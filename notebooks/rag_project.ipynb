{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f563fb",
   "metadata": {},
   "source": [
    "## installing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate qdrant-client langchain-qdrant ipywidgets langchain-community bs4 lxml markdownify trafilatura sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf0045e",
   "metadata": {},
   "source": [
    "## env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4677e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\n",
    "    \"Enter your Hugging Face API key: \"\n",
    ")\n",
    "\n",
    "os.environ[\"GROQ_API_TOKEN\"] = getpass.getpass(\n",
    "    \"Enter your GROQ API TOKEN: \"\n",
    ")\n",
    "\n",
    "# enabling langsmith tracing\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "    \"Enter your LangSmith API key: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e10c66d",
   "metadata": {},
   "source": [
    "## parsing html docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563e5633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово. Обработано документов: 918\n"
     ]
    }
   ],
   "source": [
    "import trafilatura\n",
    "import markdownify\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "documents = []\n",
    "\n",
    "def safe_preprocess_formulas(html_content):\n",
    "    \"\"\"\n",
    "    Аккуратно заменяет формулы на текст, не ломая HTML-структуру.\n",
    "    Обрабатывает формулы в формате KaTeX (Яндекс) с data-content и annotation.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # 1. Обработка формул в формате yfm-latex (Яндекс)\n",
    "    # Ищем все span с классом yfm-latex, которые содержат формулы\n",
    "    for yfm_latex in soup.find_all('span', class_='yfm-latex'):\n",
    "        latex_code = None\n",
    "        is_display = False\n",
    "        \n",
    "        # Пытаемся извлечь LaTeX из data-content (URL-encoded)\n",
    "        data_content = yfm_latex.get('data-content', '')\n",
    "        if data_content:\n",
    "            try:\n",
    "                latex_code = urllib.parse.unquote(data_content)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Если не получилось из data-content, ищем в annotation\n",
    "        if not latex_code:\n",
    "            annotation = yfm_latex.find('annotation', encoding=\"application/x-tex\")\n",
    "            if annotation:\n",
    "                latex_code = annotation.get_text()\n",
    "        \n",
    "        # Определяем display mode из data-options\n",
    "        data_options = yfm_latex.get('data-options', '')\n",
    "        if data_options:\n",
    "            try:\n",
    "                options_str = urllib.parse.unquote(data_options)\n",
    "                options = json.loads(options_str)\n",
    "                is_display = options.get('displayMode', False)\n",
    "            except:\n",
    "                # Если не получилось распарсить, проверяем по классу родителя\n",
    "                if yfm_latex.find_parent(class_=\"katex-display\"):\n",
    "                    is_display = True\n",
    "        \n",
    "        # Если display mode не определился, проверяем по классу родителя\n",
    "        if not is_display:\n",
    "            if yfm_latex.find_parent(class_=\"katex-display\"):\n",
    "                is_display = True\n",
    "        \n",
    "        # Если нашли LaTeX код, заменяем весь блок\n",
    "        if latex_code:\n",
    "            # Формируем строку замены\n",
    "            if is_display:\n",
    "                new_text = f\"\\n$${latex_code}$$\\n\"\n",
    "            else:\n",
    "                new_text = f\"${latex_code}$\"\n",
    "            \n",
    "            # Заменяем весь yfm-latex блок на текст\n",
    "            yfm_latex.replace_with(soup.new_string(new_text))\n",
    "    \n",
    "    # 2. Обработка формул через annotation (fallback для других форматов)\n",
    "    for annotation in soup.find_all('annotation', encoding=\"application/x-tex\"):\n",
    "        # Проверяем, не обработали ли мы уже этот annotation через yfm-latex\n",
    "        if annotation.find_parent('span', class_='yfm-latex'):\n",
    "            continue  # Уже обработано выше\n",
    "        \n",
    "        latex_code = annotation.get_text()\n",
    "        if not latex_code:\n",
    "            continue\n",
    "        \n",
    "        # Ищем ближайший контейнер KaTeX\n",
    "        math_container = annotation.find_parent(class_=\"katex\")\n",
    "        \n",
    "        if math_container:\n",
    "            # Проверяем на display mode\n",
    "            is_display = False\n",
    "            if math_container.find_parent(class_=\"katex-display\"):\n",
    "                is_display = True\n",
    "            \n",
    "            # Формируем строку замены\n",
    "            if is_display:\n",
    "                new_text = f\"\\n$${latex_code}$$\\n\"\n",
    "            else:\n",
    "                new_text = f\"${latex_code}$\"\n",
    "            \n",
    "            # Заменяем контейнер формулы\n",
    "            top_node = math_container\n",
    "            if is_display:\n",
    "                parent_display = math_container.find_parent(class_=\"katex-display\")\n",
    "                if parent_display:\n",
    "                    top_node = parent_display\n",
    "            \n",
    "            top_node.replace_with(soup.new_string(new_text))\n",
    "\n",
    "    # 3. MathJax (старый формат)\n",
    "    for script in soup.find_all('script', type='math/tex'):\n",
    "        script.replace_with(f\"${script.get_text()}$\")\n",
    "        \n",
    "    for script in soup.find_all('script', type='math/tex; mode=display'):\n",
    "        script.replace_with(f\"\\n$${script.get_text()}$$\\n\")\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "for index in range(2, 70):\n",
    "    file_path = f\"data/page_{index}.html\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            html_content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Файл {file_path} не найден\")\n",
    "        continue\n",
    "\n",
    "    # 1. Аккуратно внедряем формулы как текст внутрь HTML\n",
    "    html_with_math = safe_preprocess_formulas(html_content)\n",
    "\n",
    "    # 2. Используем trafilatura ТОЛЬКО для очистки мусора (меню, футеры)\n",
    "    # Но просим вернуть HTML (markdown)\n",
    "    md_file = trafilatura.extract(\n",
    "        html_with_math,\n",
    "        output_format=\"markdown\",\n",
    "        include_formatting=True,\n",
    "        include_links=True,\n",
    "        include_images=True,\n",
    "        include_tables=True,\n",
    "        include_comments=False\n",
    "    )\n",
    "\n",
    "    if not md_file:\n",
    "        print(f\"Ошибка при очистке HTML от мусора {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # 4. Сплиттер\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False \n",
    "    )\n",
    "    md_header_splits = markdown_splitter.split_text(md_file)\n",
    "    \n",
    "    # Добавляем метаданные\n",
    "    for doc in md_header_splits:\n",
    "        doc.metadata[\"source_file\"] = f\"page_{index}.html\"\n",
    "    \n",
    "    documents.extend(md_header_splits)\n",
    "\n",
    "print(f\"Готово. Обработано документов: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99ba3a",
   "metadata": {},
   "source": [
    "## vector db creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1ac22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': 'cuda'}, # или 'cuda' если есть GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd2fcbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удаляю старую коллекцию RAG_ML_HANDBOOK...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690134892a3049c7938a0c68b0df8851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Успешно добавлено 918 документов.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "COLLECTION_NAME = \"RAG_ML_HANDBOOK\"\n",
    "VECTOR_SIZE = len(embedder.embed_query(\"тестовый текст\"))\n",
    "\n",
    "# intialize qdrant client\n",
    "client = QdrantClient(path=\"./qdrant_db\")\n",
    "\n",
    "# create collection if not exists\n",
    "if client.collection_exists(COLLECTION_NAME):\n",
    "    print(f\"Удаляю старую коллекцию {COLLECTION_NAME}...\")\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# initialize vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embedder,\n",
    ")\n",
    "\n",
    "for doc in tqdm(documents):\n",
    "    _ = vector_store.add_documents([doc])\n",
    "\n",
    "print(f\"Успешно добавлено {len(documents)} документов.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09c83809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего векторов: count=918\n"
     ]
    }
   ],
   "source": [
    "count = client.count(collection_name=COLLECTION_NAME)\n",
    "print(f\"Всего векторов: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac3723",
   "metadata": {},
   "source": [
    "## vector db init from saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f044b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': 'cuda'}, # или 'cuda' если есть GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae84ad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "База успешно загружена с диска.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Те же настройки\n",
    "COLLECTION_NAME = \"RAG_ML_HANDBOOK\"\n",
    "\n",
    "# Просто указываем путь к папке, куда сохранили данные ранее\n",
    "client = QdrantClient(path=\"./qdrant_db\")\n",
    "\n",
    "# LangChain сам поймет, что коллекция уже есть внутри клиента\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embedder,\n",
    ")\n",
    "\n",
    "# Все готово к использованию!\n",
    "print(\"База успешно загружена с диска.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02cb93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего векторов: count=918\n"
     ]
    }
   ],
   "source": [
    "count = client.count(collection_name=COLLECTION_NAME)\n",
    "print(f\"Всего векторов: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce52b4",
   "metadata": {},
   "source": [
    "## generator init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f162f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "PROXY_URL = \"http://hqCxDo:Q7BLoT@196.19.177.20:8000\" # USA proxy for 1 month\n",
    "http_client = httpx.Client(\n",
    "    proxy=PROXY_URL,\n",
    "    verify=False\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    api_key=os.environ[\"GROQ_API_TOKEN\"],\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    http_client=http_client,\n",
    "\n",
    "    temperature=0.3,\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63679ce7",
   "metadata": {},
   "source": [
    "## creating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "896299a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"prompts.yaml\", \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "ReActSysPrompt = prompts[\"ReActPrompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86ed962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# tool with \"content\" enables to get \"serialized\" - what llm sees after tool calling\n",
    "@tool(response_format=\"content\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=5)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized\n",
    "\n",
    "# checkpointer to save states between invoke operations (storing session context) | we can implement the same functionality through \n",
    "checkpointer = InMemorySaver()\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"test_2\",  # Уникальный ID сессии по которому БД достает сообщения и прокидывает их в State[\"messages\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[retrieve_context],\n",
    "    system_prompt=ReActSysPrompt,\n",
    "    checkpointer=checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44f12e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_output = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"сколько градусов на солнце\"\n",
    "            }\n",
    "        ],\n",
    "        \"config\": config\n",
    "    },\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ceb88f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='сколько градусов на солнце', additional_kwargs={}, response_metadata={}, id='0b9cff88-2c4a-47de-8102-ab2978c7cf60'),\n",
       "  AIMessage(content='Информация о температуре Солнца не содержится в учебнике по машинному обучению от Яндекса, так как это астрономический параметр, а не тема, связанная с Data Science или ML. Если вас интересует общая информация: температура поверхности Солнца (фотосфера) составляет около 5500 °C, а в ядре — до 15 миллионов °C.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 410, 'prompt_tokens': 956, 'total_tokens': 1366, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 302, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None, 'queue_time': 0.283056751, 'prompt_time': 0.050602755, 'completion_time': 1.001225559, 'total_time': 1.051828314}, 'model_provider': 'openai', 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'id': 'chatcmpl-0c657875-21ac-48d8-92dc-67906fc6a6ff', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--8aa685c7-3230-43e7-8270-e4cab66ac6db-0', usage_metadata={'input_tokens': 956, 'output_tokens': 410, 'total_tokens': 1366, 'input_token_details': {}, 'output_token_details': {'reasoning': 302}})]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9eb131",
   "metadata": {},
   "source": [
    "## watch the available models of GROQ provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1df87475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "api_key = os.environ.get(\"GROQ_API_TOKEN\")\n",
    "\n",
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "\n",
    "headers = {\n",
    "\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "\n",
    "    \"Content-Type\": \"application/json\"\n",
    "\n",
    "}\n",
    "\n",
    "proxies = {\n",
    "    \"http\": PROXY_URL,\n",
    "    \"https\": PROXY_URL,\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers, proxies=proxies)\n",
    "\n",
    "\n",
    "ans = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f7442bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 'list',\n",
       " 'data': [{'id': 'meta-llama/llama-guard-4-12b',\n",
       "   'object': 'model',\n",
       "   'created': 1746743847,\n",
       "   'owned_by': 'Meta',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 1024},\n",
       "  {'id': 'groq/compound',\n",
       "   'object': 'model',\n",
       "   'created': 1756949530,\n",
       "   'owned_by': 'Groq',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 8192},\n",
       "  {'id': 'playai-tts-arabic',\n",
       "   'object': 'model',\n",
       "   'created': 1740682783,\n",
       "   'owned_by': 'PlayAI',\n",
       "   'active': True,\n",
       "   'context_window': 8192,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 8192},\n",
       "  {'id': 'llama-3.3-70b-versatile',\n",
       "   'object': 'model',\n",
       "   'created': 1733447754,\n",
       "   'owned_by': 'Meta',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 32768},\n",
       "  {'id': 'groq/compound-mini',\n",
       "   'object': 'model',\n",
       "   'created': 1756949707,\n",
       "   'owned_by': 'Groq',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 8192},\n",
       "  {'id': 'allam-2-7b',\n",
       "   'object': 'model',\n",
       "   'created': 1737672203,\n",
       "   'owned_by': 'SDAIA',\n",
       "   'active': True,\n",
       "   'context_window': 4096,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 4096},\n",
       "  {'id': 'meta-llama/llama-prompt-guard-2-22m',\n",
       "   'object': 'model',\n",
       "   'created': 1748632101,\n",
       "   'owned_by': 'Meta',\n",
       "   'active': True,\n",
       "   'context_window': 512,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 512},\n",
       "  {'id': 'whisper-large-v3-turbo',\n",
       "   'object': 'model',\n",
       "   'created': 1728413088,\n",
       "   'owned_by': 'OpenAI',\n",
       "   'active': True,\n",
       "   'context_window': 448,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 448},\n",
       "  {'id': 'moonshotai/kimi-k2-instruct-0905',\n",
       "   'object': 'model',\n",
       "   'created': 1757046093,\n",
       "   'owned_by': 'Moonshot AI',\n",
       "   'active': True,\n",
       "   'context_window': 262144,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 16384},\n",
       "  {'id': 'meta-llama/llama-prompt-guard-2-86m',\n",
       "   'object': 'model',\n",
       "   'created': 1748632165,\n",
       "   'owned_by': 'Meta',\n",
       "   'active': True,\n",
       "   'context_window': 512,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 512},\n",
       "  {'id': 'playai-tts',\n",
       "   'object': 'model',\n",
       "   'created': 1740682771,\n",
       "   'owned_by': 'PlayAI',\n",
       "   'active': True,\n",
       "   'context_window': 8192,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 8192},\n",
       "  {'id': 'meta-llama/llama-4-maverick-17b-128e-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1743877158,\n",
       "   'owned_by': 'Meta',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 8192},\n",
       "  {'id': 'openai/gpt-oss-20b',\n",
       "   'object': 'model',\n",
       "   'created': 1754407957,\n",
       "   'owned_by': 'OpenAI',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 65536},\n",
       "  {'id': 'llama-3.1-8b-instant',\n",
       "   'object': 'model',\n",
       "   'created': 1693721698,\n",
       "   'owned_by': 'Meta',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 131072},\n",
       "  {'id': 'meta-llama/llama-4-scout-17b-16e-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1743874824,\n",
       "   'owned_by': 'Meta',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 8192},\n",
       "  {'id': 'moonshotai/kimi-k2-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1752435491,\n",
       "   'owned_by': 'Moonshot AI',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 16384},\n",
       "  {'id': 'whisper-large-v3',\n",
       "   'object': 'model',\n",
       "   'created': 1693721698,\n",
       "   'owned_by': 'OpenAI',\n",
       "   'active': True,\n",
       "   'context_window': 448,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 448},\n",
       "  {'id': 'openai/gpt-oss-safeguard-20b',\n",
       "   'object': 'model',\n",
       "   'created': 1761708789,\n",
       "   'owned_by': 'OpenAI',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 65536},\n",
       "  {'id': 'openai/gpt-oss-120b',\n",
       "   'object': 'model',\n",
       "   'created': 1754408224,\n",
       "   'owned_by': 'OpenAI',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 65536},\n",
       "  {'id': 'qwen/qwen3-32b',\n",
       "   'object': 'model',\n",
       "   'created': 1748396646,\n",
       "   'owned_by': 'Alibaba Cloud',\n",
       "   'active': True,\n",
       "   'context_window': 131072,\n",
       "   'public_apps': None,\n",
       "   'max_completion_tokens': 40960}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf7543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
