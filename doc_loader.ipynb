{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cec6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "                \n",
    "\n",
    "def parse_html_with_langchain(data_dir):\n",
    "    file_paths = [os.path.join(data_dir, file) for file in os.listdir(\n",
    "        data_dir) if file.endswith(\".html\")]\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        loader = BSHTMLLoader(file_path, bs_kwargs={\"features\": \"html.parser\"})\n",
    "        docs = loader.load()\n",
    "        documents.extend(docs)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13561f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/page_57.html', 'title': 'Методы второго порядка'}, page_content=\"Методы второго порядкаЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/414.1.Оптимизация в ML14.2.Проксимальные методы14.3.Методы второго порядкаМетод НьютонаКвазиньютоновские методыПрактические аспекты14.4.Сходимость SGD15.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Методы второго порядка14.3. Методы второго порядкаАвторыТяпкин ДаниилОт\\xa0метода Ньютона до\\xa0LBFGSВ этом разделе мы сконцентрируемся сначала на методах, которые используют информацию о гессиане функции, а затем рассмотрим, как, сохраняя высокоуровневую идею метода Ньютона, обойтись без гессиана.\\nМетод Ньютона\\nИтак, наша задача – безусловная оптимизация гладкой функции\\nf(x)→min\\u2061x∈Rd.    f(x) \\\\to \\\\min_{x \\\\in \\\\mathbb{R}^d}.\\nf(x)→x∈Rdmin\\u200b.Как и при оптимизации методом градиентного спуска, мы будем искать направление уменьшения функционала. Но в этот раз мы будем использовать не линейное приближение, а квадратичное:\\nf(x+Δx)≈f(x)+⟨∇f(x),Δx⟩+12⟨Δx,B(x)Δx⟩.    f(x + \\\\Delta x) \\\\approx f(x) + \\\\langle \\\\nabla f(x), \\\\Delta x \\\\rangle + \\\\frac{1}{2}\\\\langle \\\\Delta x, B(x) \\\\Delta x \\\\rangle.\\nf(x+Δx)≈f(x)+⟨∇f(x),Δx⟩+21\\u200b⟨Δx,B(x)Δx⟩.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.ВступитьФормула Тейлора говорит нам брать B(x)=∇2f(x)B(x) = \\\\nabla^2 f(x)B(x)=∇2f(x). Приравняв к нулю градиент этой квадратичной аппроксимации, мы получаем направление спуска для метода Ньютона:\\nΔx=[B(x)]−1∇f(x).    \\\\Delta x = [B(x)]^{-1} \\\\nabla f(x).\\nΔx=[B(x)]−1∇f(x).Обозначим Bk=B(xk),Hk=Bk−1B_k = B(x_k), H_k = B_k^{-1}Bk\\u200b=B(xk\\u200b),Hk\\u200b=Bk−1\\u200b. В таком случае мы можем записать итеративный алгоритм спуска:\\nxk+1=xk−αk⋅Hk∇f(xk).    x_{k+1} = x_{k} - \\\\alpha_k \\\\cdot H_k \\\\nabla f(x_k).\\nxk+1\\u200b=xk\\u200b−αk\\u200b⋅Hk\\u200b∇f(xk\\u200b).В литературе методом Ньютона называется такой метод при αk=1\\\\alpha_k = 1αk\\u200b=1, при другом размере шаге αk∈(0,1)\\\\alpha_k \\\\in (0, 1)αk\\u200b∈(0,1) этот метод называют дэмпированным (damped) методом Ньютона.\\nОбсудим, в чем главная особенность метода Ньютона и в чем заключается выигрыш по сравнению с классическим градиентным спуском. Таких особенностей две.\\nСкорость сходимости метода Ньютона\\nПервая связана со скоростью его сходимости. А именно – в окрестности решения он сходится квадратично.\\nТеорема. Пусть функция fff имеет достаточно гладкий гессиан и сильно выпукла в точке оптимума x∗x^*x∗. Тогда ∃r>0\\\\exists r > 0∃r>0, что для всякого x0:∥x0−x∗∥≤rx_0 : \\\\Vert x_0 - x^*\\\\Vert \\\\leq rx0\\u200b:∥x0\\u200b−x∗∥≤r для метода Ньютона с αk=1\\\\alpha_k = 1αk\\u200b=1 верно ∥xk+1−x∗∥≤c∥xk−x∗∥2\\\\Vert x_{k+1} - x^* \\\\Vert \\\\leq c \\\\Vert x_k - x^* \\\\Vert^2∥xk+1\\u200b−x∗∥≤c∥xk\\u200b−x∗∥2 для константы ccc зависящей только от fff.\\nНабросок доказательства для интересующихся.Немного поясним терминологию. Под достаточно гладким гессианом мы подразумеваем то, что он должен быть липшицевым и дифференцируемым, из чего следует, что\\n∥∇2f(x)−∇2f(y)∥≤L∥x−y∥,    \\\\Vert \\\\nabla^2 f(x) - \\\\nabla^2 f(y) \\\\Vert \\\\leq L \\\\Vert x - y \\\\Vert,\\n∥∇2f(x)−∇2f(y)∥≤L∥x−y∥,а также\\n∥[∇3f(x)](x−y,x−y,⋅)∥≤L∥x−y∥2,    \\\\Vert \\\\left[\\\\nabla^3 f(x)\\\\right] (x - y, x - y, \\\\cdot) \\\\Vert \\\\leq L \\\\Vert x - y\\\\Vert^2,\\n∥[∇3f(x)](x−y,x−y,⋅)∥≤L∥x−y∥2,где [∇3f(x)](x−y,x−y,⋅)\\\\left[\\\\nabla^3 f(x)\\\\right] (x - y, x - y, \\\\cdot)[∇3f(x)](x−y,x−y,⋅) – результат подстановки (x−y)(x - y)(x−y) в качестве двух первых аргументов в трилинейную форму [∇3f(x)]\\\\left[\\\\nabla^3 f(x)\\\\right][∇3f(x)]. Под сильной выпуклостью мы подразумеваем здесь μ\\\\muμ-сильную выпуклость, которую здесь строго определять излишне (подробнее см. в этой статье), но из которой следует, что гессиан отделён от нуля:\\nλmin\\u2061(∇2f(x))≥μ\\\\lambda_{\\\\min}(\\\\nabla^2 f(x)) \\\\geq \\\\mu\\nλmin\\u200b(∇2f(x))≥μРазложим градиент по формуле Тейлора в точке xkx_kxk\\u200b с остаточным членом в форме Лагранжа:\\n0=∇f(x∗)=∇f(xk)+∇2f(xk)(x∗−xk)+12∇3f(ξk)[x∗−xk]2.    0 = \\\\nabla f(x^*) = \\\\nabla f(x_k) + \\\\nabla^2 f(x_k) (x^* - x_k) + \\\\frac{1}{2} \\\\nabla^3 f(\\\\xi_k) [x^* - x_k]^2.\\n0=∇f(x∗)=∇f(xk\\u200b)+∇2f(xk\\u200b)(x∗−xk\\u200b)+21\\u200b∇3f(ξk\\u200b)[x∗−xk\\u200b]2.Умножим с обеих сторон на обратный гессиан и получаем:\\nxk−[∇2f(xk)]−1∇f(xk)−x∗=12[∇2f(xk)]−1∇3f(ξk)[x∗−xk]2.    x_k - [\\\\nabla^2 f(x_k)]^{-1} \\\\nabla f(x_k) - x^* = \\\\frac{1}{2} [\\\\nabla^2 f(x_k)]^{-1}\\\\nabla^3 f(\\\\xi_k) [x^* - x_k]^2.\\nxk\\u200b−[∇2f(xk\\u200b)]−1∇f(xk\\u200b)−x∗=21\\u200b[∇2f(xk\\u200b)]−1∇3f(ξk\\u200b)[x∗−xk\\u200b]2.Воспользуемся формулой шага xk+1=xk−[∇2f(xk)]−1∇f(xk)x_{k+1} = x_k - [\\\\nabla^2 f(x_k)]^{-1} \\\\nabla f(x_k)xk+1\\u200b=xk\\u200b−[∇2f(xk\\u200b)]−1∇f(xk\\u200b), тогда в левой части равенства у нас xk+1−x∗x_{k+1} - x^*xk+1\\u200b−x∗. Посчитаем норму и воспользуемся гладкостью:\\n∥xk+1−x∗∥≤L2∥[∇2f(xk)]−1∥∥x∗−xk∥2.    \\\\Vert x_{k+1} - x^* \\\\Vert \\\\leq \\\\frac{L}{2}  \\\\Vert [\\\\nabla^2 f(x_k)]^{-1} \\\\Vert  \\\\Vert x^* - x_k \\\\Vert^2.\\n∥xk+1\\u200b−x∗∥≤2L\\u200b∥[∇2f(xk\\u200b)]−1∥∥x∗−xk\\u200b∥2.Теперь воспользуемся сильной выпуклостью в точке оптимума. Поскольку мы предполагаем, что точка старта достаточно близко к точке оптимума, то по гладкости можем считать, что в точке xkx_kxk\\u200b у нас есть хотя бы μ/2\\\\mu/2μ/2-сильная выпуклость. Тогда получаем:\\n∥xk+1−x∗∥≤L4μ∥xk−x∗∥2.    \\\\Vert x_{k+1} - x^* \\\\Vert \\\\leq \\\\frac{L}{4\\\\mu} \\\\Vert x_k - x^* \\\\Vert^2.\\n∥xk+1\\u200b−x∗∥≤4μL\\u200b∥xk\\u200b−x∗∥2.Метод Ньютона и плохо обусловленные задачи\\nВторое приятное свойство заключается в устойчивости метода Ньютона к плохой обусловленности задачи (в отличие от метода градиентного спуска). Разберёмся, что это значит. Когда мы говорим о плохой обусловленности задачи, мы имеем в виду, что гессиан в точке оптимума плохо обусловлен, то есть отношение максимального и минимального собственных чисел является большим числом. Геометрически это значит, что линии уровня функции вблизи оптимума похожи на очень вытянутые эллипсоиды; мы уже обсуждали, что в такой ситуации градиентный спуск может работать медленно. А как справится метод Ньютона? Оказывается, намного лучше. И связано это с его инвариантностью к линейным преобразованиям.\\nА именно, рассмотрим функцию f^(y)=f(Ay)\\\\hat f(y) = f(Ay)f^\\u200b(y)=f(Ay) для некоторой невырожденной матрицы AAA. Обозначим x=Ayx = Ayx=Ay. Посмотрим, как связаны градиент и гессиан новой функции с градиентом и гессианом старой. Воспользуемся производной сложной функции:\\n∇yf^=Ax⊤∇f,    \\\\nabla_y \\\\hat f = A^\\\\top_x \\\\nabla f,\\n∇y\\u200bf^\\u200b=Ax⊤\\u200b∇f,∇y2f^=A⊤∇x2fA    \\\\nabla^2_y \\\\hat f = A^\\\\top \\\\nabla^2_x f A\\n∇y2\\u200bf^\\u200b=A⊤∇x2\\u200bfAРассмотрим теперь траекторию x0,x1,…,xKx_0, x_1, \\\\ldots, x_Kx0\\u200b,x1\\u200b,…,xK\\u200b метода Ньютона, запущенного из точки x0x_0x0\\u200b для поиска минимума функции fff, и траекторию y0,y1,…,yKy_0, y_1, \\\\ldots,y_Ky0\\u200b,y1\\u200b,…,yK\\u200b метода Ньютона, запущенного для поиска минимума функции f^\\\\hat ff^\\u200b. Если x0=Ay0x_0 = A y_0x0\\u200b=Ay0\\u200b, то для всех kkk будет верно xk=Aykx_k = A y_kxk\\u200b=Ayk\\u200b, то есть траектории получаются одна из другой при помощи этого линейного преобразования, другими словами, траектории исходной и новой функции подобны.\\nДля интересующихся доказательствами.Докажем по индукции. Для k=0k=0k=0 это дано по условию. Теперь докажем шаг индукции:\\nyk+1=yk−αk[∇yk2f^]−1∇ykf^=    y_{k+1} = y_{k} - \\\\alpha_k [ \\\\nabla^2_{y_k} \\\\hat f]^{-1} \\\\nabla_{y_k} \\\\hat f = \\nyk+1\\u200b=yk\\u200b−αk\\u200b[∇yk\\u200b2\\u200bf^\\u200b]−1∇yk\\u200b\\u200bf^\\u200b=yk−αk(A−1[∇Ayk2f]A−⊤)(A⊤∇Aykf).    y_k - \\\\alpha_k (A^{-1} [\\\\nabla^2_{A y_k} f] A^{-\\\\top}) (A^{\\\\top} \\\\nabla_{A y_k} f).\\nyk\\u200b−αk\\u200b(A−1[∇Ayk\\u200b2\\u200bf]A−⊤)(A⊤∇Ayk\\u200b\\u200bf).По предположению индукции xk=Aykx_k = A y_kxk\\u200b=Ayk\\u200b, тогда получаем:\\nyk+1=A−1(xk−α[∇2f^(xk)]∇f(xk))=A−1xk+1⇒xk+1=Ayk+1.    y_{k+1} = A^{-1} ( x_k - \\\\alpha [\\\\nabla^2 \\\\hat f(x_k)] \\\\nabla f(x_k) ) = A^{-1} x_{k+1} \\\\Rightarrow x_{k+1} = A y_{k+1}.\\nyk+1\\u200b=A−1(xk\\u200b−α[∇2f^\\u200b(xk\\u200b)]∇f(xk\\u200b))=A−1xk+1\\u200b⇒xk+1\\u200b=Ayk+1\\u200b.Вернёмся теперь к плохо обусловленной задаче минимизации функции fff. Рассмотрим линейное преобразование A=(∇x∗2f)−1/2A = (\\\\nabla^2_{x^*} f)^{-1/2}A=(∇x∗2\\u200bf)−1/2 и функцию f^(x)=f(Ax)\\\\hat f(x) = f(Ax)f^\\u200b(x)=f(Ax). Тогда для функции f^\\\\hat ff^\\u200b число обусловленности гессиана в точке оптимума равно в точность единице (проверьте это!), а траектории для этой новой, хорошо обусловленной функции, и старой, плохо обусловленной, подобны. В частности, метод Ньютона не будет, как градиентный спуск, долго метаться где-то на задворках вытянутой эллиптической «ямки» вокруг оптимума, а быстро ринется к центру.\\nМожно сказать, что метод Ньютона правильно улавливает кривизну линий уровня функции и это позволяет ему быстрее сходиться к оптимуму. Эту идею стоит запомнить, она появляется в некоторых вдохновлённых методами второго порядка модификациях SGD.\\nТакже еще можно заметить, что свойства, которые мы требуем от функции в теореме о квадратичной сходимости, вообще говоря, не сохраняются при линейных преобразованиях: могут поменяться константы липшицевости и сильной выпуклости. Это простое замечание побудило исследователей ввести класс самосогласованных функций, более широкий и линейно инвариантный, для которого метод Ньютона также сходится. Подробнее об этом можно узнать в разделе 9.6 книги S. Boyd & L. Vandenberghe, Convex Optimization.\\nСлабости метода Ньютона\\nОт хорошего переходим к плохому: к слабостям метода Ньютона. Во-первых, мы имеем квадратичную скорость сходимости только в окрестности оптимума. А если мы стартуем из произвольно удалённой точки, то нам, как и в случае градиентного спуска, требуется подбор шага αk\\\\alpha_kαk\\u200b при помощи линейного поиска (что нам вряд ли по карману). Если подбирать шаг не хочется, можно прибегнуть к интересному теоретическому методу получения гарантий на глобальную сходимость – добавлению кубической регуляризации.\\nНемного деталей для пытливыхВ случае кубической регуляризации мы хотим обеспечить не просто аппроксимацию, но и оценку сверху:\\nf(x+Δx)≤f(x)+⟨∇f(x),Δx⟩+12⟨Δx,∇2f(x)Δx⟩+M6∥Δx∥23.    f(x + \\\\Delta x) \\\\leq f(x) + \\\\langle \\\\nabla f(x), \\\\Delta x \\\\rangle + \\\\frac{1}{2}\\\\langle \\\\Delta x, \\\\nabla^2 f(x) \\\\Delta x \\\\rangle + \\\\frac{M}{6} \\\\Vert \\\\Delta x \\\\Vert^3_2.\\nf(x+Δx)≤f(x)+⟨∇f(x),Δx⟩+21\\u200b⟨Δx,∇2f(x)Δx⟩+6M\\u200b∥Δx∥23\\u200b.По сути, мы задаем кубическую модель, которую мы можем уже оптимзировать по Δx\\\\Delta xΔx. Тогда мы получаем следующую итеративную процедуру проксимального вида:\\nxk+1=arg\\u2061min\\u2061y{f(xk)+⟨∇f(xk),y−xk⟩+12⟨y−xk,∇2f(xk)(y−xk)⟩+M6∥y−xk∥23}    x_{k+1} = \\\\arg\\\\min_{y} \\\\biggl\\\\{ f(x_k) + \\\\langle \\\\nabla f(x_k), y - x_k \\\\rangle + \\\\frac{1}{2}\\\\langle y-x_k, \\\\nabla^2 f(x_k) (y - x_k) \\\\rangle + \\\\frac{M}{6} \\\\Vert y - x_k \\\\Vert^3_2\\n    \\\\biggl\\\\}\\nxk+1\\u200b=argymin\\u200b{f(xk\\u200b)+⟨∇f(xk\\u200b),y−xk\\u200b⟩+21\\u200b⟨y−xk\\u200b,∇2f(xk\\u200b)(y−xk\\u200b)⟩+6M\\u200b∥y−xk\\u200b∥23\\u200b}В таком случае можно использовать, например, постоянный размер шага и иметь гарантии на сходимость. Этот метод в последнее время стал пользоваться популярностью в теоретических исследованиях, в том числе в распределенной оптимизации.\\nТакже можно задаться простым вопросом: а можем ли мы пользоваться подобным разложением с регуляризацией для большего числа членов разложения по формуле Тейлора? На самом деле да, только коэффициент MMM нужно подбирать чуть более специфично. Такие методы называются тензорными методами.\\nДругая проблема кроется в формуле пересчета следующей итерации: вычисление и обращение гессиана. Конечно, вместо обращения гессиана можно честно решать систему линейных уравнений, но асимптотика остается прежней: O(d3)O(d^3)O(d3), а от затрат памяти на хранение матрицы O(d2)O(d^2)O(d2) вообще некуда деться. А это значит, что, например, решать линейную регрессию с ~10000 признаками методом Ньютона попросту невозможно.\\nЕсть и третья, малозаметная проблема: дословно метод Ньютона не работает для невыпуклых задач, поскольку ∇2f(x)\\\\nabla^2 f(x)∇2f(x) не будет положительно опредленной и Δx\\\\Delta xΔx перестанет быть направлением спуска. Для решения этой проблемы можно немного «подпортить» нашу аппроксимацию и рассмотреть матрицу вида Bk=∇2f(xk)+ΔkB_k = \\\\nabla^2 f(x_k) + \\\\Delta_kBk\\u200b=∇2f(xk\\u200b)+Δk\\u200b, такую что BkB_kBk\\u200b станет положительно определенной, и уже её подставлять в нашу квадратичную модель. Идея подмены гессиана на что-то более подходящее – это главная идея квазиньютоновских методов, обсуждаемых далее.\\nИтак, общие выводы:\\n\\nМетод Ньютона – теоретически оптимальный метод, который автоматически улавливает кривизну функции в окрестности оптимума.\\nДля размерности d>1000d > 1000d>1000 он уже не является эффективным, поскольку требует вычисления и хранения гессиана, а также решения системы линейных уравнений с его участием (что может быть в общем случае очень дорого).\\n\\nКвазиньютоновские методы\\nЧтобы придумать, как бороться с проблемами метода Ньютона, нужно посмотреть на него с другой стороны, а для этого мы обратимся ненадолго к решению задачи нахождения нуля векторной функции.\\nМетод касательной\\nИтак, рассмотрим совершенно новую задачу. Пусть дана функция g\\u2009\\u2063:Rn→Rng \\\\colon \\\\mathbb{R}^n \\\\to \\\\mathbb{R}^ng:Rn→Rn и нужно найти её ноль, то есть такое x∗x^*x∗, что g(x∗)=0g(x^*) = 0g(x∗)=0. Связь с оптимизацией (по крайней мере в выпуклом случае) довольно проста: если взять g(x)=∇f(x)g(x) = \\\\nabla f(x)g(x)=∇f(x), то корень уравнения g(x)=0g(x) = 0g(x)=0 и будет точкой оптимума.\\nСначала рассмотрим одномерный случай d=1d=1d=1. Как найти ноль функции с помощью итеративной процедуры? Логично поступить следующим образом: проводим касательную y=g′(xn)(x−xn)+g(xn)y = g'(x_n)(x - x_n) + g(x_n)y=g′(xn\\u200b)(x−xn\\u200b)+g(xn\\u200b) к графику функции и находим точку xn+1x_{n+1}xn+1\\u200b, в которой линейная аппроксимация обнуляется:\\n0=g′(xn)(xn+1−xn)+g(xn),0 = g'(x_n)(x_{n+1} - x_n) + g(x_n),\\n0=g′(xn\\u200b)(xn+1\\u200b−xn\\u200b)+g(xn\\u200b),откуда получаем формулу пересчета\\nxn+1=xn−g(xn)g′(xn).    x_{n+1} = x_n - \\\\frac{g(x_n)}{g'(x_n)}.\\nxn+1\\u200b=xn\\u200b−g′(xn\\u200b)g(xn\\u200b)\\u200b.\\nИзвестно, что этот метод обладает квадратичной скоростью сходимости в одномерном мире, что очень перекликается с методом Ньютона для оптимизации – и не просто так.\\nЕсли рассмотреть многомерный случай, то вычисление производной заменяется на вычисление якобиана векторнозначной функции ggg. В случае g=∇fg = \\\\nabla fg=∇f наш якобиан становится гессианом и получаем в точности обычный метод Ньютона для оптимизации:\\nxn+1=xn−[∇xn2f]−1∇xnf.    x_{n+1} = x_n - \\\\left[\\\\nabla^2_{x_n} f\\\\right]^{-1} \\\\nabla_{x_n} f.\\nxn+1\\u200b=xn\\u200b−[∇xn\\u200b2\\u200bf]−1∇xn\\u200b\\u200bf.Метод секущей и общая схема квазиньютоновских методов\\nПусть мы хотим найти такую точку x∗x^*x∗, что g(x∗)=0g(x^*) = 0g(x∗)=0. В одномерном случае мы можем подменить вычисление g′(xn)g'(x_n)g′(xn\\u200b) вычислением её приближения g(xn)−g(xn−1)/(xn−xn−1)g(x_n) - g(x_{n-1}) / (x_n - x_{n-1})g(xn\\u200b)−g(xn−1\\u200b)/(xn\\u200b−xn−1\\u200b). Откуда получаем формулу пересчета:\\nxn+1=xn−xn−xn−1g(xn)−g(xn−1)g(xn)    x_{n+1} = x_{n} - \\\\frac{x_n - x_{n-1}}{g(x_n) - g(x_{n-1})} g(x_n)\\nxn+1\\u200b=xn\\u200b−g(xn\\u200b)−g(xn−1\\u200b)xn\\u200b−xn−1\\u200b\\u200bg(xn\\u200b)Графически, этот метод выглядит следующим образом:\\n\\nСкорость сходимости этого метода несколько ниже, чем у метода Ньютона (линейная, а не квадратичная), но зато мы теперь не должны вычислять производную! В текущем виде, используя просто подмену градиента на его конечно-разностную аппроксимацию, не очевидно, как обобщить этот метод на произвольную размерность. Но, если посмотреть на название метода и на картинку, как он работает, мы видим, что мы по сути проводим через два предыдущих приближения секущую, а затем выбираем ноль этой секущей в качестве следующей точки. В многомерном случае мы можем выписать соответствующее ей уравнение y=Bk(x−xk)+g(xk)y = B_k(x - x^k) + g(x^{k})y=Bk\\u200b(x−xk)+g(xk), где BkB_kBk\\u200b – матрица размера d×dd \\\\times dd×d, которая должна удовлетворять так называемому уравнению секущей (secant equation):\\nBk(xk−xk−1)=g(xk)−g(xk−1).    B_k(x^k - x^{k-1}) = g(x^k) - g(x^{k-1}).\\nBk\\u200b(xk−xk−1)=g(xk)−g(xk−1).Теперь, чтобы выбрать следующую точку, нужно найти ноль секущей, то есть\\nBk(xk+1−xk)+g(xk)=0\\u2005\\u200a⟺\\u2005\\u200axk+1=xk−Bk−1g(xk).    B_k(x^{k+1} - x^{k}) + g(x^k) = 0 \\\\iff  x^{k+1} = x^k - B_k^{-1} g(x^k).\\nBk\\u200b(xk+1−xk)+g(xk)=0⟺xk+1=xk−Bk−1\\u200bg(xk).А теперь рассмотрим g(x)=∇f(x)g(x) = \\\\nabla f(x)g(x)=∇f(x) и добавим в итеративную схему выше размер шага. Тогда мы получаем общую итеративную схему квазиньютоновских методов:\\nxk+1=xk−αkBk−1∇f(xk).    x^{k+1} = x^k - \\\\alpha_k B_k^{-1} \\\\nabla f(x^k).\\nxk+1=xk−αk\\u200bBk−1\\u200b∇f(xk).При этом необходимо выбирать такие BkB_kBk\\u200b, чтобы они\\n(а) были симметричными и положительно определенными и\\n(б) удовлетворяли уравнению секущей\\nBk(xk−xk−1)=∇f(xk)−∇f(xk−1)B_k(x^{k} - x^{k-1}) = \\\\nabla f(x^k) - \\\\nabla f(x^{k-1})\\nBk\\u200b(xk−xk−1)=∇f(xk)−∇f(xk−1)Первое требование восходит к двум соображениям. Первое – BkB_kBk\\u200b должно приближать гессиан, а он в идеале в окрестности точки минимума как раз является симметричным и положительно определенным. Второе соображение проще: в противном случае dk=−Bk−1∇f(xk)d_k = -B_k^{-1} \\\\nabla f(x^k)dk\\u200b=−Bk−1\\u200b∇f(xk) попросту не будет направлением спуска. Несмотря на эти два свойства, выбор по прежнему остается достаточно широким, откуда возникает большое разнообразие квазиньютоновских методов. Мы рассмотрим один классический и широко известный метод BFGS (Broyden, Fletcher, Goldfarb, Shanno).\\nBFGS\\nСначала заметим, что в самом алгоритме в первую очередь используется обратная матрица к BkB_kBk\\u200b, которую мы обозначим Hk=Bk−1H_k = B_k^{-1}Hk\\u200b=Bk−1\\u200b. Тогда выбирать BkB_kBk\\u200b – это тоже самое, что выбирать HkH_kHk\\u200b. Введем еще два стандартных обозначения, чтобы можно было проще записывать все последующие формулы: sk=xk+1−xk=αkdks_k = x_{k+1} - x_{k} = \\\\alpha_k d_{k}sk\\u200b=xk+1\\u200b−xk\\u200b=αk\\u200bdk\\u200b и yk=∇f(xk+1)−∇f(xk)y_k = \\\\nabla f(x^{k+1}) - \\\\nabla f(x^k)yk\\u200b=∇f(xk+1)−∇f(xk). В их терминах уравнение секущей для HkH_kHk\\u200b выглядит максимально просто: Hkyk−1=sk−1H_{k} y_{k-1} = s_{k-1}Hk\\u200byk−1\\u200b=sk−1\\u200b.\\nТеперь введем некоторое искусственное требование, которое гарантирует единственность Hk+1H_{k+1}Hk+1\\u200b – выберем ближайшую подходящую матрицу к HkH_kHk\\u200b, удовлетворяющую описанным выше условиям:\\nHk+1=argminH{12∥H−Hk∥∣Н=H⊤,\\xa0\\xa0Hyk=sk}    H_{k+1} = \\\\text{argmin}_H\\\\left\\\\{\\\\left.\\\\frac12\\\\Vert H - H_k \\\\Vert\\\\right| Н = H^\\\\top, \\\\ \\\\ H y_k = s_k\\\\right\\\\}\\nHk+1\\u200b=argminH\\u200b{21\\u200b∥H−Hk\\u200b∥\\u200bН=H⊤,\\xa0\\xa0Hyk\\u200b=sk\\u200b}Вообще говоря, при выборе разных норм ∥⋅∥\\\\Vert \\\\cdot \\\\Vert∥⋅∥ мы будем получать разные квазиньютоновские алгоритмы. Рассмотрим один достаточно общий класс норм (аналог взвешенных ℓ2\\\\ell_2ℓ2\\u200b норм в матричном мире):\\n∥A∥:=∥W1/2AW1/2∥F,    \\\\Vert A \\\\Vert := \\\\Vert W^{1/2} A W^{1/2} \\\\Vert_F,\\n∥A∥:=∥W1/2AW1/2∥F\\u200b,где ∥⋅∥F\\\\Vert \\\\cdot \\\\Vert_F∥⋅∥F\\u200b – это Фробениусова норма\\n∥C∥F2=⟨C,C⟩F=tr(C⊤C)=∑i,jCij2,\\\\Vert C \\\\Vert_F^2 = \\\\langle C, C\\\\rangle_F = \\\\text{tr}(C^\\\\top C) = \\\\sum_{i,j} C_{ij}^2,\\n∥C∥F2\\u200b=⟨C,C⟩F\\u200b=tr(C⊤C)=i,j∑\\u200bCij2\\u200b,а WWW – некоторая симметричная и положительно определенная матрица весов, которую мы выберем таким образом, что она будет сама по себе удовлетворять уравнению секущей Wsk=ykWs_k = y_kWsk\\u200b=yk\\u200b.\\nСразу уточним, что матрица весов в таком случае меняется на каждой итерации и, по сути, на каждой итерации мы имеем разные задачи оптимизации, само же предположение задает дополнительную похожесть на обратный гессиан, поскольку можно взять в качестве весов усредненый гессиан\\nW=Gˉk=[∫01∇2f(xk+ταkpk)dτ]W = \\\\bar G_k = [\\\\int_0^1 \\\\nabla^2 f(x_k + \\\\tau \\\\alpha_k p_k) d\\\\tau]\\nW=Gˉk\\u200b=[∫01\\u200b∇2f(xk\\u200b+ταk\\u200bpk\\u200b)dτ]Решив описанную выше оптимизационную задачу, мы получаем матрицу Hk+1H_{k+1}Hk+1\\u200b, не зависящую явным образом от матрицы весов:\\nЭта формула как раз является ключевой в алгоритме BFGS. Чтобы заметить одно крайне важное свойство этой формулы, раскроем скобки:\\nHk+1=Hk−ρk(Hkyksk⊤+skyk⊤Hk)+ρk2(skyk⊤Hkyksk⊤)+ρksksk⊤.    H_{k+1} = H_k - \\\\rho_k (H_k y_k s_k^{\\\\top} + s_k y_k^\\\\top H_k) + \\\\rho_k^2 (s_k y_k^\\\\top H_k y_k s_k^\\\\top)  + \\\\rho_k s_k s_k^\\\\top.\\nHk+1\\u200b=Hk\\u200b−ρk\\u200b(Hk\\u200byk\\u200bsk⊤\\u200b+sk\\u200byk⊤\\u200bHk\\u200b)+ρk2\\u200b(sk\\u200byk⊤\\u200bHk\\u200byk\\u200bsk⊤\\u200b)+ρk\\u200bsk\\u200bsk⊤\\u200b.Отсюда мы видим, что нам в этой формуле достаточно умножать матрицу на вектор и складывать матрицы, что можно делать за O(d2)O(d^2)O(d2) операций! То есть мы победили один из самых страшных минусов метода Ньютона. Воспользовавшись тем, что yk⊤Hkyky_k^\\\\top H_k y_kyk⊤\\u200bHk\\u200byk\\u200b и 1/ρk=yk⊤sk=sk⊤yk1/\\\\rho_k = y_k^\\\\top s_k = s_k^\\\\top y_k1/ρk\\u200b=yk⊤\\u200bsk\\u200b=sk⊤\\u200byk\\u200b – числа, перепишем формулу в более computational friendly стиле:\\nHk+1=Hk+ρk2(1/ρk+yk⊤Hkyk)(sksk⊤)−ρk(Hkyksk⊤+skyk⊤Hk).    H_{k+1} = H_k + \\\\rho_k^2 (1/\\\\rho_k + y_k^\\\\top H_k y_k)(s_k s_k^\\\\top) - \\\\rho_k (H_k y_k s_k^{\\\\top} + s_k y_k^\\\\top H_k).\\nHk+1\\u200b=Hk\\u200b+ρk2\\u200b(1/ρk\\u200b+yk⊤\\u200bHk\\u200byk\\u200b)(sk\\u200bsk⊤\\u200b)−ρk\\u200b(Hk\\u200byk\\u200bsk⊤\\u200b+sk\\u200byk⊤\\u200bHk\\u200b).Общие выводы:\\n\\nИтерации BFGS вычислительно проще итераций метода Ньютона и не требуют вычисления гессиана;\\nПо скорости сходимости BFGS уступает методу Ньютона, но все равно является достаточно быстрым;\\nПо прежнему требуется O(d2)O(d^2)O(d2) памяти, что по-прежнему вызывает проблемы при большой размерности (104−10510^4-10^5104−105).\\nВремя выполнения итерации O(d2)O(d^2)O(d2) гораздо лучше, чем O(d3)O(d^3)O(d3) метода Ньютона, но всё ещё оставляет желать лучшего.\\n\\nКазалось бы, избавиться от O(d2)O(d^2)O(d2) нельзя принципиально, ведь нужно как-то взаимодействовать с матрицей HkH_kHk\\u200b размера O(d2)O(d^2)O(d2), а она не факт что разреженная. Но и в этом случае можно добиться улучшения до линейной сложности (как у градиентных методов!).\\nL-BFGS\\nПри взаимодействии с матрицами существует два основных способа хранить их дешевле, чем «по-честному». Первый способ – пользоваться разреженностью матрицы, а второй – низкоранговыми разложениями или чем-то близким. Поскольку сейчас мы не хотим добавлять предположений на задачу, которую мы решаем, то единственный выход – это пользоваться структурой HkH_kHk\\u200b, возникающей в BFGS.\\nЕсли внимательно взглянуть на формулы обновления, то их можно переписать в следующем виде:\\nHk+1=V(sk,yk)⊤HkV(sk,yk)+U(sk,yk),    H_{k+1} = V(s_k, y_k)^\\\\top H_k V(s_k, y_k) + U(s_k, y_k),\\nHk+1\\u200b=V(sk\\u200b,yk\\u200b)⊤Hk\\u200bV(sk\\u200b,yk\\u200b)+U(sk\\u200b,yk\\u200b),V(sk,yk)=I−ρkyksk⊤,\\xa0\\xa0\\xa0U(sk,yk)=ρksksk⊤    V(s_k, y_k) = I - \\\\rho_k y_k s_k^\\\\top, \\\\ \\\\ \\\\ U(s_k,y_k) = \\\\rho_k s_k s_k^\\\\top\\nV(sk\\u200b,yk\\u200b)=I−ρk\\u200byk\\u200bsk⊤\\u200b,\\xa0\\xa0\\xa0U(sk\\u200b,yk\\u200b)=ρk\\u200bsk\\u200bsk⊤\\u200bДля того, чтобы перейти от HkH_kHk\\u200b к Hk+1H_{k+1}Hk+1\\u200b, можно хранить не матрицу HkH_{k}Hk\\u200b, а набор пар из k пар (si,yi)i=1,…,k(s_i, y_i)_{i=1,\\\\ldots,k}(si\\u200b,yi\\u200b)i=1,…,k\\u200b и начальное приближение H0H_0H0\\u200b (например, H0=γIH_0 = \\\\gamma IH0\\u200b=γI для некоторого γ>0\\\\gamma > 0γ>0), чтобы «восстановить» HkH_kHk\\u200b. Пользуясь такой структурой, мы можем хранить матрицу Hk+1H_{k+1}Hk+1\\u200b при помощи лишь (k+1)⋅2d+1(k+1) \\\\cdot 2d + 1(k+1)⋅2d+1 чисел, а не d2d^2d2. К сожалению, такая структура имеет довольно простую проблему: при k>d/2k > d/2k>d/2 затраты памяти становятся только выше.\\nВозникает простая идея – а давайте хранить только последние m=constm = \\\\text{const}m=const обновлений! Таким образом, мы получаем алгоритм L-BFGS, который имеет уже линейные O(md)O(md)O(md) затраты памяти и, что немаловажно, такие же линейные затраты O(md)O(md)O(md) на итерацию, ведь умножение матриц VVV и UUU на вектор может осуществляться за линейное время.\\nОбщие выводы:\\n\\nL-BFGS обладает линеной сложностью итерации, линейными требованиями по дополнительной памяти и к тому же требует вычислять только градиенты!\\nПроизводительность сильно зависит от константы mmm, отвечающей за точность аппроксимации гессиана;\\nКак и все методы из этого раздела, требует точного, а не стохастического вычисления градиентов.\\n\\nПрактические аспекты\\nИз всех перечисленных в этом разделе методов важнее всего отметить L-BFGS как самый практичный. Он реализован в любой* библиотеке, которая имеет дело с оптимизацией чего-либо и может быть эффективным, если удаётся вычислить градиенты (и значения функций для линейного поиска размера шага). К сожалению, это получается не всегда: при больших размерах датасета вычисление честного градиента и значения для функционалов вида суммы\\nL(X,Y)=∑i=1NL(xi,yi)    L(X,Y) = \\\\sum_{i=1}^N L(x_i, y_i)\\nL(X,Y)=i=1∑N\\u200bL(xi\\u200b,yi\\u200b)не представляется возможным за разумное время. В таком случае мы вынуждены вернуться в мир стохастического градиентного спуска. Общая идея более тонкого учёта геометрии линий уровня функции потерь, в чём-то напоминающая происходящее в методе Ньютона, находит применение и в ряде вариаций SGD, но, конечно, порождает совершенно другие методы.\\nЧто же касается самого метода Ньютона, его можно несколько оптимизировать, если смириться с тем, что всё вычисляется неточно. Во-первых, обратную матрицу к гессиану матрицу на самом деле не нужно ни хранить, ни даже вычислять. Давайте разберёмся, почему. Умножить (∇2f)−1(\\\\nabla^2f)^{-1}(∇2f)−1 на вектор vvv – это то же самое, что решить систему с левой частью ∇2f\\\\nabla^2f∇2f и правой частью vvv, а для решения систем уравнений существуют эффективные итеративные методы, не меняющие левой части системы, а требующие лишь уметь умножать её на разные векторы. При этом умножать гессиан на вектор можно при помощи автоматического дифференцирования. Кроме того, можно на кажом шаге неточно решать систему, получая таким образом неточный метод Ньютона. Теория предписывает решать систему все точнее с ростом номера итерации, но на практике нередко используют фиксированное и небольшое число шагов итеративных методов решения систем линейных уравнений.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф14.2. Проксимальные методыКак оптимизировать функции потерь с\\xa0$L_1$-регуляризациейСледующий параграф14.4. Сходимость SGDПочему он\\xa0всё-таки сходитсяЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_41.html', 'title': 'Кластеризация'}, page_content='КластеризацияЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/510.1.КластеризацияЗадача кластеризацииПримеры задач кластеризацииПростейшие методы кластеризации с помощью графовМетод K среднихИерархическая агломеративная кластеризацияDBSCANКакой метод кластеризации выбирать?Оценка качества кластеризации10.2.Временные ряды10.3.Аналитика временных рядов10.4.Модели вида ARIMA10.5.Задача ранжирования11.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Кластеризация10.1. КластеризацияАвторыКантор ВикторМетоды кластеризации: K-Means, агломеративная кластеризация, DBSCAN. Оценка качества кластеризацииЗадача кластеризации\\nВ задаче классификации мы имели дело с восстановлением отображения из множества объектов в конечный набор меток классов. При этом классы были зафиксированы заранее, то есть мы с самого начала примерно понимали, какого рода объекты должны относиться к каждому из них, и мы располагали обучающей выборкой с примерами объектов и классов, к которым они относятся. В задаче кластеризации мы тоже разбиваем объекты на конечное множество классов, но у нас нет ни обучающей выборки, ни понимания, какой будет природа этих классов. То, что модель кластеризации какие-то объекты сочла «похожими», отнеся к одному классу, будет новой информацией, «открытием», сделанным этой моделью. Обучающей выборки у нас также не будет: ведь мы не знаем заранее, что за классы получатся (а иногда и сколько их будет). Таким образом, кластеризация — это задача обучения без учителя. Из-за общего сходства постановок задач в литературе кластеризацию иногда называют unsupervised classification.\\nМетоды кластеризации часто применяют, когда фактически нужно решить задачу классификации, но обучающую выборку собрать затруднительно (дорого или долго). При этом валидационную выборку для оценки результатов кластеризации собрать значительно проще, так как для неё требуется меньше примеров. При этом стоит помнить, что точность работы supervised-методов значительно выше. Поэтому, если обучающую выборку всё-таки можно собрать, лучше решать задачу классификации, чем задачу кластеризации.\\nПримеры задач кластеризации\\nХороший пример применения методов кластеризации — анализ геоданных. В мобильных приложениях, собирающих геоданные пользователей, часто требуется понять, где именно пользователь находился. GPS-координаты известны с некоторой погрешностью, пользователь тоже обычно двигается, поэтому вместо точного положения часто приходится иметь дело с роем точек. Положение усугубляется, когда мы пытаемся анализировать поведение сразу тысяч людей в какой-то локации — например, определить, в каких точках люди чаще всего садятся в такси у аэропорта. Может показаться, что достаточно посмотреть на данные — и мы увидим в точности нужные нам кластеры. Изображение ниже показывает, как может выглядеть ситуация всего для нескольких пользователей: согласно данным GPS, такси подбирают пассажиров и внутри здания аэропорта, и на взлётной полосе, и там, где это происходит на самом деле:\\nВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nПодобная задача решалась в Яндекс.Такси при разработке пикап-пойнтов (наиболее удобных точек вызова такси, подсвечиваемых в приложении). Координаты точек заказа кластеризовались таким образом, чтобы кластер соответствовал какому-то одному, удобному для пользователя месту, и центры кластеров использовались как кандидаты в пикап-пойнты. Те кандидаты, которые удовлетворяли простым фильтрам (например, не попадали в здание или в воду), использовались в приложении. При этом не обходилось и без вручную проставленных пикап-пойнтов: например, такое решение использовалось в окрестностях аэропортов.\\nДругой пример кластеризации геоданных, который всегда рядом с нами, — это интерфейсы для просмотра фотографий в вашем смартфоне. Почти наверняка вы можете просмотреть их в привязке к местам, где они были сделаны, и по мере масштабирования карты вы будете видеть разное количество кластеров фотографий. Кстати, если говорить об интерфейсах, то есть и другой интересный пример: если нужно подстроить цветовую схему вашего интерфейса под выбираемое пользователем изображение (например, фоновую картинку), достаточно кластеризовать цвета из пользовательского изображения, используя RGB-представление (или любое другое) как признаки цвета, и воспользоваться для оформления цветами, соответствующими центрам кластеров.\\nПростейшие методы кластеризации с помощью графов\\nМожно приводить примеры не только про геоаналитику, однако тема геоданных поможет нам придумать пару наиболее простых и наглядных методов кластеризации. Представим, что перед нами рой геокоординат и нам нужно предложить по этим данным пикап-пойнты для такси. Разберём пару очевидных методов.\\nВыделение компонент связности\\nЛогично попробовать объединить точки, которые находятся друг от друга на расстоянии двух-трёх метров, а потом просто выбрать наиболее популярные места. Для этого давайте построим на известных нам точках граф: точки, расстояние между которыми в пределах трёх метров, мы соединим рёбрами. Выделим в этом графе компоненты связности, они и будут нашими кластерами.\\nУ этого способа есть пара очевидных недостатков. Во-первых, может найтись сколько угодно длинная цепочка точек, в которой соседние отстоят друг от друга на пару метров, — и вся она попадёт в одну компоненту связности. В итоге наша отсечка по трём метрам имеет очень опосредованное отношение к диаметру кластеров, а сами кластеры будут получаться значительно больше, чем нам хотелось бы. Во-вторых (и с первой проблемой это тоже связано), непонятно, как мы выбираем максимальное расстояние, при котором соединяем точки ребром. В данной задаче ещё можно предъявить хоть какую-то логику, а вот если бы мы кластеризовали не геометки, а что-то многомерное, например электронные письма по их тематике, придумать отсечку было бы уже сложнее. Если наша цель — не только решить практическую задачу, но и придумать достаточно общий метод кластеризации, понятно, что нам хочется понимать, как подбирать параметры этого метода (в данном случае условие добавления рёбер в граф). Эти соображения могут привести нас к другому решению.\\nМинимальное остовное дерево\\nВместо того чтобы проводить рёбра в графе, давайте их удалять. Построим минимальное остовное дерево, считая расстояния между точками весами рёбер. Тогда, удалив NNN рёбер с наибольшим весом, мы получим N+1N+1N+1 компоненту связности, которые, как и в прошлом подходе, будем считать кластерами. Различие в том, что теперь нам нужно задавать не расстояние, при котором проводится ребро, а количество кластеров. С одной стороны, если мы решаем задачу расчёта пикап-пойнтов в какой-то конкретной локации (аэропорт, торговый центр, жилой дом), нам может быть понятно, сколько примерно пикап-пойнтов мы хотим получить. С другой стороны, даже без локального рассмотрения можно просто сделать достаточно много кластеров, чтобы было из чего выбирать, но при этом достаточно мало, чтобы в каждый кластер попадало репрезентативное количество точек. Аналогичная логика будет справедлива и во многих других задачах кластеризации: количество кластеров — достаточно общий и достаточно хорошо интерпретируемый параметр, чтобы настраивать его вручную, поэтому во многих методах кластеризации количество кластеров выступает как гиперпараметр.\\nДалее будем рассматривать некоторую обобщённую задачу кластеризации без привязки к нашему примеру с анализом геоданных. Мы приведём три наиболее популярных метода кластеризации — k-средних, иерархическую кластеризацию и DBSCAN, а затем рассмотрим вопросы оценки качества кластеризации.\\nМетод K средних\\nПожалуй, один из наиболее популярных методов кластеризации — это метод K-средних (K-means). Основная идея метода — итеративное повторение двух шагов:\\n\\nраспределение объектов выборки по кластерам;\\nпересчёт центров кластеров.\\n\\nВ начале работы алгоритма выбираются KKK случайных центров в пространстве признаков. Каждый объект выборки относят к тому кластеру, к центру которого объект оказался ближе. Далее центры кластеров пересчитывают как среднее арифметическое векторов признаков всех вошедших в этот кластер объектов (то есть центр масс кластера). Как только мы обновили центры кластеров, объекты заново перераспределяются по ним, а затем можно снова уточнить положение центров. Процесс продолжается до тех пор, пока центры кластеров не перестанут меняться.\\n\\nВыбор начального приближения\\nПервый вопрос при выборе начального положения центров — как, выбирая центры из некоторого случайного распределения, не попасть в область пространства признаков, где нет точек выборки. Базовое решение — просто выбрать в качестве центров какие-то из объектов выборки.\\nВторая потенциальная проблема — кучное размещение центров. В этом случае их начальное положение с большой вероятностью окажется далёким от итогового положения центров кластеров. Например, для таких изначальных положений центров\\n\\nмы получим неправильную кластеризацию.\\n\\nЧтобы бороться с этим явлением, выгодно брать максимально удаленные друг от друга центры.\\nНа практике работает следующая эвристика:\\n\\nпервый центр выбираем случайно из равномерного распределения на точках выборки;\\nкаждый следующий центр выбираем из случайного распределения на объектах выборки, в котором вероятность выбрать объект пропорциональна квадрату расстояния от него до ближайшего к нему центра кластера.\\nМодификация K-means, использующая эту эвристику для выбора начальных приближений, называется K-means++.\\n\\nВыбор метрик\\nТак как работа метода K-средних состоит из последовательного повторения до сходимости двух шагов, обоснованность применения различных метрик (расстояний между точками, а не метрик качества 😃 или функций близости связана с тем, «ломают» они какой-либо из этих шагов или нет.\\nПервый шаг с отнесением объектов к ближайшим центрам не зависит от вида метрики. Второй шаг предполагает пересчёт центров как среднего арифметического входящих в кластер точек, и вот здесь будет подвох: к оптимальности выбора центров в среднем арифметическом приводит именно евклидова метрика (подробнее в разделе «Что оптимизирует K-means»).\\nОднако на практике никто не мешает использовать метод и без должного обоснования, поэтому можно экспериментировать с любыми расстояниями, с той лишь оговоркой, что не будет никаких теоретических гарантий, что метод сработает. Наиболее распространённая альтернатива евклидовой метрике — это косинусная мера близости векторов (она особенно популярна в задачах анализа текстов):\\nCosineSimilarity(μk,xi)=<μk,xi>∥μk∥2⋅∥xi∥2CosineSimilarity(\\\\mu_k, x_i)=\\\\frac{<\\\\mu_k, x_i>}{\\\\|\\\\mu_k\\\\|_2 \\\\cdot \\\\|x_i\\\\|_2}\\nCosineSimilarity(μk\\u200b,xi\\u200b)=∥μk\\u200b∥2\\u200b⋅∥xi\\u200b∥2\\u200b<μk\\u200b,xi\\u200b>\\u200bПри её использовании стоит не забывать, что косинусная мера — это функция близости, а не расстояние, так что чем больше её значения, тем ближе друг к другу векторы.\\nMini-batch K-means\\nНесложно заметить, что, если считать KKK и размерность пространства признаков константами, оба шага алгоритма работают за O(n)O(n)O(n), где n — количество объектов обучающей выборки. Отсюда возникает идея ускорения работы алгоритма. В mini-batch K-means мы не считаем шаги сразу на всей выборке, а на каждой итерации выбираем случайную подвыборку (мини-батч) и работаем на ней. В случае когда исходная выборка очень велика, переход к пакетной обработке не приводит к большой потере качества, зато значительно ускоряет работу алгоритма.\\nПонижение размерности\\nС другой стороны, вычисление расстояний и средних делается за O(d)O(d)O(d), где ddd — размерность пространства признаков, так что другая идея ускорения K-means — это предварительно понизить размерность пространства признаков (с помощью PCA или эмбеддингов). Особенно удачно эта идея работает в задачах кластеризации текстов, когда K-means применяют на эмбеддингах слов: получается выиграть не только в скорости работы, но и в интерпретируемости результатов кластеризации.\\nКстати, сам алгоритм кластеризации тоже можно использовать как метод понижения размерности. Если вы решаете задачу обучения с учителем и пространство признаков очень разнообразно (то есть обучающая выборка не даёт вам достаточно статистики при столь большом числе признаков), можно выполнить кластеризацию объектов выборки на 500 или 1000 кластеров и оперировать попаданием объектов в какой-то кластер как признаком. Такой подход называется квантизацией пространства признаков (feature space quantization) и часто помогает на практике, когда нужно огрубить признаки, добавить им интерпретируемости или же, наоборот, обезличить.\\nХрестоматийный пример такого использования кластеризации — метод bag of visual words, расширяющий bag of words из анализа текстов на работу с изображениями. Идея метода в том, чтобы строить признаковое описание изображений на основе входящих в него фрагментов: так, изображения с лицами будут содержать фрагменты с носом, глазами, ртом, а изображения с машинами — колёса, зеркала, двери. Но проблема здесь в том, что нарезать такие фрагменты из обучающей выборки и искать точные совпадения в новых примерах изображений, которые нужно классифицировать, — безнадёжная затея. В жизни фрагменты изображений не повторяются в других изображениях с попиксельной точностью. Решение этой проблемы оказалось возможным при помощи алгоритмов кластеризации (исторически использовался именно K-means): фрагменты изображений из обучающей выборки кластеризовали на 100–1000 кластеров («визуальных слов»), а проходясь по новым изображениям, также нарезали их на фрагменты и относили к одному из этих кластеров. В итоге как новые изображения, так и изображения из обучающей выборки можно было описать количеством вхождений в них фрагментов из различных кластеров («визуальных слов»), так же как в анализе текстов описывают текст количеством вхождений в него слов из словаря. В таком признаковом пространстве уже можно было успешно обучать модели машинного обучения.\\nСейчас выделение «визуальных слов» в задаче классификации изображений происходит автоматически: с одной стороны, задачи компьютерного зрения теперь решаются нейросетями, но с другой стороны — если мы визуализируем отдельные слои этих нейросетей, станет понятно, что их логика работы во многом похожа на описанную выше. При этом идея квантизации признаков не утратила своей актуальности. Вот лишь несколько современных примеров её применения:\\n\\nЕсли вам необходимо дать возможность заказчику (например, внешней компании) анализировать используемые вами признаки — отсутствие провалов в данных и какие-то другие общие показатели, но нельзя отдавать признаки как есть (например, из-за законодательства, регулирующего передачу пользовательских данных), возможное решение — это агрегировать признаки по кластерам.\\nТа же цель может быть отчасти достигнута, если перейти к самим кластерам как к признакам, чтобы скрыть исходные признаки.\\nПереход к кластерам может быть сделан не с целью что-то скрыть, а наоборот, с целью повысить интерпретируемость: исходные сырые данные часто не вполне понятны бизнесу, но позволяют построить маркетинговые сегменты по различным коммерческим интересам пользователей, из-за чего становится удобно показывать принадлежность к этим сегментам как исходные признаки, не вдаваясь в детали о том, на каких данных эти сегменты построены.\\nДля ускорения поиска похожих объектов в пространстве признаков вы также можете проводить поиск внутри того же кластера и соседних кластеров, так что за счёт «огрублённого» вида признаков какие-то процессы можно ещё и ускорить.\\n\\nЧто оптимизирует K-means\\nПроговорим на интуитивном уровне, какую оптимизационную задачу решает K-means.\\nОба шага алгоритма работают на уменьшение среднего квадрата евклидова расстояния от объектов до центров их кластеров:\\nΦ0=1nK∑k=1K∑i=1n(μk−xi)2I[a(xi)=k]\\\\Phi_0 = \\\\frac{1}{nK} \\\\sum\\\\limits_{k=1}^{K} \\\\sum\\\\limits_{i=1}^{n} (\\\\mu_k - x_i)^2 \\\\mathbb{I}[a(x_i)=k]\\nΦ0\\u200b=nK1\\u200bk=1∑K\\u200bi=1∑n\\u200b(μk\\u200b−xi\\u200b)2I[a(xi\\u200b)=k]На шаге отнесения объектов к одному из кластеров мы выбираем кластер с ближайшим центроидом, то есть минимизируем каждое слагаемое в Φ0\\\\Phi_0Φ0\\u200b: все потенциально большие слагаемые мы зануляем, а оставляем ненулевыми только наименьшие из возможных (при условии фиксирования центров кластеров).\\nНа шаге пересчёта центров кластеров мы выбираем центр таким образом, чтобы при фиксированном наборе объектов, относящихся к кластеру, для всех kkk минимизировать выражение, стоящее под суммой по kkk:\\n∑i=1n(μk−xi)2I[a(xi)=k]\\\\sum\\\\limits_{i=1}^{n}  (\\\\mu_k - x_i)^2 \\\\mathbb{I}[a(x_i)=k]\\ni=1∑n\\u200b(μk\\u200b−xi\\u200b)2I[a(xi\\u200b)=k]Здесь уже становится принципиально, что мы определяем квадрат расстояния как квадрат разности векторов, так как именно отсюда при дифференцировании по μk\\\\mu_kμk\\u200b и записи необходимого условия экстремума получается, что центры кластеров нужно пересчитывать как средние арифметические xix_ixi\\u200b, принадлежащих кластеру.\\nЭтих соображений, конечно, недостаточно, чтобы утверждать, что мы найдём минимум Φ0\\\\Phi_0Φ0\\u200b. Более того, гарантии того, что мы найдём глобальный минимум, вообще говоря, нет. Однако, потратив чуть больше усилий, можно доказать, что процесс сойдётся в один из локальных минимумов.\\nТакже можно справедливо заметить, что, так как любой центр кластера — это среднее арифметическое входящих в кластер объектов xix_ixi\\u200b, на выборке фиксированного размера есть лишь конечное множество потенциальных центров кластеров. Если предположить, что в ходе работы K-means не зацикливается, отсюда следует, что рано или поздно центры кластеров не изменятся на следующем шаге и алгоритм сойдётся. При этом фактическая сходимость, конечно же, происходит задолго до полного перебора всех возможных центров кластеров.\\nИерархическая агломеративная кластеризация\\nДругой классический метод кластеризации — это иерархическая кластеризация. Иногда дополнительно уточняют: иерархическая агломеративная кластеризация. Название указывает сразу на два обстоятельства.\\nВо-первых, есть деление алгоритмов кластеризации на агломеративные (agglomerative) и дивизивные, или дивизионные (divisive). Агломеративные алгоритмы начинают с небольших кластеров (обычно с кластеров, состоящих из одного объекта) и постепенно объединяют их в кластеры побольше. Дивизивные начинают с больших кластеров (обычно – с одного единственного кластера) и постепенно делят на кластеры поменьше.\\n\\nВо-вторых, кластеризация бывает, по аналогии с оргструктурой в организациях, плоской (когда все кластеры равноправны и находятся на одном уровне кластеризации) и иерархической (когда кластеры бывают вложены друг в друга и образуют древовидную структуру).\\nВ случае иерархической агломеративной кластеризации мы действительно будем начинать с кластеров из одного объекта, постепенно объединяя их, а уже последовательность этих объединений даст структуру вложенности кластеров. Даже если в итоге мы будем использовать кластеры с одного уровня, не углубляясь ни в какую вложенность, кластеризация всё равно называется иерархической, так как иерархия естественным образом возникает в процессе работы алгоритма.\\nСам алгоритм выглядит предельно просто:\\n\\nСоздаём столько кластеров, сколько у нас объектов в выборке, каждый объект — в своём отдельном кластере.\\nПовторяем итеративно слияние двух ближайших кластеров, пока не выполнится критерий останова.\\n\\nРасстояния в иерархической кластеризации\\nКак измерить расстояние между кластерами из одного объекта? Нужно просто взять расстояние между этими объектами. Остаётся вопрос, как обобщить расстояние между объектами до расстояния между кластерами (если в них более одного объекта). Традиционные решения — брать среднее расстояние между объектами кластеров, минимальное расстояние или максимальное. Если обозначить кластеры UUU и VVV, расстояние между ними в этом случае можем вычислять по одной из формул:\\ndavg(U,V)=1∣U∣⋅∣V∣∑u∈U∑v∈Vρ(u,v)d_{avg}(U, V) = \\\\frac{1}{|U| \\\\cdot |V|} \\\\sum\\\\limits_{u \\\\in U} \\\\sum\\\\limits_{v \\\\in V} \\\\rho(u,v)\\ndavg\\u200b(U,V)=∣U∣⋅∣V∣1\\u200bu∈U∑\\u200bv∈V∑\\u200bρ(u,v)dmin(U,V)=min\\u2061(u,v)∈U×Vρ(u,v)d_{min}(U, V) = \\\\min\\\\limits_{(u,v) \\\\in U \\\\times V} \\\\rho(u,v)\\ndmin\\u200b(U,V)=(u,v)∈U×Vmin\\u200bρ(u,v)dmax(U,V)=max\\u2061(u,v)∈U×Vρ(u,v)d_{max}(U, V) = \\\\max\\\\limits_{(u,v) \\\\in U \\\\times V} \\\\rho(u,v)\\ndmax\\u200b(U,V)=(u,v)∈U×Vmax\\u200bρ(u,v)Используемая формула расстояния между кластерами — один из гиперпараметров алгоритма. Кроме приведённых стандартных вариантов бывают и более экзотичные, например расстояние Уорда (Ward distance). В наиболее общем виде способы задания расстояния между кластерами даются формулой Ланса — Уильямса (Lance — Williams; более подробно вы можете почитать в этой статье). Сами же расстояния между объектами можно задавать любой метрикой, как евклидовой, так и манхэттенским расстоянием или, например, косинусной мерой (с той лишь поправкой, что это мера близости, а не расстояние).\\nУсловия окончания работы алгоритма (критерии останова)\\nВ качестве условия для завершения работы алгоритма можем выбрать либо получение нужного количества кластеров (количество кластеров может быть гиперпараметром алгоритма), либо выполнение эвристик на основе расстояния между объединяемыми кластерами (например, если расстояние сливаемых кластеров значительно выросло по сравнению с прошлой итерацией). На практике же обычно кластеризацию проводят вплоть до одного кластера, включающего в себя всю выборку, а затем анализируют получившуюся иерархическую структуру с помощью дендрограммы.\\nДендрограмма\\nПо мере объединения кластеров, каждой итерации алгоритма соответствует пара объединяемых на этой итерации кластеров, а также расстояние между кластерами в момент слияния. Расстояния с ростом итерации будут только увеличиваться, поэтому возникает возможность построить следующую схему, называемую дендрограммой:\\n\\nЗдесь по горизонтали внизу отмечены объекты кластеризуемой выборки, под горизонтальной осью подписаны номера объектов, а их расположение вдоль оси продиктовано только эстетическими соображениями: нам удобно строить дендрограмму так, чтобы никакие дуги в ней не пересекались. По вертикали отложены расстояния между кластерами в момент слияния. Когда происходит объединение кластеров, состоящих из нескольких объектов, соответствующая этой итерации алгоритма дуга идёт не до конкретных объектов выборки, а до дуги другого кластера.\\nТаким образом мы получаем наглядную визуализацию древовидной структуры процесса кластеризации. В частности, на дендрограмме мы можем визуально заметить, в какой момент происходит скачок расстояний между кластерами, и попытаться определить «естественное» количество кластеров в нашей задаче. На практике же это соображение, как правило, остаётся лишь красивой теорией, так как любую кластеризацию можно делать в разной степени «мелкой» и «естественного» количества кластеров в практических задачах часто не существует. В случае же если данные были получены таким образом, что в них действительно есть какое-то естественное количество кластеров, иерархическая кластеризация обычно справляется с определением числа кластеров по дендрограмме заметно хуже, чем DBSCAN. Именно алгоритму DBSCAN мы и посвятим следующий раздел.\\nDBSCAN\\nАлгоритм DBSCAN (Density-based spatial clustering of applications with noise) развивает идею кластеризации с помощью выделения связных компонент.\\nПрежде чем перейти к построению графа, введём понятие плотности объектов выборки в пространстве признаков. Плотность в DBSCAN определяется в окрестности каждого объекта выборки xix_ixi\\u200b как количество других точек выборки в шаре B(ε,xi)B(\\\\varepsilon, x_i)B(ε,xi\\u200b). Кроме радиуса ε\\\\varepsilonε окрестности в качестве гиперпараметра алгоритма задается порог N0N_0N0\\u200b по количеству точек в окрестности.\\nДалее все объекты выборки делятся на три типа: внутренние / основные точки (core points), граничные (border points) и шумовые точки (noise points). К основным относятся точки, в окрестности которых больше N0N_0N0\\u200b объектов выборки. К граничным — точки, в окрестности которых есть основные, но общее количество точек в окрестности меньше N0N_0N0\\u200b. Шумовыми называют точки, в окрестности которых нет основных точек и в целом содержится менее N0N_0N0\\u200b объектов выборки.\\nАлгоритм кластеризации выглядит следующим образом:\\n\\nШумовые точки убираются из рассмотрения и не приписываются ни к какому кластеру.\\nОсновные точки, у которых есть общая окрестность, соединяются ребром.\\nВ полученном графе выделяются компоненты связности.\\nКаждая граничная точка относится к тому кластеру, в который попала ближайшая к ней основная точка.\\n\\nУдобство DBSCAN заключается в том, что он сам определяет количество кластеров (по модулю задания других гиперпараметров — ε\\\\varepsilonε и N0N_0N0\\u200b), а также в том, что метод успешно справляется даже с достаточно сложными формами кластеров. Кластеры могут иметь вид протяжённых лент или быть вложенными друг в друга как концентрические гиперсферы. На изображении ниже показан пример выделения кластеров достаточно сложной формы с помощью DBSCAN:\\n\\nDBSCAN — один из самых сильных алгоритмов кластеризации, но работает он, как правило, заметно дольше, чем mini-batch K-means, к тому же весьма чувствителен к размерности пространства признаков, поэтому используется на практике DBSCAN только тогда, когда успевает отрабатывать за приемлемое время.\\nКакой метод кластеризации выбирать?\\nЕсли сравнивать частоту использования K-means, иерархической кластеризации и DBSCAN, то на первом месте, бесспорно, будет K-means, а второе место будут делить иерархический подход и DBSCAN. Иерархическая кластеризация — более известный и простой в понимании метод, чем DBSCAN, но довольно редко отрабатывающий качественно. Частая проблема иерархической кластеризации — раннее образование одного гигантского кластера и ряда очень небольших, что приводит к сильной несбалансированности количества объектов в итоговых кластерах. В то же время DBSCAN — менее широко известный подход, но, когда его можно применить, качество, как правило, получается выше, чем в K-means или иерархической кластеризации.\\nОценка качества кластеризации\\nДалее приведём список основных метрик качества кластеризации и обсудим некоторые особенности их применения.\\nСреднее внутрикластерное расстояние\\nСмысл среднего внутрикластерного расстояния максимально соответствует названию:\\nF0=∑i=1n∑j=inρ(xi,xj)I[a(xi)=a(xj)]∑i=1n∑j=inI[a(xi)=a(xj)]F_0 = \\\\frac{\\\\sum\\\\limits_{i=1}^{n} \\\\sum\\\\limits_{j=i}^{n} \\\\rho(x_i, x_j) \\\\mathbb{I}[a(x_i)=a(x_j)]}{\\\\sum\\\\limits_{i=1}^{n} \\\\sum\\\\limits_{j=i}^{n} \\\\mathbb{I}[a(x_i)=a(x_j)]}\\nF0\\u200b=i=1∑n\\u200bj=i∑n\\u200bI[a(xi\\u200b)=a(xj\\u200b)]i=1∑n\\u200bj=i∑n\\u200bρ(xi\\u200b,xj\\u200b)I[a(xi\\u200b)=a(xj\\u200b)]\\u200bСумма расстояний между точками из одного и того же кластера делится на количество пар точек, принадлежащих к одному кластеру. В приведённой выше формуле пары вида (xi,xi)(x_i, x_i)(xi\\u200b,xi\\u200b) включены в рассмотрение, чтобы избежать неопределённости 00\\\\frac{0}{0}00\\u200b в случае, когда в каждом кластере ровно по одному объекту. Однако иногда записывают суммы по i<ji < ji<j, просто доопределяя F0F_0F0\\u200b в описанном случае нулём.\\nРешая задачу кластеризации, мы хотим по возможности получать как можно более кучные кластеры, то есть минимизировать F0F_0F0\\u200b.\\nВ случае если у кластеров есть центры μk\\\\mu_kμk\\u200b, часто рассматривается аналогичная метрика — средний квадрат внутрикластерного расстояния:\\nΦ0=1nK∑k=1K∑i=1nρ(μk,xi)2I[a(xi)=k]\\\\Phi_0 = \\\\frac{1}{nK} \\\\sum\\\\limits_{k=1}^{K} \\\\sum\\\\limits_{i=1}^{n} \\\\rho(\\\\mu_k, x_i)^2 \\\\mathbb{I}[a(x_i)=k]\\nΦ0\\u200b=nK1\\u200bk=1∑K\\u200bi=1∑n\\u200bρ(μk\\u200b,xi\\u200b)2I[a(xi\\u200b)=k]Среднее межкластерное расстояние\\nАналогично среднему внутрикластерному расстоянию вводится среднее межкластерное:\\nF1=∑i=1n∑j=inρ(xi,xj)I[a(xi)≠a(xj)]∑i=1n∑j=inI[a(xi)≠a(xj)]F_1 = \\\\frac{\\\\sum\\\\limits_{i=1}^{n} \\\\sum\\\\limits_{j=i}^{n} \\\\rho(x_i, x_j) \\\\mathbb{I}[a(x_i) \\\\neq a(x_j)]}{\\\\sum\\\\limits_{i=1}^{n} \\\\sum\\\\limits_{j=i}^{n} \\\\mathbb{I}[a(x_i) \\\\neq a(x_j)]}\\nF1\\u200b=i=1∑n\\u200bj=i∑n\\u200bI[a(xi\\u200b)\\ue020=a(xj\\u200b)]i=1∑n\\u200bj=i∑n\\u200bρ(xi\\u200b,xj\\u200b)I[a(xi\\u200b)\\ue020=a(xj\\u200b)]\\u200bСреднее межкластерное расстояние, напротив, нужно максимизировать, то есть имеет смысл выделять в разные кластеры наиболее удалённые друг от друга объекты.\\nГомогенность\\nДля измерения следующих метрик (гомогенности, полноты и V-меры) нам уже потребуется разметка выборки. Будем обозначать кластеры, к которым наш алгоритм кластеризации относит каждый объект, буквами k∈{1,...,K}k \\\\in \\\\lbrace 1, ..., K \\\\rbracek∈{1,...,K}, а классы, к которым объекты отнесены разметкой, — буквами с∈{1,...,С}с \\\\in \\\\lbrace 1, ..., С \\\\rbraceс∈{1,...,С}. Разумный вопрос при наличии разметки — зачем нам решать задачу кластеризации, если с разметкой можно поставить задачу как задачу классификации. Это и правда хороший вопрос в том случае, если размеченных данных достаточно много для обучения классификатора. На практике же часто встречаются ситуации, когда данных достаточно для оценки качества кластеризации, но всё ещё не хватает для использования методов обучения с учителем.\\nПусть nnn — общее количество объектов в выборке, nkn_knk\\u200b — количество объектов в кластере номер kkk, mcm_cmc\\u200b — количество объектов в классе номер ссс, а nckn_{ck}nck\\u200b — количество объектов из класса ccc в кластере kkk. Рассмотрим следующие величины:\\nHclass=−∑c=1Cmcnlog\\u2061mcnH_{class} = - \\\\sum\\\\limits_{c = 1}^{C} \\\\frac{m_c}{n} \\\\log {\\\\frac{m_c}{n}}\\nHclass\\u200b=−c=1∑C\\u200bnmc\\u200b\\u200blognmc\\u200b\\u200bHclust=−∑k=1Knknlog\\u2061nknH_{clust} = - \\\\sum\\\\limits_{k = 1}^{K} \\\\frac{n_k}{n} \\\\log {\\\\frac{n_k}{n}}\\nHclust\\u200b=−k=1∑K\\u200bnnk\\u200b\\u200blognnk\\u200b\\u200bHclass∣clust=−∑c=1C∑k=1Kncknlog\\u2061ncknkH_{class | clust} = - \\\\sum\\\\limits_{c = 1}^{C}\\\\sum\\\\limits_{k = 1}^{K} \\\\frac{n_{ck}}{n} \\\\log {\\\\frac{n_{ck}}{n_k}}\\nHclass∣clust\\u200b=−c=1∑C\\u200bk=1∑K\\u200bnnck\\u200b\\u200blognk\\u200bnck\\u200b\\u200bНесложно заметить, что эти величины соответствуют формуле энтропии и условной энтропии для мультиномиальных распределений mcn\\\\frac{m_c}{n}nmc\\u200b\\u200b, nkn\\\\frac{n_k}{n}nnk\\u200b\\u200b и ncknk\\\\frac{n_{ck}}{n_k}nk\\u200bnck\\u200b\\u200b соответственно.\\nГомогенность кластеризации определяется следующим выражением:\\nHomogeneity=1−Hclass∣clustHclassHomogeneity = 1 - \\\\frac{H_{class | clust}}{H_{class}}\\nHomogeneity=1−Hclass\\u200bHclass∣clust\\u200b\\u200bОтношение Hclass∣clustHclass\\\\frac{H_{class \\\\vert clust}}{H_{class}}Hclass\\u200bHclass∣clust\\u200b\\u200b показывает, во сколько раз энтропия изменяется за счёт того, что мы считаем известной принадлежность объектов к выделенным нашим алгоритмом кластерам. Худший случай — когда отношение оказывается равным единице (энтропия не изменилась, условная энтропия совпала с обычной), лучший — когда каждый кластер содержит элементы только одного класса и номер кластера, таким образом, точно определяет номер класса (в этом случае h=1h=1h=1).\\nТривиальный способ максимизировать гомогенность кластеризации — выделить каждый объект выборки в отдельный кластер.\\nПолнота\\nПолнота задаётся аналогично гомогенности, с той лишь разницей, что вводится величина Hclust∣classH_{clust \\\\vert class}Hclust∣class\\u200b, симметричная Hclass∣clustH_{class \\\\vert clust}Hclass∣clust\\u200b:\\nCompleteness=1−Hclust∣classHclustCompleteness = 1 - \\\\frac{H_{clust | class}}{H_{clust}}\\nCompleteness=1−Hclust\\u200bHclust∣class\\u200b\\u200bПолнота равна единице, когда все объекты класса всегда оказываются в одном кластере.\\nТривиальный способ максимизировать полноту кластеризации — объединить всю выборку в один кластер.\\nV-мера\\nГомогенность и полнота кластеризации – это в некотором смысле аналоги точности и полноты классификации. Аналог F-меры для задачи кластеризации тоже есть, он называется V-мерой и связан с гомогенностью и полнотой той же формулой, что и F-мера с точностью и полнотой:\\nVβ=(1+β)⋅Homogeneity⋅Completenessβ⋅Homogeneity+CompletenessV_{\\\\beta} = \\\\frac{(1 + \\\\beta) \\\\cdot Homogeneity \\\\cdot Completeness}{\\\\beta \\\\cdot Homogeneity + Completeness}\\nVβ\\u200b=β⋅Homogeneity+Completeness(1+β)⋅Homogeneity⋅Completeness\\u200bВ частности, V1V_1V1\\u200b по аналогии с F1F_1F1\\u200b-мерой в классификации (не путать со средним межкластерным расстоянием, которое мы тоже обозначали F1F_1F1\\u200b выше) будет просто средним гармоническим гомогенности и полноты:\\nV1=2⋅Homogeneity⋅CompletenessHomogeneity+CompletenessV_{1} = \\\\frac{2 \\\\cdot Homogeneity \\\\cdot Completeness}{Homogeneity + Completeness} \\nV1\\u200b=Homogeneity+Completeness2⋅Homogeneity⋅Completeness\\u200bV-мера комбинирует в себе гомогенность и полноту таким образом, чтобы максимизация итоговой метрики не приводила к тривиальным решениям.\\nКоэффициент силуэта\\nЕщё одна метрика кластеризации, на этот раз уже не требующая разметки, это коэффициент силуэта (silhouette coefficient). Изначально коэффициент определяется для каждого объекта выборки, а метрика для результатов кластеризации всей выборки вводится как средний коэффициент силуэта для всех объектов выборки.\\nЧтобы ввести коэффициент силуэта S(xi)S(x_i)S(xi\\u200b), нам потребуются две вспомогательные величины. Первая, A(xi)A(x_i)A(xi\\u200b), — это среднее расстояние между xix_ixi\\u200b и объектами того же кластера. Вторая, B(xi)B(x_i)B(xi\\u200b), — это среднее расстояние между xix_ixi\\u200b и объектами следующего ближайшего кластера. Коэффициент силуэта вводится следующим образом:\\nS(xi)=B(xi)−A(xi)max\\u2061(B(xi),A(xi))S(x_i) = \\\\frac{B(x_i) - A(x_i)}{\\\\max(B(x_i), A(x_i))}\\nS(xi\\u200b)=max(B(xi\\u200b),A(xi\\u200b))B(xi\\u200b)−A(xi\\u200b)\\u200bВ идеальном случае объекты «родного» кластера xix_ixi\\u200b должны быть ближе к xix_ixi\\u200b, чем объекты соседнего кластера, то есть A(xi)<B(xi)A(x_i) < B(x_i)A(xi\\u200b)<B(xi\\u200b). Однако это неравенство выполняется далеко не всегда. Если «родной» кластер xix_ixi\\u200b, например, имеет форму очень протяжённой ленты или просто большой размер, а недалеко от xix_ixi\\u200b есть кластер поменьше, может оказаться, что A(xi)>B(xi)A(x_i) > B(x_i)A(xi\\u200b)>B(xi\\u200b). Таким образом, если мы посмотрим на разность B(xi)−A(xi)B(x_i) - A(x_i)B(xi\\u200b)−A(xi\\u200b), она может оказаться как положительной, так и отрицательной, но в идеальном сценарии всё же следует ожидать положительное значение. Сам же коэффициент S(xi)S(x_i)S(xi\\u200b) принимает значения от –1 до +1 и максимизируется, когда кластеры кучные и хорошо отделены друг от друга.\\nКоэффициент силуэта особенно полезен (по сравнению с другими приведёнными метриками) тем, что одновременно и не требует разметки, и позволяет подбирать количество кластеров. См. подробнее в примере из документации scikit-learn.\\nРазличия и выбор метрик качества кластеризации\\nПодводя итог в теме метрик качества в задаче кластеризации, отметим, что есть несколько разных сценариев использования этих метрик. Если вы уже определились с количеством кластеров, можно оптимизировать среднее внутрикластерное или среднее межкластерное расстояние. Если у вас ещё и есть разметка — гомогенность и полноту. V-мера за счёт сочетания гомогенности и полноты в целом позволяет выполнять и подбор количества кластеров.\\nОднако разметка, с одной стороны, есть далеко не всегда, а с другой стороны, в задаче кластеризации часто очень субъективна. Сложность кластеризации в том, что на одной и той же выборке нас вполне могут устроить сразу несколько различных вариантов кластеризации, то есть задача поставлена некорректно и имеет более одного решения. Формализовать, какие решения нас устроят, на практике довольно сложно, поэтому сама по себе задача кластеризации решается не слишком хорошо.\\nЕсли разметки нет и число кластеров не фиксировано, лучшая метрика на практике — коэффициент силуэта. Исключение — ситуация, когда результат кластеризации используется далее для решения некоторой задачи обучения с учителем (как было в примере классификации изображений с помощью visual bag of words). В этом случае можно абстрагироваться от качества кластеризации и выбирать такой алгоритм кластеризации и такие его гиперпараметры, которые позволят лучше всего решить итоговую задачу.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф9.4. Хорошие свойства рекомендательных системСледующий параграф10.2. Временные рядыЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_16.html', 'title': 'Как оценивать вероятности'}, page_content=\"Как оценивать вероятностиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/74.1.Вероятностный подход в ML4.2.Экспоненциальный класс распределений и принцип максимальной энтропии4.3.Обобщённые линейные модели4.4.Как оценивать вероятностиЧто же такое вероятность класса, если объект либо принадлежит этому классу, либо нет?Вам скажут: логистическая регрессия корректно действительно предсказывает вероятностиНо почему же все твердят, что логистическая регрессия хорошо калибрована?!Как же всё-таки предсказать вероятности: методы калибровкиКак измерить качество калибровки4.5.Генеративный подход к классификации4.6.Байесовский подход к оцениванию4.7.Модели с латентными переменными5.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Как оценивать вероятности4.4. Как оценивать вероятностиАвторыФедотов СтаниславКак правильно оценить вероятности классов в\\xa0задаче классификацииМы уже упоминали, что оценивать вероятности классов как softmax(fw(xi))softmax(f_w(x_i))softmax(fw\\u200b(xi\\u200b)) для какой-то произвольной функции fwf_wfw\\u200b — это дело подозрительное.\\nВ этом разделе мы поговорим о том, как это делать хорошо и правильно.\\nЧто же такое вероятность класса, если объект либо принадлежит этому классу, либо нет?\\nОграничимся пока случаем двухклассовой классификации — с классами 0 и 1. Если утверждается, что мы предсказываем корректную вероятность класса 1 (обозначим её q(xi)q(x_i)q(xi\\u200b)), то прогноз «объект xix_ixi\\u200b принадлежит классу 1 с вероятностью 23\\\\frac2332\\u200b» должен сбываться в 23\\\\frac2332\\u200b случаев.\\nТо есть, условно говоря, если мы возьмём все объекты, то среди них что-то около двух третей действительно имеет класс 1.\\nНа математическом языке это можно сформулировать так: Если p^\\\\widehat{p}p\\u200b — предсказанная вероятность класса 1, то P(yi=1∣q(xi)=p^)=p^P(y_i = 1 \\\\vert q(x_i) = \\\\widehat{p}) = \\\\widehat{p}P(yi\\u200b=1∣q(xi\\u200b)=p\\u200b)=p\\u200b.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nК сожалению, в реальной жизни p^\\\\widehat{p}p\\u200b — это скорее всего вещественные числа, которые будут различными для различных yiy_iyi\\u200b, и никаких вероятностей мы не посчитаем, но мы можем разбить отрезок [0,1][0,1][0,1] на бины, внутри каждого из которых уже вычислить, каковая там доля объектов класса 1, и сравнить эту долю со средним значением вероятности в бине:\\n\\nУ модели, которая идеально предсказывает вероятности (как обычно говорят, у идеально калиброванной модели) жёлтые точки на диаграмме калибровки должны совпадать с розовыми.\\nА вот на картинке выше это не так: жёлтые точки всегда ниже розовых. Давайте поймём, что это значит. Получается, что наша модель систематически завышает предсказанную вероятность (розовые точки), и порог отсечения нам, выходит, тоже надо было бы сдвинуть вправо:\\n\\nНо такая картинка, пожалуй, говорит о какой-то серьёзной патологии классификатора. Гораздо чаще встречаются следующие две ситуации:\\n\\n\\nСлишком уверенный (overconfident) классификатор:\\n\\nТакое случается с сильными классификаторыми (например, нейросетями), которые учились на метки классов, а не на вероятности: тем самым процесс обучения стимулировал их всегда давать ответ, как можно более близкий к 0 или 1.\\n\\n\\nНеуверенный (underconfident) классификатор:\\n\\n\\n\\nТакое может случиться, например, если мы слишком много обращаем внимания на трудные для классификации объекты на границе классов (как, скажем, в SVM), в каком-то смысле в ущерб более однозначно определяемым точкам. Этим же могут и грешить модели на основе бэггинга (например, случайный лес). Грубо говоря, среднее нескольких моделей предскажет что-то близкое к единице только если все слагаемые предскажут что-то, близкое к единице — но из-за дисперсии моделей это будет случаться реже, чем могло бы. Подробнее можно почитать в статье.\\nВам скажут: логистическая регрессия корректно действительно предсказывает вероятности\\nВам даже будут приводить какие-то обоснования. Важно понимать, что происходит на самом деле, и не дать ввести себя в заблуждение. В качестве противоядия от иллюзий предлагаем рассмотреть два примера.\\nРассмотрим датасет c двумя классами (ниже на картинке обучающая выборка)\\n\\nОбучим на нём логистическую регрессию из sklearn безо всяких параметров (то есть L2L^2L2-регуляризованную, но это не так важно). Классы не так-то просто разделить, вот и логистическая регрессия так себе справляется. Ниже изображена часть тестовой выборки вместе с предсказанными вероятностями классов для всех точек области\\n\\nВидим, что модель не больно-то уверена в себе, и ясно почему: признаковое описание достаточно бедное и не позволяет нам хорошо разделить классы, хотя, казалось бы, это можно довольно неплохо сделать.\\nПопробуем поправить дело, добавив полиномиальные фичи, то есть все xjykx^jy^kxjyk для 0⩽j,k⩽50\\\\leqslant j,k\\\\leqslant 50⩽j,k⩽5 в качестве признаков, и обучив поверх этих данных логистическую регрессию. Снова нарисуем некоторые точки тестовой выборки и предсказания вероятностей для всех точек области:\\n\\nВидим, что у нас сочетание двух проблем: неуверенности посередине и очень уверенных ошибок по краям.\\nНарисуем теперь калибровочные кривые для обеих моделей:\\n\\nКалибровочные кривые весьма примечательны; в любом случае ясно, что с предсказанием вероятностей всё довольно плохо. Посмотрим ещё, какие вероятности наши классификаторы чаще приписывают объектам:\\n\\nКак и следовало ожидать, предсказания слабого классификатора тяготеют к серединке (та самая неуверенность), а среди предсказаний переобученного очень много крайне уверенных — и совсем не всегда правильных.\\nНо почему же все твердят, что логистическая регрессия хорошо калибрована?!\\nПопробуем понять и простить её.\\nКак мы помним, логистическая регрессия учится путём минимизации функционала\\nl(X,y)=−∑i=1N(yilog\\u2061(σ(⟨w,xi⟩))+(1−yi)log\\u2061(1−σ(⟨w,xi⟩)))l(X, y) = -\\\\sum_{i=1}^N(y_i\\\\log(\\\\sigma(\\\\langle w, x_i\\\\rangle)) + (1 - y_i)\\\\log(1 - \\\\sigma(\\\\langle w, x_i\\\\rangle)))\\nl(X,y)=−i=1∑N\\u200b(yi\\u200blog(σ(⟨w,xi\\u200b⟩))+(1−yi\\u200b)log(1−σ(⟨w,xi\\u200b⟩)))Отметим между делом, что каждое слагаемое — это кроссэнтропия распределения PPP, заданного вероятностями P(0)=1−σ(⟨w,xi⟩)P(0) = 1 - \\\\sigma(\\\\langle w, x_i\\\\rangle)P(0)=1−σ(⟨w,xi\\u200b⟩) и P(1)=σ(⟨w,xi⟩)P(1) = \\\\sigma(\\\\langle w, x_i\\\\rangle)P(1)=σ(⟨w,xi\\u200b⟩), и тривиального распределения, которое равно yiy_iyi\\u200b с вероятностью 111.\\nДопустим, что мы обучили по всему универсуму данных X\\\\mathbb{X}X идеальную логистическую регрессию с идеальными весами w∗w^{\\\\ast}w∗. Пусть, далее, оказалось, что у нас есть nnn объектов x1,…,xnx_1,\\\\ldots,x_nx1\\u200b,…,xn\\u200b с одинаковым признаковым описанием (то есть по сути представленных одинаковыми векторами xix_ixi\\u200b), но, возможно, разными истинными метками классов y1,…,yny_1,\\\\ldots,y_ny1\\u200b,…,yn\\u200b. Тогда соответствующий им кусок функции потерь имеет вид\\n−(∑i=1nyi)log\\u2061(σ(⟨w,x1⟩))−(∑i=1n(1−yi))log\\u2061(1−σ(⟨w,x1⟩))=-\\\\left(\\\\sum_{i=1}^ny_i\\\\right)\\\\log(\\\\sigma(\\\\langle w, x_1\\\\rangle)) -\\\\left(\\\\sum_{i=1}^n (1 - y_i)\\\\right)\\\\log(1 - \\\\sigma(\\\\langle w, x_1\\\\rangle)) =\\n−(i=1∑n\\u200byi\\u200b)log(σ(⟨w,x1\\u200b⟩))−(i=1∑n\\u200b(1−yi\\u200b))log(1−σ(⟨w,x1\\u200b⟩))==−n(12p0log\\u2061(σ(⟨w,x1⟩))+p1log\\u2061(1−σ(⟨w,x1⟩)))=-n\\\\left(\\\\vphantom{\\\\frac12}p_0\\\\log(\\\\sigma(\\\\langle w, x_1\\\\rangle)) + p_1\\\\log(1 - \\\\sigma(\\\\langle w, x_1\\\\rangle))\\\\right)\\n=−n(21\\u200bp0\\u200blog(σ(⟨w,x1\\u200b⟩))+p1\\u200blog(1−σ(⟨w,x1\\u200b⟩)))где pjp_jpj\\u200b — частота jjj-го класса среди истинных меток. В скобках также стоит кросс-энтропия распределения, задаваемого частотой меток истинных классов, и распределения, предсказываемого логистической регрессией. Минимальное значение кросс-энтропии (и минимум функции потерь) достигается, когда\\nσ(⟨w,x1⟩)=p0,1−σ(⟨w,x1⟩)=p1\\\\sigma(\\\\langle w, x_1\\\\rangle) = p_0,\\\\quad 1 - \\\\sigma(\\\\langle w, x_1\\\\rangle) = p_1\\nσ(⟨w,x1\\u200b⟩)=p0\\u200b,1−σ(⟨w,x1\\u200b⟩)=p1\\u200bРезультат, полученный для nnn совпадающих точек будет приблизительно верным и для nnn достаточно близких точек в случае, когда:\\n\\nпризнаковое описание данных достаточно хорошее — классы не перемешаны как попало и всё-таки близки к разделимым;\\nмодель не переобученная — то есть, предсказания вероятностей не скачут очень уж резко — вспомните второй пример.\\n\\nНа всех этих точках модель будет выдавать примерно долю положительных, то есть тоже хорошую оценку вероятности.\\nКак же всё-таки предсказать вероятности: методы калибровки\\nПусть наша модель (бинарной классификации) для каждого объекта xix_ixi\\u200b выдаёт некоторое число q(xi)∈[0,1]q(x_i)\\\\in[0,1]q(xi\\u200b)∈[0,1]. Как же эти числа превратить в корректные вероятности?\\n\\nГистограммная калибровка. Мы разбиваем отрезок [0,1][0,1][0,1] на бины B1,…,Bk\\\\mathbb{B}_1,\\\\ldots,\\\\mathbb{B}_kB1\\u200b,…,Bk\\u200b (одинаковой ширины или равномощные) и хотим на каждом из них предсказывать всегда одну и ту же вероятность: θj\\\\theta_jθj\\u200b, если q(xi)∈Bjq(x_i)\\\\in \\\\mathbb{B}_jq(xi\\u200b)∈Bj\\u200b. Вероятности θi\\\\theta_iθi\\u200b подбираются так, чтобы они как можно лучше приближали средние метки классов на соответствующих бинах. Иными словами, мы решаем задачу\\n\\n∑j=1k∣∑i=1NI{q(xi)∈Bj}yi∣Bj∣−θj∣⟶min\\u2061(θ1,…,θk)\\\\sum_{j=1}^k\\\\left|\\\\frac{\\\\sum_{i=1}^N\\\\mathbb{I}\\\\{q(x_i)\\\\in\\\\mathbb{B}_j\\\\}y_i}{ \\\\vert \\\\mathbb{B}_j \\\\vert } - \\\\theta_j\\\\right|\\\\longrightarrow\\\\min\\\\limits_{(\\\\theta_1,\\\\ldots,\\\\theta_k)}\\nj=1∑k\\u200b\\u200b∣Bj\\u200b∣∑i=1N\\u200bI{q(xi\\u200b)∈Bj\\u200b}yi\\u200b\\u200b−θj\\u200b\\u200b⟶(θ1\\u200b,…,θk\\u200b)min\\u200bВместо разности модулей можно рассматривать и разность квадратов.\\nМетод довольно простой и понятный, но требует подбора числа бинов и предсказывает лишь дискретное множество вероятностей.\\nИзотоническая регрессия. Этот метод похож на предыдущий, только мы будем, во-первых, настраивать и границы 0=b0,b1,…,bk=10=b_0,b_1,\\\\ldots,b_k = 10=b0\\u200b,b1\\u200b,…,bk\\u200b=1 бинов Bj={t∣bj−1⩽bj}\\\\mathbb{B}_j = \\\\{t \\\\vert  b_{j-1}\\\\leqslant b_j\\\\}Bj\\u200b={t∣bj−1\\u200b⩽bj\\u200b}, а кроме того, накладываем условие θ1⩽…⩽θk\\\\theta_1\\\\leqslant\\\\ldots\\\\leqslant\\\\theta_kθ1\\u200b⩽…⩽θk\\u200b. Искать bjb_jbj\\u200b и θj\\\\theta_jθj\\u200b мы будем, приближая yiy_iyi\\u200b кусочно постоянной функцией ggg от q(xi)q(x_i)q(xi\\u200b):\\n∑i=1N(yi−g(q(xi)))2⟶min\\u2061g\\\\sum_{i=1}^N(y_i - g(q(x_i)))^2\\\\longrightarrow\\\\min_{g}\\ni=1∑N\\u200b(yi\\u200b−g(q(xi\\u200b)))2⟶gmin\\u200b\\nМинимизация осуществляется при помощи pool adjacent violators algorithm, и эти страницы слишком хрупки, чтобы выдержать его формулировку.\\n\\nКалибровка Платта представляет собой по сути применение сигмоиды поверх другой модели (то есть самый наивный способ получения «вероятностей»). Более точно, если q(xi)q(x_i)q(xi\\u200b) — предсказанная вероятность, то мы полагаем\\n\\nP(yi=1∣xi)=σ(aq(xi)+b)=11+e−aq(xi)−bP(y_i = 1\\\\mid x_i) = \\\\sigma(aq(x_i) + b) = \\\\frac1{1 + e^{-aq(x_i) - b}}\\nP(yi\\u200b=1∣xi\\u200b)=σ(aq(xi\\u200b)+b)=1+e−aq(xi\\u200b)−b1\\u200bгде aaa и bbb подбираются методом максимального правдоподобия на отложенной выборке:\\n−∑i=1N(12yilog\\u2061(σ(q(xi)))+(1−yi)log\\u2061(1−σ(q(xi))))⟶min\\u2061a,b-\\\\sum_{i=1}^N(\\\\vphantom{\\\\frac12}y_i\\\\log(\\\\sigma(q(x_i))) + (1 - y_i)\\\\log(1 - \\\\sigma(q(x_i))))\\\\longrightarrow\\\\min\\\\limits_{a,b}\\n−i=1∑N\\u200b(21\\u200byi\\u200blog(σ(q(xi\\u200b)))+(1−yi\\u200b)log(1−σ(q(xi\\u200b))))⟶a,bmin\\u200bДля избежания переобучения Платт предлагал также заменить метки yiy_iyi\\u200b и (1−yi)(1 - y_i)(1−yi\\u200b) на регуляризованные вероятности таргетов:\\nt0=1#{i∣yi=0}+2,t1=#{i∣yi=1}+1#{i∣yi=0}+2t_0 = \\\\frac1{\\\\#\\\\{i \\\\vert y_i = 0\\\\} + 2},\\\\quad t_1 = \\\\frac{\\\\#\\\\{i \\\\vert y_i = 1\\\\} + 1}{\\\\#\\\\{i \\\\vert y_i = 0\\\\} + 2}\\nt0\\u200b=#{i∣yi\\u200b=0}+21\\u200b,t1\\u200b=#{i∣yi\\u200b=0}+2#{i∣yi\\u200b=1}+1\\u200bКалибровка Платта неплохо справляется с выколачиванием вероятностей из SVM, но для более хитрых классификаторов может спасовать. В целом, можно показать, что этот метод хорошо работает, если для каждого из истинных классов предсказанные вероятности q(xi)q(x_i)q(xi\\u200b) распределы нормально с одинаковыми дисперсиями. Подробнее об этом вы можете почитать в этой статье. Там же описано обобщение данного подхода — бета-калибровка.\\nС большим количеством других методов калибровки вы можете познакомиться в этой статье\\nКак измерить качество калибровки\\nКалибровочные кривые хорошо показывают, что есть проблемы, но как оценить наши усилия по улучшению предсказания вероятностей? Хочется иметь какую-то численную метрику. Мы упомянем две разновидности — прямое воплощение описанных выше идей.\\n\\nExpected/Maximum calibration error. Самый простой способ, впрочем — он наследник идеи с калибровочной кривой. А именно, разобьём отрезок [0,1][0,1][0,1] на бины B1,…,Bk\\\\mathbb{B}_1,\\\\ldots,\\\\mathbb{B}_kB1\\u200b,…,Bk\\u200b по предсказанным вероятностям и вычислим\\n\\n∑j=1k#BjN∣y‾(Bj)−q‾(Bj)∣\\\\sum_{j=1}^k\\\\frac{\\\\#\\\\mathbb{B}_j}{N}\\\\left|\\\\overline{y}(\\\\mathbb{B}_j) - \\\\overline{q}(\\\\mathbb{B}_j)\\\\right|\\nj=1∑k\\u200bN#Bj\\u200b\\u200b∣y\\u200b(Bj\\u200b)−q\\u200b(Bj\\u200b)∣или\\nmax\\u2061j=1,…,k∣y‾(Bj)−q‾(Bj)∣\\\\max\\\\limits_{j=1,\\\\ldots,k}\\\\left|\\\\overline{y}(\\\\mathbb{B}_j) - \\\\overline{q}(\\\\mathbb{B}_j)\\\\right|\\nj=1,…,kmax\\u200b∣y\\u200b(Bj\\u200b)−q\\u200b(Bj\\u200b)∣где y‾(Bj)\\\\overline{y}(\\\\mathbb{B}_j)y\\u200b(Bj\\u200b) — среднее значение yiy_iyi\\u200b, а q‾(Bj)\\\\overline{q}(\\\\mathbb{B}_j)q\\u200b(Bj\\u200b) — среднее значение q(xi)q(x_i)q(xi\\u200b) для xix_ixi\\u200b, таких что q(xi)∈Bjq(x_i)\\\\in\\\\mathbb{B}_jq(xi\\u200b)∈Bj\\u200b. Проблема этого способа в том, что мы можем очень по-разному предсказывать в каждом из бинов вероятности (в том числе константой) без ущерба для метрики.\\n\\nBrier score. Одна из популярных метрик, которая попросту измеряет разницу между предсказанными вероятностями и yiy_iyi\\u200b:\\n\\n∑i=1N(yi−q(xi))2\\\\sum_{i=1}^N(y_i - q(x_i))^2\\ni=1∑N\\u200b(yi\\u200b−q(xi\\u200b))2Казалось бы, в чём смысл? Немного подрастить мотивацию помогает следующий пример. Допустим, наши таргеты совершенно случайны, то есть P(yi=1∣xi)=P(yi)P(y_i = 1 \\\\vert x_i) = P(y_i)P(yi\\u200b=1∣xi\\u200b)=P(yi\\u200b). Тогда хорошо калиброванный классификатор должен для каждого xix_ixi\\u200b предсказывать вероятность 12\\\\frac1221\\u200b; соответственно, его brier score равен 14\\\\frac1441\\u200b. Если же классификатор хоть в одной точке выдаёт вероятность p>12p>\\\\frac12p>21\\u200b, то в маленькой окрестности он должен выдавать примерно такие же вероятности.\\nПоскольку же таргет случаен, локальный кусочек суммы из brier score будет иметь вид N′2p2+N′2(1−p)2<N′2\\\\frac{N'}{2}p^2 + \\\\frac{N'}{2}(1-p)^2 < \\\\frac{N'}22N′\\u200bp2+2N′\\u200b(1−p)2<2N′\\u200b, что хуже, чем получил бы всегда выдающий 12\\\\frac1221\\u200b классификатор.\\nНе обязательно брать квадратичную ошибку; сгодится и наш любимый log-loss:\\n∑i=1N(12yilog\\u2061q(xi)+(1−yi)log\\u2061(1−q(xi)))\\\\sum_{i=1}^N\\\\left(\\\\vphantom{\\\\frac12}y_i\\\\log{q(x_i)} + (1 - y_i)\\\\log(1 - q(x_i))\\\\right)\\ni=1∑N\\u200b(21\\u200byi\\u200blogq(xi\\u200b)+(1−yi\\u200b)log(1−q(xi\\u200b)))Это же и помогает высветить ограничения подхода, если вспомнить рассуждения о калиброванности логистической регрессии. Для достаточно гладких классификатора и датасета brier score и log-loss будут адекватными средствами оценки, но если нет — возможно всякое.\\nВопрос на засыпку: а как быть, если у нас классификация не бинарная, а многоклассовая? Что такое хорошо калиброванный классификатор? Как это определить численно? Как заставить произвольный классификатор предсказывать вероятности?\\nМы не будем про это рассказывать, но призываем читателя подумать над этим самостоятельно или, например, посмотреть туториал с ECML KDD 2020.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф4.3. Обобщённые линейные моделиКак прокачать линейную модель с\\xa0помощью распределений из\\xa0экспоненциального классаСледующий параграф4.5. Генеративный подход к классификацииКак использовать распределение меток классов в\\xa0задаче классификации. LDA, QDA и\\xa0наивный байесЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_61.html', 'title': 'Регуляризация в онлайн-обучении'}, page_content='Регуляризация в онлайн-обученииЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/415.1.Введение в онлайн-обучение15.2.Адаптивный FTRL15.3.Регуляризация в онлайн-обученииИдея неразложения регуляризаторов в субградиентную оценку-регуляризация регуляризация: проекция на выпуклое множество 15.4.Методы оптимизации в Deep Learning16.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Регуляризация в онлайн-обучении15.3. Регуляризация в онлайн-обученииАвторыАлексей МорозовВ этом параграфе мы поговорим о регуляризации, но использовать мы её будем не для стабилизации обучения, а для того, чтобы накладывать ограничение на получаемое нами решение. Чтобы отличать их от стабилизирующих слагаемых, для таких регуляризаторов будем использовать обозначение ψt(w)\\\\psi_t(w)ψt\\u200b(w)\\nВ теории от регуляризатора требуется только выпуклость, но на практике широко используются лишь три вида:\\n\\nL1=∣∣w∣∣1L_1 = \\\\vert\\\\vert w\\\\vert\\\\vert_1L1\\u200b=∣∣w∣∣1\\u200b и его собрат L1/2L_{1/2}L1/2\\u200b;\\nL2=∣∣w∣∣22L_2 = \\\\vert\\\\vert w\\\\vert\\\\vert_2^2L2\\u200b=∣∣w∣∣22\\u200b;\\nПроекция на выпуклое множество χ\\\\chiχ:\\n\\nψ(w)=Iχ(w)={∞w∉χ0w∈χ\\\\psi(w) = I_{\\\\chi}(w) = \\\\begin{cases}\\n      \\\\infty & w \\\\not\\\\in \\\\chi \\\\\\\\\\n      0 & w \\\\in \\\\chi\\n   \\\\end{cases}\\nψ(w)=Iχ\\u200b(w)={∞0\\u200bw\\ue020∈χw∈χ\\u200bКлассическим способом введения регуляризации является прибавление к оптимизируемому функционалу:\\nf^t(w)=ft(w)+ψt(w)\\\\hat{f}_t(w) = f_t(w) + \\\\psi_t(w)\\nf^\\u200bt\\u200b(w)=ft\\u200b(w)+ψt\\u200b(w)Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступитьс последующим применением любых методов оптимизации «из коробки». Яркий пример — L2L_2L2\\u200b регуляризация:\\nf^t(w)=ft(w)+12λ2∣∣w∣∣22,\\\\hat{f}_t(w) = f_t(w) + \\\\frac{1}{2\\\\lambda_2}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2,\\nf^\\u200bt\\u200b(w)=ft\\u200b(w)+2λ2\\u200b1\\u200b∣∣w∣∣22\\u200b,которая не портит гладкости функционала.\\nИдея неразложения регуляризаторов в субградиентную оценку\\nВспомним вывод linearized FTRL. В ходе линеаризации мы заменяли все функции f^t(w)\\\\hat{f}_t(w)f^\\u200bt\\u200b(w) на их субградиентную оценку в точке wtw_twt\\u200b. Для регуляризованного функционала f^t(w)=ft(w)+ψt(w)\\\\hat{f}_t(w) = f_t(w) + \\\\psi_t(w)f^\\u200bt\\u200b(w)=ft\\u200b(w)+ψt\\u200b(w) получалась бы такая оценка:\\nf^t(w)≥f^t(wt)+(gt+∂ψt)T(w−wt),\\\\hat{f}_t(w) \\\\geq \\\\hat{f}_t(w_t) + (g_t + \\\\partial\\\\psi_t)^T (w - w_t),\\nf^\\u200bt\\u200b(w)≥f^\\u200bt\\u200b(wt\\u200b)+(gt\\u200b+∂ψt\\u200b)T(w−wt\\u200b),где через ∂ψt\\\\partial\\\\psi_t∂ψt\\u200b мы обозначили для краткости субградиент ψt\\\\psi_tψt\\u200b в точке wtw_twt\\u200b. Теперь субградиентную оценку можно подставить в метод FTRL:\\nwt+1=argmin\\u2061w[(g1:t+∂ψ1:t)Tw+∑s=1t∣∣w−ws∣∣σs2]w_{t+1} = arg\\\\min\\\\limits_w \\\\Big[(g_{1:t} + \\\\partial\\\\psi_{1:t})^Tw + \\\\sum\\\\limits_{s=1}^t\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{\\\\sigma_s}^2\\\\Big]\\nwt+1\\u200b=argwmin\\u200b[(g1:t\\u200b+∂ψ1:t\\u200b)Tw+s=1∑t\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200b]Идея неразложения состоит в следующем: заменим на субградиентную оценку только ft(w)f_t(w)ft\\u200b(w), а регуляризатор будем подбирать так, чтобы задача FTRL решалась аналитически. Интуитивно, оценка\\nf^t(w)=ft(w)+ψt(w)≥ft(wt)+gtT(w−wt)+ψt(w)\\\\hat{f}_t(w) = f_t(w) + \\\\psi_t(w) \\\\geq f_t(w_t) + g_t^T(w - w_t) + \\\\psi_t(w)\\nf^\\u200bt\\u200b(w)=ft\\u200b(w)+ψt\\u200b(w)≥ft\\u200b(wt\\u200b)+gtT\\u200b(w−wt\\u200b)+ψt\\u200b(w)должна быть точнее оценки\\nf^t(w)=ft(w)+ψt(w)≥ft(wt)+ψt(wt)+(gt+∂ψt)T(w−wt)\\\\hat{f}_t(w) = f_t(w) + \\\\psi_t(w) \\\\geq f_t(w_t) + \\\\psi_t(w_t) + (g_t + \\\\partial\\\\psi_t)^T (w - w_t)\\nf^\\u200bt\\u200b(w)=ft\\u200b(w)+ψt\\u200b(w)≥ft\\u200b(wt\\u200b)+ψt\\u200b(wt\\u200b)+(gt\\u200b+∂ψt\\u200b)T(w−wt\\u200b)а значит, и метод оптимизации будет точнее и эффективнее.\\nЭта идея очень важна для построения регуляризованных алгоритмов онлайн-обучения.\\nДавайте выпишем, как будут выглядеть с учётом этой идеи регуляризованные алгоритмы.\\nComposite Objective FTRL\\nwt+1=argmin\\u2061w[g1:tTw+ψ1:t(w)+∑s=1t∣∣w−ws∣∣σs2]w_{t+1} = arg\\\\min\\\\limits_w \\\\Big[g_{1:t}^Tw + \\\\psi_{1:t}(w) + \\\\sum\\\\limits_{s=1}^t\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{\\\\sigma_s}^2\\\\Big]\\nwt+1\\u200b=argwmin\\u200b[g1:tT\\u200bw+ψ1:t\\u200b(w)+s=1∑t\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200b]Online Mirror Descent, Proximal Gradient Descent, (F)ISTA\\nwt+1=argmin\\u2061w[gtTw+ψt(w)+∣∣w−wt∣∣σt2]w_{t+1} = arg\\\\min\\\\limits_w \\\\Big[g_t^Tw + \\\\psi_t(w) + \\\\vert\\\\vert w - w_t\\\\vert\\\\vert_{\\\\sigma_t}^2 \\\\Big]\\nwt+1\\u200b=argwmin\\u200b[gtT\\u200bw+ψt\\u200b(w)+∣∣w−wt\\u200b∣∣σt\\u200b2\\u200b]Напомним, что три названия в заголовке соответствуют трём способам восприятия этой формулы:\\n\\nOnline Mirror Descent — метод онлайн-обучения;\\nProximal Gradient Descent — метод (стохастической) батч-оптимизации. В стохастическом случае он неотличим от Mirror Descent;\\n(F)ISTA — по сути, это название аналитического решения указанного уравнения для L1L_1L1\\u200b-регуляризации.\\n\\nСвязь между Composite-Objective FTRL и Proximal Gradient Descent. Lazy vs Greedy представления\\nВ этом подразделе мы будем проводить рассуждения на примере L1L_1L1\\u200b-регуляритора.  для других регуляризаторов выкладки будут аналогичными.\\nВыпишем Proximal (он же Mirror) Gradient Descent с L1L_1L1\\u200b-регуляризацией:\\nwt+1=argmin\\u2061wgtTw+λ1∣∣w∣∣1+12ηt∣∣w−wt∣∣22w_{t+1} = arg\\\\min\\\\limits_w g_t^Tw + \\\\lambda_1\\\\vert\\\\vert w\\\\vert\\\\vert_1 + \\\\frac{1}{2\\\\eta_t}\\\\vert\\\\vert w - w_t\\\\vert\\\\vert_2^2\\nwt+1\\u200b=argwmin\\u200bgtT\\u200bw+λ1\\u200b∣∣w∣∣1\\u200b+2ηt\\u200b1\\u200b∣∣w−wt\\u200b∣∣22\\u200bНеобходимым условием минимума явняется равенство нулю градиента (а в данном случае субградиента) всего выражения:\\n0=gt+g^t+1ηt(wt+1−wt)0 = g_t + \\\\hat{g}_t + \\\\frac{1}{\\\\eta_t}(w_{t+1} - w_t)\\n0=gt\\u200b+g^\\u200bt\\u200b+ηt\\u200b1\\u200b(wt+1\\u200b−wt\\u200b)где g^t\\\\hat{g}_tg^\\u200bt\\u200b - субградиент регуляризатора λ1∣∣w∣∣1\\\\lambda_1\\\\vert\\\\vert w\\\\vert\\\\vert_1λ1\\u200b∣∣w∣∣1\\u200b в точке wtw_{t}wt\\u200b. Отсюда получаем\\nwt+1=wt−ηt(gt+g^T)w_{t+1} = w_{t} - \\\\eta_t (g_t + \\\\hat{g}^T)\\nwt+1\\u200b=wt\\u200b−ηt\\u200b(gt\\u200b+g^\\u200bT)Если же переписать формулы в духе FTRL, мы получим\\nwt+1=argmin\\u2061wg1:tTw+g^1:t−1Tw+λ∣∣w∣∣1+12∑s=0t∣∣w−ws∣∣σs2w_{t+1} = arg\\\\min\\\\limits_{w} g_{1:t}^Tw + \\\\hat{g}_{1:{t-1}}^Tw + \\\\lambda \\\\vert\\\\vert w\\\\vert\\\\vert_1 + \\\\frac{1}{2}\\\\sum\\\\limits_{s=0}^t\\\\vert\\\\vert w-w_s\\\\vert\\\\vert_{\\\\sigma_s}^2\\nwt+1\\u200b=argwmin\\u200bg1:tT\\u200bw+g^\\u200b1:t−1T\\u200bw+λ∣∣w∣∣1\\u200b+21\\u200bs=0∑t\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200bПолучился метод, который оптимизирует L1L_1L1\\u200b-регуляризатор в явном виде только на текущей итерации ttt, а для остальных использует субоптимальные субградиентные оценки. Заметим, что тем же выражением можно ограничить сверху и функционал:\\nwt+1=argmin\\u2061wg1:tTw+tλ∣∣w∣∣1+12∑s=0t∣∣w−ws∣∣σs2w_{t+1} = arg\\\\min\\\\limits_{w} g_{1:t}^Tw + t\\\\lambda \\\\vert\\\\vert w\\\\vert\\\\vert_1 + \\\\frac{1}{2}\\\\sum\\\\limits_{s=0}^t\\\\vert\\\\vert w-w_s\\\\vert\\\\vert_{\\\\sigma_s}^2\\nwt+1\\u200b=argwmin\\u200bg1:tT\\u200bw+tλ∣∣w∣∣1\\u200b+21\\u200bs=0∑t\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200bМы получили метод FTRL с incremental L1L_1L1\\u200b — более сильным и стабильным вариантом регуляризации, чем Mirror Descent. Подробнее его анализом мы займемся в параграфе про продвинутую L1L_1L1\\u200b-регуляризацию.\\nL1L_1L1\\u200b-регуляризация\\nОтбор параметров разреженных моделей\\nПредположим, что мы хотим обучить модель минимального размера и при этом как можно лучшего качества. В этом нам поможет отбор параметров. А именно, давайте постараемся оставить только те из них, которые оказывают наиболее влияние на лосс f1:T(w)f_{1:T}(w)f1:T\\u200b(w).\\nОпределение. Будем называть параметр wiw_iwi\\u200b разреженным, если он не используется (пропускается) при предсказании некоторых ft(w)f_t(w)ft\\u200b(w). «Некоторых» может означать как десятую часть, так и 0.999990.999990.99999 прогнозов ft(w)f_t(w)ft\\u200b(w), главное — что такие объекты просто есть. Частым мы будем называть параметр, у которого частота пропусков низкая (например, 10%10\\\\%10% пропусков), а редким — тот, у которого она высокая (второй случай).\\nПример. Рассмотрим модель разреженной линейной регрессии ft(w)=(wTxt−yt)2f_t(w) = (w^Tx_t - y_t)^2ft\\u200b(w)=(wTxt\\u200b−yt\\u200b)2. Обычно она применяется в ситуациях, когда элементы вектора признаков xt,ix_{t,i}xt,i\\u200b — это 000 или 111 (например, «встретилось ли iii-е слово в ttt-м документе»), причем на практике доля единиц обычно бывает очень маленькой. Поэтому существенная часть параметров wiw_iwi\\u200b при прогнозе на шаге ttt будет умножаться на нули и, таким образом, не будет использоваться.\\nОбратите внимание: как правило, в литературе по онлайн-обучению говорят о разреженных параметрах, а не признаках. Впрочем, подавляющее большинство моделей на разреженных признаках устроены так, что каждому такому признаку сопоставляется некий набор параметров, поэтому определения «разреженный признак» и «разреженные параметры» взаимозаменяемы. В линейной модели, как в примере выше, каждому признаку xix_ixi\\u200b сопоставляется параметр wiw_iwi\\u200b. В более сложных моделях признаку xix_ixi\\u200b может сопоставляться вектор параметров wiw_iwi\\u200b — эмбеддинг этого признака.\\nДавайте теперь поймём, что означает фраза «признак влияет на лосс f1:T(w)f_{1:T}(w)f1:T\\u200b(w)». Оказывать влияние можно двумя способами:\\n\\nКачеством. Если параметр wiw_iwi\\u200b редкий, но очень хорошо прогнозирует свой небольшой набор объектов, его стоит оставить. За счет того, что мы оставим достаточное количество таких параметров, мы можем покрыть большое число объектов. Такие параметры называются memorization parameters (они как будто запоминают «свои» объекты).\\nКоличеством. Если параметр wiw_iwi\\u200b часто встречается, то он в любом случае должен остаться в модели и помогать с суммарным качеством прогноза.\\n\\nУбирать мы хотим только слабые и редкие параметры. Таких, как правило, больше 99%99\\\\%99%.\\nОбратите внимание: мы не хотим убирать слабые, но часто встречающиеся параметры. Тому есть две причины:\\n\\nМеста они много не занимают, а количества данных в large scale задачах достаточно, чтобы правильно выучить эти параметры. Они будут вносить свой, пусть и небольшой, вклад в общее качество;\\nЧастые параметры хорошо запоминают среднее поведение на всех данных, а разреженные — поведение на конкретных объектах. Если наша цель — оставить как можно меньше параметров, то выгоднее хорошо выучить среднее поведение на всех данных, а отклонения от среднего запомнить с помощью memorization parameters. Если в модели есть только супер-разреженные параметры, то из-за огромной вариативности в их возможных комбинациях в данных каждому параметру придется доучивать среднее поведение. Подробнее на этой проблеме мы остановимся в конце параграфа.\\n\\nИнициализация разреженных параметров\\nВ обучении разреженных моделей все параметры, на которые накладывается L1L_1L1\\u200b-регуляризация, инициализируются нулями. С точки зрения здравого смысла такая инициализация довольно естественна, однако есть и более формальное обоснование;\\n\\nЕсли параметры инициализируются нулями, то мы по мере обучения смотрим на градиенты этих параметров и в зависимости от градиентов принимаем решение, нужен нам параметр для прогноза или не нужен. Все параметры стартуют в равных условиях, и модель понемногу выходит из состояния «абсолютная разреженность», выучивая что-то содержательное.\\nЕсли же параметры инициализируются случайно, то нам надо сначала доучить все параметры до какого-то более или менее разумного значения, а потом уже пытаться понять, нужен ли он нам. Момент, когда модель начинает эффективно разреживаться, тем самым очень сильно отдалается.\\n\\nComposite-objective FTRL с L1L_1L1\\u200b-регуляризацией\\nНапомним формулировку задачи:\\nwt+1=argmin\\u2061wg1:tTw+λ1,t∣∣w∣∣1+12∑s=1T∣∣w−ws∣∣σs2w_{t+1} = arg\\\\min\\\\limits_w g_{1:t}^Tw + \\\\lambda_{1,t}\\\\vert\\\\vert w\\\\vert\\\\vert_1 + \\\\frac{1}{2}\\\\sum\\\\limits_{s=1}^T\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{\\\\sigma_s}^2\\nwt+1\\u200b=argwmin\\u200bg1:tT\\u200bw+λ1,t\\u200b∣∣w∣∣1\\u200b+21\\u200bs=1∑T\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200bРешение можно выписать в явном виде. Для этого введём следующие обозначения:\\n\\nztz_tzt\\u200b будет аккумулировать сумму градиентов, z0=0z_0 = 0z0\\u200b=0,\\nntn_tnt\\u200b будет аккумулировать сумму поэлементных квадратов градиентов, n0=0n_0 = 0n0\\u200b=0,\\nα\\\\alphaα — это learning rate.\\n\\nСледующие формулы выписаны отдельно для каждой координаты. В них iii — индекс параметра модели, ttt — номер итерации.\\nσt,i=1ηt,i−1ηt−1,i=1α(nt,i+gt,i2−nt,i)\\\\sigma_{t,i} = \\\\frac{1}{\\\\eta_{t,i}} - \\\\frac{1}{\\\\eta_{t-1,i}} = \\\\frac{1}{\\\\alpha}\\\\Big(\\\\sqrt{n_{t,i} + g_{t,i}^2} - \\\\sqrt{n_{t,i}}\\\\Big)\\nσt,i\\u200b=ηt,i\\u200b1\\u200b−ηt−1,i\\u200b1\\u200b=α1\\u200b(nt,i\\u200b+gt,i2\\u200b\\u200b−nt,i\\u200b\\u200b)zt+1,i=zt,i+gt,i−σt,iwt,iz_{t+1,i} = z_{t,i}  + g_{t,i} - \\\\sigma_{t,i}w_{t,i}\\nzt+1,i\\u200b=zt,i\\u200b+gt,i\\u200b−σt,i\\u200bwt,i\\u200bnt+1,i=nt,i+gt,i2n_{t+1,i} = n_{t,i} + g_{t,i}^2\\nnt+1,i\\u200b=nt,i\\u200b+gt,i2\\u200bwt+1,i={0∣zt+1,i∣≤λ1,t−αnt+1,i+αλ2,t(zt+1,i−sgn(zt+1,i)λ1,t)∣zt+1,i∣>λ1,t(∗)w_{t+1,i} = \\\\begin{cases}\\n      0 & \\\\vert z_{t+1,i}\\\\vert  \\\\leq \\\\lambda_{1,t}\\\\\\\\\\n      -\\\\frac{\\\\alpha}{\\\\sqrt{n_{t+1,i}} + \\\\alpha\\\\lambda_{2,t}} \\\\Big(z_{t+1,i} - sgn(z_{t+1,i})\\\\lambda_{1,t} \\\\Big) & \\\\vert z_{t+1,i}\\\\vert  > \\\\lambda_{1,t}\\n\\\\end{cases}\\\\qquad(\\\\ast)wt+1,i\\u200b={0−nt+1,i\\u200b\\u200b+αλ2,t\\u200bα\\u200b(zt+1,i\\u200b−sgn(zt+1,i\\u200b)λ1,t\\u200b)\\u200b∣zt+1,i\\u200b∣≤λ1,t\\u200b∣zt+1,i\\u200b∣>λ1,t\\u200b\\u200b(∗)Вывод этих формул хорошо расписан в конспекте курса Д. А. Кропотова.\\nАнализ аналитического решения\\nПри регуляризаторе ∣∣w∣∣1\\\\vert\\\\vert w\\\\vert\\\\vert_1∣∣w∣∣1\\u200b в оптимизируемом функционале стоят коэффициенты λ1,t\\\\lambda_{1,t}λ1,t\\u200b, которые могут как-то зависеть от ttt. Обычно рассматривают три вида зависимости:\\n\\nFixed: λ1,t=λ\\\\lambda_{1,t} = \\\\lambdaλ1,t\\u200b=λ.\\nSquared incremental: λ1,t=tλ\\\\lambda_{1,t} = \\\\sqrt{t}\\\\lambdaλ1,t\\u200b=t\\u200bλ\\nLinear incremental: λ1,t=tλ\\\\lambda_{1,t} = t\\\\lambdaλ1,t\\u200b=tλ\\n\\nИх также можно комбинировать, получая коэффициенты регуляризации\\nλ1,t=λ1,global+tλ1,sqrt+tλ1,incremental\\\\lambda_{1,t} = \\\\lambda_{1,global} + \\\\sqrt{t}\\\\lambda_{1,sqrt} + t\\\\lambda_{1,incremental}\\nλ1,t\\u200b=λ1,global\\u200b+t\\u200bλ1,sqrt\\u200b+tλ1,incremental\\u200bНапомним, что все веса wiw_{i}wi\\u200b мы инициализируем нулями. По формулам (∗)(\\\\ast)(∗) из нуля на шаге ttt выводятся веса wiw_iwi\\u200b, для которых\\n∣g1:t,i−∑σsws,i∣>λ1,t.\\\\vert g_{1:t,i} - \\\\sum\\\\limits\\\\sigma_sw_{s,i}\\\\vert  > \\\\lambda_{1,t}.\\n∣g1:t,i\\u200b−∑σs\\u200bws,i\\u200b∣>λ1,t\\u200b.Таким образом, начальное условие выхода параметров из нуля имеет вид\\n∣g1:t,i∣>λ1,t.\\\\vert g_{1:t,i}\\\\vert  > \\\\lambda_{1,t}.\\n∣g1:t,i\\u200b∣>λ1,t\\u200b.Попробуем понять физический смысл этого неравенства.\\nНапоминание. Говорят, что функция f(w)f(w)f(w) имеет липшиц-непрерывный градиент с константой LLL, если\\n∣∣∇f(x)−∇f(y)∣∣22≤L2∣∣x−y∣∣22\\\\vert\\\\vert \\\\nabla f(x) - \\\\nabla f(y)\\\\vert\\\\vert_2^2 \\\\leq \\\\frac{L}{2} \\\\vert\\\\vert x - y\\\\vert\\\\vert_2^2\\n∣∣∇f(x)−∇f(y)∣∣22\\u200b≤2L\\u200b∣∣x−y∣∣22\\u200bПредположим, что это выполняется (ниже мы покажем, что это не слишком обременительное ограничение). Тогда, подставив в качестве yyy точку оптимума функции ft(w)f_t(w)ft\\u200b(w) (не путайте с глобальным wT∗w_T^*wT∗\\u200b из regret!), мы получим\\n∣∣∇f(x)∣∣22≤L2∣∣x−x∗∣∣22\\\\vert\\\\vert \\\\nabla f(x)\\\\vert\\\\vert_2^2 \\\\leq \\\\frac{L}{2}\\\\vert\\\\vert x - x_*\\\\vert\\\\vert_2^2\\n∣∣∇f(x)∣∣22\\u200b≤2L\\u200b∣∣x−x∗\\u200b∣∣22\\u200bЭто означает, что для достаточно хорошей функции норма градиента является оценкой снизу на расстояние до точки оптимума в пространстве параметров. Чем больше норма градиента, тем дальше мы от оптимальных параметров www.\\nВернемся к выражению ∣g1:t,i∣>λ1,t\\\\vert g_{1:t,i}\\\\vert  > \\\\lambda_{1,t}∣g1:t,i\\u200b∣>λ1,t\\u200b. Здесь мы имеем дело (а) отдельно с каждой из координат и (б) с нормой суммы градиентов (а не с суммой норм). Хорошая новость: утверждение выше верно и для функций одной переменной, то есть ∣gs,i∣\\\\vert g_{s,i}\\\\vert∣gs,i\\u200b∣, грубо говоря, показывает, насколько мы далеки от оптимума по iii-й координате. Знак gs,ig_{s,i}gs,i\\u200b говорит о том, в какую сторону мы будем сдвигаться по iii-й координате www на sss-м шаге. Если сдвиги были в основном в одну сторону, то g1:t,ig_{1:t,i}g1:t,i\\u200b будет больше, а если они всё время в разную сторону, то отдельные слагаемые могут скомпенсировать друг друга, и g1:t,ig_{1:t,i}g1:t,i\\u200b может быть малым.\\nОтметим ещё, что абсолютная величина компоненты g1:t,ig_{1:t,i}g1:t,i\\u200b на первых итерациях может отражать прогнозирующую силу параметра wiw_iwi\\u200b: в самом деле, неверное значение важного для предсказания параметра может вести к большим ошибкам, что будет давать большие градиенты.\\nПосмотрим теперь, как будет вести себя разреженная модель в зависимости от вида λ1,t\\\\lambda_{1,t}λ1,t\\u200b.\\nLinear incremental (λ1,t=tλ1\\\\lambda_{1,t} = t\\\\lambda_1λ1,t\\u200b=tλ1\\u200b)\\nУсловие выхода wiw_iwi\\u200b из нуля принимает вид\\n∣g1:t,i∣>tλ1,\\\\vert g_{1:t,i}\\\\vert  > t\\\\lambda_1,\\n∣g1:t,i\\u200b∣>tλ1\\u200b,что равносильно\\n∣1tg1:t,i∣>λ1\\\\left| \\\\frac{1}{t}g_{1:t,i}\\\\right|  > \\\\lambda_1\\n\\u200bt1\\u200bg1:t,i\\u200b\\u200b>λ1\\u200bОграничение на среднее значение компоненты градиента означает, что для выхода из нуля параметр wiw_iwi\\u200b должен иметь определённую прогнозирующую силу. Это противоречит нашему требованию о том, чтобы частые маломощные параметры все равно присутствовали в модели и выучивали среднее поведение.\\nОбратите внимание. Выше мы показали, что проксимальный градиентный спуск с обычным L1L_1L1\\u200b\\nwt+1=argmin\\u2061wgtTw+λ1∣∣w∣∣1+1ηt∣∣w−wt∣∣22w_{t+1} = arg\\\\min\\\\limits_w g_t^Tw + \\\\lambda_1 \\\\vert\\\\vert w\\\\vert\\\\vert_1 + \\\\frac{1}{\\\\eta_t}\\\\vert\\\\vert w - w_t\\\\vert\\\\vert_2^2\\nwt+1\\u200b=argwmin\\u200bgtT\\u200bw+λ1\\u200b∣∣w∣∣1\\u200b+ηt\\u200b1\\u200b∣∣w−wt\\u200b∣∣22\\u200bв некотором смысле эквивалентен Composite-Objective FTRL с инкрементальным L1L_1L1\\u200b. Таким образом, обычная L1L_1L1\\u200b-регуляризация в классическом градиентном спуске эквивалентна именно инкрементальному L1L_1L1\\u200b, который, как мы выяснили, субоптимален. Ниже мы рассмотрим специфический для FTRL вариант L1L_1L1\\u200b-регуляризации, который лишен этих недостатков.\\nФиксированный (λ1,t=tλ1\\\\lambda_{1,t} = t\\\\lambda_1λ1,t\\u200b=tλ1\\u200b)\\nЭто самый мощный и полезный на практике режим.\\nЗдесь мы не нормируем на 1t\\\\frac{1}{t}t1\\u200b (то есть не берём среднее), и это означает, что выйти из нуля может и слабый, но частый параметр, который за много итераций накопит достаточно большую сумму частных производных.\\nСвойства фильтрации с фиксированным регуляризатором в точности совпадают с продуктовыми требованиями:\\n\\nРедкий параметр с мощной прогнозирующей силой на старте будет иметь большие по модулю градиенты одного знака, и он выйдет из нуля;\\nРедкий параметр с малой прогнозной силой не выйдет из нуля;\\nЧастые параметры в любом случае выйдут из нуля.\\n\\nSquared incremental: (λ1,t=tλ\\\\lambda_{1,t} = \\\\sqrt{t}\\\\lambdaλ1,t\\u200b=t\\u200bλ)\\nВ этой статье было теоретически обосновано, что если параметр частый, но нерелевантный и абсолютно шумный, то дисперсия ∣g1:t∣\\\\vert g_{1:t}\\\\vert∣g1:t\\u200b∣ будет иметь асимптотику O(t)O(\\\\sqrt{t})O(t\\u200b). Из этого следует, что, если сделать регуляризацию порядка t\\\\sqrt{t}t\\u200b, мы лишим такой случайный шум почти любых шансов выйти из нуля.\\nК сожалению, ни в игрушечных примерах вроде Avazu, ни в продакшен задачах улучшений качества прогноза или степени разреживания модели без потери качества достичь не удалось. Возможно, вам повезет больше.\\nПолезность частых параметров для разреживания модели\\nРассмотрим две линейных модели\\nft(w)=wTxt+b,gt(w)=wTxt,f_t(w) = w^Tx_t + b,\\\\quad g_t(w) = w^Tx_t,\\nft\\u200b(w)=wTxt\\u200b+b,gt\\u200b(w)=wTxt\\u200b,в которых все параметры wiw_iwi\\u200b разреженные. Давайте считать, что в первой модели есть константный (и совсем даже не разреженный) признак xb=1x_b = 1xb\\u200b=1, которому и соответствует параметр bbb.\\nТеперь в каждой из моделей наложим на www регуляризацию L1L_1L1\\u200b и сравним, что получится:\\n\\nВ модели ftf_tft\\u200b параметрам wiw_iwi\\u200b нужно запомнить «отклонение» от среднего bbb;\\nВ модели gtg_tgt\\u200b параметрам wiw_iwi\\u200b нужно запомнить абсолютное значение предсказания.\\n\\nНетрудно понятно, что при наличии bias нормы градиентов в первой модели в среднем будут намного меньше, потому что мы на каждом шаге оптимизации будем стартовать с точки, которая в среднем ближе к точке оптимума (bias и есть наше среднее). Поэтому меньше весов смогут преодолеть порог по модулю суммы градиентов и выйти из нуля. Таким образом, несмотря на одинаковый оптимум без регуляризации, при введении L1L_1L1\\u200b-регуляризации модель с bias будет обладать более хорошим соотношением разреженность/качество прогноза.\\nЭта логика легко обобщается на более сложные случаи, когда вместо bias у нас есть неразреженные контентные признаки. Вывод такой: модели, в которых есть только очень разреженные параметры, обладают гораздо худшим соотношением разреженность/качество, чем модели, в которых есть и контентные, и разреженные параметры.\\nУбедиться в этих эффектах мы сможем в разделе с практикой на линейных моделях.\\nL2L_2L2\\u200b регуляризация\\nWeight decay\\nРассмотрим обыкновенный SGD.\\nwt+1=wt−αgtw_{t+1} = w_t - \\\\alpha g_t\\nwt+1\\u200b=wt\\u200b−αgt\\u200bWeight decay состоит во введение штрафа на размер текущих весов:\\nwt+1=(1−λ)wt−αgt,0≤λ<1w_{t+1} = (1 - \\\\lambda)w_t - \\\\alpha g_t,\\\\quad 0 \\\\leq \\\\lambda < 1\\nwt+1\\u200b=(1−λ)wt\\u200b−αgt\\u200b,0≤λ<1Внимательные читатели уже заметили, что в случае с SGD это эквивалентно введению L2L_2L2\\u200b-регуляризации. Давайте разберёмся, как это сделать правильно.\\nDecoupled weight decay\\nПопробуем заменить ft(w)f_t(w)ft\\u200b(w) на\\nf^t(w)=ft(w)+λ2∣∣w∣∣22\\\\hat{f}_t(w) = f_t(w) + \\\\lambda_2 \\\\vert\\\\vert w\\\\vert\\\\vert_2^2\\nf^\\u200bt\\u200b(w)=ft\\u200b(w)+λ2\\u200b∣∣w∣∣22\\u200bи запустить любой адаптивный метод, например, AdaGrad. Если мы беспечно заменим на градиентную оценку всю функцию f^t(w)\\\\hat{f}_t(w)f^\\u200bt\\u200b(w) (забыв, что с регуляризатором этого делать не стоит), то алгоритм примет вид\\nwt+1=wt−η^tgt,w_{t+1} = w_t - \\\\hat{\\\\eta}_tg_t,\\nwt+1\\u200b=wt\\u200b−η^\\u200bt\\u200bgt\\u200b,где\\nη^t=α∑s=1t(gs+λ2ws)2\\\\hat{\\\\eta}_t = \\\\frac{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{s=1}^t (g_s + \\\\lambda_2 w_s)^2 }}\\nη^\\u200bt\\u200b=s=1∑t\\u200b(gs\\u200b+λ2\\u200bws\\u200b)2\\u200bα\\u200bВ этих формулах нехороши две вещи:\\n\\nКоэффициенты α\\\\alphaα и λ2\\\\lambda_2λ2\\u200b нетривиальным образом взаимодействуют. Это крайне неудобно при переборе гиперпараметров: изменение learning rate α\\\\alphaα должно влечь за собой переподбор коэффициента регуляризации λ2\\\\lambda_2λ2\\u200b по полной сетке;\\nВ квадратах градиентов мы хотим видеть только адаптивность к кривизне самой функции ftf_tft\\u200b, но теперь там ещё добавка λ2ws\\\\lambda_2w_sλ2\\u200bws\\u200b.\\n\\nЭта проблема была впервые замечена в Decoupled weight decay regularization. Авторы также рассматривали влияние на momentum, к этому мы вернёмся в параграфе про AdamW.\\nАвторы статьи предлагают модифицировать метод AdaGrad следующим образом:\\nwt+1=wt−α∑s=1tgs2gt−λ2wtw_{t+1} = w_t - \\\\frac{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{s=1}^t g_s^2}} g_t - \\\\lambda_2 w_t\\nwt+1\\u200b=wt\\u200b−s=1∑t\\u200bgs2\\u200b\\u200bα\\u200bgt\\u200b−λ2\\u200bwt\\u200bСразу отметим сходство с исходными формулами weight decay — его и добивались авторы.\\nDecoupled weight decay — это адаптивный L2L_2L2\\u200b\\nЛегко видеть, что формула\\nwt+1=wt−α∑s=1tgs2gt−λ2wtw_{t+1} = w_t - \\\\frac{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{s=1}^t g_s^2}} g_t - \\\\lambda_2 w_t\\nwt+1\\u200b=wt\\u200b−s=1∑t\\u200bgs2\\u200b\\u200bα\\u200bgt\\u200b−λ2\\u200bwt\\u200bописывает обыкновенный покоординатный градиентный спуск с некоторым линеаризованным L2L_2L2\\u200b-регуляризатором. Давайте «проинтегрируем» это выражение обратно до аргминимума, из которого бы получились такие формулы обновления весов:\\nwt+1=argmin\\u2061w[gtTw+λ2ηtwtTw+12ηt∣∣w−wt∣∣22]w_{t+1} = arg\\\\min\\\\limits_w \\\\Big[ g_t^Tw + \\\\frac{\\\\lambda_2}{\\\\eta_t}\\\\color{#E06A27}{w_t^T}w + \\\\frac{1}{2\\\\eta_t}\\\\vert\\\\vert w - w_t\\\\vert\\\\vert_2^2 \\\\Big]\\nwt+1\\u200b=argwmin\\u200b[gtT\\u200bw+ηt\\u200bλ2\\u200b\\u200bwtT\\u200bw+2ηt\\u200b1\\u200b∣∣w−wt\\u200b∣∣22\\u200b]Получается, что decoupled weight decay — это адаптивный L2L_2L2\\u200b-centered регуляризатор. Его можно усовершенствовать, вспомним наше важное правило не заменять регуляризатор на субградиентную оценку. Перейдём к задаче\\nwt+1=argmin\\u2061w[gtTw+λ22ηt∣∣w∣∣22+12ηt∣∣w−wt∣∣22]w_{t+1} = arg\\\\min\\\\limits_w \\\\Big[ g_t^Tw + \\\\frac{\\\\lambda_2}{2\\\\eta_t}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2 + \\\\frac{1}{2\\\\eta_t}\\\\vert\\\\vert w - w_t\\\\vert\\\\vert_2^2 \\\\Big]\\nwt+1\\u200b=argwmin\\u200b[gtT\\u200bw+2ηt\\u200bλ2\\u200b\\u200b∣∣w∣∣22\\u200b+2ηt\\u200b1\\u200b∣∣w−wt\\u200b∣∣22\\u200b]Она отличается от предыдущей заменой wtTw\\\\color{#E06A27}{w_t^T}wwtT\\u200bw на wTw=∣∣w∣∣2\\\\color{#348FEA}{w^T}w = \\\\vert\\\\vert w\\\\vert\\\\vert^2wTw=∣∣w∣∣2. Её решение имеет вид\\nwt+1=11+λ2(wt−ηtgt)w_{t+1} = \\\\color{#C81D6B}{\\\\frac{1}{1 + \\\\lambda_2}}\\\\Big(w_t - \\\\eta_t g_t \\\\Big)\\nwt+1\\u200b=1+λ2\\u200b1\\u200b(wt\\u200b−ηt\\u200bgt\\u200b)Поскольку мы меньше огрубляем оптимизируемый функционал, обучение может стать немного стабильнее.\\nОбратите внимание, что в оптимизационной задаче у нас теперь стоит не просто λ2\\\\lambda_2λ2\\u200b, а λ22ηt\\\\frac{\\\\lambda_2}{2\\\\eta_t}2ηt\\u200bλ2\\u200b\\u200b.\\nDecoupled L2L_2L2\\u200b-регуляризация в Composite-Objective FTRL\\nТеперь посмотрим, как decoupled weight decay будте работать с Composite-Objective FTRL. Линеаризованная задача имеет вид:\\nwt+1=argmin\\u2061w[g1:tTw+λ22∣∣w∣∣σ1:s2+12∑s=1t∣∣w−ws∣∣σs2]w_{t+1} = arg\\\\min\\\\limits_w \\\\left[g_{1:t}^Tw + \\\\frac{\\\\lambda_2}{2}\\\\vert\\\\vert w\\\\vert\\\\vert_{\\\\sigma_{1:s}}^2 + \\\\frac{1}{2}\\\\sum\\\\limits_{s=1}^t\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{\\\\sigma_s}^2\\\\right]\\nwt+1\\u200b=argwmin\\u200b[g1:tT\\u200bw+2λ2\\u200b\\u200b∣∣w∣∣σ1:s\\u200b2\\u200b+21\\u200bs=1∑t\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200b]Перепишем её:\\nwt+1=argmin\\u2061w[g1:tTw+12∑s=1t(∣∣w−ws∣∣σs2+λ2∣∣w∣∣σs2)]w_{t+1} = arg\\\\min\\\\limits_w \\\\left[g_{1:t}^Tw + \\\\frac{1}{2}\\\\sum\\\\limits_{s=1}^t\\\\Big(\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{\\\\sigma_s}^2 + \\\\lambda_2\\\\vert\\\\vert w\\\\vert\\\\vert_{\\\\sigma_s}^2 \\\\Big)\\\\right]\\nwt+1\\u200b=argwmin\\u200b[g1:tT\\u200bw+21\\u200bs=1∑t\\u200b(∣∣w−ws\\u200b∣∣σs\\u200b2\\u200b+λ2\\u200b∣∣w∣∣σs\\u200b2\\u200b)]Нетрудно показать, что решение имеет вид\\nzt+1=zt+gt−σt⊙wt,z_{t+1} = z_t + g_t - \\\\sigma_t\\\\odot w_t,\\nzt+1\\u200b=zt\\u200b+gt\\u200b−σt\\u200b⊙wt\\u200b,wt+1=−11+λ21ηtzt.w_{t+1} = -\\\\color{#C81D6B}{\\\\frac{1}{1 + \\\\lambda_2}} \\\\frac{1}{\\\\eta_t}z_t.\\nwt+1\\u200b=−1+λ2\\u200b1\\u200bηt\\u200b1\\u200bzt\\u200b.Для ztz_tzt\\u200b можно написать и явную формулу: zt=g1:t−∑s=1tσs⊙wsz_{t} = g_{1:t} - \\\\sum_{s=1}^t\\\\sigma_s\\\\odot w_szt\\u200b=g1:t\\u200b−∑s=1t\\u200bσs\\u200b⊙ws\\u200b.\\nЗамечание. Чтобы оценить Regret такого метода, мы не сможем механически воспользоваться оценкой для AdaGrad: ведь она базированась на оценке на Regret, выведенной либо для целиком Proximal, либо для целиком Centered L2L_2L2\\u200b-регуляризаторов. Composite objective из теоремы 10 тут не годится, так как Centered регуляризатор в этом случае не поедет в оценку норм градиентов, а мы в текущем представлении рассматриваем Proximal и Centered как равноправные члены. Интуитивно, мы должны применить Lemma 7 к обоим регуляризаторам и получить точно такую же оценку с такой же двойственной нормой (напомним, что centered и proximal регуляризаторы имеют одинаковую двойственную норму). Двойственная норма такая же -> формулы оптимального метода AdaGrad будут такие же. Мы оставляем это читателям в качестве упражнения.\\nIχ(w)I_{\\\\chi}(w)Iχ\\u200b(w): проекция на выпуклое множество χ\\\\chiχ\\nНапоминание: множество χ\\\\chiχ называется выпуклым, если\\n∀x,y∈χ,\\xa0∀α∈[0;1]:αx+(1−α)y∈χ\\\\forall x,y\\\\in \\\\chi,\\\\ \\\\forall\\\\alpha \\\\in [0; 1]: \\\\quad \\\\alpha x + (1-\\\\alpha)y \\\\in \\\\chi\\n∀x,y∈χ,\\xa0∀α∈[0;1]:αx+(1−α)y∈χПроекцией на это множество называют функцию\\nIχ(w)={∞w∉χ0w∈χI_{\\\\chi}(w) = \\\\begin{cases}\\n      \\\\infty & w \\\\not\\\\in \\\\chi \\\\\\\\\\n      0 & w \\\\in \\\\chi\\n\\\\end{cases}Iχ\\u200b(w)={∞0\\u200bw\\ue020∈χw∈χ\\u200bДокажем, что Iχ(w)I_{\\\\chi}(w)Iχ\\u200b(w) — выпуклый регуляризатор. Для этого нам нужно проверить неравенство\\nαI(x)+(1−α)I(y)≥I(αx+(1−α)y).\\\\alpha I(x) + (1-\\\\alpha)I(y) \\\\geq I(\\\\alpha x + (1 - \\\\alpha) y).\\nαI(x)+(1−α)I(y)≥I(αx+(1−α)y).Единственный шанс, когда это может быть нарушено — это I(αx+(1−α)y)=∞I(\\\\alpha x + (1 - \\\\alpha) y) = \\\\inftyI(αx+(1−α)y)=∞, I(x)=0I(x) = 0I(x)=0, I(y)=0I(y) = 0I(y)=0. Это значит, что x,y∈χx,y\\\\in \\\\chix,y∈χ, а αx+(1−α)y∉χ\\\\alpha x + (1 - \\\\alpha) y \\\\notin \\\\chiαx+(1−α)y∈/χ, что противоречит выпуклости χ\\\\chiχ.\\nВернемся к формулам FTRL. Здесь ситуация сильно проще — от накидывания любых последовательностей α1:T\\\\alpha_{1:T}α1:T\\u200b на регуляризатор ничего не изменится, так что его всегда оставляют просто as is\\nwt+1=argmin\\u2061wg1:tTw+Iχ(w)+12∑s=1T∣∣w−ws∣∣σs2w_{t+1} = arg\\\\min\\\\limits_w g_{1:t}^Tw + I_{\\\\chi}(w) + \\\\frac{1}{2}\\\\sum\\\\limits_{s=1}^T\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{\\\\sigma_s}^2\\nwt+1\\u200b=argwmin\\u200bg1:tT\\u200bw+Iχ\\u200b(w)+21\\u200bs=1∑T\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200bАналитические решения для каждого вида χ\\\\chiχ нужно искать отдельно. Примерно все решения получаются путем выноса Iχ(w)I_{\\\\chi}(w)Iχ\\u200b(w) из оптимизируемого функционала и превращения его в ограничение, после чего можно применить метод множителей Лагранжа.\\nПроекция на шар x:∣∣x∣∣≤c{x: \\\\vert\\\\vert x\\\\vert\\\\vert  \\\\leq c}x:∣∣x∣∣≤c\\nРешим аналитически задачу проекции на шар\\n{g1:tTw+12∑s=1T∣∣w−ws∣∣σs2⟶min\\u2061w,∣∣w∣∣2≤c. \\\\begin{cases}\\ng_{1:t}^Tw + \\\\frac{1}{2}\\\\sum\\\\limits_{s=1}^T\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{\\\\sigma_s}^2\\\\longrightarrow\\\\min_w,\\\\\\\\\\n\\\\vert\\\\vert w\\\\vert\\\\vert_2 \\\\leq c.\\n\\\\end{cases}⎩⎨⎧\\u200bg1:tT\\u200bw+21\\u200bs=1∑T\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200b⟶minw\\u200b,∣∣w∣∣2\\u200b≤c.\\u200bФункция Лагранжа будет иметь вид\\nL(w,λ)=g1:tTw+12∑s=1T∣∣w−ws∣∣σs2+λ(∣∣w∣∣2−c),L(w, \\\\lambda) = g_{1:t}^Tw + \\\\frac{1}{2}\\\\sum\\\\limits_{s=1}^T\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{\\\\sigma_s}^2 + \\\\lambda(\\\\vert\\\\vert w\\\\vert\\\\vert_2 - c),\\nL(w,λ)=g1:tT\\u200bw+21\\u200bs=1∑T\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200b+λ(∣∣w∣∣2\\u200b−c),а её градиент равен\\n∇wL(w,λ)=g1:tT+∑s=1T(w−ws)⊙σs+λw∣∣w∣∣2,\\\\nabla_wL(w, \\\\lambda) = g_{1:t}^T + \\\\sum\\\\limits_{s=1}^T(w - w_s)\\\\odot\\\\sigma_s + \\\\lambda \\\\frac{w}{\\\\vert\\\\vert w\\\\vert\\\\vert_2},\\n∇w\\u200bL(w,λ)=g1:tT\\u200b+s=1∑T\\u200b(w−ws\\u200b)⊙σs\\u200b+λ∣∣w∣∣2\\u200bw\\u200b,где σs\\\\sigma_sσs\\u200b - вектор, а ⊙\\\\odot⊙ — поэлементное умножение векторов. Приравнивая к нулю градиент, получаем\\nzt+σ1:s⊙w+λw∣∣w∣∣2=0,z_t + \\\\sigma_{1:s}\\\\odot w + \\\\lambda \\\\frac{w}{\\\\vert\\\\vert w\\\\vert\\\\vert_2} = 0,\\nzt\\u200b+σ1:s\\u200b⊙w+λ∣∣w∣∣2\\u200bw\\u200b=0,где мы, как обычно, обозначили zt=g1:t−∑s=1tσs⊙wsz_{t} = g_{1:t} - \\\\sum_{s=1}^t\\\\sigma_s\\\\odot w_szt\\u200b=g1:t\\u200b−∑s=1t\\u200bσs\\u200b⊙ws\\u200b.\\nПроанализируем условие дополняющей нежесткости λ(∣∣w∣∣−c)=0\\\\lambda(\\\\vert\\\\vert w\\\\vert\\\\vert -c) = 0λ(∣∣w∣∣−c)=0. Если λ=0\\\\lambda = 0λ=0, то решение www уже находится внутри шара и имеет вид\\nw=−ztσ1:sw = \\\\frac{-z_t}{\\\\sigma_{1:s}}\\nw=σ1:s\\u200b−zt\\u200b\\u200bПри практической реализации мы просто сначала посчитаем это выражение и проверим, не попадаем ли мы в шар. Если попадаем — отлично, если нет — то дальше говорим, что ∣∣w∣∣=c\\\\vert\\\\vert w\\\\vert\\\\vert  = c∣∣w∣∣=c и решаем продолжаем решение\\nzt+σ1:s∗w+λwс=0z_t + \\\\sigma_{1:s}*w + \\\\lambda \\\\frac{w}{с} = 0\\nzt\\u200b+σ1:s\\u200b∗w+λсw\\u200b=0w=−ztσ1:s+λcw = \\\\frac{-z_t}{\\\\sigma_{1:s} + \\\\frac{\\\\lambda}{c}}\\nw=σ1:s\\u200b+cλ\\u200b−zt\\u200b\\u200bТеперь подставим это в ∣∣w∣∣=c\\\\vert\\\\vert w\\\\vert\\\\vert  = c∣∣w∣∣=c и получим\\n∣∣w∣∣=∣∣zt∣∣σ1:s+λc=c\\\\vert\\\\vert w\\\\vert\\\\vert  = \\\\frac{\\\\vert\\\\vert z_t\\\\vert\\\\vert }{\\\\sigma_{1:s} + \\\\frac{\\\\lambda}{c}} = c\\n∣∣w∣∣=σ1:s\\u200b+cλ\\u200b∣∣zt\\u200b∣∣\\u200b=cλ=∣∣zt∣∣−σ1:sc\\\\lambda = \\\\vert\\\\vert z_t\\\\vert\\\\vert  - \\\\sigma_{1:s}c\\nλ=∣∣zt\\u200b∣∣−σ1:s\\u200bcw=c−zt∣∣zt∣∣w = c\\\\frac{-z_t}{\\\\vert\\\\vert z_t\\\\vert\\\\vert }\\nw=c∣∣zt\\u200b∣∣−zt\\u200b\\u200bПолучаем, что если мы находимся внутри шара, то мы действуем согласно обыкновенному adaptive алгоритму со всеми хорошими свойствами, иначе — проекция побеждает.\\nАналогично L1L_1L1\\u200b регуляризации, здесь тоже есть различия между lazy и greedy представлением этого регуляризатора. Однако, в классических DL задачах эти методы встречаются не слишком часто и здесь сложно привести какой-нибудь значимый успех, который мог бы улучшить качество в важной задача. Навскидку мы можем вспомнить разве что Adversatial White-Box learning, в котором можно было бы это попробовать.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф15.2. Адаптивный FTRLСледующий параграф15.4. Методы оптимизации в Deep LearningЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_36.html', 'title': 'Языковые модели'}, page_content='Языковые моделиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/68.1.Введение в генеративное моделирование8.2.Variational Autoencoder (VAE)8.3.Генеративно-состязательные сети (GAN)8.4.Нормализующие потоки8.5.Диффузионные модели8.6.Языковые моделиЧто такое языковые модели?Развитие языковых моделейТрансформерыСовременные подходыПодводкиInstructGPTChatGPTКак обучить свою LLM?Итог9.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Языковые модели8.6. Языковые моделиАвторыЕвгений КосаревВ 2023 году ChatGPT стал самой узнаваемой языковой моделью машинного обучения во всём мире — причём как среди специалистов, так и среди обычных людей.\\nСпособность вести осмысленный диалог, отвечать на практически любые вопросы и быть применимыми без дообучения в большом спектре задач с высоким качеством — вот залог их  популярности.\\nВ этом параграфе мы расскажем, что такое языковые модели, как они устроены, как развивались, а также как изменились за последнее время.\\nЧто такое языковые модели?\\nГоворя простым языком, языковые модели — это алгоритмы, способные продолжать тексты. Если чуть усложнить, то это вероятностные алгоритмы, и к ним сразу можно задать эмпирический критерий качества: хорошая модель даёт разумные продолжения данных ей текстов.\\nВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nДавайте разберём пример выше.\\nМодель высчитывает вероятность возможных продолжений текста и предлагает их нам. Слово «фрукт» — наименее разумное продолжение нашей фразы, в то время как слово «наука» — наиболее разумное. И действительно, это часть определения машинного обучения, которое мы давали в начале этого учебника.\\nТаким образом, нам осталось лишь научить алгоритм моделировать эти вероятности и максимизировать их для разумных предложений. Но как это сделать? По ходу развития языковых моделей подходы менялись, мы расскажем о каждом из них в хронологическом порядке.\\nНачнём с краткого экскурса в историю — поговорим о статистических моделях, рекуррентных нейронных сетях и трансформерах. А затем перейдём к современным — GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT и LLaMa.\\nРазвитие языковых моделей\\nСтатистические модели\\nИдея модели лежит на поверхности, много где применяется в самых разных вариациях даже в ХХ веке, поэтому сложно назвать авторов или точную дату создания. Однако этот метод популярен до сих пор — используется в клавиатурах смартфонов для исправления опечаток и быстрого набора текстов через Т9.\\nТеперь подробнее о методе.\\nНапомним вероятностную формулировку цепей Маркова в общем виде:\\nP(w1,w2,...,wN)=P(wN∣w1,w2,...,wN−1)×P(w1,w2,...,wN−1)=P(wN∣w1,w2,...,wN−1)×P(wN−1∣w1,w2,...,wN−2)×...×P(w2∣w1)×P(w1)P(w_1,w_2, ..., w_N) = P(w_N | w_1,w_2, ..., w_{N - 1}) \\\\times P(w_1,w_2, ..., w_{N - 1}) = P(w_N | w_1,w_2, ..., w_{N - 1}) \\\\times P(w_{N-1} | w_1,w_2, ..., w_{N - 2}) \\\\times ... \\\\times P(w_2| w_1) \\\\times P(w_1)\\nP(w1\\u200b,w2\\u200b,...,wN\\u200b)=P(wN\\u200b∣w1\\u200b,w2\\u200b,...,wN−1\\u200b)×P(w1\\u200b,w2\\u200b,...,wN−1\\u200b)=P(wN\\u200b∣w1\\u200b,w2\\u200b,...,wN−1\\u200b)×P(wN−1\\u200b∣w1\\u200b,w2\\u200b,...,wN−2\\u200b)×...×P(w2\\u200b∣w1\\u200b)×P(w1\\u200b)Если представить, что wiw_iwi\\u200b  — это слово, а набор этих омега — это предложение, то по формуле становится возможным посчитать вероятность предложения w1,w2,...,wNw_1, w_2, ..., w_Nw1\\u200b,w2\\u200b,...,wN\\u200b С практической точки зрения всё чуть сложнее, ведь распределение слов в реальном языке (какое, с какими и как часто встречается), вообще говоря, неизвестно.\\nЕго принято аппроксимировать на основе корпуса текстов (например, всего интернета) — в этом случае считаются совстречаемости слов друг с другом, и по ним считаются вероятности.\\nВ условной вероятности число переменных, от которых зависит распределение следующего слова, называется контекстом. Например, в выражении P(wN∣w1,w2,...,wN−1)P(w_N | w_1,w_2, ..., w_{N - 1})P(wN\\u200b∣w1\\u200b,w2\\u200b,...,wN−1\\u200b) длина контекста равна N−1N - 1N−1. На практике же редко считают вероятности с контекстом больше трёх, на это есть несколько причин:\\n\\nСложность в подсчёте и хранении каждого возможного уникального контекста длины KKK. Если корпус текстов состоит из NNN  различных слов, то стоимость хранения счётчиков встречаемости для выбранной длины контекста равна NKN^KNK, что очень много при больших KKK.\\nБольшой контекст реже встречается. То есть слова «яблоку», «негде» и «упасть» поодиночке встречаются чаще, чем их комбинация «яблоку негде упасть». Отсюда достаточность статистик падает с ростом длины контекста.\\n\\nВ учебном примере предлагается ограничиться шириной контекста размера 1:\\nP(w1,w2,...,wN)=P(wN∣wN−1)×P(wN−1∣wN−2)×...×P(w2∣w1)×P(w1)P(w_1,w_2, ..., w_N) = P(w_N | w_{N - 1}) \\\\times P(w_{N-1} | w_{N - 2}) \\\\times ... \\\\times P(w_2| w_1) \\\\times P(w_1)\\nP(w1\\u200b,w2\\u200b,...,wN\\u200b)=P(wN\\u200b∣wN−1\\u200b)×P(wN−1\\u200b∣wN−2\\u200b)×...×P(w2\\u200b∣w1\\u200b)×P(w1\\u200b)\\nИнтересно, что такой подход достаточно популярен до сих пор. Например, он используется в умных клавиатурах, чтобы подсказать следующее слово.\\nДостоинства статистических моделей:\\n\\nПростота имплементации.\\nВысокая скорость работы алгоритма.\\nНизкая вычислительная стоимость обучения и инференса.\\n\\nНедостатки статистических моделей:\\n\\nНе сможет сгенерировать слова, которые не шли подряд в обучающем корпусе.\\nОчень маленький контекст.\\nДлинные последовательности равновероятны ≈ нулю (в цепях Маркова для длинных последовательностей много множителей меньше нуля, поэтому их произведение уже практически равно нулю для любых множителей). Отсюда алгоритм не может выдавать разумные продолжения большой длины.\\n\\nТокенизация\\nЯзыковые модели, да и вообще все модели, которые оперируют текстом, используют понятие токена. Токен — это единица текста, которую понимают алгоритмы. В примере выше токен\\xa0— это отдельное слово wiw_iwi\\u200b (этот подход называется мешком слов), однако текст можно разбивать на токены и иначе.\\n\\nРаньше предложение разбивалось на слова по пробелам, знакам препинания, исключались стоп-слова и так далее (назовем это CountVectorizer). Но у этого подхода возникали две проблемы с разными словоформами. Они:\\n\\nЛибо обозначались разными токенами,\\xa0что не совсем верно, ведь слово-то одно и то же. И получалось, что похожим смыслом обладало сразу несколько токенов.\\nЛибо приводились к начальной форме — и в итоге терялся падеж, время, число.\\n\\nСовременные токенизаторы построены на алгоритме BPE (Byte Pair Encoding; об устройстве BPE более подробно можно прочитать в\\xa0учебнике Лены Войта). Решение требует фиксации определённого числа токенов. Как только это сделано, в словарь добавляются все символы из текста, ищутся самые частые их сочетания и снова добавляются. Этот процесс продолжается до тех пор, пока число токенов не станет равно заданному значению.\\nТокенизатор SentencePiece в определённом смысле совершеннее, чем BPE, — он наследует логику Unigram- и BPE-токенизаторов, иначе работает с пробелами (добавляет _  перед соответствующим токеном) и не построен на логике разбиения слов по разделителям.\\nПоэтому, в отличие от BPE, он способен работать с такими языками, как японский или китайский. Подробнее о его устройстве можно прочитать здесь.\\n\\n💡 Токенизаторы не разделяют входной поток по значимости. Например, число 12345 BPE могут разбить на два токена — 1 и 2345, что явно не соответствует логике написанного выражения. Также они будут неправильно выделять всё число в отдельный токен, так как чисел бесконечное количество. Сейчас используется идея о разбиении всех чисел на цифры, чтобы множеством из десяти токенов представить всё многообразие чисел.\\n\\nРекуррентные нейронные сети (RNN)\\nПоявились после статистических моделей, подробнее о хронологии здесь. Рекуррентные нейронные сети концептуально можно описать формулой, где:\\nААА — некоторая модель;\\nhth_tht\\u200b  — внутреннее состояние модели на момент времени ttt;\\nxtx_txt\\u200b — токен, который сейчас обрабатывается.\\nТогда следующий токен xt+1x_{t+1}xt+1\\u200b получается так:\\nxt+1=g(ht);\\xa0ht=A(ht−1,xt)x_{t+1} = g(h_{t}); \\\\space h_{t} = A(h_{t-1}, x_t)\\nxt+1\\u200b=g(ht\\u200b);\\xa0ht\\u200b=A(ht−1\\u200b,xt\\u200b)\\n\\n\\nссылка на источник картинки\\n\\n\\nПодробно об устройстве RNN мы рассказываем в параграфе Нейросети для работы с последовательностями. Здесь же коротко отметим, что существуют различные модификации рекуррентных сетей, которые усложняют структуру алгоритма ААА, даже добавляют механизм внимания Attention. Если коротко, то он позволяет лучше оценивать взаимосвязи токенов в тексте. Все они в разной степени помогают модели усваивать более длинные и сложные последовательности токенов.\\nДостоинства RNN:\\n\\nВысокая скорость инференса и сравнительно низкая стоимость.\\nБолее качественный текст, чем у моделей на статистиках.\\nТеоретически понимает контекст в сотни слов (а с Attention ещё больше).\\nТочно учитывает весь контекст документа.\\n\\nНедостатки RNN:\\n\\nНевозможность параллельного обучения на многих устройствах, отсюда не получится просто так обучить большую RNN.\\nМодель «хорошо помнит» лишь несколько последних токенов контекста (без Attention).\\nПроблемы с обучением (exploading/vanishing gradients).\\n\\nТрансформеры\\nБолее подробно трансформеры и их устройство описаны в параграфе Трансформеры. Последней и наиболее успешной с точки зрения качества оказалась архитектура трансформеров. Она состоит из двух частей: encoder (на изображении слева) и decoder (на изображении справа).\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nИзначально был популярен подход обучать части отдельно. Так на базе encoder-блоков были построены BERT-модели. Идея обучения звучит несложно: давайте из входного текста замаскируем токеном MASK 15% имеющихся токенов и обучим модель угадывать, какие именно токены были скрыты. Тогда, если модель обучится это делать, она сможет очень хорошо понимать текст.\\nТаким образом, энкодеры обладают следующими особенностями:\\n\\nАнализируют входной текст и связи между токенами.\\nВыделяют важные токены для определённой задачи.\\nНичего не генерируют.\\n\\nНа базе декодеров сделаны GPT-модели. Они обучаются предсказывать следующий токен на основе предыдущих. На инференсе, когда очередной токен сгенерирован, он добавляется в контекст, и уже на основе него выбирается новый токен. Таким образом модель:\\n\\nгенерирует токен за токеном.\\nсмотрит на весь контекст, архитектурно, нет забывания токенов.\\nимеет возможность (как и BERT-модели) обучаться параллельно.\\nобладает достаточно высокой вычислительной стоимостью инференса.\\n\\n\\n💡 Контекст в случае трансформеров определяется числом токенов, которые они могут обработать за раз. Архитектурно за понимание контекста отвечает блок Attention, и размеры матриц в нём как раз определяют размер контекста. \\nРазмер матриц конечен: чем они больше, тем сложнее вычислять блок внимания, поэтому контекст существенно ограничен. На момент написания параграфа разработаны различные модификации Attention, позволяющие растить понимаемый контекст, однако они имеют ряд проблем, с которыми предлагаем ознакомиться читателю самостоятельно.\\n\\nСовременные подходы\\nGPT-1 & GPT-2\\nНачнём немного издалека, с моделей GPT-1 и GPT-2.\\nПервая была обучена в 2018 году на 7000 книг и имела размер контекста в 512 токенов. И она сразу получилась довольно сильной: после дообучения на специализированные задачи (бенчмарки) показывала на них лучшее на то время качество.\\nТак, в задачах CoLA (бенчмарк классификационный, в нём надо определить грамматическую корректность предложения) результат вырос до 45,4 против прежнего результата в 35,0 у RNN. А в GLUE — с 72,8 до 68,9.\\nВторая модель была обучена в 2019 году. Она состояла из рекордных для того времени 1,5 млрд параметров (то есть была в ~10 раз больше первой), имела контекст в 1024 токена и была обучена на 40 ГБ текстовых данных. GPT-2 снова побеждала предыдущие подходы, включая GPT-1, на многих бенчмарках.\\nПо сравнению с первой версией модели у второй произошел качественный рост: теперь она могла генерировать разумные тексты — а не только предложения. Правда, не всегда и не с первой попытки.\\nGPT-3\\nGPT-3 стала революцией с точки зрения качества и размеров. В 2020 году была получена модель размером в 175 млрд параметров, она обучалась на 570 ГБ текстовых данных с контекстом в 2048 токенов. Модель могла решать целый спектр задач, включая перевод, суммаризацию и ответы на вопросы, с качеством, близким к человеческому уровню, а также отличалась высокой способностью генерировать креативный контент. Демонстрацию работы модели лучше посмотреть в этой статье на 28 странице и далее.\\n\\n\\n\\n\\nМодель\\n\\n\\nЧисло обучающих данных\\n\\n\\nКонтекст\\n\\n\\nЧисло параметров\\n\\n\\nDecoder-слои\\n\\n\\nHidden-size (размерность тензоров внутри модели)\\n\\n\\nTrain batchsize (размер батча при обучении)\\n\\n\\n\\n\\nGPT\\n\\n\\n7000 книг\\n\\n\\n512\\n\\n\\n117 млн\\n\\n\\n12\\n\\n\\n768\\n\\n\\n64\\n\\n\\n\\n\\nGPT-2\\n\\n\\n40 ГБ текстовых данных\\n\\n\\n1024\\n\\n\\n1,5 млрд\\n\\n\\n48\\n\\n\\n1600\\n\\n\\n512\\n\\n\\n\\n\\nGPT-3\\n\\n\\n570 ГБ текстовых данных\\n\\n\\n2048\\n\\n\\n175 млрд\\n\\n\\n96\\n\\n\\n12 288\\n\\n\\n3 200 000\\n\\n\\n\\n\\nМодель демонстрировала действительно впечатляющие результаты: собрав обучающие данные, можно было с высоким качеством решить практически любую текстовую задачу.\\nОднако для применения таких решений остаётся проблема со стоимостью их обучения. Для обучения GPT-2 авторы использовали 16 GPU (иначе говоря —\\xa0графических процессоров, видеокарт), а для GPT-3 уже 3200. Для дообучения модели под определенную задачу, конечно, понадобится меньше ресурсов, но всё равно достаточно много.\\nЧто с этим делать? Использовать подводки.\\nПодводки\\nFew-shot обучение\\nОказывается, что обучать большие языковые модели решать определённые задачи не всегда нужно (как мы говорили ранее, это ресурсоёмко): можно составить few-shot подводку. Подводка — словесное описание поставленной задачи, составленное определенным образом.\\nПредставим, что мы хотим осуществить перевод с английского на французский. Для обучения нам необходимо было бы составить пары (X,y)(X,y)(X,y), где XXX — слово на английском, а yyy — на французском. Сделаем иначе — опишем задание на естественном языке:\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nЗдесь на английском языке сформулировано задание и предлагается слово «cheese» перевести на французский. Назовем такую конструкцию zero-shot-примером. Такой запрос GPT-3, возможно, поймёт, но работать будет плохо.\\nДавайте увеличим количество примеров в подводке и назовем эту конструкцию one-shot:\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nИли больше, и это будет few-shot:\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nПри этом приёме не тратятся ресурсы на обучение модели, она лишь смотрит на контекст и генерирует продолжение. Оказывается, этого достаточно, чтобы сравняться с downstream-обучением. Продемонстрируем преимущество такого подхода на двух бенчмарках.\\n\\nTriviaQA — вопросно-ответный бенчмарк, составленный на основе Википедии. Он помогает оценивать знания модели и ее ответы на вопросы.\\nLambada — оценивает меморизацию длинного контекста модели. Чем выше скор, тем лучше модель на обоих бенчмарках.\\n\\n\\n\\n\\nссылка на источник картинки\\n\\n\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nГрафики выше демонстрируют несколько особенностей:\\n\\nFew-shot позволяет получать качество, сравнимое с дообучением на определённом датасете, и стремится к человеческому качеству.\\nС ростом числа обучаемых параметров модели растет её качество.\\nНа правом графике few-shot-примеры начинают работать лучше zero-shot-примеров лишь с некоторого размера модели. Это говорит о том, что модель начинает демонстрировать «умные» свойства лишь начиная с некоторого размера.\\n\\n\\n💡 На самом деле последний пункт достаточно часто встречается в языковых моделях. Случается так, что определённые приёмы не работают с маленькими моделями, но показывают себя лишь на больших. \\nЭто можно назвать фазовым переходом, когда языковая модель вместе с увеличением размера и числа пройденных текстов на обучении обретает большую обобщающую способность.\\n\\nФормулировка имеет значение\\nFew-shot действительно полезен и помогает получать от модели нужный результат без обучения, но всё же недостаточно хорошо.\\nПредположим, мы хотим узнать у модели, как приготовить любимое блюдо. Пусть это будет лазанья:\\n\\nМожно заметить, что запрос к модели можно задать по-разному, но ответ ожидается обычно какой-то конкретный. Авторы этой статьи заметили, что сама по себе конструкция few-shot-примера не приводит к стабильному результату. Качество решения задачи очень зависит от:\\n\\nТекстового описания задачи.\\nЧисла примеров в подводке.\\nПорядка, в котором примеры следуют друг за другом в подводке.\\nФормате составления few-shot.\\n\\nЧтобы улучшить качество решения задачи, авторы предлагают осуществлять калибровку подводок. В статье они заметили, что модели смещены относительно подводок, то есть переформулировка запроса ведёт к смещению в ответе модели, а также к росту разброса ответов.\\nНапример, модели задают вопрос и её задача — ответить «да» или «нет». Если few-shot состоит из четырёх примеров и они идут в порядке «да», «да», «нет»,  «нет», то, вероятнее всего, дальше модель ответит «нет» на любой вход, просто потому что слово «нет» встречалось последним.\\nКалибровать модель предлагается с помощью выученного линейного преобразования:\\nq^=softmax(Wp^+b)\\\\hat{q} = softmax(W\\\\hat{p} + b)\\nq^\\u200b=softmax(Wp^\\u200b+b)В этом преобразовании:\\nWWW и bbb — обучаемые;\\np^\\\\hat{p}p^\\u200b — вероятности на выходе модели;\\nq^\\\\hat{q}q^\\u200b — откалиброванные вероятности;\\nОбучающие данные собираются так:\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nДля различных задач собираем подводки и добавляем нейтральное слово N/A. В этом примере несмещённая модель должна давать с вероятностью 50% ответ «positive» или «negative».\\nЧтобы добиться такого распределения ответов у смещённой модели, представим:\\nW=diag(p^)−1,\\xa0b=0W = diag(\\\\hat{p})^{-1}, \\\\space b = 0\\nW=diag(p^\\u200b)−1,\\xa0b=0Также все few-shot-примеры стандартизуются в специальный формат вопрос — ответ, как на картинке выше.\\nЭтот метод (синий график) по сравнению со стандартными few-shot-примерами (красный график) помог повысить качество и уменьшить разброс результата. Таким образом, оптимизировав всего 4 параметра, авторы существенно улучшили итоговый результат.\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nPromt-tuning\\nКачество работы модели зависит от подводки, и few-shot просто один из способов её построения. Эксперименты показывают, что грамотный подбор промта позволяет экономить на обучении и решать задачи с высоким качеством. Проблема в обучении больших моделей — нехватка оперативной памяти на GPU, поэтому не будем оптимизировать все параметры модели.\\nПусть необходимо решить задачу ААА, к ней имеется обучающее множество вида (X,y)(X,y)(X,y). Введём дополнительные токены, которых не было в словаре: <P1>,<P2>,…,<Pk><P_1>, <P_2>, … , <P_k><P1\\u200b>,<P2\\u200b>,…,<Pk\\u200b> — и будем добавлять в каждый текст из X согласно какому-то правилу.\\nПравило может быть таким: имеем 20 спецтокенов, добавим токены 1–10 в начало строки, а 11–20 в конец.\\nТогда, можно «заморозить» все параметры в модели, кроме этих токенов, и сэкономить на обучении. Если токенов 100 и каждый из них имеет размерность в 1024, то необходимо оптимизировать лишь 100 тысяч параметров вместо 175 млрд в случае обучения всей модели.\\n\\n\\n\\nссылка на источник картинки\\n\\n\\n\\n💡 Эффект от такого трюка достаточно многогранен:\\n\\nМеньше обучаемых параметров — меньше памяти занимает модель.\\nМеньше обучаемых параметров — быстрее происходит обучение.\\nОбычно нужно сильно меньше обучающих данных, чем при традиционном обучении всей модели для достижения высокого качества.\\nВысокое качество результата.\\n\\n\\nПолучается, что можно оптимизировать подводку, или, другими словами, находить наиболее оптимальный промт, который лучше прочих решает поставленную задачу.\\nКак повысить качество решения задач из разных категорий\\nЯзыковые модели призваны решать самый широкий спектр текстовых задач — вопросно-ответные, суммаризацию, диалоговость, перевод и многие другие.\\nПолучается, что модель должна после некого обучения (подбора подводки или оптимизации вообще всех параметров под каждую задачу) решать каждую из них на высоком уровне. Однако модель обычно учится на текстах из интернета, книгах и других доступных ресурcах. И формат задачи, который обычно требуется от модели, не соответствует тому, что алгоритм привык видеть на обучении. К этому стоит добавить, что среди веб-документов просьба что-то сократить или определить тональность документа встречается не очень часто.\\nИсправить этот недостаток призваны подходы по генерализации языковых моделей: FLAN и T0. Инструкции даются на естественном языке и для подготовки качественного обучающего множества предлагается произвести следующие действия:\\n\\nКаждой отдельной задаче (будь то перевод, написание отзывов или суммаризация) пишется по несколько различных подводок, отражающих смысл задания.\\nИтоговый датасет составляется из отдельных задач, все строчки датасета перемешиваются случайным образом.\\nАвторы стараются собрать как можно более разнообразные задачи в обучающее множество.\\n\\n\\n\\n\\nссылка на источник картинки\\n\\n\\n\\n\\n\\nссылка на источник картинки\\n\\n\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nДве картинки сверху демонстрируют FLAN- и T0- подходы по созданию датасета, а картинка снизу — рост усреднённого качества модели после обучения на смеси. Таким образом с некоторого размера модели наблюдается повышение метрик качества при дальнейших дообучениях генерализованной модели на отложенных задачах.\\nChain-of-Thought\\nПредыдущий подход со смесью датасетов помогает решать многие задачи в среднем заметно лучше. Однако есть задачи, где качество результатов модели всё ещё низкое. Например, предложить эффективный код, решающий некую алгоритмическую задачу, найти минимум некоторой аналитической функции потерь, посчитать производную фукнции в точке и так далее.\\nТакие вопросы требуют рассуждения, которое модель не может просто так провести из-за своей архитектуры. Выход — составить подводки в стиле Chain-of-Thought (CoT):\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nCoT-подводка состоит из трёх обязательных элементов:\\n\\nФормулировки задачи на естественном языке.\\nПодробного пошагового решения.\\nОтвета на задачу.\\n\\nФормирование такого промта, особенно на few-shot, заставляет модель рассуждать, как можно правильно решить задачу. Авторы этой статьи сравнили на двух математических бенчмарках способность модели решать сложные задачи.\\n\\nMultiArith — проверяет умение решать простые арифметически задачки.\\nGSM8K — более сложные.\\n\\nРезультаты демонстрируют, что наличие CoT в подводке увеличивает способность решать математические задачки у больших языковых моделей.\\n\\nInstructGPT\\nНаконец, обсудив, как готовить обучающие данные, перейдем к прародителю ChatGPT. Инструкционная модель — это та, которая обучена отвечать на пользовательские запросы в режиме zero-shot (а вообще, и few-shot, и любой человекочитаемый формат) с высоким качеством.\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nInstructGPT — это модель, и она интересна с точки зрения выработки концепции обучения всех инструкционных моделей (InstructGPT, ChatGPT, GPT-4 и других). С некоторыми нюансами обучение состоит из четырех этапов:\\n\\nПодготовка качественного претрейна. Языковая модель должна содержать в себе как можно больше знаний о мире, чтобы иметь возможность в последующем решать произвольные задачи с высоким качеством. На этом этапе необходимо озаботиться наибольшим разнообразием, чистотой и полнотой обучающих данных. Подробнее об этом мы поговорим в последнем разделе этого параграфа.\\nSFT (supervised finetuning) — обучение модели следовать инструкциям. Этот пункт мы подробно обсудили в предыдущей части параграфа (T0, FLAN, CoT). На этом этапе важно составить грамотный инструкционный датасет, где инструкция содержит произвольные запросы к модели, а ответ на неё — подробный текст, которым будущий пользователь будет доволен. Грамотный сбор таких данных довольно дорогостоящий процесс, но от него напрямую зависит, каким образом модель будет взаимодействовать с пользователем.\\nОбучение reward-модели. Каждый ответ алгоритма можно оценить с точки зрения вежливости, подробности или персонажности. Персонажность позволяет модели считать себя, например, капитаном Джеком Воробьем и общаться на пиратском говоре. Также есть менее формализуемые критерии качества ответов, их даже сложно описать словами. Например, что в основном людям ответ 1 нравится больше чем ответ 2.\\nReward-модель агрегирует эти кртитерии в число — меру качества. Чем оно выше, тем качественнее ответ модели. Для выравнивания поведения модели обычно важно уметь оценивать тысячи текстов, а вручную это делать дорого и долго, поэтому обучается специальная модель-оценщик. Про то, как обучать reward-модель, будет рассказано далее.\\nЭтап Reinforcement Learning (RL). На нём языковая модель обучается генерировать такие ответы, которые имели бы наивысшую оценку относительно reward-модели. Про то, как делать RL, будет рассказано далее.\\n\\nChatGPT\\nОдна из самых нашумевших языковых моделей в мире наследует логику обучения Instruct GPT. Основные отличия от последней заключаются в:\\n\\nДиалоговости. Модель обучена работать с диалогами, держать их в контексте и помнить историю того, что требовал пользователь. Обучение производится посредством сбора/написания диалоговых данных.\\nРазмере и качестве инструкционного датасета.\\nТом, что больше внимания уделено разметке и обучению reward-модели и этапу с RL.\\n\\nК сожалению, OpenAI не предоставили детали обучения ChatGPT, а предложили лишь общий ход действий. Также неизвестны архитектурные параметры модели.\\nКак обучить свою LLM?\\nОбсудим детально на примере доступных в open-source моделей семейства LLaMA.\\nLLaMa\\nВ качестве примера возьмём самую свежую архитектуру трансформеров на первую половину 2023 года — LLaMa, а также способы превращать её в чатовую модель, проводить Alignment на примере LLaMa-2. Вторая модель архитектурно не отличается от первой (кроме увеличенного контекста до 4096 токенов), поэтому содержание статей можно объединить в один рассказ.\\nПретрейн\\nДля обучения с нуля качественной языковой модели необходимы:\\n\\nмощный кластер на сотни видеокарт, на котором можно производить параллельное обучение модели. Больше GPU — больше модель можно обучить и быстрее по времени обучения;\\nтерабайты текстовых данных для тренировки на них;\\nархитектура, которая лучшим образом может моделировать язык.\\n\\nПоговорим подробнее о двух последних пунктах.\\nТекстовые данные\\nТекстовые данные можно брать из открытых источников, таких как CommonCrawl, C4, Taiga и прочее. Важно обеспечить:\\n\\nчистоту данных — например, убрать html-тэги, устранить дублирование текстов;\\nполноту — чтобы модель одинаково хорошо решала математические задачи, писала код или сочиняла стихотворения, текстов соответствующих доменов должно быть в достатке в обучающем корпусе;\\nразнообразие данных.\\n\\nСуществуют эмпирические законы обученности модели, но здесь остановимся на числе пройденных за обучение токенов. В LLaMa-моделях это значение варьируется от 1T до 2Т. Ниже приведены основные параметры по числу размерности внутренних эмбедингов, числу голов Attention, слоёв и параметров обучения разных моделей:\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nАрхитектура\\nУ LLaMa-моделей предлагается целый ряд архитектурных изменений. Так как в учебнике рассматривался лишь базовая архитектура трансформеров, то опишем, что в ней необходимо изменить, чтобы получить LLaMa-модель.\\n\\n\\nPre-нормализация.\\n\\n\\n\\n\\n\\nОбычно используется LayerNorm, а в LLaMa — RMSNorm.\\n\\nПусть x∈Rm,y∈Rn.x \\\\in \\\\R^m, y \\\\in \\\\R^n.x∈Rm,y∈Rn.  Тогда нелинейное преобразование в общем виде выглядит так:\\nai=∑j=1mwijxj,yi=f(ai+bi)a_i = \\\\sum_{j=1}^{m} w_{ij}x_{j}, y_i = f(a_i + b_i)\\nai\\u200b=j=1∑m\\u200bwij\\u200bxj\\u200b,yi\\u200b=f(ai\\u200b+bi\\u200b)И LayerNorm можно описать следующими формулами:\\nμ=1n∑i=1nai,\\xa0σ=1n∑i=1n(ai−μ)2yi=f(ai−μσgi+bi)\\\\mu = \\\\frac{1}{n}\\\\sum_{i=1}^n a_i, \\\\space \\\\sigma = \\\\sqrt{\\\\frac{1}{n}\\\\sum_{i=1}^n (a_i - \\\\mu) ^ 2}\\n\\\\\\\\\\ny_i = f(\\\\frac{a_i - \\\\mu}{\\\\sigma}g_i + b_i)\\nμ=n1\\u200bi=1∑n\\u200bai\\u200b,\\xa0σ=n1\\u200bi=1∑n\\u200b(ai\\u200b−μ)2\\u200byi\\u200b=f(σai\\u200b−μ\\u200bgi\\u200b+bi\\u200b)В свою очередь экспериментально RMSNorm демонстрирует лучшие результаты в сравнении с  LayerNorm и высчитывается так:\\nyi=f(ai1n∑i=1nai2gi+bi)y_i = f(\\\\frac{a_i}{\\\\sqrt{\\\\frac{1}{n}\\\\sum^{n}_{i=1} a_i^2}}g_i + b_i)\\nyi\\u200b=f(n1\\u200b∑i=1n\\u200bai2\\u200b\\u200bai\\u200b\\u200bgi\\u200b+bi\\u200b)\\n\\nSwiGLU-активация используется вместо ReLU. ⊗⊗⊗ — значок поэлементного умножения матриц.\\nReLU(x)=max(0,x)SwiGLU(x,W,V,b,c,β)=Swishβ(xW+b)⊗(xV+c)Swishβ(x)=xσ(βx)ReLU(x) = max(0, x)\\n\\\\\\\\\\nSwiGLU(x, W, V, b, c, β) = Swish_β(xW + b) ⊗ (xV + c)\\n\\\\\\\\\\nSwish_β(x) = x \\\\sigma(βx)\\nReLU(x)=max(0,x)SwiGLU(x,W,V,b,c,β)=Swishβ\\u200b(xW+b)⊗(xV+c)Swishβ\\u200b(x)=xσ(βx)\\n\\nРоторные эмбединги. Информацию о том, в каком порядке следуют токены внутри модели, хранят в себе позиционные эмбединги. Они могут быть абсолютными (кодирование синусами и косинусами, как описано в параграфе о трансформерах) или относительными (кодируется расстояние между каждой парой токенов).\\nРоторные эмбединги позволяют вычислять относительную связь между парой токенов на этапе вычисления Attention, также они выигрывают по сравнению с относительными в совместимости kernel-ов. То есть, одно из понятных не технических отличий их от других — вычисление позиционной информации на каждом слое модели при подсчёте Attention, а не только перед первым слоем. Это позволяет на каждом слое явно обрабатывать информацию об относительном расположении токенов. Роторные эмбединги показывают лучшее качество на многих задачах и являются стандартом для обучения языковых моделей. Подробнее о них можно почитать в этой статье.\\n\\n\\nСуществуют также техники ускорения обучения моделей и оптимизации использования памяти, но с этим предлагаем читателям ознакомиться самостоятельно.\\nSFT (supervised finetuning)\\nВторой этап обучения инструкционных языковых моделей требует множество инструкций. Рецепт как их готовить был подробно описан в середине этого параграфа. Снова проговорим, что для написания инструкций или сбора датасета необходимо, чтобы инструкции были:\\n\\nразнообразными;\\nкачественными;\\nимели одинаковый формат, чтобы чатовая модель могла обучиться диалоговости (где вопрос пользователя, где ее ответ);\\nинформативными;\\nподробными;\\nChain-of-Thought (CoT), few-shot и так далее.\\n\\nReward-модель\\nТретий этап в создании инструкционных моделей. Есть несколько способов собрать датасет для обучения reward-модели. Он должен содержать тексты и метки к ним. Если меток много (например, в случае балльной оценки), можно использовать разновидности ранжирующих лоссов. Разберем способ обучения модели на бинарную оценку.\\nПусть модели подается на вход инструкция xxx. Поменяв температуру, способ сэмплирования или использовав разные чек-пойнты модели, возможно получить два разнообразных ответа y1y_1y1\\u200b и y2y_2y2\\u200b. Не ограничивая общность, предположим, что, согласно некоторым предпочтениям, асессоры или пользователи установили, что первый ответ лучше второго.\\nПроделаем эту операцию много раз и получим обучающее множество, состоящее из {xi,y1i,y2i}i=1N\\\\{x^i, y_1^i, y_2^i\\\\}_{i = 1} ^ N{xi,y1i\\u200b,y2i\\u200b}i=1N\\u200b. Тогда reward-модель можно обучать минимизацией следующей функции потерь:\\nLranking=−log(σ(rθ(x,y1)−rθ(x,y2)−m(r)))\\\\mathcal{L}_{ranking} = -log(\\\\sigma(r_{\\\\theta}(x, y_1) - r_{\\\\theta}(x, y_2) - m(r)))\\nLranking\\u200b=−log(σ(rθ\\u200b(x,y1\\u200b)−rθ\\u200b(x,y2\\u200b)−m(r)))Где:\\nrθr_{\\\\theta}rθ\\u200b  — reward-модель с обучаемыми параметрами тета;\\nm(r)m(r)m(r) — некий margin, который определяет, насколько сильно модель должна отделять хороший и плохой ответы друг от друга.\\nRL (Reinforcement Learning)\\nНа четвёртом этапе, этапе выравнивания модели, можно воспользоваться разными алгоритмами. LLaMa-2 Chat была обучена последовательно сначала на Rejection Sampling fine-tuning (RL «для бедных») и Proximal Policy Optimization (PPO).\\nRejection Sampling fine-tuning. Этот подход основан на довольно простой стратегии. Пусть имеется инструкция xxx. Сгенерируем для неё NNN ответов и выберем тот, который получает наивысшую оценку у reward-модели. График ниже демонстрирует, что чем больше NNN, тем больше reward-score у лучшего ответа. Собрав пары инструкция — лучший ответ, можно обучить на них языковую модель и провести таким образом выравнивание поведения модели.\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nProximal Policy Optimization. Для лучшего понимания происходящего советуем прочесть параграф, посвященный RL.\\nОсновная задача, как обычно, следовать некой политике, которая лучшим образом отражает human feedback. Политика — наша итоговая модель, value-функция оценивает средний reward в текущем состоянии (обычно это та же самая модель с линейным слоем поверх).\\nФормализуем термины из RL для задачи выравнивания языковой модели:\\n\\nПолитика π\\\\piπ — обучаемая языковая модель.\\nValue-функция Vπ(st)V_{\\\\pi}(s_t)Vπ\\u200b(st\\u200b) — обычно та же самая модель с линейным слоем поверх, оценивает средний reward, если действовать из состояния sts_tst\\u200b согласно политике π\\\\piπ.\\nsts_tst\\u200b — состояние в момент времени ttt. Это весь контекст ttt-токенов, которые модель успела сгенерировать к текущему моменту.\\nata_tat\\u200b — действие из текущего состояния в момент времени ttt. Обозначает следующий токен, который будет сгенерирован.\\nτ\\\\tauτ — траектория, т. е. тройки {si,ai,ri}i=0∞\\\\{s_i, a_i, r_i\\\\}_{i = 0}^{\\\\infty}{si\\u200b,ai\\u200b,ri\\u200b}i=0∞\\u200b, — это состояния генерируемого токена и награды за него\\n\\nСразу можно сделать вывод, что в языковых моделях st+1=concat([st,at])s_{t+1} = concat([s_t, a_t])st+1\\u200b=concat([st\\u200b,at\\u200b]), Q(st,at)=V(st+1).Q(s_t, a_t) = V(s_{t+1}).Q(st\\u200b,at\\u200b)=V(st+1\\u200b).\\nТакже, в RL символом ∞\\\\infty∞ обозначается вся последовательность токенов, то есть на практике сюда можно подставлять количество сгенерированных токенов.\\nИнициализируем θ0,ψ0\\\\theta_0, \\\\psi_0θ0\\u200b,ψ0\\u200b — начальные веса политики и value-функции\\nДля n=0,1,2,…,N:n = 0, 1, 2, …, N:n=0,1,2,…,N:\\n\\n\\nСоберем коллекцию траекторий Dn={τi}D_n = \\\\{\\\\tau_i\\\\}Dn\\u200b={τi\\u200b} , следуя политике π(θn)\\\\pi(\\\\theta_n)π(θn\\u200b).\\n\\n\\nПосчитаем A(st,at)=Q(st,at)−V(st)A(s_t, a_t) = Q(s_t, a_t) - V(s_t)A(st\\u200b,at\\u200b)=Q(st\\u200b,at\\u200b)−V(st\\u200b). Эта формула отражает разницу между финальной наградой за выбранное действие ata_tat\\u200b в текущем состоянии sts_tst\\u200b и средней финальной наградой, которую можно было бы получить в этом состоянии. Вообще говоря, с помощью метода Generalized Advantage Estimation (GAE) её можно аппроксимировать следующим выражением:\\nAt^=rt+γVθn(st+1)−Vθn(st)\\\\hat{A_t} = r_t + \\\\gamma V_{\\\\theta_n}(s_{t+1}) - V_{\\\\theta_n}(s_{t}) \\nAt\\u200b^\\u200b=rt\\u200b+γVθn\\u200b\\u200b(st+1\\u200b)−Vθn\\u200b\\u200b(st\\u200b)\\n\\nОбновляем веса политики согласно одному из лоссов PPO. Например, используем такой:\\nθn+1=argmaxθ\\xa0E^t[πθn(at∣st)πθn−1(at∣st)A^t]−βKL(πθn−1(⋅∣st),πθn(⋅∣st))\\\\theta_{n+1} = \\\\underset{\\\\theta}{argmax} \\\\space \\\\mathbb{\\\\hat{E}}_t \\\\left[ \\\\frac{\\\\pi_{\\\\theta_n}(a_t | s_t)}{\\\\pi_{\\\\theta_{n-1}}(a_t | s_t)} \\\\hat{A}_t \\\\right] - \\\\beta KL(\\\\pi_{\\\\theta_{n-1}}(\\\\cdot | s_t), \\\\pi_{\\\\theta_{n}}(\\\\cdot | s_t))\\nθn+1\\u200b=θargmax\\u200b\\xa0E^t\\u200b[πθn−1\\u200b\\u200b(at\\u200b∣st\\u200b)πθn\\u200b\\u200b(at\\u200b∣st\\u200b)\\u200bA^t\\u200b]−βKL(πθn−1\\u200b\\u200b(⋅∣st\\u200b),πθn\\u200b\\u200b(⋅∣st\\u200b))\\n\\nС помощью MSE лосса оптимизируем значение value-функции:\\nL(ψ)=Et^[∣∣Vψn(st)−Rt^∣∣2]Rt^=∑l=0∞γlrt+l\\\\mathcal{L}(\\\\psi) = \\\\hat{\\\\mathbb{E_t}} \\\\left[ || V_{\\\\psi_n}(s_t) - \\\\hat{R_t}|| ^ 2 \\\\right]\\n\\\\\\\\\\n\\\\hat{R_t} = \\\\sum_{l=0}^{\\\\infty} \\\\gamma^l r_{t+l}\\nL(ψ)=Et\\u200b^\\u200b[∣∣Vψn\\u200b\\u200b(st\\u200b)−Rt\\u200b^\\u200b∣∣2]Rt\\u200b^\\u200b=l=0∑∞\\u200bγlrt+l\\u200b\\n\\nИтог\\nМы с вами обсудили, как развивались языковые модели, какие приёмы и техники необходимы для успешного обучения инструкционных моделей. Также на примере архитектуры LLaMa разобрали, как самостоятельно обучить языковые модели с нуля.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф8.5. Диффузионные моделиСледующий параграф9.1. Введение в рекомендательные системыА что, если попробовать дообучить уже сейчас?Хочешь применить знания о больших языковых моделях на практике? В Yandex Cloud ты можешь дообучить YandexGPT или Llama на своих данных и интегрировать их в свои проекты. Всё это — с помощью Yandex Cloud AI Studio.Попробуй AI StudioЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_20.html', 'title': 'Нейронные сети'}, page_content='Нейронные сетиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/45.1.Нейронные сети5.2.Первое знакомство с полносвязными нейросетями5.3.Метод обратного распространения ошибки5.4.Тонкости обучения6.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Нейронные сети5.1. Нейронные сетиАвторыФедотов СтаниславСиницин ФилиппНейчев РадославКраткий путеводитель по\\xa0разделуВ этом параграфе вы познакомитесь с нейронными сетями — семейством моделей, которое начиная с 2012-го постепенно добивается превосходства во всё новых и новых приложениях, во многих став де-факто стандартом.\\nОни были придуманы ещё в 70-х, но техническая возможность и понимание того, как обучать нейросети большого размера, появились лишь примерно к 2011 году, и это дало мощный толчок к их развитию. Совокупность нейросетевых подходов и сама наука о нейросетях носит название глубинного обучения или deep learning.\\nГлубинное обучение основано на двух идеях.\\n\\nВо-первых, это стремление к переходу от построения сложных пайплайнов, каждая компонента которых тренируется сама по себе решать кусочек задачи, к end-to-end обучению всей системы, как одного целого. Так, мы можем обучать не каждый слой отдельно, а все вместе, и не учить какие-нибудь линейные модели поверх случайных лесов, а работать с одной цельной моделью\\nВо-вторых, это обучение представлений объектов — информативных признаковых описаний, учитывающих структуру данных, построенных лишь на основе самих данных, зачастую неразмеченных. Это позволяет автоматизировать процесс отбора признаков: теперь вместо того, чтобы руками экспертов искать «более информативное» или «более простое» признаковое описание наших объектов, мы можем получить их автоматически, притом используя не только данные, доступные нам для задачи, но и, к примеру, все тексты мира.\\n\\nОбучению представлений будет посвящён отдельный параграф, а в этом мы постараемся убедить вас, что нейросети — это гибкий инструмент для решения самых разных задач и для работы с самыми разными типами данных.\\nНадо признать, впрочем, что современные модели весьма сложны и мало напоминают своих элегантных предшественников из 2012 года. Развитие нейросетей во многом мотивируется нуждами и возможностями индустрии, ростом производительности компьютеров и объёмов доступных данных.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nПри этом теория отчаянно не поспевает за практикой. В глубинном обучении весьма распространён чисто инженерный подход к построению алгоритмов, изобилие деталей, основанных на интуиции и обосновывающихся фразой «просто потому, что так работает, а иначе — нет», поэтому ряд учёных продолжает относиться к нейросетям скептически.\\nОднако результаты, достигнутые с их помощью за последние 10 лет, столь впечатляющие, что их нельзя игнорировать. Особенно существенный прогресс был достигнут в области анализа данных, обладающих некоторой внутренней структурой: текстов, изображений, видео, облаков точек, графов и так далее.\\nТем не менее, есть и подходы к теоретическому осмыслению того, почему нейросети работают так хорошо, и мы познакомим вас с ними в отдельном параграфе, посвящённой теории машинного обучения.\\nНо довольно предисловий! Давайте набросаем план нашего вторжения в мир глубинного обучения:\\n\\n\\nПервое знакомство с полносвязными нейросетями. Вы впервые встретитесь с самой простой нейросетевой архитектурой, узнаете, как строятся и как применяются нейронные сети.\\n\\n\\nОбратное распространение ошибки. Вы узнаете, как легко и быстро дифференцировать по параметрам сложного вычислительного графа, после чего будете (теоретически) понимать, как обучить несложную нейросеть.\\n\\n\\nТонкости обучения. Нейросети — могущественный, но капризный инструмент, и чем сложнее глубинная модель, тем труднее обучить её. В этом разделе мы начнём знакомить вас с разными приёмами, которые позволяют повысить вероятность успеха, а также с регуляризационными техниками для нейросетей.\\n\\n\\nСвёрточные нейросети. Классический пример структурированных данных — это изображения. В этом параграфе вы познакомитесь с базовым инструментарием для работы с ними — свёртками, пулингом и со свёрточными сетями в целом.\\n\\n\\nВ первых четырёх главах вы познали основы глубинного обучения, но в основном имели дело с ситуацией, когда и данные, и выходы представляют из себя непритязательные векторы. А что делать, если мы должны работать с чем-то более сложно устроенным? Оказывается, за счёт своей гибкости и возможности сочетать в себе самые разные компоненты нейросети отлично справляются с данными, имеющими однородную структуру (изображениями, текстами, облаками точек …\\\\ldots…). Для работы с каждым из этих типов данных требуются специфические инженерные решения, и мы постараемся познакомить вас с ними, разверзнув перед вами всю бездну способностей нейросетей!\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф4.7. Модели с латентными переменнымиСледующий параграф5.2. Первое знакомство с полносвязными нейросетямиОсновные понятия глубинного обучения. Базовые слои и\\xa0функции активацииЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_6.html', 'title': 'Метрические методы'}, page_content='Метрические методыЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/52.1.Линейные модели2.2.Метрические методыМетод k-ближайших соседей (KNN)Поиск ближайших соседейПоиск ближайших соседей: точные методыПоиск ближайших соседей: приближённые методы2.3.Решающие деревья2.4.Ансамбли в машинном обучении2.5.Градиентный бустинг3.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Метрические методы2.2. Метрические методыАвторыНоркин ДмитрийАлгоритмы KNN. Быстрый поиск ближайших соседейСмысл метрических методов очень хорошо раскрывает фраза «Скажи мне, кто твой друг, и я скажу, кто ты». Алгоритмы этого класса почти не имеют фазы обучения. Вместо этого они просто запоминают всю обучающую выборку, а на этапе предсказания просто ищут объекты, похожие на целевой.\\nТакой процесс называют lazy learning, потому что никакого обучения, по сути, не происходит. Также метрические модели являются непараметрическими, потому что они не делают явных допущений о глобальных законах, которым подчиняются данные. Так, линейная регрессия основывается на предположении о том, что изучаемая закономерность линейная (с неизвестными коэффициентами, которые восстанавливаются по выборке), а линейная бинарная классификация — что существует гиперплоскость, неплохо разделяющая классы. Метрические методы же локальны: они исходят из допущения, что свойства объекта можно узнать, имея представление о его соседях.\\nУказанные выше свойства могут быть полезными, особенно в случае сложно устроенных данных, для которых мы не можем придумать глобальную модель. Однако с другой стороны, из-за lazy learning алгоритм становится абсолютно неприменимым при большом количестве данных. Несмотря на то, что эти алгоритмы очень просты для понимания, они довольно точны и хорошо интерпретируемы — и часто используются как минимум в качестве бейзлайнов в разных задачах.\\nВ первой части параграфа мы расскажем об одном из самых известных метрических алгоритмов — методе k-ближайших соседей (k-nearest neighbors, KNN). Этот подход в основном чисто инженерный из-за отсутствия фазы обучения — в настоящее время уже почти нигде не применяется. Однако многие техники, на которых основан алгоритм, используются и в других методах.\\nНапример, у алгоритмов поиска ближайших соседей, — неотъемлемой часть метода, — намного более широкая область применения. Плюс ко всему KNN — очень простой и легко интерпретируемый алгоритм, поэтому изучить его всё равно полезно. Мы обсудим подробнее его преимущества, недостатки, область его применения, а также возможные обобщения.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nДля метрических методов очень важно уметь эффективно находить ближайшие объекты, поэтому задача их поиска неизбежно возникает при применении любого такого алгоритма. Поэтому во второй части параграфа мы рассмотрим возможные подходы к быстрому поиску ближайших соседей.\\nМетод k-ближайших соседей (KNN)\\nПредставим, что мы проводим классификацию объектов на два класса — красный или жёлтый. Нам дана некоторая обучающая выборка и целевой объект (серый):\\n\\nМы хотим определить, к какому классу относится серый объект. Интуитивно очевидно, что он должен быть жёлтым, потому что все его соседи жёлтые. Эта интуиция и отражает суть метода KNN — классифицировать целевой объект, исходя из того, какие классы у объектов, которые максимально похожи на него.\\nПерейдём теперь к более формальному описанию алгоритма. Рассмотрим сначала задачу многоклассовой классификации, а регрессией займёмся позже.\\nПусть дана обучающая выборка X=(xi,yi)i=1NX = (x_i, y_i)_{i=1}^NX=(xi\\u200b,yi\\u200b)i=1N\\u200b, где xi∈X,\\xa0yi∈Y=1,…,Cx_i \\\\in \\\\mathbb{X}, \\\\ y_i \\\\in \\\\mathbb{Y}=\\\\\\\\{1,\\\\ldots,C\\\\\\\\}xi\\u200b∈X,\\xa0yi\\u200b∈Y=1,…,C. Пусть также задана некоторая симметричная по своим аргументам функция расстояния ρ:X×X→[0,+∞)\\\\color{#E06A27}{\\\\rho : \\\\mathbb{X} \\\\times \\\\mathbb{X} \\\\to [0, +\\\\infty)}ρ:X×X→[0,+∞). Предположим, что требуется классифицировать новый объект u\\\\color{#97C804}{u}u. Для этого найдём kkk наиболее близких к u\\\\color{#97C804}{u}u в смысле расстояния ρ\\\\color{#E06A27}{\\\\rho}ρ объектов обучающей выборки Xk(u)=xu(1),…,xu(k)X_k(\\\\color{#97C804}{u}) = \\\\\\\\{\\\\color{#FFC100}{x^{(1)}_u},\\\\ldots,\\\\color{#FFC100}{x^{(k)}_u}\\\\\\\\}Xk\\u200b(u)=xu(1)\\u200b,…,xu(k)\\u200b:\\n∀xin∈Xk(u)\\xa0∀xout∈X∖Xk(u)ρ(u,xin)⩽ρ(u,xout).(1)\\\\forall \\\\color{#FFC100}{x_{\\\\rm in}}\\\\in X_k(\\\\color{#97C804}{u}) \\\\ \\\\forall x_{\\\\rm out} \\\\in X \\\\setminus X_k(\\\\color{#97C804}{u}) \\\\quad \\\\color{#E06A27}{\\\\rho}(\\\\color{#97C804}{u}, \\\\color{#FFC100}{x_{\\\\rm in}}) \\\\leqslant \\\\color{#E06A27}{\\\\rho}(\\\\color{#97C804}{u}, x_{\\\\rm out}). \\\\tag{1}\\n∀xin\\u200b∈Xk\\u200b(u)\\xa0∀xout\\u200b∈X∖Xk\\u200b(u)ρ(u,xin\\u200b)⩽ρ(u,xout\\u200b).(1)Метку класса объекта xu(i)\\\\color{#FFC100}{x^{(i)}_u}xu(i)\\u200b будем обозначать yu(i)\\\\color{#FFC100}{y_u^{(i)}}yu(i)\\u200b. Класс нового объекта тогда естественным образом определим как наиболее часто встречающийся класс среди объектов из Xk(u)X_k(\\\\color{#97C804}{u})Xk\\u200b(u):\\na(u)=argmax\\u2061y∈Y∑i=1kI[yu(i)=y](2)a(\\\\color{#97C804}{u}) = \\\\underset{y\\\\in \\\\mathbb{Y}}{\\\\operatorname{arg max}} \\\\sum_{i=1}^k \\\\mathbb{I}[\\\\color{#FFC100}{y_u^{(i)}} = y] \\\\tag{2}\\na(u)=y∈Yargmax\\u200bi=1∑k\\u200bI[yu(i)\\u200b=y](2)Формула может показаться страшной, но на самом деле всё довольно просто: для каждой метки класса y∈Yy \\\\in \\\\mathbb{Y}y∈Y количество соседей u\\\\color{#97C804}{u}u с такой меткой можно посчитать, просто просуммировав по всем соседям индикаторы событий, соответствующих тому, что метка соседа равна yyy.\\nЛегко заметить, что этот алгоритм позволяет также оценивать вероятности классов. Для этого достаточно просто посчитать частоты классов соседей:\\nP(u∼y)=∑i=1kI[yu(i)=y]k\\\\mathbb{P}(\\\\color{#97C804}{u}\\\\sim y) = \\\\frac{\\\\sum_{i=1}^k \\\\mathbb{I}[\\\\color{#FFC100}{y_u^{(i)}} = y]}k\\nP(u∼y)=k∑i=1k\\u200bI[yu(i)\\u200b=y]\\u200bСтоит, однако, понимать, что, хоть такая функция и удовлетворяет свойствам вероятности (она неотрицательна, аддитивна и ограничена единицей), это не более чем эвристика.\\nНесмотря на то что формально фаза обучения отсутствует, алгоритм может легко переобучиться. Вы можете убедиться в этом сами, использовав маленькое количество соседей (например, одного или двух), — границы классов оказываются довольно сложными. Происходит это из-за того, что параметрами алгоритма можно считать всю обучающую выборку, довольно большую по размеру. Из-за этого алгоритму легко подстроиться под конкретные данные.\\nПо ссылке вы можете увидеть интерактивный пример работы алгоритма. Автор примера - Анастасия Чирикова.\\nВыбор метрики\\nМожет возникнуть закономерный вопрос, как же правильно выбрать функцию расстояния ρ\\\\color{#E06A27}{\\\\rho}ρ. В подавляющем большинстве случаев обычное евклидово расстояние ρ(x,y)=∑i(xi−yi)2\\\\rho(x, y) = \\\\sqrt{\\\\sum_i (x_i - y_i)^2}ρ(x,y)=∑i\\u200b(xi\\u200b−yi\\u200b)2\\u200b будет хорошим выбором. Однако в некоторых случаях другие функции будут подходить лучше, поэтому давайте разберём ещё несколько функций, наиболее используемых на практике.\\n\\n\\nМанхэттенская метрика\\nρ(x,y)=∑i∣xi−yi∣\\\\rho(x, y) = \\\\sum_i \\\\vert x_i - y_i \\\\vert\\nρ(x,y)=i∑\\u200b∣xi\\u200b−yi\\u200b∣Часто используется в высокоразмерных пространствах из-за лучшей устойчивости к выбросам. Представим, что два объекта в 1000-размерном пространстве почти идентичны, но сильно отличаются по одному из признаков. Это почти наверняка свидетельствует о выбросе в этом признаке, и объекты, скорее всего, очень близки. Однако евклидово расстояние усилит различие в единственном признаке и сделает их более далёкими друг от друга. Этого недостатка лишена манхэттенская метрика — в ней вместо квадрата используется модуль.\\nМетрика Минковского\\nρ(x,y)=(∑i∣xi−yi∣p)1/p\\\\rho(x, y) = \\\\left(\\\\sum_i \\\\vert x_i - y_i\\\\vert^p\\\\right)^{1/p}\\nρ(x,y)=(i∑\\u200b∣xi\\u200b−yi\\u200b∣p)1/pЯвляется обобщением евклидовой (p=2p=2p=2) и манхэттенской (p=1p=1p=1) метрик.\\nКосинусное расстояние\\nρ(x,y)=1−cos\\u2061θ=1−x⋅y∥x∥∥y∥\\\\rho(x,y) = 1 - \\\\cos \\\\theta = 1 - \\\\frac{x \\\\cdot y}{\\\\|x\\\\| \\\\|y\\\\|}\\nρ(x,y)=1−cosθ=1−∥x∥∥y∥x⋅y\\u200bЭта метрика хороша тем, что не зависит от норм векторов. Такое поведение бывает полезно в некоторых задачах, например при поиске похожих документов. В качестве признаков там часто используются количества слов. При этом интуитивно кажется, что если в тексте использовать каждое слово в два раза больше, то тема этого текста поменяться не должна. Поэтому как раз в этом случае нам не важна норма вектор-признака, и в задачах, связанных с текстами, часто применяется именно косинусное расстояние.\\nРасстояние Жаккара\\nρ(A,B)=1−∣A∩B∣∣A∪B∣\\\\rho(A, B) = 1 - \\\\frac{|A\\\\cap B|}{|A\\\\cup B|}\\nρ(A,B)=1−∣A∪B∣∣A∩B∣\\u200bЕго стоит использовать, если исследуемые объекты — это некоторые множества. Это полезно тем, что нет нужды придумывать векторные представления для этих множеств, чтобы использовать традиционные метрики.\\nВообще говоря, несмотря на некоторые эвристические соображения по выбору метрики, её можно считать гиперпараметром и подбирать соответствующими способами. Часто качество модели сильно зависит от выбора метрики, а иногда выбрать правильную метрику очень тяжело. Например, в случае когда данные имеют сильно разный масштаб, выбрать подходящую метрику почти невозможно, и нужно сперва проводить нормализацию.\\nЗамечание. Упомянутые в этом параграфе функции мы называем «метриками», но, конечно же, они не обязаны быть метриками в строгом математическом смысле. Они неотрицательны и симметричны, но могут не удовлетворять неравенству треугольника.\\nОбобщения алгоритма\\nВзвешенный KNN\\nУ оригинального алгоритма есть один большой недостаток: он никак не учитывает расстояния до соседних объектов, хотя эта информация может быть полезной.\\nДавайте попробуем придумать, как исправить этот недостаток. Нам нужно каким-то образом увеличивать вклад близких объектов и уменьшать вклад далёких. Можно заметить, что все индикаторы в формуле (2)(2)(2) учитываются в сумме с одинаковыми коэффициентами. Возникает идея — назначить этим индикаторам веса, которые тем больше, чем ближе объект к целевому. Таким образом, получаем следующую формулу:\\na(u)=argmax\\u2061y∈Y∑i=1kwiI[yu(i)=y].(3)a(\\\\color{#97C804}{u}) = \\\\underset{y\\\\in \\\\mathbb{Y}}{\\\\operatorname{arg max}} \\\\sum_{i=1}^k w_i\\\\mathbb{I}[\\\\color{#FFC100}{y_u^{(i)}} = y]. \\\\tag{3}\\na(u)=y∈Yargmax\\u200bi=1∑k\\u200bwi\\u200bI[yu(i)\\u200b=y].(3)Такой алгоритм называется взвешенным KNN (weighted KNN).\\nЕсть множество вариантов выбора весов для объектов, которые можно поделить на две большие группы. В первой группе веса зависят лишь от порядкового номера объекта в отсортированном по близости к uuu массиве Xk(u)X_k(\\\\color{#97C804}{u})Xk\\u200b(u). Чаще всего затухающие веса берутся линейно (wi=k+1−ik)\\\\left( w_i = \\\\frac{k+1-i}{k} \\\\right)(wi\\u200b=kk+1−i\\u200b) или экспоненциально (wi=qi,\\xa00<q<1)\\\\left( w_i = q^i, \\\\ 0 < q < 1\\\\right)(wi\\u200b=qi,\\xa00<q<1) .\\nОднако здесь мы также не используем всю информацию, которая нам доступна. Зачем использовать порядок соседей, порождаемый расстояниями, если можно использовать сами расстояния?\\nВо второй группе методов вес — это некоторая функция от расстояния. Давайте подумаем, какие должны быть свойства у этой функции.\\n\\nОчевидно, она должна быть положительной на своей области определения, иначе модель будет поощрять несовпадение с некоторыми ближайшими соседями.\\nТакже необходимо, чтобы функция монотонно не возрастала, чтобы вес близких соседей был больше, чем далёких.\\n\\nТаким образом вводится так называемая ядерная функция (kernel function) K:R→RK : \\\\mathbb{R} \\\\to \\\\mathbb{R}K:R→R, обладающая перечисленными выше свойствами, с помощью которой и высчитывается вес каждого соседа:\\na(u)=argmax\\u2061y∈Y∑i=1kK(ρ(u,xu(i))h)I[yu(i)=y],(4)a(\\\\color{#97C804}{u}) = \\\\underset{y\\\\in \\\\mathbb{Y}}{\\\\operatorname{arg max}} \\\\sum_{i=1}^k K\\\\left(\\\\frac{\\\\color{#E06A27}{\\\\rho}(\\\\color{#97C804}{u}, \\\\color{#FFC100}{x_u^{(i)}})}{h}\\\\right)\\\\mathbb{I}[\\\\color{#FFC100}{y_u^{(i)}} = y], \\\\tag{4}\\na(u)=y∈Yargmax\\u200bi=1∑k\\u200bK(hρ(u,xu(i)\\u200b)\\u200b)I[yu(i)\\u200b=y],(4)где hhh — некое положительное число, которое называется шириной окна.\\nОт выбора ядра зависит гладкость аппроксимации, но на её качество этот выбор почти не влияет. Примеры ядерных функций в порядке увеличения их гладкости:\\n\\nK(x)=12I[∣x∣⩽1]K(x) = \\\\frac12 \\\\mathbb{I} \\\\left[ \\\\vert x \\\\vert \\\\leqslant 1\\\\right]K(x)=21\\u200bI[∣x∣⩽1] — прямоугольное ядро;\\nK(x)=(1−∣x∣)I[∣x∣⩽1]K(x) = \\\\left(1 - \\\\vert x \\\\vert \\\\right)\\\\mathbb{I}\\\\left[\\\\vert x \\\\vert \\\\leqslant 1\\\\right]K(x)=(1−∣x∣)I[∣x∣⩽1] — треугольное ядро (непрерывное);\\nK(x)=34(1−x2)I[∣x∣⩽1]K(x) = \\\\frac34\\\\left(1 - x^2\\\\right) \\\\mathbb{I}\\\\left[ \\\\vert x \\\\vert \\\\leqslant 1\\\\right]K(x)=43\\u200b(1−x2)I[∣x∣⩽1] — ядро Епанечникова (гладкое везде, кроме –1 и 1);\\nK(x)=1516(1−x2)2I[∣x∣⩽1]K(x) = \\\\frac{15}{16}\\\\left(1 - x^2\\\\right)^2\\\\mathbb{I}\\\\left[\\\\vert x \\\\vert \\\\leqslant 1\\\\right]K(x)=1615\\u200b(1−x2)2I[∣x∣⩽1] — биквадратное ядро (гладкое везде);\\nK(x)=12πe−2x2K(x) = \\\\frac{1}{\\\\sqrt{2\\\\pi}}e^{-2x^2}K(x)=2π\\u200b1\\u200be−2x2 — гауссовское ядро (бесконечно гладкое везде).\\n\\nНа практике чаще всего используют либо прямоугольное для простоты, либо гауссовское, в случае когда важна гладкость модели (немного забегая вперёд — это особенно важно в регрессии).\\nШирина окна, в свою очередь, сильно влияет как раз на качество модели. При слишком маленькой ширине модель сильно подстраивается под обучающую выборку и теряет свою обобщающую способность. При слишком большой ширине, напротив, модель становится слишком простой. Универсальной ширины окна не существует, поэтому для каждой задачи её приходится подбирать отдельно.\\nKernel regression\\nАлгоритм KNN можно довольно легко обобщить и на задачу регрессии. Самые очевидные способы — брать для некоторого ядра KKK либо обычное среднее:\\na(u)=1k∑i=1kyu(i),(5)a(\\\\color{#97C804}{u}) = \\\\frac1k \\\\sum_{i=1}^k \\\\color{#FFC100}{y_u^{(i)}}, \\\\tag{5}\\na(u)=k1\\u200bi=1∑k\\u200byu(i)\\u200b,(5)либо взвешенный вариант:\\na(u)=∑i=1kK(ρ(u,xu(i))h)yu(i)∑i=1kK(ρ(u,xu(i))h)(6)a(\\\\color{#97C804}{u}) = \\\\frac{\\\\sum_{i=1}^k K\\\\left(\\\\frac{\\\\color{#E06A27}{\\\\rho}(\\\\color{#97C804}{u}, \\\\color{#FFC100}{x_u^{(i)}})}{h}\\\\right) \\\\color{#FFC100}{y_u^{(i)}}}{\\\\sum_{i=1}^k K\\\\left(\\\\frac{\\\\color{#E06A27}{\\\\rho}(\\\\color{#97C804}{u}, \\\\color{#FFC100}{x_u^{(i)}})}{h}\\\\right)} \\\\tag{6}\\na(u)=∑i=1k\\u200bK(hρ(u,xu(i)\\u200b)\\u200b)∑i=1k\\u200bK(hρ(u,xu(i)\\u200b)\\u200b)yu(i)\\u200b\\u200b(6)Последняя формула называется формулой Надарая — Ватсона. Она — один из непараметрических методов восстановления регрессии, объединённых названием ядерная регрессия (kernel regression).\\nВыписать ответ, конечно, просто, но возникает интересный вопрос: можно ли использовать оптимизационные формулы из задачи классификации? Сначала давайте подумаем, что выдаст алгоритм, если формулу (4)(4)(4) применить без изменений.\\nВ задаче регрессии почти наверняка все значения yu(i)y_u^{(i)}yu(i)\\u200b будут различными. Поэтому для любого yyy сумма в формуле (4)(4)(4) будет состоять из не более чем одного слагаемого, а значит, максимум будет достигаться на соседе с наибольшим весом, то есть на ближайшем соседе. Это означает, что метод всегда вырождается в 1-NN. Это не совсем то, чего мы добиваемся, поэтому давайте немного модифицируем алгоритм.\\nДавайте сперва подумаем, а для чего вообще в формуле (4)(4)(4) используется индикатор. В задаче классификации индикатор — естественная мера близости двух объектов: если объекты совпадают, то значение 111, если различаются, то 000. Проблема в том, что в задаче регрессии объекты являются действительными числами, и для них функция, которая выдаёт отличное от нуля значение лишь в одной точке y=yiy=y_iy=yi\\u200b, — плохая мера близости.\\nВ случае непрерывных значений yyy естественно использовать более гладкие функции для выражения близости. Таким образом, для обобщения формулы (4)(4)(4) на задачу регрессии нам необходимо всего лишь заменить индикатор на некоторую более гладкую функцию. При этом для действительных чисел чаще всего рассматривают не близость, а расстояние между ними, то есть некоторую метрику. Например, в качестве такой метрики можно взять квадрат евклидова расстояния (y−yu(i))2(y- y_u^{(i)})^2(y−yu(i)\\u200b)2. Отметим, что максимизация близости эквивалентна минимизации расстояния, и получим следующую формулу:\\na(u)=argmin\\u2061y∈R∑i=1kK(ρ(u,xu(i))h)(y−yu(i))2.(7)a(\\\\color{#97C804}{u}) = \\\\underset{y\\\\in \\\\mathbb{R}}{\\\\operatorname{arg min}} \\\\sum_{i=1}^k K\\\\left(\\\\frac{\\\\color{#E06A27}{\\\\rho}(\\\\color{#97C804}{u}, \\\\color{#FFC100}{x_u^{(i)}})}{h}\\\\right)(y- \\\\color{#FFC100}{y_u^{(i)}})^2. \\\\tag{7}\\na(u)=y∈Rargmin\\u200bi=1∑k\\u200bK(hρ(u,xu(i)\\u200b)\\u200b)(y−yu(i)\\u200b)2.(7)Выбор именно этой функции хорош тем, что у этой оптимизационной задачи есть точное решение, и оно записывается как раз формулой (6)(6)(6).\\nДля ядерной регрессии справедливы те же рассуждения про выбор ядра и ширины окна, которые были приведены в прошлом разделе про классификацию.\\nВлияние ширины окна и вида ядра на вид функции:\\n\\n\\n\\nПреимущества и недостатки\\nСперва поговорим о преимуществах алгоритма.\\n\\nНепараметрический, то есть не делает явных предположений о распределении данных.\\nОчень простой в объяснении и интерпретации.\\nДостаточно точный, хоть и чаще всего уступает градиентному бустингу и случайному лесу в accuracy.\\nМожет быть использован как для классификации, так и для регрессии.\\n\\nНесмотря на большие преимущества, алгоритм не лишён и минусов.\\n\\nНеэффективный по памяти, поскольку нужно хранить всю обучающую выборку.\\nВычислительно дорогой по той же причине.\\nЧувствителен к масштабу данных, а также к неинформативным признакам.\\nДля применения алгоритма необходимо, чтобы метрическая близость объектов совпадала с их семантической близостью, чего не всегда просто добиться. Представим, например, что мы решаем задачу нахождения похожих изображений. Мы хотим, чтобы картинки с лесом находились близко друг к другу, однако, если взять любую попиксельную метрику, такие картинки могут быть очень далеки друг от друга. Зачастую для решения этой проблемы вначале обучают представления.\\n\\nПрименение\\nИз-за своих недостатков алгоритм очень неэффективен в задачах с большим количеством данных. Однако у него всё равно есть много применений в реальном мире. Приведём лишь некоторые из них:\\n\\nРекомендательные системы. Если посмотреть на саму формулировку задачи «предложить пользователю что-то похожее на то, что он любит», то KNN прямо напрашивается в качестве решения. Несмотря на то что сейчас часто используются более совершенные алгоритмы, метод ближайших соседей всё равно применяется в качестве хорошего бейзлайна.\\nПоиск семантически похожих документов. Если векторные представления близки друг к другу, то темы документов схожи.\\nПоиск аномалий и выбросов. Из-за того что алгоритм запоминает обучающую выборку полностью, ему легко посмотреть, насколько целевой объект похож на все данные, которые он видел.\\nЗадача кредитного скоринга. Рейтинги двух людей, у которых примерно одинаковая зарплата, схожие должности и кредитные истории, не должны сильно отличаться, поэтому KNN отлично подходит для решения такой задачи.\\n\\nВопрос сложности алгоритма неочевиден и требует детального анализа, который будет частично проведён в следующем разделе.\\nПоиск ближайших соседей\\nДля того чтобы применять метод ближайших соседей, нужно уметь как-то находить этих самых соседей. С первого взгляда может показаться, что никакой проблемы нет: действительно, можно ведь просто перебрать все объекты из обучающей выборки X=(xi,yi)i=1NX = (x_i, y_i)_{i=1}^{N}X=(xi\\u200b,yi\\u200b)i=1N\\u200b, посчитать для каждого из них расстояние до тестового объекта и затем найти минимум.\\nОднако несмотря на то что сложность такого поиска линейная по NNN, она также зависит и от размерности пространства признаков. Если x∈RDx \\\\in \\\\mathbb{R}^Dx∈RD, то сложность такого алгоритма поиска O(ND)O(N D)O(ND). Если вспомнить, что в типичной задаче машинного обучения количество признаков DDD может быть порядка 100100100, а размер выборки и вовсе может исчисляться десятками и сотнями тысяч объектов, то становится ясно, что такая сложность никуда не годится. Проблема осложняется ещё и тем, что данный поиск необходимо выполнять на этапе применения модели, который должен быть быстрым. Всё это означает, что возникает необходимость в более быстрых методах поиска ближайших соседей, чем простой перебор.\\nВсе такие методы можно поделить на две основные группы: точные и приближённые. Последние, как следует из их названия, находят соседей лишь приближённо, то есть найденные объекты хоть и будут действительно близки, но не обязательно будут самыми близкими. В этом разделе мы подробнее рассмотрим методы из каждой группы.\\nПеред началом обзора стоит сказать, что хоть мы и рассматриваем алгоритмы поиска соседей именно в контексте их использования в KNN, область их применения значительно шире, и она не ограничивается исключительно машинным обучением. Например, на их основе работает любая информационно-поисковая система, от поиска в «Гугле» или «Яндексе» до всем известных алгоритмов «Ютьюба».\\nПоиск ближайших соседей: точные методы\\nТочных методов существует довольно мало. Можно сказать, что их, по сути, два.\\n\\nПервый — полный перебор с различными эвристиками. Например, можно выбрать подмножество признаков и считать расстояние только по ним. Оно будет оценкой снизу на реальное расстояние, поэтому если оно уже больше, чем до текущего ближайшего объекта, то можно сразу отбросить этот объект и переходить к следующему. Такие эвристики хоть и могут давать некоторый выигрыш по времени, но не улучшат асимптотическую сложность.\\nВторой — k-d-деревья, о которых стоит поговорить подробнее.\\n\\nK-d-деревья\\nПредставим на секунду, что у нас есть всего лишь один признак, то есть объекты выражаются вещественными числами, а не векторами. В этом случае для поиска ближайшего соседа напрашивается всем вам известное бинарное дерево поиска, которое позволяет находить элементы за логарифмическое время. Оказывается, существует аналог данной структуры в многомерном пространстве, который называется k-d-дерево (k-d tree, сокращение от k-dimensional tree).\\nКак и в обычном дереве поиска, в k-d-дереве каждый узел является объектом обучающей выборки, который особым образом делит пространство на два полупространства. Таким образом, всё пространство оказывается поделено на множество малых областей, и такое деление оказывается очень полезным при поиске ближайших соседей.\\nРассмотрим подробнее, как строится такое дерево. Трудность в применении обычного дерева поиска состоит в том, что мы не можем напрямую сравнить два вектора так же, как два вещественных числа. Чтобы эту проблему преодолеть, узлы дерева будут делить пространство лишь по одной оси. При движении вниз по дереву оси, по которым точки делят пространство, циклически сменяют друг друга. Например, в двумерном пространстве корень будет отвечать за деление по x-координате, его сыновья — за деление по y-координате, а внуки — снова за x-координату, и т. д. Посмотрим, как это работает на примере:\\n\\n\\n\\nИсточник\\n\\n\\nНа картинке выше корень (30,40)(30, 40)(30,40) делит все точки по оси х: слева оказываются точки, у которых x<30x < 30x<30, а справа — те, у которых x⩾30x\\\\geqslant 30x⩾30. Аналогично левый сын корня (5,25)(5, 25)(5,25) делит своё поддерево по оси y: слева оказываются точки, у которых y<25y < 25y<25, а справа — те, у которых y⩾25y\\\\geqslant 25y⩾25.\\nОстаётся вопрос — как выбирать точки, которые будут делить пространство пополам? Чтобы дерево было сбалансированным, нужно находить точку с медианой, соответствующей уровню поддерева координаты. На практике часто ограничиваются выбором случайной точки или любой эвристикой по приближённому поиску медианы (например, медиана некоторого подмножества точек). Это позволяет ускорить построение дерева, но убирает все гарантии на его сбалансированность.\\nДобавлять новые точки можно так же, как и в одномерном дереве поиска. Спускаясь по дереву, можно однозначно определить лист, к которому нужно подвесить новую точку, чтобы не нарушить все свойства дерева. При добавлении большого количества точек, однако, дерево может перестать быть сбалансированным, и нужно проводить ребалансировку. Также существуют варианты k-d-деревьев, которые сохраняют сбалансированность при добавлении / удалении точек.\\nПоговорим теперь про то, как же находить ближайших соседей с помощью такого дерева. Будем производить обход дерева в глубину с двумя модификациями.\\n\\nВо-первых, будем запоминать наиболее близкую точку. Это позволит не заходить в поддеревья, задающие области, которые заведомо дальше, чем текущая наиболее близкая точка, поэтому не имеет смысла искать в них ближайших соседей.\\nВо-вторых, будем прежде всего обходить те поддеревья, которые задают наиболее близкие области, а значит, с большей вероятностью содержат ближайшего соседа.\\n\\n\\n\\n\\nИсточник\\n\\n\\nСложность метода по размеру обучающей выборки в среднем равна O(log\\u2061N)O(\\\\log N)O(logN) при равномерном распределении точек. При большой размерности пространства, однако, алгоритму приходится посещать больше ветвей дерева, чтобы найти ближайших соседей. Например, если N≈DN \\\\approx DN≈D, то сложность становится примерно такой же, как и в случае полного перебора. В общем случае считается, что для того чтобы асимптотика действительно была логарифмической, нужно, чтобы N≳2DN \\\\gtrsim 2^DN≳2D. Поэтому уже при количестве признаков порядка сотни алгоритм не даёт существенных преимуществ перед полным перебором.\\nПочитать по теме:\\n\\nХорошая презентация, объясняющая структуру и поиск соседей.\\nБалансировка деревьев.\\n\\nПоиск ближайших соседей: приближённые методы\\nПочти всегда находить именно самых близких соседей необязательно. Например, в задаче подбора рекомендаций фильмов пользователю чаще всего не нужны наиболее похожие картины, достаточно, к примеру, 10 из 15 наиболее близких. Поэтому, чтобы ускорить процесс поиска соседей, используют приближённые методы. Разберём основные идеи, которые применяются в таких методах.\\nRandom projection trees\\nАлгоритмы, основанные на деревьях, очень часто применяются в задачах поиска соседей. Идея всех таких методов заключается в итеративном разделении пространства случайными гиперплоскостями и построении на базе этого разделения дерева, в листах которого содержится малое число объектов.\\nОдним из наиболее ярких представителей этого семейства является Annoy — алгоритм, который используется Spotify для рекомендаций музыки. Задача подобных рекомендательных систем довольно простая — нужно посоветовать пользователю композиции, которые он ещё не слушал, но которые при этом с высокой вероятностью ему понравятся. Простая и рабочая идея — предлагать композиции, похожие на те, которые он уже слушает. Здесь на помощь как раз и приходят методы поиска ближайших соседей.\\nAnnoy в какой-то степени похож на k-d-деревья. Сначала выбираются два случайных объекта обучающей выборки и проводится гиперплоскость, симметрично их разделяющая. Затем для каждого полученного полупространства итеративно запускается такая же процедура, которая продолжается до тех пор, пока в каждой области будет не более MMM объектов (MMM — гиперпараметр).\\n\\n\\n\\nИсточник\\n\\n\\nТаким образом задаётся бинарное дерево с глубиной порядка O(log\\u2061N)O(\\\\log N)O(logN) в среднем.\\n\\n\\n\\nИсточник\\n\\n\\nСпускаясь по этому дереву, можно найти область, в которой лежит целевой объект и некоторое количество близких к нему элементов обучающей выборки. Проблема в том, что это не обязательно будут самые близкие объекты, поэтому для увеличения точности составляется лес из таких деревьев и берётся объединение соответствующих целевому объекту областей.\\n\\n\\n\\nИсточник\\n\\n\\nЧем больше таких деревьев берётся, тем более точным будет результат, но придётся тратить большее время на его поиск.\\nПреимущество алгоритма — простота нахождения компромисса между скоростью работы и точностью с помощью тюнинга гиперпараметров. К минусам можно отнести то, что алгоритм плохо параллелится и переносится на GPU, не работает эффективно с батчами, а также то, что для добавления новой точки в обучающую выборку придётся перезапускать процедуру с самого начала.\\nПочитать по теме:\\n\\nОтличная статья с иллюстрациями и подробным описанием алгоритма.\\n\\nLocality-sensitive hashing (LSH)\\nПредположим, что мы можем построить такую хеш-функцию, которая переводит близкие объекты в один бакет. Тогда близких соседей целевого объекта можно найти, посчитав его хеш и посмотрев на коллизии. Оказывается, такие хеш-функции существуют, и на этой идее основано несколько алгоритмов, которые объединяются названием Locality-sensitive hashing (LSH). К этому классу алгоритмов относится, например, FAISS, используемый Facebook.\\n\\n\\n\\nИсточник\\n\\n\\nОпределим формально семейство хеш-функций, которое мы хотим использовать. Нам нужно, чтобы вероятность коллизии на близких объектах была высокая, а на далёких — низкая. Назовём семейство хеш-функций H\\\\mathcal{H}H (R,cR,p1,p2)(R, cR, p_1, p_2)(R,cR,p1\\u200b,p2\\u200b)-чувствительным, если для любой h(x)∈Hh(x)\\\\in\\\\mathcal{H}h(x)∈H:\\n\\nдля ρ(x,y)<R\\\\rho(x, y) < Rρ(x,y)<R вероятность коллизии Pr\\u2061[h(x)=h(y)]>p1\\\\Pr\\\\left[h(x)=h(y)\\\\right] > p_1Pr[h(x)=h(y)]>p1\\u200b;\\nдля ρ(x,y)>cR\\\\rho(x, y) > cRρ(x,y)>cR вероятность коллизии Pr\\u2061[h(x)=h(y)]<p2\\\\Pr\\\\left[h(x)=h(y)\\\\right] < p_2Pr[h(x)=h(y)]<p2\\u200b.\\n\\nФормулы могут выглядеть сложными, но это всего лишь формализация нашей интуиции. Картинка ниже поясняет определение: для близких красных объектов в шаре радиусом RRR вероятность коллизии больше p1p_1p1\\u200b, для далёких синих объектов на расстоянии больше cRcRcR вероятность коллизии меньше p2p_2p2\\u200b, а про серые объекты в слое между RRR и cRcRcR мы ничего не знаем.\\n\\n\\n\\nИсточник\\n\\n\\nДля каждой функции расстояния, используемой в задаче, существует своё подходящее семейство хеш-функций. Например, для евклидовой и манхэттенской метрик используются случайные проекции, где хеш-функция имеет следующий вид:\\nhw,b(x)=⌊wTx+br⌋,h_{\\\\boldsymbol{w}, b}(\\\\boldsymbol{x}) = \\\\left\\\\lfloor \\\\frac{\\\\boldsymbol{w}^T\\\\boldsymbol{x} + b}{r}\\\\right\\\\rfloor, \\nhw,b\\u200b(x)=⌊rwTx+b\\u200b⌋,где w\\\\boldsymbol{w}w и bbb — случайные параметры, а rrr выбирается пользователем. bbb выбирается равномерно из отрезка [0,r][0, r][0,r], а w\\\\boldsymbol{w}w генерируется либо из нормального распределения, что соответствует евклидовой метрике, либо из распределения Коши — для манхэттенской метрики.\\nПо сути, такая функция разбивает всё пространство на слои в направлении вектора w\\\\boldsymbol{w}w. Параметр rrr при этом задаёт ширину слоя.\\n\\n\\n\\nИсточник\\n\\n\\nНа практике при использовании лишь одной хеш-функции разница между p1p_1p1\\u200b и p2p_2p2\\u200b оказывается очень маленькой, поэтому применяют различные методы для её увеличения.\\n\\nПервый способ — уменьшать размер бакетов в хеш-таблице путём использования композиции разных хеш-функций из одного семейства g(x)=(h1(x),…,hm(x))g(x) = (h_1(x),\\\\ldots,h_m(x))g(x)=(h1\\u200b(x),…,hm\\u200b(x)). Преимущество этого способа как раз хорошо видно на примере случайных проекций. При использовании лишь одной хеш-функции бакетами являются слои бесконечного объёма. Однако при использовании композиции размером, как минимум равным количеству признаков DDD, из-за случайности выбора вектора w\\\\boldsymbol{w}w бакеты почти наверное станут замкнутыми фигурами с конечным объёмом.\\nВторой способ повышения эффективности алгоритма — использовать несколько хеш-таблиц и искать соседей среди коллизий в каждой из них. На практике используют оба метода сразу, подбирая mmm и LLL — количество хеш-таблиц как гиперпараметры.\\n\\nК плюсам алгоритма можно отнести хорошие теоретические гарантии на сублинейное время и, как и в Annoy, простой поиск компромисса между точностью и скоростью работы. Минусами можно назвать высокую потребность в памяти, плохую адаптируемость под GPU, а также тот факт, что, несмотря на теоретические гарантии в среднем, на практике алгоритм может работать даже чуть дольше полного перебора из-за того, что, помимо самого поиска, требуется искать хеши объектов.\\nПочитать по теме:\\n\\nОтличная статья с объяснением в иллюстрациях и примерами хеш-функций для других метрик.\\nЕщё одна статья, в которой шаг за шагом выводится алгоритм на примере расстояния Жаккара.\\n\\nProximity graphs & Hierarchical navigable small world (HNSW)\\nСледующий класс алгоритмов основан на построении специального графа близости (proximity graph) на объектах выборки и дальнейшем жадном поиске по этому графу. Алгоритмы этого семейства сейчас считаются state-of-the-art (SotA) для многих задач.\\nРассмотрим подробнее этот класс алгоритмов на примере одного из наиболее популярных из них под названием Navigable small world (NSW). Идея его в следующем: на данных строится граф (он также называется NSW), который удовлетворяет двум следующим свойствам:\\n\\nМежду любыми двумя точками существует короткий путь, или, более формально, матожидание числа кратчайшего пути между двумя случайно выбранными вершинами растёт как O(log\\u2061N)O(\\\\log N)O(logN).\\nСредняя степень вершины мала.\\n\\nНа первый взгляд может показаться, что тяжело выполнить одновременно оба свойства, но на самом деле большая часть графов в реальном мире являются NSW-графами. Самый простой пример — это известное правило шести рукопожатий: любые два случайных человека соединены короткой последовательностью личных контактов длиной не более шести, несмотря на то, что количество знакомых у среднего человека (100100100–100010001000) мало по сравнению с населением Земли.\\nВ таких графах существует очень простой метод поиска соседей. Нужно выбрать случайную точку, среди её соседей выбрать того, который ближе всего к целевому объекту, и повторить процедуру уже для него. Показано, что такой жадный поиск имеет полилогарифмическую асимптотику.\\n\\n\\n\\nИсточник\\n\\n\\nПроблема такого подхода в том, что можно попасть в плотный кластер и очень долго оттуда выбираться. Для решения этой проблемы используется иерархия NSW, или Hierarchical navigable small world (HNSW). Исходный граф является нулевым слоем. Каждый следующий слой строится в два шага:\\n\\nКаждая вершина текущего слоя попадает в следующий с некоторой вероятностью ppp.\\nНа всех вершинах, попавших в новый слой, строится NSW.\\n\\nПо построению количество слоёв будет O(log\\u2061N)O(\\\\log N)O(logN).\\n\\n\\n\\nИсточник\\n\\n\\nПоиск начинается в самом верхнем слое. После нахождения ближайшей к целевому объекту вершины спускаемся на слой ниже и начинаем поиск из этой вершины. Повторяем процедуру, пока не спустимся до нулевого слоя. Таким образом, на каждом слое мы всё больше уточняем наш ответ. Стоит отметить, что для ускорения работы иногда поиск останавливают не при нахождении ближайшей вершины, а раньше, используя критерии остановки.\\nИнтуитивно легко понять, почему такая иерархическая структура решает проблему плотных кластеров: в верхних слоях вершин мало, а расстояния между ними в среднем большие, а значит, таких кластеров там почти нет. Поэтому, попадая в нижний слой, мы чаще всего оказываемся уже в нужном кластере и просто уточняем результат работы алгоритма.\\nHNSW, так же как и рассмотренные ранее приближённые методы, позволяет искать трейд-офф между точностью и скоростью работы. Плюс ко всему на реальных данных он часто работает лучше других методов и сейчас считается SotA. Однако этот способ поиска не лишён и недостатков. Главный заключается в том, что нельзя добавлять точки в обучающую выборку без перестройки структуры. Помимо этого, он довольно требователен по памяти из-за того, что для каждого слоя приходится хранить как вершины, которые в него входят, так и связи между этими вершинами.\\nПодробнее — в оригинальной статье.\\nВ завершение стоит сказать, что не существует универсального метода поиска соседей — каждый из описанных методов может быть лучше других в определённой задаче. К тому же, несмотря на то что приближённые методы имеют лучшую асимптотику, многие из них плохо переносятся на GPU. Из-за этого на практике полный перебор бывает быстрее любого из таких приближённых методов.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф2.1. Линейные моделиЛинейные модели от\\xa0линейной до\\xa0логистической регрессии. Регуляризация, работа с\\xa0категориальными признаками, многоклассовая классификацияСледующий параграф2.3. Решающие деревьяОбучение древесных моделей для классификации и\\xa0регрессии. Эффективное построение решающих деревьевЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_7.html', 'title': 'Решающие деревья'}, page_content='Решающие деревьяЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/52.1.Линейные модели2.2.Метрические методы2.3.Решающие деревьяОпределение решающего дереваПочему построение оптимального решающего дерева — сложная задача?Жадный алгоритм построения решающего дереваОсобенности данныхМетоды регуляризации решающих деревьевАлгоритмические трюкиИсторическая справка2.4.Ансамбли в машинном обучении2.5.Градиентный бустинг3.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Решающие деревья2.3. Решающие деревьяАвторыСиницин ФилиппОбучение древесных моделей для классификации и\\xa0регрессии. Эффективное построение решающих деревьевВ этом параграфе мы рассмотрим ещё одно семейство моделей машинного обучения — решающие деревья (decision trees).\\nРешающее дерево предсказывает значение целевой переменной с помощью применения последовательности простых решающих правил (которые называются предикатами). Этот процесс в некотором смысле согласуется с естественным для человека процессом принятия решений.\\nХотя обобщающая способность решающих деревьев невысока, их предсказания вычисляются довольно просто, из-за чего решающие деревья часто используют как кирпичики для построения ансамблей — моделей, делающих предсказания на основе агрегации предсказаний других моделей. О них мы поговорим в следующем параграфе.\\nПример решающего дерева\\nНачнём с небольшого примера. На картинке ниже изображено дерево, построенное для задачи классификации на пять классов:\\nВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nОбъекты в этом примере имеют два признака с вещественными значениями: XXX и YYY. Решение о том, к какому классу будет отнесён текущий объект выборки, будет приниматься с помощью прохода от корня дерева к некоторому листу.\\nВ каждом узле этого дерева находится предикат. Если предикат верен для текущего примера из выборки, мы переходим в правого потомка, если нет — в левого. В данном примере все предикаты — это просто взятие порога по значению какого-то признака:\\nB(x,j,t)=[xj≤t]    B(x, j, t) = [ x_j \\\\le t ] \\nB(x,j,t)=[xj\\u200b≤t]В листьях записаны предсказания (например, метки классов). Как только мы дошли до листа, мы присваиваем объекту ответ, записанный в вершине.\\nНа картинке ниже визуализирован процесс построения решающих поверхностей, порождаемых деревом (правая часть картинки):\\n\\nКаждый предикат порождает разделение текущего подмножества пространства признаков на две части. На первом этапе, когда происходило деление по [X≤X1][ X \\\\le X_1 ][X≤X1\\u200b], вся плоскость была поделена на две соответствующие части. На следующем уровне часть плоскости, для которой выполняется X≤X1X \\\\le X_1X≤X1\\u200b, была поделена на две части по значению второго признака Y≤Y1Y \\\\le Y_1Y≤Y1\\u200b — так образовались области 1 и 2. То же самое повторяется для правой части дерева — и так далее до листовых вершин: получится пять областей на плоскости. Теперь любому объекту выборки будет присваиваться один из пяти классов в зависимости от того, в какую из образовавшихся областей он попадает.\\nЭтот пример хорошо демонстрирует, в частности, то, что дерево осуществляет кусочно-постоянную аппроксимацию целевой зависимости. Ниже приведён пример визуализации решающей поверхности, которая соответствует дереву глубины 4, построенному для объектов данных из Ames Housing Dataset, где из всех признаков, описывающих объекты недвижимости, были выбраны ширина фасада (Lot_Frontage) и площадь (Lot_Area), а предсказать нужно стоимость.\\nДля более понятной визуализации перед построением дерева из датасета были выкинуты объекты с Lot_Frontage > 150 и с Lot_Area > 20000. Вот что получилось — в каждой из прямоугольных областей предсказывается одна и та же стоимость:\\n\\nОпределение решающего дерева\\nРазобравшись с приведёнными выше примерами, мы можем дать определение решающего дерева. Пусть задано бинарное дерево, в котором:\\n\\nкаждой внутренней вершине vvv приписан предикат Bv:X→{0,1}B_v: \\\\mathbb{X} \\\\to \\\\{ 0, 1 \\\\}Bv\\u200b:X→{0,1};\\nкаждой листовой вершине vvv приписан прогноз cv∈Yc_v \\\\in \\\\mathbb{Y}cv\\u200b∈Y, где Y\\\\mathbb{Y}Y — область значений целевой переменной (в случае классификации листу может быть также приписан вектор вероятностей классов).\\n\\nВ ходе предсказания осуществляется проход по этому дереву к некоторому листу. Для каждого объекта выборки xxx движение начинается из корня. В очередной внутренней вершине vvv проход продолжится вправо, если Bv(x)=1B_v(x) = 1Bv\\u200b(x)=1, и влево, если Bv(x)=0B_v(x) = 0Bv\\u200b(x)=0. Проход продолжается до момента, пока не будет достигнут некоторый лист, и ответом алгоритма на объекте xxx считается прогноз cvc_vcv\\u200b, приписанный этому листу.\\nВообще, предикат BvB_vBv\\u200b может иметь, произвольную структуру, но на практике чаще используют просто сравнение с порогом t∈Rt \\\\in \\\\mathbb{R}t∈R по какому-то jjj-му признаку:\\nBv(x,j,t)=[xj≤t]    B_v(x, j, t) = [ x_j \\\\le t ] \\nBv\\u200b(x,j,t)=[xj\\u200b≤t]При проходе через узел дерева с данным предикатом объекты будут отправлены в правое поддерево, если значение jjj-го признака у них меньше либо равно ttt, и в левое — если больше. В дальнейшем рассказе мы будем по умолчанию использовать именно такие предикаты.\\nИз структуры дерева решений следует несколько интересных свойств:\\n\\nвыученная функция — кусочно-постоянная, из-за чего производная равна нулю везде, где задана. Следовательно, о градиентных методах при поиске оптимального решения можно забыть;\\nдерево решений (в отличие от, например, линейной модели) не сможет экстраполировать зависимости за границы области значений обучающей выборки;\\nдерево решений способно идеально приблизить обучающую выборку и ничего не выучить (то есть такой классификатор будет обладать низкой обобщающей способностью): для этого достаточно построить такое дерево, в каждый лист которого будет попадать только один объект. Следовательно, при обучении нам надо не просто приближать обучающую выборку как можно лучше, но и стремиться оставлять дерево как можно более простым, чтобы результат обладал хорошей обобщающей способностью.\\n\\nНесколько иллюстраций для закрепленияСгенерируем для начала небольшой синтетический датасет для задачи классификации и обучим на нём решающее дерево, не ограничивая его потенциальную высоту.\\n\\nВыученная закономерность очень сложная: иногда она выделяет прямоугольником одну точку. И действительно, как мы увидим дальше, деревья умеют идеально подстраиваться под обучающую выборку. Это чемпионы переобучения. Помешать этому может лишь ограничение на высоту дерева. Вот как будет выглядеть дерево высоты 3, построенное на том же датасете:\\n\\nНе всё идеально, но классификатор уже не такой безумный. Теперь обратимся к задаче регрессии. Обучим решающее дерево, не ограничивая его высоту.\\n\\nФиолетовая ступенчатая пунктирная линия — это восстановленная деревом зависимость. На графике она мечется между точками, идеально следуя за обучающей выборкой. Кроме того (и это не лечится ограничением глубины дерева) за пределами обучающей выборки дерево делает константные предсказания. Это и имеют в виду, когда говорят, что древесные модели неспособны к экстраполяции.\\nПочему построение оптимального решающего дерева — сложная задача?\\nПусть, как обычно, у нас задан датасет (X,y)(X, y)(X,y), где y={yi}i=1N⊂RNy=\\\\{y_i\\\\}_{i=1}^N \\\\subset \\\\mathbb{R}^Ny={yi\\u200b}i=1N\\u200b⊂RN — вектор таргетов, а X={xi}i=1N∈RN×D,xi∈RDX=\\\\{x_i\\\\}_{i = 1}^N \\\\in \\\\mathbb{R}^{N \\\\times D}, x_i \\\\in \\\\mathbb{R}^DX={xi\\u200b}i=1N\\u200b∈RN×D,xi\\u200b∈RD — матрица признаков, в которой iii-я строка — это вектор признаков iii-го объекта выборки. Пусть у нас также задана функция потерь L(f,X,y)L(f, X, y)L(f,X,y), которую мы бы хотели минимизировать.\\nНаша задача — построить решающее дерево, наилучшим образом предсказывающее целевую зависимость. Однако, как уже было замечено выше, оптимизировать структуру дерева с помощью градиентного спуска не представляется возможным. Как ещё можно было бы решить эту задачу? Давайте начнём с простого — научимся строить решающие пни, то есть решающие деревья глубины 1.\\nКак и раньше, мы будем рассматривать только самые простые предикаты:\\nBj,t(xi)=[xij≤t]    B_{j, t}(x_i) = [ x_{ij} \\\\le t ] \\nBj,t\\u200b(xi\\u200b)=[xij\\u200b≤t]Ясно, что задачу можно решить полным перебором: существует не более (N−1)D(N - 1) D(N−1)D предикатов такого вида. Действительно, индекс jjj (номер признака) пробегает значения от 111 до DDD, а всего значений порога ttt, при которых меняется значение предиката, может быть не более N−1N - 1N−1:\\n\\nРешение, которое мы ищем, будет иметь вид:\\n(jopt,topt)=arg\\u2061min\\u2061j,tL(Bj,t,X,y)    (j_{opt}, t_{opt}) = \\\\arg \\\\min_{j, t} L(B_{j, t}, X, y)\\n(jopt\\u200b,topt\\u200b)=argj,tmin\\u200bL(Bj,t\\u200b,X,y)Для каждого из предикатов Bj,tB_{j, t}Bj,t\\u200b нам нужно посчитать значение функции потерь на всей выборке, что, в свою очередь, тоже занимает O(N)O(N)O(N).\\nСледовательно, полный алгоритм выглядит так:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1min_loss = inf\\n2optimal_border = None\\n3\\n4for j in range(D):\\n5    for t in X[:, j]:     # Можно брать сами значения признаков в качестве порогов\\n6        loss = calculate_loss(t, j, X, y)\\n7        if loss < min_loss:\\n8           min_loss, optimal_border = loss, (j, t)\\n\\n\\nСложность алгоритма — O(N2D)O(N^2 D)O(N2D). Это не заоблачная сложность, хотя, конечно, не идеальная. Но это была схема возможного алгоритма поиска оптимального дерева высоты 1.\\nКак обобщить алгоритм для дерева произвольной глубины? Мы можем сделать наш алгоритм поиска решающего пня рекурсивным и в теле цикла вызывать исходную функцию для всех возможных разбиений. Как мы упоминали выше, так можно построить дерево, идеально запоминающее всю выборку, однако на тестовых данных такой алгоритм вряд ли покажет высокое качество.\\nМожно поставить другую задачу: построить оптимальное с точки зрения качества на обучающей выборке дерево минимальной глубины (чтобы снизить переобучение). Проблема в том, что поиск такого дерева — NP-полная задача, то есть человечеству пока неизвестны способы решить её за полиномиальное время. Как быть?\\nИдеального ответа на этот вопрос нет, но до некоторой степени ситуацию можно улучшить двумя не исключающими друг друга способами:\\n\\nРазрешить себе искать не оптимальное решение, а просто достаточно хорошее. Начать можно с того, чтобы строить дерево с помощью жадного алгоритма, то есть не искать всю структуру сразу, а строить дерево этаж за этажом. Тогда в каждой внутренней вершине дерева будет решаться задача, схожая с задачей построения решающего пня. Для того чтобы этот подход хоть как-то работал, его придётся прокачать внушительным набором эвристик.\\nЗаняться оптимизацией с точки зрения computer science — наивную версию алгоритма (перебор наборов возможных предикатов и порогов) можно ускорить и асимптотически, и в константу раз.\\n\\nЭти две идеи мы и будем обсуждать в дальнейшем. Сначала попытаемся подробно разобраться с первой — как использовать жадный алгоритм.\\nЖадный алгоритм построения решающего дерева\\nПусть XXX — исходное множество объектов обучающей выборки, а XmX_mXm\\u200b — множество объектов, попавших в текущий лист (в самом начале Xm=XX_m = XXm\\u200b=X). Тогда жадный алгоритм можно верхнеуровнево описать следующим образом:\\n\\nСоздаём вершину vvv.\\nЕсли выполнен критерий остановки Stop(Xm)Stop(X_m)Stop(Xm\\u200b), то останавливаемся, объявляем эту вершину листом и ставим ей в соответствие ответ Ans(Xm)Ans(X_m)Ans(Xm\\u200b), после чего возвращаем её.\\nИначе: находим предикат (иногда ещё говорят сплит) Bj,tB_{j, t}Bj,t\\u200b, который определит наилучшее разбиение текущего множества объектов XmX_mXm\\u200b на две подвыборки XℓX_{\\\\ell}Xℓ\\u200b и XrX_rXr\\u200b, максимизируя критерий ветвления Branch(Xm,j,t)Branch(X_m, j, t)Branch(Xm\\u200b,j,t).\\nДля XℓX_\\\\ellXℓ\\u200b и XrX_rXr\\u200b рекурсивно повторим процедуру.\\n\\nДанный алгоритм содержит в себе несколько вспомогательных функций, которые надо выбрать так, чтобы итоговое дерево было способно минимизировать LLL:\\nAns(Xm)Ans(X_m)Ans(Xm\\u200b), вычисляющая ответ для листа по попавшим в него объектам из обучающей выборки, может быть, например:\\n\\nв случае задачи классификации — меткой самого частого класса или оценкой дискретного распределения вероятностей классов для объектов, попавших в этот лист;\\nв случае задачи регрессии — средним, медианой или другой статистикой;\\nпростой моделью. К примеру, листы в дереве, задающем регрессию, могут быть линейными функциями или синусоидами, обученными на данных, попавших в лист. Впрочем, везде ниже мы будем предполагать, что в каждом листе просто предсказывается константа.\\n\\nКритерий остановки Stop(Xm)Stop(X_m)Stop(Xm\\u200b) — функция, которая решает, нужно ли продолжать ветвление или пора остановиться. Это может быть какое-то тривиальное правило: например, остановиться только в тот момент, когда объекты в листе получились достаточно однородными и/или их не слишком много. Более детально мы поговорим о критериях остановки в параграфе про регуляризацию деревьев.\\nКритерий ветвления Branch(Xm,feature,value)Branch(X_m, feature, value)Branch(Xm\\u200b,feature,value) — пожалуй, самая интересная компонента алгоритма. Это функция, измеряющая, насколько хорош предлагаемый сплит. Чаще всего эта функция оценивает, насколько улучшится некоторая финальная метрика качества дерева в случае, если получившиеся два листа будут терминальными, по сравнению с ситуацией, когда сама исходная вершина — это лист. Выбирается такой сплит, который даёт наиболее существенное улучшение.\\nВпрочем, есть и другие подходы. При этом строгой теории, которая бы связывала оптимальность выбора разных вариантов этих функций и разных метрик классификации и регрессии, в общем случае не существует. Однако есть набор интуитивных и хорошо себя зарекомендовавших соображений, с которыми мы вас сейчас познакомим.\\nКритерии ветвления: общая идея\\nДавайте теперь по очереди посмотрим на популярные постановки задач ML и под каждую подберём свой критерий.\\nОтветы дерева будем кодировать так: c∈Rc \\\\in \\\\mathbb{R}c∈R — для ответов регрессии и меток класса. Для случаев, когда надо ответить дискретным распределением на классах, c∈RKc \\\\in \\\\mathbb{R}^Kc∈RK будет вектором вероятностей:\\nc=(c1,…,cK),∑i=1Kci=1c = (c_1, \\\\ldots, c_K), \\\\quad \\\\sum_{i = 1}^K c_i = 1 \\nc=(c1\\u200b,…,cK\\u200b),i=1∑K\\u200bci\\u200b=1Предположим также, что задана некоторая функция потерь L(yi,c)L(y_i, c)L(yi\\u200b,c). О том, что это может быть за функция, мы поговорим ниже.\\nВ момент, когда мы ищем оптимальный сплит Xm=Xl⊔XrX_m = X_l\\\\sqcup X_rXm\\u200b=Xl\\u200b⊔Xr\\u200b, мы можем вычислить для объектов из XmX_mXm\\u200b тот константный таргет ccc, которые предсказало бы дерево, будь текущая вершина терминальной, и связанное с ними значение исходного функционала качества LLL. А именно — константа ccc должна минимизировать среднее значение функции потерь:\\n1∣Xm∣∑(xi,yi)∈XmL(yi,c)    \\\\frac{1}{\\\\vert X_m\\\\vert}\\\\sum\\\\limits_{(x_i, y_i) \\\\in X_m} L(y_i, c)\\n∣Xm\\u200b∣1\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200bL(yi\\u200b,c)Оптимальное значение этой величины\\nH(Xm)=min\\u2061c∈Y1∣Xm∣∑(xi,yi)∈XmL(yi,c)    H(X_m) = \\\\min\\\\limits_{c \\\\in Y} \\\\frac{1}{\\\\vert X_m\\\\vert}\\\\sum\\\\limits_{(x_i, y_i) \\\\in X_m} L(y_i, c)\\nH(Xm\\u200b)=c∈Ymin\\u200b∣Xm\\u200b∣1\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200bL(yi\\u200b,c)обычно называют информативностью, или impurity. Чем она ниже, тем лучше объекты в листе можно приблизить константным значением.\\nПохожим образом можно определить информативность решающего пня. Пусть, как и выше, XlX_lXl\\u200b — множество объектов, попавших в левую вершину, а XrX_rXr\\u200b — в правую; пусть также clc_lcl\\u200b и crc_rcr\\u200b — константы, которые предсказываются в этих вершинах. Тогда функция потерь для всего пня в целом будет равна\\n1∣Xm∣(∑xi∈XlL(yi,cl)+∑xi∈XrL(yi,cr))    \\\\frac1{|X_m|}\\\\left(\\\\sum_{x_i\\\\in X_l}L(y_i, c_l) + \\\\sum_{x_i\\\\in X_r}L(y_i, c_r)\\\\right)\\n∣Xm\\u200b∣1\\u200b(xi\\u200b∈Xl\\u200b∑\\u200bL(yi\\u200b,cl\\u200b)+xi\\u200b∈Xr\\u200b∑\\u200bL(yi\\u200b,cr\\u200b))Вопрос на подумать. Как информативность решающего пня связана с информативностью его двух листьев?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Преобразуем выражение:\\n1∣Xm∣(∑xi∈XlL(yi,cl)+∑xi∈XrL(yi,cr))=\\\\frac1{|X_m|}\\\\left(\\\\sum_{x_i\\\\in X_l}L(y_i, c_l) + \\\\sum_{x_i\\\\in X_r}L(y_i, c_r)\\\\right) =\\n∣Xm\\u200b∣1\\u200b(xi\\u200b∈Xl\\u200b∑\\u200bL(yi\\u200b,cl\\u200b)+xi\\u200b∈Xr\\u200b∑\\u200bL(yi\\u200b,cr\\u200b))==1∣Xm∣(∣Xl∣⋅1∣Xl∣∑xi∈XlL(yi,cl)+∣Xr∣⋅1∣Xr∣∑xi∈XrL(yi,cr))= \\\\frac1{|X_m|}\\\\left(|X_l|\\\\cdot\\\\frac{1}{|X_l|}\\\\sum_{x_i\\\\in X_l}L(y_i, c_l) + |X_r|\\\\cdot\\\\frac1{|X_r|}\\\\sum_{x_i\\\\in X_r}L(y_i, c_r)\\\\right)\\n=∣Xm\\u200b∣1\\u200b(∣Xl\\u200b∣⋅∣Xl\\u200b∣1\\u200bxi\\u200b∈Xl\\u200b∑\\u200bL(yi\\u200b,cl\\u200b)+∣Xr\\u200b∣⋅∣Xr\\u200b∣1\\u200bxi\\u200b∈Xr\\u200b∑\\u200bL(yi\\u200b,cr\\u200b))Эта сумма будет минимальна при оптимальном выборе констант clc_lcl\\u200b и crc_rcr\\u200b, и информативность пня будет равна\\n∣Xl∣∣Xm∣H(Xl)+∣Xr∣∣Xm∣H(Xr)\\\\frac{|X_l|}{|X_m|}H(X_l) + \\\\frac{|X_r|}{|X_m|}H(X_r)\\n∣Xm\\u200b∣∣Xl\\u200b∣\\u200bH(Xl\\u200b)+∣Xm\\u200b∣∣Xr\\u200b∣\\u200bH(Xr\\u200b)Теперь для того чтобы принять решение о разделении, мы можем сравнить значение информативности для исходного листа и для получившегося после разделения решающего пня.\\nРазность информативности исходной вершины и решающего пня равна\\nH(Xm)−∣Xl∣∣Xm∣H(Xl)−∣Xr∣∣Xm∣H(Xr)H(X_m) - \\\\frac{|X_l|}{|X_m|}H(X_l) - \\\\frac{|X_r|}{|X_m|}H(X_r)\\nH(Xm\\u200b)−∣Xm\\u200b∣∣Xl\\u200b∣\\u200bH(Xl\\u200b)−∣Xm\\u200b∣∣Xr\\u200b∣\\u200bH(Xr\\u200b)Для симметрии её принято умножить на ∣Xm∣\\\\vert X_m\\\\vert∣Xm\\u200b∣; тогда получится следующий критерий ветвления:\\nBranch(Xm,j,t)=∣Xm∣⋅H(Xm)−∣Xl∣⋅H(Xl)−∣Xr∣⋅H(Xr)\\\\color{#348FEA}{Branch (X_m, j, t) = |X_m| \\\\cdot H(X_m) -  |X_l| \\\\cdot H(X_l) -  |X_r| \\\\cdot H(X_r)}\\nBranch(Xm\\u200b,j,t)=∣Xm\\u200b∣⋅H(Xm\\u200b)−∣Xl\\u200b∣⋅H(Xl\\u200b)−∣Xr\\u200b∣⋅H(Xr\\u200b)Получившаяся величина неотрицательна: ведь, разделив объекты на две кучки и подобрав ответ для каждой, мы точно не сделаем хуже. Кроме того, она тем больше, чем лучше предлагаемый сплит.\\nТеперь посмотрим, какими будут критерии ветвления для типичных задач.\\nИнформативность в задаче регрессии: MSE\\nПосмотрим на простой пример — регрессию с минимизацией среднеквадратичной ошибки:\\nL(yi,c)=(yi−c)2    L(y_i, c) = (y_i -c)^2\\nL(yi\\u200b,c)=(yi\\u200b−c)2Информативность листа будет выглядеть следующим образом:\\nH(Xm)=1∣Xm∣min\\u2061c∈Y∑(xi,yi)∈Xm(yi−c)2    H(X_m) = \\\\frac{1}{\\\\vert X_m\\\\vert} \\\\min\\\\limits_{c \\\\in Y} \\\\sum\\\\limits_{(x_i, y_i) \\\\in X_m} (y_i - c)^2\\nH(Xm\\u200b)=∣Xm\\u200b∣1\\u200bc∈Ymin\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200b(yi\\u200b−c)2Как мы уже знаем, оптимальным предсказанием константного классификатора для задачи минимизации MSE является среднее значение, то есть\\nc=∑yi∣Xm∣    c = \\\\frac{\\\\sum y_i}{|X_m|}\\nc=∣Xm\\u200b∣∑yi\\u200b\\u200bПодставив в формулу информативности сплита, получаем:\\nH(Xm)=∑(xi,yi)∈Xm(yi−y‾)2∣Xm∣,\\xa0где\\xa0y‾=1∣Xm∣∑iyi\\\\color{#348FEA}{H(X_m) = \\\\sum\\\\limits_{(x_i, y_i) \\\\in X_m}\\\\frac{\\\\left(y_i - \\\\overline{y} \\\\right)^2}{|X_m|}, ~ \\\\text{где} ~ \\\\overline{y} = \\\\frac{1}{\\\\vert X_m \\\\vert} \\\\sum_i y_i}\\nH(Xm\\u200b)=(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200b∣Xm\\u200b∣(yi\\u200b−y\\u200b)2\\u200b,\\xa0где\\xa0y\\u200b=∣Xm\\u200b∣1\\u200bi∑\\u200byi\\u200bТо есть при жадной минимизации MSE информативность — это оценка дисперсии таргетов для объектов, попавших в лист. Получается очень стройная картинка: оценка значения в каждом листе — это среднее, а выбирать сплиты надо так, чтобы сумма дисперсий в листьях была как можно меньше.\\nИнформативность в задаче регрессии: MAE\\nL(yi,c)=∣yi−c∣    L(y_i, c) = |y_i -c|\\nL(yi\\u200b,c)=∣yi\\u200b−c∣Случай средней абсолютной ошибки так же прост: в листе надо предсказывать медиану, ведь именно медиана таргетов для обучающих примеров минимизирует MAE констатного предсказателя (мы это обсуждали в параграфе про линейные модели).\\nВ качестве информативности выступает абсолютное отклонение от медианы:\\nH(Xm)=∑(xi,yi)∈Xm∣yi−MEDIAN(Y)∣∣Xm∣H(X_m) = \\\\sum\\\\limits_{(x_i, y_i) \\\\in X_m}\\\\frac{|y_i - MEDIAN(Y)|}{|X_m|}\\nH(Xm\\u200b)=(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200b∣Xm\\u200b∣∣yi\\u200b−MEDIAN(Y)∣\\u200bКритерий информативности в задаче классификации: misclassification error\\nПусть в нашей задаче KKK классов, а pkp_kpk\\u200b — доля объектов класса kkk в текущей вершине XmX_mXm\\u200b:\\npk=1∣Xm∣∑(xi,yi)∈XmI[yi=k]    p_k = \\\\frac{1}{|X_m|} \\\\sum_{(x_i, y_i) \\\\in X_m} \\\\mathbb{I}[ y_i = k ]\\npk\\u200b=∣Xm\\u200b∣1\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200bI[yi\\u200b=k]Допустим, мы заботимся о доле верно угаданных классов, то есть функция потерь — это индикатор ошибки:\\nL(yi,c)=I[yi≠c]    L(y_i, c) = \\\\mathbb{I}[y_i \\\\ne c]\\nL(yi\\u200b,c)=I[yi\\u200b\\ue020=c]Пусть также предсказание модели в листе — один какой-то класс. Информативность для такой функции потерь выглядит так:\\nH(Xm)=min\\u2061c∈Y1∣Xm∣∑(xi,yi)∈XmI[yi≠c]    H(X_m) = \\\\min_{c \\\\in Y} \\\\frac{1}{|X_m|} \\\\sum_{(x_i, y_i) \\\\in X_m} \\\\mathbb{I}[y_i \\\\ne c]\\nH(Xm\\u200b)=c∈Ymin\\u200b∣Xm\\u200b∣1\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200bI[yi\\u200b\\ue020=c]Ясно, что оптимальным предсказанием в листе будет наиболее частотный класс k∗k_{\\\\ast}k∗\\u200b, а выражение для информативности упростится следующим образом:\\nH(Xm)=1∣Xm∣∑(xi,yi)∈XmI[yi≠k∗]=1−pk∗    H(X_m) = \\\\frac{1}{|X_m|} \\\\sum_{(x_i, y_i) \\\\in X_m} \\\\mathbb{I}[y_i \\\\ne k_{\\\\ast}] = 1 - p_{k_{\\\\ast}}\\nH(Xm\\u200b)=∣Xm\\u200b∣1\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200bI[yi\\u200b\\ue020=k∗\\u200b]=1−pk∗\\u200b\\u200bИнформативность в задаче классификации: энтропия\\nЕсли же мы собрались предсказывать вероятностное распределение классов (c1,…,cK)(c_1, \\\\ldots, c_K)(c1\\u200b,…,cK\\u200b), то к этому вопросу можно подойти так же, как мы поступали при выводе логистической регрессии: через максимизацию логарифма правдоподобия (= минимизацию минус логарифма) распределения Бернулли. А именно, пусть в вершине дерева предсказывается фиксированное распределение ccc (не зависящее от xix_ixi\\u200b), тогда правдоподобие имеет вид\\nP(y∣x,c)=P(y∣c)=∏(xi,yi)∈XmP(yi∣c)=∏(xi,yi)∈Xm∏k=1KckI[yi=k],    P(y\\\\vert x, c) = P(y\\\\vert c) = \\\\prod\\\\limits_{(x_i, y_i) \\\\in X_m}P(y_i\\\\vert c) = \\\\prod\\\\limits_{(x_i, y_i) \\\\in X_m}\\n    \\\\prod\\\\limits_{k = 1}^K c_k^{\\\\mathbb{I}[ y_i = k ]},\\nP(y∣x,c)=P(y∣c)=(xi\\u200b,yi\\u200b)∈Xm\\u200b∏\\u200bP(yi\\u200b∣c)=(xi\\u200b,yi\\u200b)∈Xm\\u200b∏\\u200bk=1∏K\\u200bckI[yi\\u200b=k]\\u200b,откуда\\nH(Xm)=min\\u2061∑kck=1(−1∣Xm∣∑(xi,yi)∈Xm∑k=1KI[yi=k]log\\u2061ck)    H(X_m) = \\\\min\\\\limits_{\\\\sum\\\\limits_{k} c_k = 1} \\n    \\\\left( \\n        -\\\\frac{1}{|X_m|} \\\\sum\\\\limits_{(x_i, y_i) \\\\in X_m} \\\\sum\\\\limits_{k = 1}^K \\\\mathbb{I}[ y_i = k ] \\\\log c_k\\n    \\\\right)\\nH(Xm\\u200b)=k∑\\u200bck\\u200b=1min\\u200b\\u200b−∣Xm\\u200b∣1\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200bk=1∑K\\u200bI[yi\\u200b=k]logck\\u200b\\u200bТо, что оценка вероятностей в листе ckc_kck\\u200b, минимизирующая H(Xm)H(X_m)H(Xm\\u200b), должна быть равна pkp_kpk\\u200b, то есть доле попавших в лист объектов этого класса, до некоторой степени очевидно, но это можно вывести и строго.\\nДоказательство для любопытныхИз-за наличия условия на ∑kck=1\\\\sum_k c_k = 1∑k\\u200bck\\u200b=1 нам придётся минимизировать лагранжиан\\nL(c,λ)=min\\u2061c,λ((−1∣Xm∣∑(xi,yi)∈Xm∑k=1KI[yi=k]log\\u2061ck)+λ∑k=1Kck)L(c, \\\\lambda) = \\\\min_{c, \\\\lambda} \\\\left( \\n\\\\left( \\n    -\\\\frac{1}{|X_m|} \\\\sum\\\\limits_{(x_i, y_i) \\\\in X_m} \\\\sum\\\\limits_{k = 1}^K \\\\mathbb{I}[ y_i = k ] \\\\log c_k\\n\\\\right) + \\\\lambda \\\\sum\\\\limits_{k = 1}^K c_k \\\\right) \\nL(c,λ)=c,λmin\\u200b\\u200b\\u200b−∣Xm\\u200b∣1\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200bk=1∑K\\u200bI[yi\\u200b=k]logck\\u200b\\u200b+λk=1∑K\\u200bck\\u200b\\u200bКак обычно, возьмём частную производную и решим уравнение:\\n0=∂∂cjL(c,λ)=    0 = \\\\frac{\\\\partial }{\\\\partial c_j}L(c, \\\\lambda) = \\n0=∂cj\\u200b∂\\u200bL(c,λ)==∂∂cj((−1∣Xm∣∑(xi,yi)∈Xm∑k=1KI[yi=k]log\\u2061ck)+λ∑k=1Kck)== \\\\frac{\\\\partial }{\\\\partial c_j} \\\\left( \\n\\\\left( \\n    -\\\\frac{1}{|X_m|} \\\\sum\\\\limits_{(x_i, y_i) \\\\in X_m} \\\\sum\\\\limits_{k = 1}^K \\\\mathbb{I}[ y_i = k ] \\\\log c_k\\n\\\\right) + \\\\lambda \\\\sum\\\\limits_{k = 1}^K c_k \\\\right) =\\n=∂cj\\u200b∂\\u200b\\u200b\\u200b−∣Xm\\u200b∣1\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200bk=1∑K\\u200bI[yi\\u200b=k]logck\\u200b\\u200b+λk=1∑K\\u200bck\\u200b\\u200b==((−1∣Xm∣∑(xi,yi)∈XmI[yi=j]1cj)+λ)=−pjcj+λ,= \\\\left( \\n\\\\left( \\n    -\\\\frac{1}{|X_m|} \\\\sum\\\\limits_{(x_i, y_i) \\\\in X_m} \\\\mathbb{I}[ y_i = j ] \\\\frac{1}{c_j}\\n\\\\right) + \\\\lambda \\\\right) = - \\\\frac{p_j}{c_j} + \\\\lambda,\\n=\\u200b\\u200b−∣Xm\\u200b∣1\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200bI[yi\\u200b=j]cj\\u200b1\\u200b\\u200b+λ\\u200b=−cj\\u200bpj\\u200b\\u200b+λ,откуда выражаем cj=pjλc_j = \\\\frac{p_j}{\\\\lambda}cj\\u200b=λpj\\u200b\\u200b. Суммируя эти равенства, получим:\\n1=∑k=1Kck=1λ∑k=1Kpk=1λ,1 = \\\\sum_{k = 1}^K c_k = \\\\frac{1}{\\\\lambda} \\\\sum_{k = 1}^K p_k = \\\\frac{1}{\\\\lambda},\\n1=k=1∑K\\u200bck\\u200b=λ1\\u200bk=1∑K\\u200bpk\\u200b=λ1\\u200b,откуда λ=1\\\\lambda = 1λ=1, а значит, ck=pkc_k = p_kck\\u200b=pk\\u200b.\\nПодставляя вектор c=(p1,…,pK)c = (p_1,\\\\ldots,p_K)c=(p1\\u200b,…,pK\\u200b) в выражение выше, мы в качестве информативности получим энтропию распределения классов:\\nH(Xm)=−∑k=1Kpklog\\u2061pk\\\\color{#348FEA}{H(X_m) = -\\\\sum_{k = 1}^K p_k \\\\log p_k}\\nH(Xm\\u200b)=−k=1∑K\\u200bpk\\u200blogpk\\u200bНемного подробнее об энтропииВеличина\\nH(Xm)=−∑kpklog\\u2061pkH(X_m) = -\\\\sum_k p_k \\\\log p_k\\nH(Xm\\u200b)=−k∑\\u200bpk\\u200blogpk\\u200bназывается информационной энтропией Шеннона и измеряет непредсказуемость реализации случайной величины. В оригинальном определении, правда, речь шла не о значениях случайной величины, а о символах (первичного) алфавита, так как Шеннон придумал эту величину, занимаясь вопросами кодирования строк. Для данной задачи энтропия имеет вполне практический смысл — среднее количество битов, которое необходимо для кодирования одного символа сообщения при заданной частоте символов алфавита.\\nТак как pk∈[0,1]p_k\\\\in[0,1]pk\\u200b∈[0,1], энтропия неотрицательна. Если случайная величина принимает только одно значение, то она абсолютно предсказуема и её энтропия равна\\n−1log\\u2061(1)=0- 1\\\\log(1) = 0\\n−1log(1)=0Наибольшего значения энтропия достигает для равномерно распределённой случайной величины — и это отражает тот факт, что среди всех величин с данной областью значений она наиболее «непредсказуема». Для равномерно распределённой на множестве {1,…,K}\\\\{1,\\\\ldots,K\\\\}{1,…,K} случайной величины значение энтропии будет равно:\\n−∑k=1K1Klog\\u20611K=log\\u2061K-\\\\sum_{k = 1}^K \\\\frac{1}{K}\\\\log\\\\frac{1}{K} = \\\\log K−∑k=1K\\u200bK1\\u200blogK1\\u200b=logK\\nНа следующем графике приведены три дискретных распределения на множестве {0,1,…,20}\\\\{0, 1, \\\\ldots, 20\\\\}{0,1,…,20} с их энтропиями. Как и указано выше, максимальную энтропию будет иметь равномерное распределение; у двух других проявляются пики разной степени остроты — и тем самым реализации этих величин обладают меньшей неопределённостью: мы можем с большей уверенностью говорить, что будет сгенерировано.\\n\\nРазберём на примере игрушечной задачи классификации то, как энтропия может выступать в роли impurity. Рассмотрим три разбиения синтетического датасета и посмотрим, какие значения энтропии они дают. В подписях указано, каким становится соотношение классов в половинках.\\n\\nВ изначальном датасете по 25 точек каждого класса; энтропия состояния равна\\nS0=−2550log\\u206122550−2550log\\u206122550=1S_0 = -\\\\frac{25}{50}\\\\log_2{\\\\frac{25}{50}}-\\\\frac{25}{50}\\\\log_2{\\\\frac{25}{50}} = 1\\nS0\\u200b=−5025\\u200blog2\\u200b5025\\u200b−5025\\u200blog2\\u200b5025\\u200b=1Для первого разбиения, по [X1⩽3][X_1 \\\\leqslant 3][X1\\u200b⩽3] в левую часть попадают 252525 точек класса 0 и 121212 точек класса 1, а в правую — 000 точек класса 0 и 131313 точек класса 1. Энтропия левой группы равна\\nSl=−2537log\\u206122537−1237log\\u206121237≈0.9S_l = -\\\\frac{25}{37}\\\\log_2{\\\\frac{25}{37}}-\\\\frac{12}{37}\\\\log_2{\\\\frac{12}{37}}\\\\approx 0.9\\nSl\\u200b=−3725\\u200blog2\\u200b3725\\u200b−3712\\u200blog2\\u200b3712\\u200b≈0.9Энтропия правой группы, строго говоря, не определена (под логарифмом ноль), но если заменить несуществующее значение на lim\\u2061t→0+tlog\\u20612t=0\\\\lim_{t\\\\rightarrow 0+}t\\\\log_2{t} = 0limt→0+\\u200btlog2\\u200bt=0, то получится\\nSr=−0−1log\\u206121=0S_r = - 0 - 1\\\\log_2{1}=0\\nSr\\u200b=−0−1log2\\u200b1=0Что, в принципе, логично: с вероятностью 1 выпадает единица, мы всегда уверены в результате, и энтропия у такого, вырожденного распределения тоже минимальная, равная нулю.\\nКак можно заметить, энтропия в обеих группах уменьшилась по сравнению с изначальной. Получается, что, разделив шарики по значению координаты, равному 3, мы уменьшили общую неопределённость системы. Но какое из разбиений даёт самое радикальное уменьшение? После несложных вычислений, мы получаем для нарисованных выше разбиений:\\nBranch(1)≈16.4Branch^{(1)} \\\\approx 16.4\\nBranch(1)≈16.4Branch(2)≈30.5Branch^{(2)} \\\\approx 30.5\\nBranch(2)≈30.5Branch(3)≈7Branch^{(3)} \\\\approx 7\\nBranch(3)≈7Ожидаемо, не так ли? Второе разбиение практически идеально разделяет классы, делая из исходного, почти равномерного распределения, два почти вырожденных. При остальных разбиениях в каждой из половинок неопределённость тоже падает, но не так сильно.\\nКстати, Шеннон изначально собирался назвать информационную энтропию или «информацией», или «неопределённостью», но в итоге выбрал название «энтропия», потому что концепция со схожим смыслом в статистической механике уже была названа энтропией. Употребление термина из другой научной области выглядело убедительным преимуществом при ведении научных споров.\\nИнформативность в задаче классификации: критерий Джини\\nПусть предсказание модели — это распределение вероятностей классов (c1,…,ck)(c_1, \\\\ldots, c_k)(c1\\u200b,…,ck\\u200b). Вместо логарифма правдоподобия в качестве критерия можно выбрать, например, метрику Бриера (за которой стоит всего лишь идея посчитать MSE от вероятностей). Тогда информативность получится равной\\nH(Xm)=min\\u2061∑kck=11∣Xm∣∑(xi,yi)∈Xm∑k=1K(ck−I[yi=k])2H(X_m) = \\\\min_{\\\\sum_k c_k = 1} \\\\frac{1}{|X_m|} \\\\sum_{(x_i, y_i) \\\\in X_m} \\\\sum_{k = 1}^K (c_k - \\\\mathbb{I}[ y_i = k ] )^2\\nH(Xm\\u200b)=∑k\\u200bck\\u200b=1min\\u200b∣Xm\\u200b∣1\\u200b(xi\\u200b,yi\\u200b)∈Xm\\u200b∑\\u200bk=1∑K\\u200b(ck\\u200b−I[yi\\u200b=k])2Можно показать, что оптимальное значение этой метрики, как и в случае энтропии, достигается на векторе ccc, состоящем из выборочных оценок частот классов (p1,…,pk)(p_1, \\\\ldots, p_k)(p1\\u200b,…,pk\\u200b), pi=1∣Xm∣∑iI[yi=k]p_i = \\\\frac{1}{\\\\vert X_m\\\\vert}\\\\sum_i \\\\mathbb{I}[ y_i = k ]pi\\u200b=∣Xm\\u200b∣1\\u200b∑i\\u200bI[yi\\u200b=k]. Если подставить (p1,…,pk)(p_1, \\\\ldots, p_k)(p1\\u200b,…,pk\\u200b) в выражение выше и упростить его, получится критерий Джини:\\nH(Xm)=∑k=1Kpk(1−pk)\\\\color{#348FEA}{H(X_m) = \\\\sum_{k = 1}^K p_k (1 - p_k)}\\nH(Xm\\u200b)=k=1∑K\\u200bpk\\u200b(1−pk\\u200b)Критерий Джини допускает и следующую интерпретацию: H(Xm)H(X_m)H(Xm\\u200b) равно математическому ожиданию числа неправильно классифицированных объектов в случае, если мы будем приписывать им случайные метки из дискретного распределения, заданного вероятностями (p1,…,pk)(p_1, \\\\ldots, p_k)(p1\\u200b,…,pk\\u200b).\\nНеоптимальность полученных критериев\\nКазалось бы, мы вывели критерии информативности для всех популярных задач, и они довольно логично следуют из их постановок, но получилось ли у нас обмануть NP-полноту и научиться строить оптимальные деревья легко и быстро?\\nКонечно, нет. Простейший пример — решение задачи XOR с помощью жадного алгоритма и любого критерия, который мы построили выше:\\n\\n\\n\\nИсточник\\n\\n\\nВне зависимости от того, что вы оптимизируете, жадный алгоритм не даст оптимального решения задачи XOR. Но этим примером проблемы не исчерпываются. Скажем, бывают ситуации, когда оптимальное с точки зрения выбранной метрики дерево вы получите с критерием ветвления, построенным по другой метрике (например, MSE-критерий для MAE-задачи или Джини для misclassification error).\\nОсобенности данных\\nКатегориальные признаки\\nНа первый взгляд, деревья прекрасно могут работать с категориальными переменными. А именно, если признак xix^ixi принимает значения из множества C=c1,…,cMC = {c_1,\\\\ldots,c_M}C=c1\\u200b,…,cM\\u200b, то при очередном разбиении мы можем рассматривать по этому признаку произвольные сплиты вида C=Cl⊔CrC = C_l\\\\sqcup C_rC=Cl\\u200b⊔Cr\\u200b (предикат будет иметь вид [xi∈Cr][x^i\\\\in C_r][xi∈Cr\\u200b]). Это очень логично и естественно, но проблема в том, что при больших MMM у нас будет 2M−1−12^{M-1}-12M−1−1 сплитов, и перебирать их будет слишком долго. Было бы здорово уметь каким-то образом упорядочивать значения cmc_mcm\\u200b, чтобы работать с ними так же, как с обычными числами: разделяя на значения, «не превосходящие» и «большие» определённого порога.\\nОказывается, что для некоторых задач такое упорядочение можно построить вполне естественным образом.\\nТак, для задачи бинарной классификации значения cmc_mcm\\u200b можно упорядочить по неубыванию доли объектов класса 1 с xi=cmx^i = c_mxi=cm\\u200b, после чего работать с ними, как со значениями вещественного признака. Показано, что в случае, если мы выбираем таким образом сплит, оптимальный с точки зрения энтропийного критерия или критерия Джини, то он будет оптимальным среди всех 2M−1−12^{M-1}-12M−1−1 сплитов.\\nДля задачи регрессии с функцией потерь MSE значения cmc_mcm\\u200b можно упорядочивать по среднему значению таргета на подмножестве X∣xi=cm\\\\\\\\{X\\\\mid x^i = c_m\\\\\\\\}X∣xi=cm\\u200b. Полученный таким образом сплит тоже будет оптимальным.\\nРабота с пропусками\\nОдна из приятных особенностей деревьев — это способность обрабатывать пропуски в данных. Разберёмся, что при этом происходит на этапе обучения и на этапе применения дерева.\\nПусть у нас есть некоторый признак xix^ixi, значение которого пропущено у некоторых объектов. Как обычно, обозначим через XmX_mXm\\u200b множество объектов, пришедших в рассматриваемую вершину, а через VmV_mVm\\u200b — подмножество XmX_mXm\\u200b, состоящее из объектов с пропущенным значением xix^ixi. В момент выбора сплитов по этому признаку мы будем просто игнорировать объекты из VmV_mVm\\u200b, а когда сплит выбран, мы отправим их в оба поддерева. При этом логично присвоить им веса: ∣Xl∣∣Xm∣\\\\frac{\\\\vert X_l\\\\vert}{\\\\vert X_m\\\\vert}∣Xm\\u200b∣∣Xl\\u200b∣\\u200b для левого поддерева и ∣Xr∣∣Xm∣\\\\frac{\\\\vert X_r\\\\vert}{\\\\vert X_m\\\\vert}∣Xm\\u200b∣∣Xr\\u200b∣\\u200b для правого. Веса будут учитываться как коэффициенты при L(yi,c)L(y_i, c)L(yi\\u200b,c) в формуле информативности.\\nВопрос на подумать. Во всех критериях ветвления участвуют мощности множеств XmX_mXm\\u200b, XlX_lXl\\u200b и XrX_rXr\\u200b. Нужно ли уменьшение размера выборки учитывать в формулах для информативности? Если нужно, то как?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Чтобы ответить на этот вопрос, вспомним, как работают критерии ветвления. Их суть в сравнении информативности H(Xm)H(X_m)H(Xm\\u200b) для исходной вершины с информативностью для решающего пня, порождённого рассматриваемым сплитом. Величина H(Xm)H(X_m)H(Xm\\u200b) никак не меняется, надо понять, что будет с пнём. Для него можно посчитать информативность так, словно объектов с пропущенными значениями нет:\\n1∣Xm∖Vm∣(∑xi∈Xl∖VmL(yi,cl)+∑xi∈Xr∖VmL(yi,cr))\\\\frac1{|X_m\\\\setminus V_m|}\\\\left(\\\\sum_{x_i\\\\in X_l\\\\setminus V_m}L(y_i, c_l) + \\\\sum_{x_i\\\\in X_r\\\\setminus V_m}L(y_i, c_r)\\\\right)\\n∣Xm\\u200b∖Vm\\u200b∣1\\u200b\\u200bxi\\u200b∈Xl\\u200b∖Vm\\u200b∑\\u200bL(yi\\u200b,cl\\u200b)+xi\\u200b∈Xr\\u200b∖Vm\\u200b∑\\u200bL(yi\\u200b,cr\\u200b)\\u200bТогда в критерии ветвления нужно поменять коэффициенты, иначе мы не будем адекватным образом сравнивать такой сплит со сплитами по другим признакам:\\nBranch=∣Xm∖Vm∣⋅H(Xm)−∣Xl∖Vm∣⋅H(Xl∖Vm)−∣Xr∖Vm∣⋅H(Xr∖Vm)Branch = |X_m\\\\setminus V_m|\\\\cdot H(X_m) - |X_l\\\\setminus V_m|\\\\cdot H(X_l\\\\setminus V_m) - |X_r\\\\setminus V_m|\\\\cdot H(X_r\\\\setminus V_m)\\nBranch=∣Xm\\u200b∖Vm\\u200b∣⋅H(Xm\\u200b)−∣Xl\\u200b∖Vm\\u200b∣⋅H(Xl\\u200b∖Vm\\u200b)−∣Xr\\u200b∖Vm\\u200b∣⋅H(Xr\\u200b∖Vm\\u200b)Если же мы предполагаем, что объекты из VmV_mVm\\u200b отправляются в новые листья с весами, как описано выше, то формула оказывается другой, и коэффициенты можно не менять:\\n1∣Xm∣(∑xi∈Xl∖VmL(yi,cl)+∣Xl∣∣Xm∣∑xi∈VmL(yi,cl)⏟Левый\\xa0лист+∑xi∈Xr∖VmL(yi,cr)+∣Xr∣∣Xm∣∑xi∈VmL(yi,cr)⏟Правый\\xa0лист)\\\\frac1{|X_m|}\\\\left(\\\\underbrace{\\\\sum_{x_i\\\\in X_l\\\\setminus V_m}L(y_i, c_l) + \\\\frac{|X_l|}{|X_m|}\\\\sum_{x_i\\\\in V_m}L(y_i, c_l)}_{\\\\text{Левый лист}} + \\\\underbrace{\\\\sum_{x_i\\\\in X_r\\\\setminus V_m}L(y_i, c_r) + \\\\frac{|X_r|}{|X_m|}\\\\sum_{x_i\\\\in V_m}L(y_i, c_r)}_{\\\\text{Правый лист}}\\\\right)\\n∣Xm\\u200b∣1\\u200b\\u200bЛевый\\xa0листxi\\u200b∈Xl\\u200b∖Vm\\u200b∑\\u200bL(yi\\u200b,cl\\u200b)+∣Xm\\u200b∣∣Xl\\u200b∣\\u200bxi\\u200b∈Vm\\u200b∑\\u200bL(yi\\u200b,cl\\u200b)\\u200b\\u200b+Правый\\xa0листxi\\u200b∈Xr\\u200b∖Vm\\u200b∑\\u200bL(yi\\u200b,cr\\u200b)+∣Xm\\u200b∣∣Xr\\u200b∣\\u200bxi\\u200b∈Vm\\u200b∑\\u200bL(yi\\u200b,cr\\u200b)\\u200b\\u200b\\u200bТеперь рассмотрим этап применения дерева. Допустим, в вершину, где сплит идёт по iii-му признаку, пришёл объект x0x_0x0\\u200b с пропущенным значением этого признака. Предлагается отправить его в каждую из дальнейших веток и получить по ним предсказания y^l\\\\widehat{y}_ly\\u200bl\\u200b и y^r\\\\widehat{y}_ry\\u200br\\u200b. Эти предсказания мы усредним с весами ∣Xl∣∣Xm∣\\\\frac{\\\\vert X_l\\\\vert}{\\\\vert X_m\\\\vert}∣Xm\\u200b∣∣Xl\\u200b∣\\u200b и ∣Xr∣∣Xm∣\\\\frac{\\\\vert X_r\\\\vert}{\\\\vert X_m\\\\vert}∣Xm\\u200b∣∣Xr\\u200b∣\\u200b (которые мы запомнили в ходе обучения):\\ny^=∣Xl∣∣Xm∣y^l+∣Xr∣∣Xm∣y^r\\\\widehat{y} = \\\\frac{\\\\vert X_l\\\\vert}{\\\\vert X_m\\\\vert}\\\\widehat{y}_l + \\\\frac{\\\\vert X_r\\\\vert}{\\\\vert X_m\\\\vert}\\\\widehat{y}_r\\ny\\u200b=∣Xm\\u200b∣∣Xl\\u200b∣\\u200by\\u200bl\\u200b+∣Xm\\u200b∣∣Xr\\u200b∣\\u200by\\u200br\\u200bДля задачи регрессии это сразу даст нам таргет, а в задаче бинарной классификации — оценку вероятности класса 1.\\nЗамечание. Если речь идёт о категориальном признаке, может оказаться хорошей идеей ввести дополнительное значение «пропущено» для категориального признака и дальше работать с пропусками, как с обычным значением. Особенно это актуально в ситуациях, когда пропуски имеют системный характер и их наличие несёт в себе определённую информацию.\\nМетоды регуляризации решающих деревьев\\nМы уже упоминали выше, что деревья легко переобучаются и процесс ветвления надо в какой-то момент останавливать.\\nДля этого есть разные критерии, обычно используются все сразу:\\n\\nограничение по максимальной глубине дерева;\\nограничение на минимальное количество объектов в листе;\\nограничение на максимальное количество листьев в дереве;\\nтребование, чтобы функционал качества BranchBranchBranch при делении текущей подвыборки на две улучшался не менее чем на sss процентов.\\n\\nДелать это можно на разных этапах работы алгоритма, что не меняет сути, но имеет разные устоявшиеся названия:\\n\\nможно проверять критерии прямо во время построения дерева, такой способ называется pre-pruning или early stopping;\\nа можно построить дерево жадно без ограничений, а затем провести стрижку (pruning), то есть удалить некоторые вершины из дерева так, чтобы итоговое качество упало не сильно, но дерево начало подходить под условия регуляризации. При этом качество стоит измерять на отдельной, отложенной выборке.\\n\\nАлгоритмические трюки\\nТеперь временно снимем шапочку ML-аналитика, наденем шапочку разработчика и специалиста по computer science и посмотрим, как можно сделать полученный алгоритм более вычислительно эффективным.\\nВ базовом алгоритме мы в каждой вершине дерева для всех возможных значений сплитов вычисляем информативность. Если в вершину пришло qqq объектов, то мы рассматриваем qDqDqD сплитов и для каждого тратим O(q)O(q)O(q) операций на подсчёт информативности. Отметим, что в разных вершинах, находящихся в нашем дереве на одном уровне, оказываются разные объекты, то есть сумма этих qqq по всем вершинам заданного уровня не превосходит NNN, а значит, выбор сплитов во всех вершинах уровня потребует O(N2D)O(N^2 D)O(N2D) операций.\\nТаким образом, общая сложность построения дерева — O(hN2D)O(hN^2 D)O(hN2D) (где hhh — высота дерева), и доминирует в ней перебор всех возможных предикатов на каждом уровне построения дерева. Посмотрим, что с этим можно сделать.\\nДинамическое программирование\\nПостараемся оптимизировать процесс выбора сплита в одной конкретной вершине.\\nВместо того чтобы рассматривать все O(ND)O(ND)O(ND) возможных сплитов, для каждого тратя O(N)O(N)O(N) на вычисление информативности, можно использовать одномерную динамику. Для этого заметим, что если отсортировать объекты по какому-то признаку, то, проходя по отсортированному массиву, можно одновременно и перебирать все значения предикатов, и поддерживать все необходимые статистики для пересчёта значений информативности за O(1)O(1)O(1) для каждого следующего варианта сплита (против изначальных O(N)O(N)O(N)).\\nДавайте разберём, как это работает, на примере построения дерева для MSE. Чтобы оценить информативность для листа, нам нужно знать несколько вещей:\\n\\nдисперсию и среднее значение таргета в текущем листе;\\nдисперсию и среднее значение таргета в обоих потомках для каждого потенциального значения сплита.\\n\\nДисперсию и среднее текущего листа легко посчитать за O(n)O(n)O(n).\\nС дисперсией и средним для всех значений сплитов чуть сложнее, но помогут следующие оценки математического ожидания и дисперсии:\\nY‾=1N∑yi,\\\\overline{Y} = \\\\frac1N\\\\sum y_i,\\nY=N1\\u200b∑yi\\u200b,σ2(Y)=Y2‾−(Y‾)2=1N∑yi2−1N2(∑yi)2\\\\sigma^2 (Y) = \\\\overline{Y^2} - (\\\\overline{Y})^2 = \\\\frac1N\\\\sum y_i^2 - \\\\frac{1}{N^2}(\\\\sum y_i)^2 \\nσ2(Y)=Y2−(Y)2=N1\\u200b∑yi2\\u200b−N21\\u200b(∑yi\\u200b)2Следовательно, нам достаточно для каждого потенциального значения сплита знать количество элементов в правом и левом поддеревьях, их сумму и сумму их квадратов. Впрочем, всё это необходимо знать только для одной из половинок сплита, а для второй это можно получить, вычитая значения для первой из полных сумм. Это можно сделать за один проход по массиву, просто накапливая значения частичных сумм.\\nЕсли в вершину дерева пришло qqq объектов, сложность построения одного сплита складывается из DDD сортировок каждая по O(qlog\\u2061q)O(q\\\\log q)O(qlogq) и одного линейного прохода с динамикой, всего O(qDlog\\u2061q+qD)=O(qDlog\\u2061q)O(qD\\\\log q + qD) = O(qD\\\\log q)O(qDlogq+qD)=O(qDlogq), что лучше исходного O(q2D)O(q^2D)O(q2D). Итоговая сложность алгоритма построения дерева — O(hNDlog\\u2061N)O(hND\\\\log N)O(hNDlogN) (где hhh – высота дерева) против hN2DhN^2DhN2D в наивной его версии.\\nКакие именно статистики накапливать (средние, медианы, частоты), зависит от критерия, который вы используете.\\nГистограммный метод\\nЕсли бы мощность множества значений признаков была ограничена какой-то разумной константой b≪Nb \\\\ll Nb≪N, то сортировку в предыдущем способе можно было бы заменить сортировкой подсчётом и за счёт этого существенно ускорить алгоритм: ведь сложность такой сортировки — O(N)O(N)O(N).\\nЧтобы провернуть это с любой выборкой, мы можем искусственно дискретизировать значения всех признаков. Это приведёт к локально менее оптимальным значениям сплитов, но, учитывая, что наш алгоритм и без этого был весьма приблизительным, это не ухудшит ничего драматически, а вот ускорение получается очень неплохое.\\nСамый популярный и простой способ дискретизации основан на частотах значений признаков: отрезок между максимальным и минимальным значением признака разбивается на bbb подотрезков, длины которых выбираются так, чтобы в каждый попадало примерно равное число обучающих примеров. После чего значения признака заменяются на номера отрезков, на которые они попали.\\n\\nАналогичная процедура проводится для всех признаков выборки. Полная сложность предобработки — O(DNlog\\u2061N)O(DN\\\\log N)O(DNlogN) — сортировка за O(Nlog\\u2061N)O(N\\\\log N)O(NlogN) для каждого из DDD признаков.\\nТеперь в процедуре динамического алгоритма поиска оптимального сплита нам надо перебирать не все NNN объектов выборки, а всего лишь bbb подготовленных заранее границ подотрезков. Частичные суммы статистик тоже придётся поддерживать не для исходного массива данных, а для списка из bbb возможных сплитов. А для того чтобы делать это эффективно, необходим объект, называемый гистограммой: упорядоченный словарь, сопоставляющий каждому значению дискретизированного признака сумму необходимой статистики от таргета на отрезке [B[i-1], B[i]].\\nФинальный вид алгоритма таков:\\n\\nДискретизируем каждый из признаков на bbb значений. Сложность O(DNlog\\u2061N)O(DN\\\\log N)O(DNlogN).\\nСоздаём корневую вершину root.\\nВызываем build_tree_recursive(root, data).\\n\\nФункция build_tree_recursive выглядит следующим образом:\\n\\nПроверяем, не пора ли остановиться. Если пора — считаем значение в листе.\\nТеперь мы снова используем динамический алгоритм, но объекты будем сортировать не по исходным значениям признаков, а по их дискретизированным версиям, упорядочивая их с помощью сортировки подсчётом (для вершины, в которую попало qqq объектов, сложность будет равна O(qD)O(qD)O(qD) против O(qlog\\u2061q⋅D)O(q\\\\log q\\\\cdot D)O(qlogq⋅D) в стандартной динамике).\\nНаходим оптимальный сплит за O(qD)O(qD)O(qD).\\nДелим данные, запускаем процедуру рекурсивно для обоих поддеревьев.\\n\\nОбщая сложность: O(DNlog\\u2061N+hND)O(DN\\\\log N + hND)O(DNlogN+hND)\\nГистограммный метод на примереДавайте посчитаем информативность сплита для MSE на данных с одним признаком, по которому мы уже отсортировали данные:\\n\\n\\n\\n\\nx\\n\\n\\n-3\\n\\n\\n-2\\n\\n\\n-0.05\\n\\n\\n1\\n\\n\\n1\\n\\n\\n2\\n\\n\\n6\\n\\n\\n8\\n\\n\\n\\n\\ny\\n\\n\\n0\\n\\n\\n0.5\\n\\n\\n-1\\n\\n\\n0\\n\\n\\n1\\n\\n\\n2\\n\\n\\n1\\n\\n\\n4\\n\\n\\n\\n\\nДискретизируем на три отрезка с границами B=[−1,1.5]B = [-1, 1.5]B=[−1,1.5]\\n\\n\\n\\n\\ndiscretized_x\\n\\n\\n0\\n\\n\\n0\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n2\\n\\n\\n2\\n\\n\\n2\\n\\n\\n\\n\\ny\\n\\n\\n0\\n\\n\\n0.5\\n\\n\\n-1\\n\\n\\n0\\n\\n\\n1\\n\\n\\n2\\n\\n\\n1\\n\\n\\n4\\n\\n\\n\\n\\nСтроим гистограмму частичных сумм, сумм квадратов и количества объектов для двух отрезков (для третьего можно посчитать, используя общую сумму):\\n\\n\\n\\n\\nb\\n\\n\\n0\\n\\n\\n1\\n\\n\\n\\n\\ncnt\\n\\n\\n2\\n\\n\\n3\\n\\n\\n\\n\\nsum_y\\n\\n\\n0.5\\n\\n\\n0\\n\\n\\n\\n\\nsum_y_sq\\n\\n\\n0.25\\n\\n\\n2\\n\\n\\n\\n\\nВсего объектов восемь, сумма таргетов — \\xa0sum_total\\xa0=7.5\\\\text { sum\\\\_total }=7.5\\xa0sum_total\\xa0=7.5, сумма квадратов — \\xa0sum_sq_total\\xa0=23.25\\\\text { sum\\\\_sq\\\\_total }=23.25\\xa0sum_sq_total\\xa0=23.25.\\nИнформативность текущего листа:\\nD[Xd]=\\xa0sum_sq_total\\xa0\\xa0cnt_total\\xa0−(\\xa0sum_total\\xa0\\xa0cnt_total\\xa0)2=D\\\\left[X_d\\\\right]=\\\\frac{\\\\text { sum\\\\_sq\\\\_total }}{\\\\text { cnt\\\\_total }}-\\\\left(\\\\frac{\\\\text { sum\\\\_total }}{\\\\text { cnt\\\\_total }}\\\\right)^2=\\nD[Xd\\u200b]=\\xa0cnt_total\\xa0\\xa0sum_sq_total\\xa0\\u200b−(\\xa0cnt_total\\xa0\\xa0sum_total\\xa0\\u200b)2==23.258−(7.58)2=2.0273= \\\\frac{23.25}{8} - \\\\left( \\\\frac{7.5}{8} \\\\right)^{2} = 2.0273\\n=823.25\\u200b−(87.5\\u200b)2=2.0273Посчитаем значение критерия ветвления для b=−1b=-1b=−1. Информативность левого листа, то есть дисперсия в нём, равна:\\nD[Xl]=E[Xl2]−E2[Xl]=D[X_l] = E[X_l^2] - E^2[X_l] = \\\\\\\\ \\nD[Xl\\u200b]=E[Xl2\\u200b]−E2[Xl\\u200b]==su_y_s1[0]cnt\\u2061[0]−(\\xa0sum\\xa0_ycnt\\u2061[0])2==\\\\frac{s_u \\\\_y_{\\\\_} s_1[0]}{\\\\operatorname{cnt}[0]}-\\\\left(\\\\frac{\\\\text { sum } \\\\_y}{\\\\operatorname{cnt}[0]}\\\\right)^2=\\n=cnt[0]su\\u200b_y_\\u200bs1\\u200b[0]\\u200b−(cnt[0]\\xa0sum\\xa0_y\\u200b)2==0.125−0.252=0.0625= 0.125 - 0.25 ^ 2 = 0.0625\\n=0.125−0.252=0.0625Информативность правого листа, то есть дисперсия в нём, равна:\\nD[Xr]=E[Xr2]−E2[Xr]=D[X_r] = E[X_r^2] - E^2[X_r] = \\\\\\\\ \\nD[Xr\\u200b]=E[Xr2\\u200b]−E2[Xr\\u200b]==\\xa0sum\\xa0_sq_total\\xa0−\\xa0sum\\xa0_y_sq[0]\\xa0cnt_total\\xa0−cnt\\u2061[0]−(\\xa0sum_total\\xa0−sum\\u2061_y\\xa0cnt_total\\xa0−\\xa0cnt\\xa0[0])2≈2.47=\\\\frac{\\\\text { sum } \\\\_ \\\\text {sq\\\\_total }- \\\\text { sum } \\\\_y \\\\_s q[0]}{\\\\text { cnt\\\\_total }-\\\\operatorname{cnt}[0]}-\\\\left(\\\\frac{\\\\text { sum\\\\_total }-\\\\operatorname{sum} \\\\_y}{\\\\text { cnt\\\\_total }- \\\\text { cnt }[0]}\\\\right)^2 \\\\approx 2.47\\n=\\xa0cnt_total\\xa0−cnt[0]\\xa0sum\\xa0_sq_total\\xa0−\\xa0sum\\xa0_y_sq[0]\\u200b−(\\xa0cnt_total\\xa0−\\xa0cnt\\xa0[0]\\xa0sum_total\\xa0−sum_y\\u200b)2≈2.47Полное значение критерия для разбиения по b=−1b=-1b=−1:\\nBranch(b=−1)=8⋅D[Xd]−2⋅D[Xl]−6⋅D[Xr]=  Branch(b = -1) = 8 \\\\cdot D[X_d] - 2 \\\\cdot D[X_l] - 6 \\\\cdot D[X_r] = \\\\\\\\ \\nBranch(b=−1)=8⋅D[Xd\\u200b]−2⋅D[Xl\\u200b]−6⋅D[Xr\\u200b]==1.2734  = 1.2734\\n=1.2734Mixed integer optimization\\nЕсли вам действительно хочется построить оптимальное (или хотя бы очень близкое к оптимальному) дерево, то на сегодня для решения этой проблемы не нужно придумывать кучу эвристик самостоятельно, а можно воспользоваться специальными солверами, которые решают NP-полные задачи приближённо, но всё-таки почти точно. Так что единственной (и вполне решаемой) проблемой будет представить исходную задачу в понятном для солвера виде. По ссылке — пример построения оптимального дерева с помощью решения задачи целочисленного программирования.\\nИсторическая справка\\nКак вы, может быть, уже заметили, решающие деревья — это одна большая эвристика для решения NP-полной задачи, практически лишённая какой-либо стройной теоретической подоплёки. В 1970–1990-e годы интерес к ним был весьма велик как в индустрии, где был полезен хорошо интерпретируемый классификатор, так и в науке, где учёные интересовались способами приближённого решения NP-полных задач.\\nВ связи с этим сложилось много хорошо работающих наборов эвристик, у которых даже были имена: например, ID3 был первой реализацией дерева, минимизирующего энтропию, а CART — первым деревом для регрессии. Некоторые из них были запатентованы и распространялись коммерчески.\\nНа сегодня это всё потеряло актуальность в связи с тем, что существуют хорошо написанные библиотеки (например, sklearn, в которой реализована оптимизированная версия CART).\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф2.2. Метрические методыАлгоритмы KNN. Быстрый поиск ближайших соседейСледующий параграф2.4. Ансамбли в машинном обученииКак смешать несколько моделей в\\xa0одну. Стэкинг, бэггинг, случайные лесаЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_21.html', 'title': 'Первое знакомство с полносвязными нейросетями'}, page_content='Первое знакомство с полносвязными нейросетямиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/45.1.Нейронные сети5.2.Первое знакомство с полносвязными нейросетямиForward & backward propagationАрхитектуры для простейших задачПопулярные функции активацииНемного о мощи нейросетей5.3.Метод обратного распространения ошибки5.4.Тонкости обучения6.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Первое знакомство с полносвязными нейросетями5.2. Первое знакомство с полносвязными нейросетямиАвторыСиницин ФилиппНейчев РадославОсновные понятия глубинного обучения. Базовые слои и\\xa0функции активацииОсновные определения\\nИскусственная нейронная сеть (далее — нейронная сеть) — это сложная дифференцируемая функция, задающая отображение из исходного признакового пространства в пространство ответов, все параметры которой могут настраиваться одновременно и взаимосвязанно (то есть сеть может обучаться end-to-end).\\nВ частном (и наиболее частом) случае представляет собой последовательность дифференцируемых параметрических преобразований.\\nВнимательный читатель может заметить, что под указанное выше определение нейронной сети подходят и логистическая, и линейная регрессия. Это верное замечание: и линейная, и логистическая регрессии могут рассматриваться как нейронные сети, задающие отображения в пространство ответов и логитов соответственно.\\nСложную функцию удобно представлять в виде суперпозиции простых, и нейронные сети обычно предстают перед программистом в виде конструктора, состоящего из более-менее простых блоков (слоёв, layers). Вот две простейшие их разновидности:\\n\\n\\nЛинейный слой (linear layer, dense layer) — линейное преобразование над входящими данными. Его обучаемые параметры — это матрица WWW и вектор bbb: x↦xW+bx \\\\mapsto xW + bx↦xW+b (W∈Rd×k,x∈Rd,b∈RkW \\\\in \\\\mathbb{R}^{d \\\\times k}, x \\\\in \\\\mathbb{R}^{d}, b \\\\in \\\\mathbb{R}^{k}W∈Rd×k,x∈Rd,b∈Rk). Такой слой преобразует ddd-мерные векторы в kkk-мерные.\\n\\n\\nФункция активации (activation function) — нелинейное преобразование, поэлементно применяющееся к пришедшим на вход данным. Благодаря функциям активации нейронные сети способны порождать более информативные признаковые описания, преобразуя данные нелинейным образом. Может использоваться, например, ReLU (rectified linear unit) ReLU(x)=max(0,x)\\\\text{ReLU}(x) = \\\\text{max}(0, x)ReLU(x)=max(0,x) или уже знакомая вам из логистической регрессии сигмоида σ(x)=11+e−x\\\\sigma(x) = \\\\frac1{1 + e^{-x}}σ(x)=1+e−x1\\u200b. К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее.\\n\\n\\nДаже самые сложные нейронные сети обычно собираются из относительно простых блоков, подобных этим. Таким образом, их можно представить в виде вычислительного графа (computational graph), где промежуточным вершинам соответствуют преобразования. На иллюстрации ниже приведён вычислительный граф для логистической регрессии.\\nВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nНе правда ли, похоже на слоёный пирог из преобразований? Отсюда и слои.\\nГрафы могут быть и более сложными, в том числе нелинейными:\\n\\nДавайте разберёмся, что тут происходит.\\nInput — это вход нейросети, который получает исходные данные. Обычно требуется, чтобы они имели вид матрицы («объекты-признаки») или тензора (многомерной матрицы). Вообще говоря, входов может быть несколько: например, мы можем подавать в нейросеть картинку и какие-нибудь ещё сведения о ней — преобразовывать их мы будем по-разному, поэтому логично предусмотреть два входа в графе.\\nДальше к исходным данным X0X^0X0 применяются два линейных слоя, которые превращают их в промежуточные (внутренние, скрытые) представления X1X^1X1 и X2X^2X2. В литературе они также называются активациями (не путайте с функциями активации).\\nКаждое из представлений X1X^1X1 и X2X^2X2 подвергается нелинейному преобразованию, превращаясь в новые промежуточные представления X3X^3X3 и X4X^4X4 соответственно. Переход от X0X^0X0 к двум новым матрицам (или тензорам) X3X^3X3 и X4X^4X4 можно рассматривать как построение двух новых (возможно, более информативных) признаковых описаний исходных данных.\\nЗатем представления X3X^3X3 и X4X^4X4 конкатенируются (то есть признаковые описания всех объектов объединяются).\\nДальше следует ещё один линейный слой и ещё одна активация, и полученный результат попадает на выход сети, то есть отдаётся обратно пользователю.\\nНейросеть, в которой есть только линейные слои и различные функции активации, называют полносвязной (fully connected) нейронной сетью или многослойным перцептроном (multilayer perceptron, MLP).\\nПосмотрим, что происходит с размерностями, если на вход подаётся матрица N×dN\\\\times dN×d:\\n\\nПримечание о терминологииВ литературе, увы, нет единства терминологии.\\nТак, например, никто не мешает нам объявить «единым и неделимым слоем» композицию линейного слоя и активации (в ознаменование того, что мы почти никогда не используем просто линейный слой без нелинейности). Например, в фреймворке keras активацию можно указать в линейном слое в качестве параметра.\\nТакже в ряде источников слоями называется то, что мы называем промежуточными представлениями. Нам, впрочем, кажется, что промежуточные результаты XiX^iXi правильнее называть именно представлениями: ведь это новые признаковые описания, представляющие исходные объекты. Кроме того, во всех нейросетевых фреймворках слои — это именно преобразования, поэтому и нам кажется правильным объявлять слоями именно преобразования, связывающие промежуточные представления.\\nА вот и настоящий пример из реальной жизни. GoogLeNet (она же Inception-v1), показавшая SotA-результат на ILSVRC 2014 (ImageNet challenge), выглядит так:\\n\\nЗдесь каждый кирпичик — это некоторое относительно простое преобразование, а белым помечены входы и выходы вычислительного графа.\\nСовременные же сети часто выглядят и ещё сложней, но всё равно они собираются из достаточно простых кирпичиков-слоёв.\\nПримечание. Впрочем, в общем случае нейронная сеть — это просто некоторая сложная функция (или, что эквивалентно, граф вычислений). Поэтому в некоторых (очень нетривиальных) случаях нет смысла разбивать её на слои.\\nВ качестве иллюстрации ниже приведены структуры агностических нейронных сетей WANN, представленных в работе Weight Agnostic Neural Networks, NeurIPS 2019.\\n\\nForward & backward propagation\\nИнформация может течь по графу в двух направлениях.\\nПрименение нейронной сети к данным (вычисление выхода по заданному входу) часто называют прямым проходом, или же forward propagation (forward pass). На этом этапе происходит преобразование исходного представления данных в целевое и последовательно строятся промежуточные (внутренние) представления данных — результаты применения слоёв к предыдущим представлениям. Именно поэтому проход называют прямым.\\n\\nПри обратном проходе, или же backward propagation (backward pass), информация (обычно об ошибке предсказания целевого представления) движется от финального представления (а чаще даже от функции потерь) к исходному через все преобразования.\\nМеханизм обратного распространения ошибки, играющий важнейшую роль в обучении нейронных сетей, как раз предполагает обратное движение по вычислительному графу сети. С ним вы познакомитесь в следующем параграфе.\\n\\nАрхитектуры для простейших задач\\nКак мы уже упоминали выше, нейросети — это универсальный конструктор, который из простых блоков позволяет собрать орудия для решения самых разных задач. Давайте посмотрим на конкретные примеры. Безусловно, мир намного разнообразнее того, что мы покажем вам в этом параграфе, но с чего-то ведь надо начинать, не так ли?\\nВ тех несложных ситуациях, которые мы сейчас рассмотрим, архитектура будет отличаться лишь самыми последними этапами вычисления (у сетей будут разные «головы»). Для иллюстрации приведём примеры нескольких игрушечных архитектур для решения игрушечных задач классификации и регрессии на двумерных данных:\\n\\nБинарная классификация\\nДля решения задачи бинарной классификации подойдёт любая архитектура, на выходе у которой одно число от 000 до 111, интерпретируемое как «вероятность класса 1». Обычно этого добиваются, взяв\\ny^=σ(f(Xm)),\\\\widehat{y} = \\\\sigma(f(X^m)),\\ny\\u200b=σ(f(Xm)),где fff — некоторая функция, превращающая представление XmX^mXm в число (если XmX^mXm — матрица, то подойдёт f(Xm)=Xmw+bf(X^m) = X^mw + bf(Xm)=Xmw+b, где www — вектор-столбец), а σ\\\\sigmaσ — наша любимая сигмоида. При этом XmX^mXm может получаться как угодно, лишь бы хватало оперативной памяти и не было переобучения.\\nВ качестве функции потерь удобно брать уже знакомый нам log loss.\\nМногоклассовая классификация\\nРаботая с другими моделями, мы порой вынуждены были выдумывать сложные стратегии многоклассовой классификации; нейросети позволяют это делать легко и элегантно.\\nДостаточно построить сеть, которая будет выдавать KKK неотрицательных чисел, суммирующихся в 1 (где KKK — число классов); тогда им можно придать смысл вероятностей классов и предсказывать тот класс, «вероятность» которого максимальна.\\nПревратить произвольный набор из KKK чисел в набор из неотрицательных чисел, суммирующихся в 1, позволяет, к примеру, функция\\nsoftmax(x1,…,xK)=(ex1∑iexi,…,exK∑iexi)\\\\text{softmax}(x_1,\\\\ldots,x_K) = \\\\left( \\\\frac{e^{x_1}}{\\\\sum_ie^{x_i}},\\\\ldots,\\\\frac{e^{x_K}}{\\\\sum_ie^{x_i}} \\\\right)\\nsoftmax(x1\\u200b,…,xK\\u200b)=(∑i\\u200bexi\\u200bex1\\u200b\\u200b,…,∑i\\u200bexi\\u200bexK\\u200b\\u200b)Наиболее популярные архитектуры для многоклассовой классификации имеют вид\\ny^=softmax(f(Xm)),\\\\widehat{y} = \\\\text{softmax}(f(X^m)),\\ny\\u200b=softmax(f(Xm)),где fff — функция, превращающая XmX^mXm в матрицу B×KB\\\\times KB×K (где BBB — размер батча), а XmX^mXm может быть получен любым приятным вам образом.\\nНо какой будет функция потерь для такой сети? Мы должны научиться сравнивать «распределение вероятностей классов» с истинным (в котором на месте истинного класса стоит 1, а в остальных местах 0). Сделать это позволяет кросс-энтропия, она же negative log-likelihood — некоторый аналог расстояния между распределениями:\\nL(y^,y)=−1B∑i=1B∑k=1Kyiklog\\u2061y^ik,\\\\mathcal{L}(\\\\widehat{y}, y) = -\\\\frac1B\\\\sum_{i=1}^B\\\\sum_{k=1}^Ky_{ik}\\\\log{\\\\widehat{y}_{ik}},\\nL(y\\u200b,y)=−B1\\u200bi=1∑B\\u200bk=1∑K\\u200byik\\u200blogy\\u200bik\\u200b,где снова BBB — размер батча, а KKK — число классов. Легко видеть, что при K=2K = 2K=2 получается та самая функция потерь, которую мы использовали для обучения бинарной классификации.\\n(Множественная) регрессия\\nС помощью нейросетей легко создать модель, которая предсказывает не одно число, а сразу несколько. Например, координаты ключевых точек лица — кончика носа, уголков рта и так далее.\\nДостаточно сделать, чтобы последнее представление было матрицей B×MB\\\\times MB×M, где BBB — размер батча, а MMM — количество предсказываемых чисел. Особенностью большинства моделей регрессии является то, что после последнего слоя (часто линейного) не ставят функций активации. Вы тоже этого не делайте, если только чётко не понимаете, зачем вам это. В качестве функции потерь можно брать, например, MSEMSEMSE по всей матрице B×MB\\\\times MB×M.\\nВсё вместе\\nЕсли вы используете нейросети, то ваши таргеты могут иметь и различную природу. Например, можно соорудить одну-единственную сеть, которая по фотографии нескольких котиков определяет их количество (регрессия) и породу каждого из них (многоклассовая классификация).\\nЛосс для такой модели может быть равен (взвешенной) сумме лоссов для каждой из задач (правда, не факт, что это хорошая идея). Так что, по крайней мере в теории, сетям подвластны любые задачи. На практике, конечно, всё гораздо хитрей: для обучения слишком сложной сети у вас может не хватить данных или вычислительных мощностей.\\nПопулярные функции активации\\nДля начала поговорим о том, зачем они нужны.\\nКазалось бы, можно последовательно выстраивать лишь линейные слои, но так не делают: после каждого линейного слоя обязательно вставляют функцию активации. Но зачем? Попробуем разобраться.\\nРассмотрим нейронную сеть из двух линейных слоёв. Что произойдёт, если между ними будет отсутствовать нелинейная функция активации?\\n\\ny^=Xout=X1W2+b2=(X0W1+b1)W2+b2=\\\\widehat{y} = X^{out} = X^{1}W^{2} + b^2 = (X^0W^1 + b^1)W^2 + b^2 = \\ny\\u200b=Xout=X1W2+b2=(X0W1+b1)W2+b2==X0W1W2+b1W2+b2=X0W~+b~=  X^0\\\\color{blue}{W^1W^2} + \\\\color{green}{b^1W^2 + b^2} = X^0\\\\color{blue}{\\\\widetilde{W}} + \\\\color{green}{\\\\widetilde{b}}\\n=X0W1W2+b1W2+b2=X0W+bЛинейная комбинация линейных отображений есть линейное отображение, то есть два последовательных линейных слоя эквивалентны одному линейному слою.\\nДобавление функций активации после линейного слоя позволяет получить нелинейное преобразование, и подобной проблемы уже не возникает. Вдобавок правильный выбор функции активации позволяет получить преобразование, обладающее подходящими свойствами.\\nВ качестве функции активации может использоваться, например, уже знакомая вам из логистической регрессии сигмоида σ(x)=11+exp\\u2061(−x)\\\\sigma(x) = \\\\frac1{1 + \\\\exp(-x)}σ(x)=1+exp(−x)1\\u200b или ReLU (Rectified linear unit) ReLU(x)=max(0,x)\\\\text{ReLU}(x) = \\\\text{max}(0, x)ReLU(x)=max(0,x). К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее.\\nПримечание. На самом деле бывают ситуации, когда два линейных слоя подряд — это полезно. Например, если вы понимаете, что у вас очень много параметров, а информации в данных не так много, вы можете заменить линейный слой, превращающий mmm-мерные векторы в nnn-мерные, на два, вставив посередине kkk-мерное представление, где k≪m,nk \\\\ll m, nk≪m,n:\\n\\nС точки зрения линейной алгебры это примерно то же самое, что потребовать, чтобы матрица исходного линейного слоя имела ранг не выше kkk. И с точки зрения сужения «информационного канала» это иногда может сработать. Но в любом случае вы должны понимать, что два линейных слоя подряд стоит ставить, только если вы хорошо понимаете, чего хотите добиться.\\nВернёмся к функциям активации. Вот наиболее популярные:\\n\\nРассмотрим их подробнее.\\nReLU, Rectified linear unit\\nФормула:\\nReLU(x)=max\\u2061(0,x),\\\\text{ReLU}(x) = \\\\max(0, x),\\nReLU(x)=max(0,x),ReLU:R→[0,+∞).\\\\text{ReLU}: \\\\mathbb{R} \\\\to [0, +\\\\infty).\\nReLU:R→[0,+∞).ReLU это простая кусочно-линейная функция. Одна из наиболее популярных функций активации. В нуле производная доопределяется нулевым значением.\\nПлюсы:\\n\\nпростота вычисления активации и производной.\\n\\nМинусы:\\n\\nобласть значений является смещённой относительно нуля;\\nдля отрицательных значений производная равна нулю, что может привести к затуханию градиента.\\n\\nReLU и её производная очень просты для вычисления: достаточно лишь сравнить значение с нулём. Благодаря этому использование ReLU позволяет достигать прироста в скорости до четырёх-шести раз относительно сигмоиды.\\nБолее подробно о затухании градиентаРассмотрим нейронную сеть из нескольких линейных слоёв с сигмоидой в качестве функции активации. Пусть\\nX1=σ(Z1)=σ(W1X0),X^1 = \\\\sigma(Z^1) = \\\\sigma(W^1 X^0),\\nX1=σ(Z1)=σ(W1X0),где в качестве Z1Z^1Z1 обозначен результат применения линейного слоя. Свободный член, как и ранее, опущен для упрощения выкладок.\\nРассмотрим график сигмоиды и её производной:\\n\\nНа «хвостах» её производная практически равна нулю (ведь сигмоида представляет собой почти константу). То есть если какое-то значение Z1Z^1Z1 было достаточно велико по абсолютной величине (например, ∣zk1∣=7.4\\\\vert z^1_k \\\\vert = 7.4∣zk1\\u200b∣=7.4), то градиент функции потерь для этой компоненты будет домножен на очень малое число и фактически станет равным нулю:\\n∂l∂Wk,:0=∂l∂Xk1∂Xk1∂Zk1∂Zk1∂Wk,:0≈∂l∂Xk1⋅0⋅∂Zk1∂Wk,:0≈0.\\\\frac{\\\\partial l}{\\\\partial W^0_{k, :}} = \\\\frac{\\\\partial l}{\\\\partial X^1_k}\\\\frac{\\\\partial X^1_k}{\\\\partial Z^1_k} \\\\frac{\\\\partial Z^1_k}{\\\\partial W^0_{k,:}} \\\\approx \\\\frac{\\\\partial l}{\\\\partial X^1_k} \\\\cdot 0 \\\\cdot \\\\frac{\\\\partial Z^1_k}{\\\\partial W^0_{k,:}} \\\\approx 0.\\n∂Wk,:0\\u200b∂l\\u200b=∂Xk1\\u200b∂l\\u200b∂Zk1\\u200b∂Xk1\\u200b\\u200b∂Wk,:0\\u200b∂Zk1\\u200b\\u200b≈∂Xk1\\u200b∂l\\u200b⋅0⋅∂Wk,:0\\u200b∂Zk1\\u200b\\u200b≈0.Получается, что kkk-ая строка матрицы W0W^0W0 не будет обновлена: ведь градиент равен нулю. Это может привести к «отмиранию» части весов: неудачное значение параметров приведёт к невозможности их обновления.\\nПримечание. Это одна из причин необходимости нормировки данных и выбора правильной инициализации для начальных значений параметров нейронной сети.\\nНо помимо явного обнуления градиента есть и вторая проблема: максимальное значение производной сигмоиды составляет 0.250.250.25. То есть она всегда уменьшает значение градиента не менее чем в четыре раза. Если же представить себе глубокую сеть с 20+ слоями, то для каждого следующего (если считать с конца) слоя градиент будет домножаться на число, не превосходящее 0.250.250.25. Так, для переменных из первого слоя коэффициент составит 4−204^{-20}4−20.\\nLeaky ReLU\\nФормула:\\nLeaky\\xa0ReLU(x)=max\\u2061(αx,x),α=const,0<α≪1\\\\text{Leaky ReLU}(x) = \\\\max(\\\\alpha x, x), \\\\quad \\\\alpha = \\\\text{const}, 0 < \\\\alpha \\\\ll 1\\nLeaky\\xa0ReLU(x)=max(αx,x),α=const,0<α≪1Leaky\\xa0ReLU:R→(−∞,+∞).\\\\text{Leaky ReLU}: \\\\mathbb{R} \\\\to (-\\\\infty, +\\\\infty).\\nLeaky\\xa0ReLU:R→(−∞,+∞).Гиперпараметр α\\\\alphaα обеспечивает небольшой уклон слева от нуля, что позволяет получить более симметричную относительно нуля область значений. Также меньше провоцирует затухание градиента благодаря наличию ненулевого градиента и слева, и справа от нуля.\\nPReLU, Parametric ReLU\\nФормула:\\nPReLU(x)=max\\u2061(αx,x),0<α≪1\\\\text{PReLU}(x) = \\\\max(\\\\alpha x, x), \\\\quad 0 < \\\\alpha \\\\ll 1\\nPReLU(x)=max(αx,x),0<α≪1PReLU:R→(−∞,+∞).\\\\text{PReLU}: \\\\mathbb{R} \\\\to (-\\\\infty, +\\\\infty).\\nPReLU:R→(−∞,+∞).Аналогична Leaky ReLU, но параметр α\\\\alphaα настраивается градиентными методами.\\nELU\\nELU – это гладкая аппроксимация ReLU. Обладает более высокой вычислительной сложностью, достаточно редко используется на практике.\\nSigmoid, сигмоида\\nФормула:\\nσ(x)=11+exp\\u2061(−x),\\\\sigma(x) = \\\\frac{1}{1 + \\\\exp(-x)},\\nσ(x)=1+exp(−x)1\\u200b,σ:R→(0,1).\\\\sigma: \\\\mathbb{R} \\\\to (0, 1).\\nσ:R→(0,1).Исторически одна из первых функций активации. Рассматривалась в том числе и как гладкая аппроксимация порогового правила, эмулирующая активацию естественного нейрона.\\nПлюсы:\\nМинусы:\\n\\nобласть значений смещена относительно нуля;\\nсигмоида (как и её производная) требует вычисления экспоненты, что является достаточно сложной вычислительной операцией. Её приближённое значение вычисляется на основе ряда Тейлора или с помощью полиномов, Stack Overflow question 1, question 2;\\nна «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента;\\nмаксимальное значение производной составляет 0.250.250.25, что также приводит к затуханию градиента.\\n\\nНа практике сигмоида редко используется внутри сетей, чаще всего в случаях, когда внутри модели решается задача бинарной классификации (например, вероятность забывания информации в LSTM).\\nTanh, гиперболический тангенс\\nФормула:\\ntanh\\u2061(x)=exp\\u2061(x)−exp\\u2061(−x)exp\\u2061(x)+exp\\u2061(−x),\\\\tanh(x) = \\\\frac{\\\\exp(x) - \\\\exp(-x)}{\\\\exp(x) + \\\\exp(-x)},\\ntanh(x)=exp(x)+exp(−x)exp(x)−exp(−x)\\u200b,tanh\\u2061:R→(−1,1).\\\\tanh: \\\\mathbb{R} \\\\to (-1, 1).\\ntanh:R→(−1,1).Плюсы:\\n\\nкак и сигмоида, имеет ограниченную область значений;\\nв отличие от сигмоиды, область значений симметрична.\\n\\nМинусы:\\n\\nтребует вычисления экспоненты, что является достаточно сложной вычислительной операцией;\\nна «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента.\\n\\nВопрос на подумать. А почему симметричность области значений может быть ценным свойством?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Разберём на примере сигмоиды. Пусть оказалось так, что все веса линейных слоёв инициализированы положительными числами.\\nСигмоида от положительного числа даёт нечто, большее 12\\\\frac1221\\u200b, и с каждым дальнейшим слоем ситуация может усугубляться, приводя к тому, что сигмоиды будут выдавать всё более близкие к единице значения. Это покарает нас, когда мы начнём считать градиенты: они начнут «затухать».\\nНемного о мощи нейросетей\\nРассмотрим для начала задачу регрессии. Ясно, что линейная модель (то есть однослойная нейросеть) может приблизить только линейную функцию, но уже двухслойная нейросеть может приблизить почти что угодно. Есть ряд теорем на эту тему, мы упомянем одну из них. Обратите внимание на год: как мы уже упоминали, нейросети начали серьёзно изучать задолго до того, как они начали превращаться в state of the art.\\nТеорема Цыбенко (1989). Для любой непрерывной функции f(x):Rm→Rf(x):\\\\mathbb{R}^m\\\\rightarrow\\\\mathbb{R}f(x):Rm→R и для любого ε>0\\\\varepsilon > 0ε>0 найдётся число NNN, а также числа w1…,wNw_1\\\\ldots,w_Nw1\\u200b…,wN\\u200b, b1,…,bNb_1,\\\\ldots,b_Nb1\\u200b,…,bN\\u200b α1,…,αN\\\\alpha_1,\\\\ldots,\\\\alpha_Nα1\\u200b,…,αN\\u200b, для которых\\n∣f(x)−∑i=1Nαiσ(⟨x,wi⟩+bi)∣<ε\\\\left|f(x) - \\\\sum_{i=1}^N\\\\alpha_i\\\\sigma(\\\\langle x, w_i\\\\rangle + b_i)\\\\right| < \\\\varepsilon\\n\\u200bf(x)−i=1∑N\\u200bαi\\u200bσ(⟨x,wi\\u200b⟩+bi\\u200b)\\u200b<εдля любых xxx из единичного куба [0,1]m[0,1]^m[0,1]m в Rm\\\\mathbb{R}^mRm.\\nВ сумме из теоремы Цыбенко легко опознать двуслойную нейросеть с сигмоидной функцией активации. В самом деле, сперва мы превращаем xxx в ⟨x,wi⟩+bi\\\\langle x, w_i\\\\rangle + b_i⟨x,wi\\u200b⟩+bi\\u200b — это можно представить в виде одной матричной операции (линейный слой!):\\nx↦x(1)=x⋅(12w1…wN)+(12b1…bN),x\\\\mapsto x^{(1)} = x\\\\cdot\\\\begin{pmatrix}\\n\\\\phantom{\\\\frac12}w_1 & \\\\ldots &\\nw_N\\n\\\\end{pmatrix} + \\\\begin{pmatrix}\\n\\\\phantom{\\\\frac12}b_1 & \\\\ldots &\\nb_N\\n\\\\end{pmatrix},x↦x(1)=x⋅(21\\u200bw1\\u200b\\u200b…\\u200bwN\\u200b\\u200b)+(21\\u200bb1\\u200b\\u200b…\\u200bbN\\u200b\\u200b),где wiw_iwi\\u200b — вектор-столбцы, а каждое из bib_ibi\\u200b прибавляется к iii-му столбцу, после чего поэлементно берём от x(1)x^{(1)}x(1) сигмоиду (активация) x(2)=σ(x(1))x^{(2)} = \\\\sigma(x^{(1)})x(2)=σ(x(1)), после чего вычисляем\\n∑i=1Nαixi(2)=(α1,…,αN)⋅x,\\\\sum_{i=1}^N\\\\alpha_ix^{(2)}_i = \\\\left(\\\\alpha_1,\\\\ldots,\\\\alpha_N\\\\right)\\\\cdot x,\\ni=1∑N\\u200bαi\\u200bxi(2)\\u200b=(α1\\u200b,…,αN\\u200b)⋅x,и это второй линейный слой (без свободного члена).\\nПравда, теорема не очень помогает находить такие функции, но это уже другое дело. В любом случае — если дать нейросети достаточно данных, она действительно может выучить почти что угодно.\\nУпражнение. Мы не будем приводить результатов, касающихся классификации, но рекомендуем воспользоваться замечательной песочницей. Убедитесь сами, что при использовании одного скрытого слоя из двух нейронов и сигмоиды в качестве функции активации, можно неплохо классифицировать данные со сложной, совсем даже не линейной границей между классами. Вы также можете поиграть с разными функциями активации.\\nА для получения решения нам необходим метод автоматической настройки всех параметров нейронной сети — метод обратного распространения ошибки, или же error backpropagation. Рассмотрим его в деталях в следующем параграфе.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф5.1. Нейронные сетиКраткий путеводитель по\\xa0разделуСледующий параграф5.3. Метод обратного распространения ошибкиКак эффективно посчитать градиенты по\\xa0весам нейронной сетиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_37.html', 'title': 'Введение в рекомендательные системы'}, page_content='Введение в рекомендательные системыЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/49.1.Введение в рекомендательные системыГде можно встретить рекомендательные системы?Формализация задачиКоллаборативная фильтрацияContent-based рекомендацииКлассический пайплайн рекомендательной системы9.2.Рекомендации на основе матричных разложений9.3.Контентные рекомендации9.4.Хорошие свойства рекомендательных систем10.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Введение в рекомендательные системы9.1. Введение в рекомендательные системыАвторыСемён ИвановГде можно встретить рекомендательные системы?\\nС рекомендательными системами можно столкнуться там, где есть большое множество товаров и пользователей, которые хотят найти нужные для себя товары. Рекомендательные системы помогают отобрать наиболее релевантные для пользователя объекты, тем самым экономя его время. Приведём несколько примеров:\\n\\nYouTube рекомендует пользователям видео;\\nНа сайтах интернет-магазинов можно встретить блоки с рекомендациями товаров;\\nМузыкальные сервисы наподобие Spotify или Яндекс.Музыки рекомендуют музыкальные треки.\\n\\n\\nЧто такое «релевантные для пользователя товары» – это нетривиальный вопрос, который решается отдельно для каждой задачи исходя из бизнес-логики.\\nПоиск vs рекомендации\\nОтметим, что, хотя задачи поиска и рекомендаций кажутся похожими и, как мы увидим, могут использовать схожие методы, у них есть одно важное отличие: в задаче поиска есть сформулированный запрос от пользователя, а в задаче рекомендаций явного запроса нет, есть только история взаимодействий пользователя с объектами и наша надежда на то, что мы верно распознали его скрытые желания. Это различие объясняет некоторые особенности дизайна рекомендательных систем, которые мы подробнее обсудим в конце этого параграфа, при разборе классического пайплайна рекомендательной системы.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nФормализация задачи\\nExplicit и Implicit feedback\\nВведём ряд обозначений. Пусть у нас есть множество пользователей UUU и множество объектов III. Для каждого пользователя u∈Uu \\\\in Uu∈U есть множество объектов Iu⊂II_u \\\\subset IIu\\u200b⊂I, с которыми он взаимодействовал и которым поставил рейтинги Ru=(rui)i∈IuR_u = \\\\left(r_{ui}\\\\right)_{i \\\\in I_u}Ru\\u200b=(rui\\u200b)i∈Iu\\u200b\\u200b. Рейтинг (его также называют фидбеком) – это некоторая характеристика взаимодействия пользователя с объектом; про него можно думать, как про некоторый таргет, который мы выбрали для оптимизации рекомендательной системы.\\nТаким образом, задачу рекомендательных систем можно переформулировать в следующем виде: для каждого пользователя u∈Uu \\\\in Uu∈U необходимо оценить значение ruir_{ui}rui\\u200b для i∈I∖Iui \\\\in I \\\\setminus I_{u}i∈I∖Iu\\u200b и выбрать несколько товаров с наибольшим r^ui\\\\hat{r}_{ui}r^ui\\u200b. Иными словами, надо научиться среди непоказанных пользователю товаров находить те, которые заинтересовали бы его больше всего.\\nПриведем несколько примеров фидбека:\\n\\nДля товара – факт добавления в корзину;\\nДля музыки – дослушали ли трек до конца;\\nДля статьи – лайк/дизлайк;\\nДля видео – время его просмотра или факт просмотра, например, наполовину.\\n\\nКак правило, фидбек разделяют на два типа – explicit и implicit. Из-за различия для каждого фидбека есть разные техники обработки и использования, которые будут обсуждаться в параграфе про матричные факторизации.\\nExplicit, или явный фидбек – это такие действия пользователя, по которым точно можно понять, понравился ли ему объект. Это может быть оценка, поставленная, фильму, лайк/дизлайк к видео или рецензия на купленный товар. Такого фидбека очень мало, но он наиболее точно характеризует отношение пользователя к товару.\\nImplicit, или неявный фидбек – это любая другая информация о действиях пользователя на сайте. Он выступает в качестве прокси к явному фидбеку. Например, факт того, что пользователь досмотрел видео до конца, не говорит о том, понравилось ли оно ему, однако можно сделать предположение, что большинству досмотревших видео до конца оно понравилось. Приведем основные примеры неявного фидбека: клик на статью, время просмотра видео, покупка товара. Обычно такого сигнала в разы больше, чем явного, однако он более шумный, и не стоит доверять ему так же, как явному. Например, при оптимизации кликов на статью может получиться так, что рекомендательная система научится находить кликбейт, а не интересные пользователю статьи – это может плохо отразиться на сервисе в долгосрочной перспективе.\\nРанжирующая модель\\nЗадачу построения рекомендательной системы можно формулировать в качестве задачи классификации (клик/не клик) или регрессию (сколько звёзд пользователь поставит объекту), но это не самые распространённые стратегии.\\nОбратим внимание, что нам на самом деле не обязательно уметь точно оценивать рейтинги ruir_{ui}rui\\u200b. Достаточно уметь для пользователя и набора объектов генерировать перестановку этих объектов в порядке убывания рейтинга. Модель, решающую данную задачу, называют ранжирующей.\\nОпишем классический пайплайн применения ранжирующей модели для одного пользователя. На вход подаются признаки пользователя и объекта, и для пары пользователь-объект на основе этих признаков выдается некоторое число, ответ модели. Далее мы сортируем объекты в порядке его убывания. Из полученной перестановки обычно берут несколько первых объектов для показа пользователю.\\nБолее подробно о том, как решается задача ранжирования, вы можете прочитать в соответствующем параграфе.\\nКоллаборативная фильтрация\\n\\nРассмотрим матрицу взамодействий пользователя, приведённую выше. Что можно порекомендовать Кате, исходя из исторических данных? Можно заметить, что взаимодействия Кати похожи на взамодействия Пети (так как они оба лайкали объекты 1 и 8). Иными словами, их интересы в чём-то похожи, поэтому Кате можно порекомендовать, например, объект 3 (так как он понравился Пете). Можно проделать аналогичное упражнение с Петей и сделать вывод, что ему не стоит рекомендовать объект 10.\\nМожно решать и транспонированную задачу: для лайкнутого пользователем объекта искать похожие, то есть те, которые пользователи достаточно часто лайкали вместе с ним. Например, объекты 1 и 8 похожи друг на друга, так как их лайкали одни и те же пользователи, и точно так же похожи 1 и 3.\\nПроиллюстрированный выше подход называют коллаборативной фильтрацией. Он объединяет семейство методов рекомендаций, использующих сходство по истории взаимодействия между пользователем и товаром. Рассмотрим конкретные простые методы коллаборативной фильтрации.\\nUser2User рекомендации\\nВведём меру похожести двух пользователей s(u,v)s(u, v)s(u,v), которая тем больше, чем выше сходство между uuu и vvv. Для пользователя uuu рассмотрим множество похожих на него пользователей N(u)={v∈U∖{u}∣s(u,v)>α},N(u) = \\\\{ v \\\\in U \\\\setminus \\\\{u\\\\} \\\\vert  s(u, v) > \\\\alpha  \\\\},N(u)={v∈U∖{u}∣s(u,v)>α}, где α\\\\alphaα – настраиваемый гиперпараметр, в чём-то аналогичный порогу бинарного классификатора.\\nДопустим, мы хотим теперь оценить рейтинг ruir_{ui}rui\\u200b, который пользователь uuu поставил бы объекту iii. Сделаем это, опираясь на рейтинги, которые ставили похожие на uuu пользователи. Например, можно взять взвешенное среднее:\\nr^ui=∑v∈N(u)s(u,v)rvi∑v∈N(u)∣s(u,v)∣\\\\hat{r}_{ui} = \\\\frac{\\\\sum_{v \\\\in N(u)} s(u, v) r_{vi}}{\\\\sum_{v \\\\in N(u)} \\\\lvert s(u, v) \\\\rvert}\\nr^ui\\u200b=∑v∈N(u)\\u200b∣s(u,v)∣∑v∈N(u)\\u200bs(u,v)rvi\\u200b\\u200bМодуль добавляется для того, чтобы корректно обработать непохожих пользователей, то есть пары с отрицательной похожестью, которая может возникнуть, если при построении N(u)N(u)N(u) взять достаточно маленькое α\\\\alphaα.\\nМожно пойти дальше и усовершенствовать метод оценивания. У пользователей могут быть разные диапазоны оценок: кто-то ставит почти всегда в диапазоне 1-3, а кто-то предпочитает ставить 4-5. Иными словами, для разных пользователей оценка «нормально» (и соответственно, оценки «хорошо» и «плохо») могут соответствовать разным значениям рейтинга. Для устранения этой проблемы, можно брать не сырой рейтинг пользователя rvir_{vi}rvi\\u200b, а его отклонение от среднего всех оценок пользователя: rvi−r‾vr_{vi} - \\\\overline{r}_{v}rvi\\u200b−rv\\u200b. Таким образом, мы учитываем только разброс вокруг среднего и итоговая оценка будет выглядеть так:\\nr^ui=r‾u+∑v∈N(u)s(u,v)(rvi−r‾v)∑v∈N(u)∣s(u,v)∣\\\\hat{r}_{ui} = \\\\overline{r}_u + \\\\frac{\\\\sum_{v \\\\in N(u)} s(u, v) \\\\left(r_{vi} - \\\\overline{r}_v\\\\right)}{\\\\sum_{v \\\\in N(u)} \\\\lvert s(u, v) \\\\rvert}\\nr^ui\\u200b=ru\\u200b+∑v∈N(u)\\u200b∣s(u,v)∣∑v∈N(u)\\u200bs(u,v)(rvi\\u200b−rv\\u200b)\\u200bМожно пойти еще дальше и учесть дисперсию оценок пользователей:\\nr^ui=r‾u+σu∑v∈N(u)s(u,v)(rvi−r‾v)/σv∑v∈N(u)∣s(u,v)∣,\\u2009где\\u2009σu=1Iu∑i∈Iu(rui−r‾u)2,\\\\hat{r}_{ui} = \\\\overline{r}_u + \\\\sigma_{u} \\\\frac{\\\\sum_{v \\\\in N(u)} s(u, v) \\\\left(r_{vi} - \\\\overline{r}_v\\\\right) / \\\\sigma_v}{\\\\sum_{v \\\\in N(u)} \\\\lvert s(u, v) \\\\rvert}, \\\\, \\\\text{где} \\\\, \\\\sigma_{u} = \\\\sqrt{ \\\\frac{1}{I_u} \\\\sum_{i \\\\in I_{u} } \\\\left(r_{ui} - \\\\overline{r}_u\\\\right) ^ 2},\\nr^ui\\u200b=ru\\u200b+σu\\u200b∑v∈N(u)\\u200b∣s(u,v)∣∑v∈N(u)\\u200bs(u,v)(rvi\\u200b−rv\\u200b)/σv\\u200b\\u200b,гдеσu\\u200b=Iu\\u200b1\\u200bi∈Iu\\u200b∑\\u200b(rui\\u200b−ru\\u200b)2\\u200b,где IuI_{u}Iu\\u200b – множество объектов, с которыми взаимодействовал пользователь uuu.\\nВ заключение приведём несколько вариантов оценки схожести пользователей:\\n\\nМера Жаккара: s(u,v)=∣Pu∩Pv∣∣Pu∪Pv∣,s(u, v) = \\\\frac{\\\\lvert P_u \\\\cap P_v \\\\rvert}{\\\\lvert P_u \\\\cup P_v \\\\rvert},s(u,v)=∣Pu\\u200b∪Pv\\u200b∣∣Pu\\u200b∩Pv\\u200b∣\\u200b, где PuP_uPu\\u200b – множество понравившихся uuu айтемов;\\nСкалярное произведение общих рейтингов: s(u,v)=∑i∈Iu∩Ivruirvis(u, v) = \\\\sum_{i \\\\in I_u \\\\cap I_v} r_{ui} r_{vi}s(u,v)=∑i∈Iu\\u200b∩Iv\\u200b\\u200brui\\u200brvi\\u200b;\\nКорреляция Пирсона:\\n\\ns(u,v)=∑i∈Iu∩Iv(rui−r‾u)(rvi−r‾v)∑i∈Iu∩Iv(rui−r‾u)2∑i∈Iu∩Iv(rvi−r‾v)2s(u, v) = \\\\frac{\\\\sum_{i \\\\in I_u \\\\cap I_v} (r_{ui} - \\\\overline{r}_u)(r_{vi} - \\\\overline{r}_v)}{\\\\sqrt{\\\\sum_{i \\\\in I_u \\\\cap I_v} (r_{ui} - \\\\overline{r}_u)^2} \\\\sqrt{\\\\sum_{i \\\\in I_u \\\\cap I_v} (r_{vi} - \\\\overline{r}_v)^2}}\\ns(u,v)=∑i∈Iu\\u200b∩Iv\\u200b\\u200b(rui\\u200b−ru\\u200b)2\\u200b∑i∈Iu\\u200b∩Iv\\u200b\\u200b(rvi\\u200b−rv\\u200b)2\\u200b∑i∈Iu\\u200b∩Iv\\u200b\\u200b(rui\\u200b−ru\\u200b)(rvi\\u200b−rv\\u200b)\\u200b\\nДисконтированная корреляция Пирсона. Так как айтемов в пересечении Iu∩IvI_u \\\\cap I_vIu\\u200b∩Iv\\u200b в действительности не всегда может быть достаточно много, можно дисконтировать похожести, посчитанные по небольшому множеству айтемов, домножая корреляцию на min\\u2061(∣Iu∩Iv∣50,1)\\\\min(\\\\frac{\\\\lvert I_u \\\\cap I_v \\\\rvert}{50}, 1)min(50∣Iu\\u200b∩Iv\\u200b∣\\u200b,1).\\n\\nItem2Item рекомендации\\nТеперь попробуем решать транспонированную задачу. Введем меру похожести объектов s(i,j)s(i, j)s(i,j). Если нам нужно оценить рейтинг, который пользователь uuu поставил бы ещё не виденному им объекту iii, то мы можем рассмотреть множество N(i)N(i)N(i) близких к iii объектов и оценить r^ui\\\\hat{r}_{ui}r^ui\\u200b аналогично user2user подходу:\\nr^ui=∑j∈N(i)s(i,j)ruj∑j∈N(i)∣s(i,j)∣\\\\hat{r}_{ui} = \\\\frac{\\\\sum_{j \\\\in N(i)} s(i, j) r_{uj}}{\\\\sum_{j \\\\in N(i)} \\\\lvert s(i, j) \\\\rvert}\\nr^ui\\u200b=∑j∈N(i)\\u200b∣s(i,j)∣∑j∈N(i)\\u200bs(i,j)ruj\\u200b\\u200bМеру схожести объектов можно задать как adjusted cosine:\\ns(i,j)=∑u∈Ui∩Uj(rui−r‾u)(ruj−r‾u)∑u∈Ui∩Uj(rui−r‾u)2∑u∈Ui∩Uj(ruj−r‾u)2s(i, j) = \\\\frac{\\\\sum_{u \\\\in U_i \\\\cap U_j} (r_{ui} - \\\\overline{r}_u)(r_{uj} - \\\\overline{r}_u)}{\\\\sqrt{\\\\sum_{u \\\\in U_i \\\\cap U_j} (r_{ui} - \\\\overline{r}_u)^2} \\\\sqrt{\\\\sum_{u \\\\in U_i \\\\cap U_j} (r_{uj} - \\\\overline{r}_u)^2}}\\ns(i,j)=∑u∈Ui\\u200b∩Uj\\u200b\\u200b(rui\\u200b−ru\\u200b)2\\u200b∑u∈Ui\\u200b∩Uj\\u200b\\u200b(ruj\\u200b−ru\\u200b)2\\u200b∑u∈Ui\\u200b∩Uj\\u200b\\u200b(rui\\u200b−ru\\u200b)(ruj\\u200b−ru\\u200b)\\u200bгде UiU_iUi\\u200b – множество пользователей, оценивших товар iii. Обратите внимание, что r‾u\\\\overline{r}_uru\\u200b – это средняя оценка пользователя, а не объекта, то есть это не корреляция Пирсона – на практике данный подход обычно работает лучше.\\nОсобенности коллаборативной фильтрации\\nВыделим ключевые особенности методов, основанных на коллаборативной фильтрации, о которых следует помнить при разработке рекомендательных систем:\\n\\nОни не опираются ни на какую дополнительную информацию кроме матрицы оценок RuiR_{ui}Rui\\u200b, предполагая, что этого должно быть достаточно для улавливания качественного сигнала о схожести пользователей и товаров;\\nПредложенные методы не применимы для новых объектов и пользователей – для них просто нет истории или она недостаточно информативна для того, чтобы методы могли давать более-менее точные оценки;\\nТак как методы коллаборативной фильтрации основаны только на истории прошлых взаимодействий, рекомендательная система, построенная исключительно на их основе будет постепенно вгонять пользователя в информационный пузырь: эти методы не предполагают открытия новых интересов у пользователя, они способны только эксплуатировать уже имеющиеся.\\n\\nContent-based рекомендации\\nТакже помимо коллаборативной фильтрации существует content-based подход для построения рекомендаций: измерение похожести между объектами на основе их содержания. Например, две статьи про то, как заменить колесо на велосипеде можно считать похожими с точки зрения содержания. Иными словами, входом для content-based модели являются разные контентные признаки и характеристики товара (например, текст статьи, время публикации, картинки), а выходом является некоторое числовое представление объекта (эмбеддинг). Отметим, что никакую коллаборативную информацию такие модели не используют, они ничего не знают про других пользователей и про их взаимодействие с объектами. Например, Bert является чисто контентной моделью – он переводит текст в эмбеддинг.\\nПусть у нас есть некоторый контентные эмбеддинги ei∈Rne_i \\\\in \\\\mathbb{R}^nei\\u200b∈Rn для каждого товара – например, мы применили обученный Bert для получения векторных представлений статей. Тогда мы можем посчитать скалярное произведение (или косинусное расстояние) до оценённых пользователем объектов и оценить рейтинги, как:\\nr^ui=max\\u2061j∈Iu,ruj>αρ(ei,ej)ruj,\\\\hat{r}_{ui} = \\\\max_{j \\\\in I_{u}, r_{uj} > \\\\alpha} \\\\rho(e_i, e_j) r_{uj},\\nr^ui\\u200b=j∈Iu\\u200b,ruj\\u200b>αmax\\u200bρ(ei\\u200b,ej\\u200b)ruj\\u200b,где ρ\\\\rhoρ – скалярное произведение или косинусное расстояние между двумя векторами, IuI_{u}Iu\\u200b – множество оценённых пользователем объектов, а α\\\\alphaα – гиперпараметр. Таким образом, высокие рейтинги получат объекты, похожие на те, что понравились пользователю – мы получили простую ранжирующую модель.\\nПлюс контентного подхода в том, что, в отличие от чисто коллаборативного подхода, он одинаково хорошо работает на новых и старых айтемах, так как контентные модели основаны только на статичной контентной информации, которая всегда доступна. Из минусов можно отметить, что похожесть по контенту может ещё больше загонять пользователя в информационный пузырь: например, контентная модель вряд ли сможет к кофемашине порекомендовать кофейные зерна, в то время как коллаборативный подход получит сигнал о том, что товары являются дополняющими напрямую из действий других пользователей.\\nОтметим, что существуют гибридные модели, совмещающие в себе коллаборативный и контентный сигналы. Например, такой моделью является DSSM.\\nПодробнее о контентных моделях вы узнаете в соответствующем параграфе.\\nКлассический пайплайн рекомендательной системы\\n\\nМы разобрали несколько классических подходов к построению рекомендаций, теперь нужно обсудить, как это скомпоновать в единую рекомендательную систему.\\nДля начала сформулируем ряд свойств, которыми должна обладать хорошая рекомендательная система:\\n\\nПри ранжировании товаров в порядке убывания r^ui\\\\hat{r}_{ui}r^ui\\u200b нам хотелось бы учитывать как можно больше сигналов/фичей (как пользователя, так и объекта);\\nРекомендательная система должна работать достаточно быстро;\\nДолжен быть несложный механизм, позволяющий понятно учитывать «бизнес-логику» (например, если при прочих равных мы больше хотим показывать свежие статьи).\\n\\nДля соблюдения первого пункта, очевидно, нужна ранжирующая модель. В качестве самой модели часто применяют бустинг – на табличных данных он, как правило, cправляется лучше, плюс он быстрее нейронных сетей с точки зрения времени применения.\\nЗдесь не будет лишним упомянуть про feedback loop. Для обучения ранжирующей модели мы обычно берем прошлую историю взаимодействия пользователей с показанными ему объектами, считаем ruir_{ui}rui\\u200b и составляем на основе этих оценок обучающий датасет. Таким образом, обучая новую модель, мы с некоторыми оговорками будем учиться предсказывать старую модель. Поэтому есть риск, что она застрянет в локальном оптимуме, из которого сложно выбраться. В качестве решения этой проблемы можно, например, подмешивать в выдачу случайные объекты и давать им больший вес в функции потерь. Таким образом, у нас появляется некоторое подмножество объектов, которые не были смоделированы нашей моделью. В качестве дополнительного плюса такого подхода мы в какой-то степени будем выбивать пользователя из его информационного пузыря, показывая объекты из категорий, которыми он еще не интересовался.\\nВ реальной рекомендательной системе обычно от нескольких миллионов товаров и хотя бы несколько сотен тысяч пользователей в день (а чаще несколько миллионов). Обученная CatBoost модель на 5000 объектов отрабатывает где-то за 100-125ms на CPU. Фичи пользователей и объектов постоянно меняются, поэтому на каждый запрос пользователя мы должны заново скорить все объекты. Но тогда только на скоринг мы будем тратить порядка 25 секунд, а если это не CatBoost, а, например, нейронная сеть, то, скорее всего, ещё больше. Это очень существенные и необоснованные затраты.\\nВ действительности, пользователю наверняка интересна лишь небольшая часть имеющихся у нас товаров. Можно попытаться сузить множество до потенциально интересных пользователю объектов и уже для них применить «тяжёлую» ранжирующую модель, которая определит финальную выдачу. Этот подход называется отбором кандидатов. К отбору кандидатов предъявляют два требования:\\n\\nон должен быть быстрым;\\nон должен иметь хорошую полноту поиска подходящих пользователю объектов, то есть в полученной после отбора кандидатов подмножестве должны в избытке находиться интересные пользователю статьи/фильмы/продукты;\\n\\nПриведем несколько подходов к отбору кандидатов:\\n\\nЭвристики: самые популярные товары, популярные за XXX последних дней, популярные среди жителей этого города, недавно опубликованные;\\nКоллаборативные: item2item или user2user рекомендации. Мы можем в оффлайне предподсчитывать все необходимые статистики и строить таблички из пользователя в множество подходящих айтемов или из айтема в айтемы. Также есть более сложные подходы на основе матричных разложений, о которых будет рассказано в соответствующем параграфе;\\nКонтентные методы: берём content-based эмбеддинги объектов и строим быстрый индекс для поиска ближайших объектов (например, HNSW). Подробнее о быстром поиске ближайших соседей вы можете почитать в параграфе про метрические методы. Далее, можем взять понравившиеся пользователю товары и найти похожие на них.\\n\\nОбычно отбор кандидатов состоит из набора разных источников кандидатов, где каждый источник по смыслу пытается покрыть какой-то пользовательский аспект.\\nДвухступенчатая рекомендательная система уже обладает двумя хорошими свойствами, осталось предложить механизм, который позволит учитывать бизнес-логику. Под бизнес-логикой здесь понимается некоторое качество рекомендательной системы, которое хотелось бы иметь, но которое достаточно нетривиально, чтобы мы не стали зашивать его в саму ранжирующую модель. Приведем примеры возможных пожеланий:\\n\\nРеже показывать старые видео в ленте;\\nРеже показывать слишком длинные видео или видео, снятые в плохом качестве;\\nОбеспечить разнообразную для пользователя выдачу. Например, если пользователь интересуется кошками и машинами, А ранжирующая модель всем видео про кошек дала большую оценку, чем любому видео про машины, то получится, что лента пользователя будет состоять только из кошек, хотя ему также интересны и машины.\\n\\nВсе эти свойства подразумевают под собой небольшое переупорядочивание объектов после применения ранжирующей формулы. Этот механизм называется переранжированием (реранкингом).\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф8.6. Языковые моделиСледующий параграф9.2. Рекомендации на основе матричных разложенийЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_60.html', 'title': 'Адаптивный FTRL'}, page_content=\"Адаптивный FTRLЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/415.1.Введение в онлайн-обучение15.2.Адаптивный FTRLСинтаксический сахарАддитивные регуляризаторыКлассы алгоритмов FTRLГарантии сходимости для алгоритмов FTRLПостроение эффективного адаптивного FTRL15.3.Регуляризация в онлайн-обучении15.4.Методы оптимизации в Deep Learning16.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Адаптивный FTRL15.2. Адаптивный FTRLАвторыАлексей МорозовВ данном разделе мы рассмотрим широкое семейство алгоритмов, позволяющее делать улучшения в способах введения регуляризации, которые невозможно добиться в классическом градиентном спуске.\\nПолезные ссылки\\nВсе написанное ниже (за исключением вывода AdaGrad) — сокращенный пересказ обзора H. Brendan McMahan A Survey of Algorithms and Analysis for Adaptive Online Learning. Везде, где мы обозначаем Lemma 4, Theorem 10 и т.д. — мы ссылаемся на соответствующие теоремы из этой статьи. То же самое с доказательствами: если мы что-то опускаем, подробности можно найти в обзоре\\nИнтуитивный вывод AdaGrad взят из статьи Adaptive Subgradient Methods for Online Learning and Stochastic Optimization . Вместо оригинальных оценок на метод Regularized Dual Averaging, требующих дополнительных понятий вроде двойственности по Фенхелю, мы использовали аналогичную оценку из обзора выше, сохранив все рассуждения автора. Опять же — строгое доказательство оценок на regret для AdaGrad есть в этом обзоре.\\nСинтаксический сахар\\nВ выкладках очень часто используются суммы, и без сокращенных обозначений читать их невозможно. В литературе про онлайн-обучение приняты вот такие сокращения:\\n\\nr0:t(w)=∑s=0trs(w)\\\\color{#348FEA}{r_{0:t}(w)} = \\\\sum\\\\limits_{s=0}^tr_s(w)r0:t\\u200b(w)=s=0∑t\\u200brs\\u200b(w);\\nОсобо отметим обозначение r0:t(wt)=∑s=0trs(wt)\\\\color{#348FEA}{r_{0:t}(w_t)} = \\\\sum\\\\limits_{s=0}^tr_s(w_t)r0:t\\u200b(wt\\u200b)=s=0∑t\\u200brs\\u200b(wt\\u200b), т.е. точка wtw_twt\\u200b фиксирована и не меняется с индексацией в сумме;\\nh0:t(w)=f1:t(w)+r0:t(w)\\\\color{#348FEA}{h_{0:t}(w)} = f_{1:t}(w) + r_{0:t}(w)h0:t\\u200b(w)=f1:t\\u200b(w)+r0:t\\u200b(w) (обычно это будет сумма функции потерь и регуляризатора);\\ngt\\\\color{#348FEA}{g_t}gt\\u200b — субградиент функции ft(w)f_t(w)ft\\u200b(w) в точке wtw_twt\\u200b.\\n\\nАддитивные регуляризаторы\\nВ новых обозначениях описанные выше алгоритмы примут вид:\\n\\nAdaptive FTRL: wT=argmin\\u2061w[f1:t(w)+RT(w)]w_T = arg\\\\min\\\\limits_w \\\\Big[f_{1:t}(w) + R_T(w)\\\\Big]wT\\u200b=argwmin\\u200b[f1:t\\u200b(w)+RT\\u200b(w)]\\nAdaptive Linearized FTRL: wT=argmin\\u2061w[∇f1:t(wt)Tw+RT(w)]w_T = arg\\\\min\\\\limits_w \\\\Big[\\\\nabla f_{1:t}(w_t)^Tw + R_T(w)\\\\Big]wT\\u200b=argwmin\\u200b[∇f1:t\\u200b(wt\\u200b)Tw+RT\\u200b(w)]\\nВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nОпишем условия, накладываемые нами на алгоритм. В обзоре они называются Setting 1.\\nSetting 1\\nОт функций RT(w)R_T(w)RT\\u200b(w) мы потребуем, чтобы они представлялись в виде:\\nRT(w)=∑t=0Trt(w)=r0:T(w)R_T(w) = \\\\sum\\\\limits_{t=0}^Tr_t(w) =  r_{0:T}(w)\\nRT\\u200b(w)=t=0∑T\\u200brt\\u200b(w)=r0:T\\u200b(w)Слагаемые должны удовлетворять следующим условиям:\\n\\nВсе rt(w)r_t(w)rt\\u200b(w) выпуклы (вниз);\\nrt(w)≥0r_t(w) \\\\geq 0rt\\u200b(w)≥0;\\nw0=argmin\\u2061wr0(w)w_0 = arg\\\\min\\\\limits_w r_0(w)w0\\u200b=argwmin\\u200br0\\u200b(w).\\n\\nТакже наложим следующие требования на h1:t=f1:t(w)+r0:t(w)h_{1:t} = f_{1:t}(w) + r_{0:t}(w)h1:t\\u200b=f1:t\\u200b(w)+r0:t\\u200b(w):\\n\\nОбласть определения h1:th_{1:t}h1:t\\u200b — непустое множество. Это требование может показаться странным, но при желании можно придумать пример h1:th_{1:t}h1:t\\u200b с пустой областью определения: достаточно взять несколько регуляризаторов-проекций Iχ(w)I_{\\\\chi}(w)Iχ\\u200b(w) на непересекающиеся выпуклые множества (подробнее о таких регуляризаторах мы расскажем в одном из следующих разделов);\\nСубдифференциал ∂wtft(w)\\\\partial_{w_t} f_t(w)∂wt\\u200b\\u200bft\\u200b(w) в точке wtw_twt\\u200b непуст.\\n\\nКлассы алгоритмов FTRL\\nБудем рассматривать аддитивные регуляризаторы rt(w)r_t(w)rt\\u200b(w) из двух семейств в зависимости от того, где у них минимум:\\n\\nFTRL-Centered: argmin\\u2061wrt(w)=w0arg\\\\min\\\\limits_w r_t(w) = w_0argwmin\\u200brt\\u200b(w)=w0\\u200b;\\nFTRL-Proximal: argmin\\u2061wrt(w)=wtarg\\\\min\\\\limits_w r_t(w) = w_targwmin\\u200brt\\u200b(w)=wt\\u200b;\\nComposite Objective: смешение первых двух семейств.\\n\\nОбратите внимание: название Proximal напрямую связано с проксимальным градиентным спуском (ссылка на учебник с проксимальными методами). В обоих случаях мы накладываем регуляризатор в текущей точке wtw_twt\\u200b.\\nОбратите внимание: для Proximal регуляризаторов зачастую требуют выполнения более сильного условия: rt(wt)=0r_t(w_t) = 0rt\\u200b(wt\\u200b)=0. Это не такое уж и серьёзное ограничение: все разумные Proximal регуляризаторы (например,  ∣∣w−wt∣∣2\\\\vert\\\\vert w - w_t\\\\vert\\\\vert^2∣∣w−wt\\u200b∣∣2) ему удовлетворяют.\\nОбратите внимание: у обоих семейств есть значимые высокоцитируемые статьи\\n\\nFTRL-Centered: метод Regularized Dual Averaging. Статья получила премию Test of Time Award на NeurIPS 2021, так как огромное количество последующих громких результатов (тот же AdaGrad) напрямую основывались на этих результатах. В названии Dual Averaging под dual average имеется в виду 1tg1:t\\\\frac1t g_{1:t}t1\\u200bg1:t\\u200b, то есть среднее по градиентам. Кардинально других техник оценок regret там нет, обзор McMahan строго улучшает все доступные там результаты.\\nFTRL-Proximal: самая известная статья от гугла Ad Click Prediction. Известна она скорее потому, что там выписаны формулы и объяснено, как правильно реализовывать метод для large-scale задач с результатами применения различных дополнительных инженерных идей. Это хороший инженерный обзор, а не математическая статья.\\n\\nРассмотрим отдельно каждую из разновидностей алгоритмов\\nFTRL-Centered\\nЗадача оптимизации имеет вид\\nwt+1=argmin\\u2061wh0:t=argmin\\u2061w[g1:tTw+r0:t(w)],w_{t+1} = arg\\\\min\\\\limits_{w} h_{0:t} = arg\\\\min\\\\limits_{w}\\\\Big[ g_{1:t}^Tw + r_{0:t}(w)\\\\Big],\\nwt+1\\u200b=argwmin\\u200bh0:t\\u200b=argwmin\\u200b[g1:tT\\u200bw+r0:t\\u200b(w)],где rt(w)r_t(w)rt\\u200b(w) таковы, что\\nargmin\\u2061wrt(w)=w0arg\\\\min\\\\limits_w r_t(w) = w_0\\nargwmin\\u200brt\\u200b(w)=w0\\u200bПример: Рассмотрим SGD с фиксированным learning rate и стартом в точке 000. Положим\\nr1(w)=12η∣∣w∣∣22r_1(w) = \\\\frac{1}{2\\\\eta}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2\\nr1\\u200b(w)=2η1\\u200b∣∣w∣∣22\\u200brt(w)=0,t>0r_t(w) = 0, \\\\quad t > 0\\nrt\\u200b(w)=0,t>0w0=0w_0 = 0\\nw0\\u200b=0wt+1=argmin\\u2061w[g1:tTw+12η∣∣w∣∣22].w_{t+1} = arg\\\\min\\\\limits_{w}\\\\Big[ g_{1:t}^Tw + \\\\frac{1}{2\\\\eta}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2\\\\Big].\\nwt+1\\u200b=argwmin\\u200b[g1:tT\\u200bw+2η1\\u200b∣∣w∣∣22\\u200b].Как мы уже знаем, итеративное обновление весов будет иметь вид\\nwt+1=wt−ηgt.w_{t+1} = w_t - \\\\eta g_t.\\nwt+1\\u200b=wt\\u200b−ηgt\\u200b.FTRL-Proximal\\nЗадача имеет похожий вид\\nwt+1=argmin\\u2061wh0:t=argmin\\u2061w[g1:tTw+r0:t(w)],w_{t+1} = arg\\\\min\\\\limits_{w} h_{0:t} = arg\\\\min\\\\limits_{w}\\\\Big[ g_{1:t}^Tw + r_{0:t}(w)\\\\Big],\\nwt+1\\u200b=argwmin\\u200bh0:t\\u200b=argwmin\\u200b[g1:tT\\u200bw+r0:t\\u200b(w)],но rt(w)r_t(w)rt\\u200b(w) выбираются так, чтобы\\nargmin\\u2061wrt(w)=wtarg\\\\min\\\\limits_w r_t(w) = w_t\\nargwmin\\u200brt\\u200b(w)=wt\\u200bПример: Рассмотрим SGD с убывающим learning rate:\\nηt=αt\\\\eta_t = \\\\frac{\\\\alpha}{\\\\sqrt{t}}\\nηt\\u200b=t\\u200bα\\u200bσt=1ηt−1ηt−1\\\\sigma_t = \\\\frac{1}{\\\\eta_t} - \\\\frac{1}{\\\\eta_{t-1}}\\nσt\\u200b=ηt\\u200b1\\u200b−ηt−1\\u200b1\\u200bПодробный вывод связи σt\\\\sigma_tσt\\u200b и ηt\\\\eta_tηt\\u200b мы приведём в одном из следующих разделов, а сейчас просто приведём результат:\\nrt(w)=σt∣∣w−wt∣∣w2r_t(w) = \\\\sigma_t\\\\vert\\\\vert w - w_t\\\\vert\\\\vert_w^2\\nrt\\u200b(w)=σt\\u200b∣∣w−wt\\u200b∣∣w2\\u200bwt+1=argmin\\u2061w[g1:tTw+∑s=1tσs∣∣w−ws∣∣w2]w_{t+1} = arg\\\\min\\\\limits_{w}\\\\Big[ g_{1:t}^Tw + \\\\sum\\\\limits_{s=1}^t\\\\sigma_s \\\\vert\\\\vert w - w_s\\\\vert\\\\vert_w^2\\\\Big]\\nwt+1\\u200b=argwmin\\u200b[g1:tT\\u200bw+s=1∑t\\u200bσs\\u200b∣∣w−ws\\u200b∣∣w2\\u200b]wt+1=wt−ηtgt=wt−αtgtw_{t+1} = w_t - \\\\eta_t g_t = w_t - \\\\frac{\\\\alpha}{\\\\sqrt{t}} g_t\\nwt+1\\u200b=wt\\u200b−ηt\\u200bgt\\u200b=wt\\u200b−t\\u200bα\\u200bgt\\u200bОбратите внимание: как правило, на практике Proximal методы работают лучше. Интуитивно, центрирование в недавних точках вместо\\nComposite-Objective FTRL\\nРассмотрим смесь центрированных и проксимальных регуляризаторов:\\nwt+1=argmin\\u2061wh0:t=argmin\\u2061w[g1:tTw+ψ0:t(w)+r0:t(w)],w_{t+1} = arg\\\\min\\\\limits_{w} h_{0:t} = arg\\\\min\\\\limits_{w}\\\\Big[ g_{1:t}^Tw + \\\\psi_{0:t}(w) + r_{0:t}(w)\\\\Big],\\nwt+1\\u200b=argwmin\\u200bh0:t\\u200b=argwmin\\u200b[g1:tT\\u200bw+ψ0:t\\u200b(w)+r0:t\\u200b(w)],где rt(w)r_t(w)rt\\u200b(w) и ψt(w)\\\\psi_t(w)ψt\\u200b(w) таковы, что\\nargmin\\u2061wrt(w)=wtarg\\\\min\\\\limits_w r_t(w) = w_t\\nargwmin\\u200brt\\u200b(w)=wt\\u200bargmin\\u2061wψt(w)=w0arg\\\\min\\\\limits_w \\\\psi_t(w) = w_0\\nargwmin\\u200bψt\\u200b(w)=w0\\u200bПример: FTRL-Proximal с L1 и L2 регуляризацией\\nwt+1=argmin\\u2061w[g1:tTw+λ1,t∣∣w∣∣1+λ2,t∣∣w∣∣w2+∑s=1tσs∣∣w−ws∣∣22]w_{t+1} = arg\\\\min\\\\limits_{w}\\\\Big[ g_{1:t}^Tw + \\\\lambda_{1,t}\\\\vert\\\\vert w\\\\vert\\\\vert_1 + \\\\lambda_{2,t}\\\\vert\\\\vert w\\\\vert\\\\vert_w^2 + \\\\sum\\\\limits_{s=1}^t\\\\sigma_s \\\\vert\\\\vert w - w_s\\\\vert\\\\vert_2^2\\\\Big]\\nwt+1\\u200b=argwmin\\u200b[g1:tT\\u200bw+λ1,t\\u200b∣∣w∣∣1\\u200b+λ2,t\\u200b∣∣w∣∣w2\\u200b+s=1∑t\\u200bσs\\u200b∣∣w−ws\\u200b∣∣22\\u200b]Обратите внимание: как правило, центрированные регуляризаторы в довесок к проксимальным вводят уже не для «дополнительной стабилизации» алгоритма, а для наложения ограничений на решение www.\\nОбратите внимание: наиболее правильные и хорошо работающие на практике способы подбора коэффициентов λ1,t\\\\lambda_{1,t}λ1,t\\u200b и λ2,t\\\\lambda_{2,t}λ2,t\\u200b мы приведём в параграфе про учет дополнительной L1L_1L1\\u200b и L2L_2L2\\u200b регуляризации.\\nГарантии сходимости для алгоритмов FTRL\\nВ этом разделе мы обсудим теоретические оценки на скорость сходимости алгоритма FTRL или, что то же самое, на скорость убывания maxRegret.\\nНапомним формулу:\\nmaxRegret(T)=max\\u2061w∗[∑t=1Tft(wt)−f1:T(w∗)]maxRegret(T) = \\\\max\\\\limits_{w^*}\\\\left[ \\\\sum\\\\limits_{t=1}^Tf_t(w_t) - f_{1:T}(w^*)\\\\right]\\nmaxRegret(T)=w∗max\\u200b[t=1∑T\\u200bft\\u200b(wt\\u200b)−f1:T\\u200b(w∗)]Чтобы делать оценки на maxRegret, нужно пытаться оценить асимптотику ряда, каждое слагаемое которого — это решение сложной оптимизационной задачи min\\u2061wf1:t(w)\\\\min\\\\limits_w f_{1:t}(w)wmin\\u200bf1:t\\u200b(w) с произвольными функциями ft(w)f_t(w)ft\\u200b(w). Работать с такой сущностью крайне сложно. Наша основная цель — сделать верхнюю оценку на regret, в которой не будет этого члена.(???)\\nStrong FTRL Lemma (Lemma 4)\\n\\nПусть ft(w)f_t(w)ft\\u200b(w) — последовательность произвольных (не обязательно) функций;\\nПусть rt(w)r_t(w)rt\\u200b(w) — последовательность выпуклых неотрицательных регуляризаторов;\\nПусть также wt+1=argmin\\u2061wh0:t(w)w_{t+1} = arg \\\\min\\\\limits_w h_{0:t}(w)wt+1\\u200b=argwmin\\u200bh0:t\\u200b(w) всегда определен (относительно слабые условия 1-2 требуют от нас это явно проговорить);\\nТогда алгоритм, выбирающий wt+1w_{t+1}wt+1\\u200b по правилу (3), удовлетворяет неравенству\\n\\nRegretT(w∗)≤r0:T(w∗)+∑t=1T[h0:t(wt)−h0:t(wt+1)−rt(wt)12]Regret_T(w^*) \\\\leq r_{0:T}(w^*) + \\\\sum\\\\limits_{t=1}^T \\\\left[h_{0:t}(w_t) - h_{0:t}(w_{t+1}) - r_t(w_t)\\\\vphantom{\\\\frac12}\\\\right]\\nRegretT\\u200b(w∗)≤r0:T\\u200b(w∗)+t=1∑T\\u200b[h0:t\\u200b(wt\\u200b)−h0:t\\u200b(wt+1\\u200b)−rt\\u200b(wt\\u200b)21\\u200b]Из чего состоит эта лемма?\\n\\n\\nСлагаемое r0:T(w∗)r_{0:T}(w^*)r0:T\\u200b(w∗) — это суммарная регуляризация в точке w∗w^*w∗. Совсем избавиться от вхождения w∗w^*w∗ не получится, но мы можем выбирать регуляризатор так, чтобы оценить сверху r0:T(w∗)r_{0:T}(w^*)r0:T\\u200b(w∗) было не очень сложно.\\n\\n\\nКаждое слагаемое суммы ∑t=1T[h0:t(wt)−h0:t(wt+1)12]\\\\sum\\\\limits_{t=1}^T \\\\left[h_{0:t}(w_t) - h_{0:t}(w_{t+1})\\\\vphantom{\\\\frac12}\\\\right]t=1∑T\\u200b[h0:t\\u200b(wt\\u200b)−h0:t\\u200b(wt+1\\u200b)21\\u200b] отражает, насколько улучшается ttt-й лосс h0:th_{0:t}h0:t\\u200b при замене wtw_twt\\u200b на wt+1=argmin\\u2061wh0:t(w)w_{t+1} = arg \\\\min\\\\limits_w h_{0:t}(w)wt+1\\u200b=argwmin\\u200bh0:t\\u200b(w). Поведение разностей h0:t(wt)−h0:t(wt+1)h_{0:t}(w_t) - h_{0:t}(w_{t+1})h0:t\\u200b(wt\\u200b)−h0:t\\u200b(wt+1\\u200b) характеризует стабильность алгоритма. Мы ожидаем, что при больших ttt у хорошо сходящегося алгоритма на очередном шаге wtw_twt\\u200b будет достаточно близок к оптимуму wt+1w_{t+1}wt+1\\u200b, то есть вся сумма будет меняться всё медленнее, и её получится разумно оценить. Пример ситуации, когда это не так, мы уже видели, когда рассматривали FTL без регуляризации для линейной функции потерь (там всё было максимально нестабильно и расходилось). К счастью, введение регуляризации обычно помогает добиться стабильности.\\n\\n\\nОбе компоненты неразрывно связаны. Добавляя регуляризацию, мы увеличиваем первую компоненту, но улучшает стабильность алгоритма, чем уменьшаем вторую, и наоборот.\\nОбратите внимание: в условиях леммы допускаются невыпуклые ft(w)f_t(w)ft\\u200b(w), и это позволяет применять её в весьма общей ситуации. Впрочем, все наши последующие выкладки все-таки будут опираться на выпуклость ft(w)f_t(w)ft\\u200b(w).\\nДоказательство Strong FTRL LemmaПреобразуем выражение для regret:\\nregret(T)=∑t=1Tft(wt)−f1:T(w∗)=regret(T) = \\\\sum\\\\limits_{t=1}^Tf_t(w_t) - f_{1:T}(w^*) =\\nregret(T)=t=1∑T\\u200bft\\u200b(wt\\u200b)−f1:T\\u200b(w∗)==∑t=0Tht(wt)−h0:T(w∗)−∑t=0Trt(wt)+r0:T(w∗)=(∗)= \\\\sum\\\\limits_{t=0}^Th_t(w_t) - h_{0:T}(w^*) - \\\\sum_{t=0}^Tr_t(w_t) + r_{0:T}(w^*) = (\\\\ast)\\n=t=0∑T\\u200bht\\u200b(wt\\u200b)−h0:T\\u200b(w∗)−t=0∑T\\u200brt\\u200b(wt\\u200b)+r0:T\\u200b(w∗)=(∗)Вспомним, что\\nwt+1=argmin\\u2061wh0:t(w),w_{t+1} = arg \\\\min\\\\limits_w h_{0:t}(w),\\nwt+1\\u200b=argwmin\\u200bh0:t\\u200b(w),откуда\\nh0:t(w∗)≥h0:t(wt+1).h_{0:t}(w^*) \\\\geq h_{0:t}(w_{t+1}).\\nh0:t\\u200b(w∗)≥h0:t\\u200b(wt+1\\u200b).Поэтому выражение выше мы можем оценить как\\n(∗)≤∑t=0Tht(wt)−h0:T(wt+1)−∑t=0Trt(wt)+r0:T(w∗)=(∗∗)(\\\\ast)\\\\leq \\\\sum\\\\limits_{t=0}^Th_t(w_t) - h_{0:T}(w_{t+1}) - \\\\sum_{t=0}^Tr_t(w_t) + r_{0:T}(w^*) = (\\\\ast\\\\ast)\\n(∗)≤t=0∑T\\u200bht\\u200b(wt\\u200b)−h0:T\\u200b(wt+1\\u200b)−t=0∑T\\u200brt\\u200b(wt\\u200b)+r0:T\\u200b(w∗)=(∗∗)Теперь займёмся первыми двумя компонентами\\n∑t=0Tht(wt)−h0:T(wt+1)=\\\\sum\\\\limits_{t=0}^Th_t(w_t) - h_{0:T}(w_{t+1}) = \\nt=0∑T\\u200bht\\u200b(wt\\u200b)−h0:T\\u200b(wt+1\\u200b)==h0(w0)+∑t=1T[h0:t(wt)−h0:t−1(wt)12]−h0:T(wT+1)= h_0(w_0) + \\\\color{#E06A27}{\\\\sum\\\\limits_{t=1}^T}\\\\left[h_{0:t}(w_t) \\\\color{#E06A27}{- h_{0:t-1}(w_t)} \\\\vphantom{\\\\frac12}\\\\right] \\\\color{#E06A27}{- h_{0:T}(w_{T+1})}\\n=h0\\u200b(w0\\u200b)+t=1∑T\\u200b[h0:t\\u200b(wt\\u200b)−h0:t−1\\u200b(wt\\u200b)21\\u200b]−h0:T\\u200b(wT+1\\u200b)Посмотрим повнимательнее на рыжие слагаемые:\\n∑t=1Th0:t−1(wt)+h0:T(wT+1)=∑t=1T∑i=0t−1hi(wt)+∑i=0Thi(wT+1)=\\\\sum\\\\limits_{t=1}^T h_{0:t-1}(w_t) + h_{0:T}(w_{T+1}) = \\\\color{#348FEA}{\\\\sum\\\\limits_{t=1}^T \\\\sum\\\\limits_{i=0}^{t-1}h_i(w_t)} + \\\\sum\\\\limits_{i=0}^T h_i(w_{T+1}) =\\nt=1∑T\\u200bh0:t−1\\u200b(wt\\u200b)+h0:T\\u200b(wT+1\\u200b)=t=1∑T\\u200bi=0∑t−1\\u200bhi\\u200b(wt\\u200b)+i=0∑T\\u200bhi\\u200b(wT+1\\u200b)==∣12s=t−1∣=∑s=0T−1∑i=0shi(ws+1)+∑i=0Thi(wT+1)== \\\\left|\\\\vphantom{\\\\frac12}\\\\color{#348FEA}{s = t - 1}\\\\right|\\n= \\\\color{#348FEA}{\\\\sum\\\\limits_{s=0}^{T-1} \\\\sum\\\\limits_{i=0}^{s}h_i(w_{s+1})} + \\\\sum\\\\limits_{i=0}^T h_i(w_{T+1}) = =\\u200b21\\u200bs=t−1\\u200b=s=0∑T−1\\u200bi=0∑s\\u200bhi\\u200b(ws+1\\u200b)+i=0∑T\\u200bhi\\u200b(wT+1\\u200b)==∑t=0T∑i=0thi(wt+1)=∑t=0Th0:t(wt+1)==\\\\sum\\\\limits_{t=0}^{T}\\\\sum\\\\limits_{i=0}^{t}h_i(w_{t+1}) = \\\\sum\\\\limits_{t=0}^Th_{0:t}(w_{t+1}) =\\n=t=0∑T\\u200bi=0∑t\\u200bhi\\u200b(wt+1\\u200b)=t=0∑T\\u200bh0:t\\u200b(wt+1\\u200b)==h0(w1)+∑t=1Th0:t(wt+1)= h_0(w_1) + \\\\sum\\\\limits_{t=1}^Th_{0:t}(w_{t+1})\\n=h0\\u200b(w1\\u200b)+t=1∑T\\u200bh0:t\\u200b(wt+1\\u200b)Подставим это:\\n(∗∗)=h0(w0)+∑t=1Th0:t(wt)−h0(w1)−∑t=1Th0:t(wt+1)−∑t=0Trt(wt)+r0:T(w∗)≤(\\\\ast\\\\ast) = h_0(w_0) + \\\\sum\\\\limits_{t=1}^Th_{0:t}(w_t) - h_0(w_1) - \\\\sum\\\\limits_{t=1}^Th_{0:t}(w_{t+1}) - \\\\sum_{t=0}^Tr_t(w_t) + r_{0:T}(w^*) \\\\leq\\n(∗∗)=h0\\u200b(w0\\u200b)+t=1∑T\\u200bh0:t\\u200b(wt\\u200b)−h0\\u200b(w1\\u200b)−t=1∑T\\u200bh0:t\\u200b(wt+1\\u200b)−t=0∑T\\u200brt\\u200b(wt\\u200b)+r0:T\\u200b(w∗)≤≤h0(w0)+r0:T(w∗)+∑t=1T[h0:t(wt)−h0:t(wt+1)−rt(wt)12].\\\\leq h_0(w_0) + r_{0:T}(w^*) + \\\\sum\\\\limits_{t=1}^T \\\\left[h_{0:t}(w_t) - h_{0:t}(w_{t+1}) - r_t(w_t)\\\\vphantom{\\\\frac12}\\\\right].\\n≤h0\\u200b(w0\\u200b)+r0:T\\u200b(w∗)+t=1∑T\\u200b[h0:t\\u200b(wt\\u200b)−h0:t\\u200b(wt+1\\u200b)−rt\\u200b(wt\\u200b)21\\u200b].Здесь мы снова воспользовались тем, что h0(w1)=r0(w1)≥0h_0(w_1) = r_0(w_1)\\\\geq 0h0\\u200b(w1\\u200b)=r0\\u200b(w1\\u200b)≥0, а также сократили h0(w0)=r0(w0)h_0(w_0) = r_0(w_0)h0\\u200b(w0\\u200b)=r0\\u200b(w0\\u200b).\\nЛемма доказана.\\nТеоретические оценки на Regret (regret bounds)\\nНиже мы представим теоремы 1,2 и 10 из обзора McMahan. Они дают оценки на regret в немного разных исходных предположениях и для разных типов регуляризаторов; асимптотика regret в каждом из случаев O(T)O(\\\\sqrt{T})O(T\\u200b), хотя константы будут различными. О важности констант в сходимости мы поговорим в одной из следующих параграфов, когда будем разбирать метод AdaGrad. В самом конце параграфа мы обсудим, какие оценки получаются для линеаризованного regret. А в следующем параграфе мы займёмся выводом конкретных алгоритмов FTRL для разных видов регуляризаторов.\\nМы не будем полностью пересказывать обзор (если вам стало интересно, рекомендуем прочитать его самостоятельно) и докажем в качестве примера теорему 2, а для остальных приведём лишь формулировки.\\nНапоминание из выпуклого анализа\\nОпределение Выпуклая функция ψ(x)\\\\psi(x)ψ(x) называется σ\\\\sigmaσ-сильно выпуклой по отношению к некоторой норме ∣∣⋅∣∣\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert∣∣⋅∣∣, если выполнено\\n∀g∈∂ψ(y)ψ(x)≥ψ(y)+gT(x−y)+σ2∣∣x−y∣∣2\\\\forall g \\\\in \\\\partial \\\\psi(y) \\\\quad \\\\psi(x) \\\\geq \\\\psi(y) + g^T(x-y) + \\\\frac{\\\\sigma}{2}\\\\vert\\\\vert x-y\\\\vert\\\\vert^2\\n∀g∈∂ψ(y)ψ(x)≥ψ(y)+gT(x−y)+2σ\\u200b∣∣x−y∣∣2Определение Двойственной нормой ∣∣⋅∣∣∗\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_*∣∣⋅∣∣∗\\u200b по отношению к норме ∣∣⋅∣∣\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert∣∣⋅∣∣ называется\\n∣∣x∣∣∗=sup\\u2061y:∣∣y∣∣≤1xTy\\\\vert\\\\vert x\\\\vert\\\\vert_* = \\\\sup\\\\limits_{y:\\\\vert\\\\vert y\\\\vert\\\\vert  \\\\leq 1} x^Ty\\n∣∣x∣∣∗\\u200b=y:∣∣y∣∣≤1sup\\u200bxTyФизический смыслЭта норма у нас возникнет в контексте работы с градиентами. С одной стороны, конечно, градиент gt=∇wtftg_t = \\\\nabla_{w_t}f_tgt\\u200b=∇wt\\u200b\\u200bft\\u200b — это вектор, но по сути он играет роль линейной функции w↦gtTww\\\\mapsto g_t^Tww↦gtT\\u200bw, и кажется логичным определять для него норму именно как для линейной функции, то есть как для элемента двойственного пространства, состоящего из ограниченных линейных функций на исходном пространстве.\\nКак можно определить норму отображения? Самый, пожалуй, естественный вариант — это рассмотреть операторую норму относительно ∣∣⋅∣∣\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert∣∣⋅∣∣:\\n∣∣x∣∣∗=sup\\u2061y≤0∣xTy∣∣∣y∣∣,\\\\vert\\\\vert x\\\\vert\\\\vert_* = \\\\sup\\\\limits_{y\\\\leq 0} \\\\frac{|x^Ty|}{\\\\vert\\\\vert y\\\\vert\\\\vert},\\n∣∣x∣∣∗\\u200b=y≤0sup\\u200b∣∣y∣∣∣xTy∣\\u200b,Это формула верна, если пространство, в котором живут xxx и yyy ненулевое (впрочем, нулевое мы вряд ли рассматриваем). Можно показать, что это выражение равно sup\\u2061y:∣∣y∣∣≤1xTy\\\\sup\\\\limits_{y:\\\\vert\\\\vert y\\\\vert\\\\vert  \\\\leq 1} x^Tyy:∣∣y∣∣≤1sup\\u200bxTy.\\nПостроенная норма называется двойственной к норме ∣∣⋅∣∣\\\\vert\\\\vert\\\\cdot\\\\vert\\\\vert∣∣⋅∣∣ на исходном пространстве.\\nБолее подробно о σ\\\\sigmaσ-сильной выпуклости и двойственных нормах вы можете почитать, например, в книге Boyd, 2004, Convex Optimization.\\nТеорема 1. General FTRL Bound\\nПусть\\n\\nОбновление параметров происходит по правилу\\n\\nwt+1=argmin\\u2061wh0:t=argmin\\u2061w[g1:tTw+r0:t(w)];w_{t+1} = arg\\\\min\\\\limits_{w} h_{0:t} = arg\\\\min\\\\limits_{w}\\\\Big[ g_{1:t}^Tw + r_{0:t}(w)\\\\Big];\\nwt+1\\u200b=argwmin\\u200bh0:t\\u200b=argwmin\\u200b[g1:tT\\u200bw+r0:t\\u200b(w)];\\nВыполнены все условия Setting 1;\\nРегуляризатор rt(w)r_t(w)rt\\u200b(w) выбирается так, чтобы выражение h0:t(w)+ft+1(w)=r0:t(w)+f1:t+1(w)h_{0:t}(w) + f_{t+1}(w) = r_{0:t}(w) + f_{1:t+1}(w)h0:t\\u200b(w)+ft+1\\u200b(w)=r0:t\\u200b(w)+f1:t+1\\u200b(w) было 1-сильно выпукло по отношению к некоторой норме ∣∣⋅∣∣t\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_{t}∣∣⋅∣∣t\\u200b (возможно, своей на каждом шаге).\\n\\nТогда\\nRegretT(w∗)≤r0:T−1(w∗)+12∑t=1T∣∣gt∣∣(t−1),∗2,Regret_T(w^*) \\\\leq r_{0:T-1}(w^*) + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g_t\\\\vert\\\\vert_{(t-1),*}^2,\\nRegretT\\u200b(w∗)≤r0:T−1\\u200b(w∗)+21\\u200bt=1∑T\\u200b∣∣gt\\u200b∣∣(t−1),∗2\\u200b,где ∣∣⋅∣∣(t−1),∗\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_{(t-1),*}∣∣⋅∣∣(t−1),∗\\u200b — норма, двойственная к норме ∣∣⋅∣∣(t−1)\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_{(t-1)}∣∣⋅∣∣(t−1)\\u200b.\\nТеорема 2. FTRL-Proximal Bound\\nПусть\\n\\nОбновление параметров происходит по правилу\\n\\nwt+1=argmin\\u2061wh0:t=argmin\\u2061w[g1:tTw+r0:t(w)];w_{t+1} = arg\\\\min\\\\limits_{w} h_{0:t} = arg\\\\min\\\\limits_{w}\\\\Big[ g_{1:t}^Tw + r_{0:t}(w)\\\\Big];\\nwt+1\\u200b=argwmin\\u200bh0:t\\u200b=argwmin\\u200b[g1:tT\\u200bw+r0:t\\u200b(w)];\\nВыполнены все условия Setting 1;\\nВсе регуляризаторы rt(w)r_t(w)rt\\u200b(w) лежат в семействе FTRL-Proximal, причём rt(wt)=0r_t(w_t) = 0rt\\u200b(wt\\u200b)=0 для всех ttt;\\nrt(w)r_t(w)rt\\u200b(w) выбирается так, чтобы выражение h0:t(w)=r0:t(w)+f1:t(w)h_{0:t}(w) = r_{0:t}(w) + f_{1:t}(w)h0:t\\u200b(w)=r0:t\\u200b(w)+f1:t\\u200b(w) было 1-сильно выпукло по отношению к некоторой норме ∣∣⋅∣∣t\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_t∣∣⋅∣∣t\\u200b (возможно, своей на каждом шаге).\\n\\nТогда\\nRegretT(w∗)≤r0:T(w∗)+12∑t=1T∣∣gt∣∣t,∗2,Regret_T(w^*) \\\\leq r_{0:T}(w^*) + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g_t\\\\vert\\\\vert_{t,*}^2,\\nRegretT\\u200b(w∗)≤r0:T\\u200b(w∗)+21\\u200bt=1∑T\\u200b∣∣gt\\u200b∣∣t,∗2\\u200b,где ∣∣⋅∣∣t,∗\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_{t,*}∣∣⋅∣∣t,∗\\u200b — норма, двойственная к норме ∣∣⋅∣∣t\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_{t}∣∣⋅∣∣t\\u200b.\\nТеорема 10. Composite Objective FTRL-Proximal Bound\\nПусть\\n\\nОбновление параметров происходит по правилу\\n\\nwt+1=argmin\\u2061wh0:t=argmin\\u2061w[g1:tTw+α1:tΨ(w)+r0:t(w)];w_{t+1} = arg\\\\min\\\\limits_{w} h_{0:t} = arg\\\\min\\\\limits_{w} \\\\Big[g_{1:t}^Tw + \\\\alpha_{1:t}\\\\Psi(w) + r_{0:t}(w)\\\\Big];\\nwt+1\\u200b=argwmin\\u200bh0:t\\u200b=argwmin\\u200b[g1:tT\\u200bw+α1:t\\u200bΨ(w)+r0:t\\u200b(w)];\\nВыполнены все условия Settning 1;\\nh^t(w)=ft(w)+αtΨ(w)+rt(w)\\\\hat{h}_t(w) = f_t(w) + \\\\alpha_t\\\\Psi(w) + r_t(w)h^t\\u200b(w)=ft\\u200b(w)+αt\\u200bΨ(w)+rt\\u200b(w);\\nαt\\\\alpha_tαt\\u200b — неубывающая последовательность;\\nΨ(w)\\\\Psi(w)Ψ(w) — Centered регуляризатор с минимумом в точке w0w_0w0\\u200b;\\nrt(w)r_t(w)rt\\u200b(w) — Proximal регуляризаторы;\\nrt(w)r_t(w)rt\\u200b(w) выбирается так, чтобы выражение h0:t^(w)=r0:t(w)+α1:tΨ(w)+f1:t(w)\\\\hat{h_{0:t}}(w) = r_{0:t}(w) + \\\\alpha_{1:t}\\\\Psi(w) + f_{1:t}(w)h0:t\\u200b^\\u200b(w)=r0:t\\u200b(w)+α1:t\\u200bΨ(w)+f1:t\\u200b(w) было 1-сильно выпукло по отношению к некоторой норме ∣∣⋅∣∣t\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_t∣∣⋅∣∣t\\u200b (возможно, своей на каждом шаге).\\n\\nТогда\\n\\nЕсли мы рассматриваем regret относительно f^t(w)=ft(w)+αtΨ(w)\\\\hat{f}_t(w) = f_t(w) + \\\\alpha_t\\\\Psi(w)f^\\u200bt\\u200b(w)=ft\\u200b(w)+αt\\u200bΨ(w), то\\n\\nRegretT(w∗)≤r0:T(w∗)+12∑t=1T∣∣gt∣∣t,∗2;Regret_T(w^*) \\\\leq r_{0:T}(w^*) + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g_t\\\\vert\\\\vert_{t,*}^2;\\nRegretT\\u200b(w∗)≤r0:T\\u200b(w∗)+21\\u200bt=1∑T\\u200b∣∣gt\\u200b∣∣t,∗2\\u200b;\\nЕсли мы рассматриваем regret относительно ft(w)f_t(w)ft\\u200b(w), то\\n\\nRegretT(w∗)≤r0:T(w∗)+α1:tΨ(w∗)+12∑t=1T∣∣gt∣∣t,∗2,Regret_T(w^*) \\\\leq r_{0:T}(w^*) + \\\\alpha_{1:t}\\\\Psi(w^*) + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g_t\\\\vert\\\\vert_{t,*}^2,\\nRegretT\\u200b(w∗)≤r0:T\\u200b(w∗)+α1:t\\u200bΨ(w∗)+21\\u200bt=1∑T\\u200b∣∣gt\\u200b∣∣t,∗2\\u200b,где ∣∣⋅∣∣(t),∗\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_{(t),*}∣∣⋅∣∣(t),∗\\u200b — норма, двойственная к норме ∣∣⋅∣∣t\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_{t}∣∣⋅∣∣t\\u200b.\\nОбратите внимание. Оценки Proximal и General отличаются индексацией: до ttt или до t−1t-1t−1 соответственно. Это чисто техническое различие, однако именно из-за него с Proximal регуляризаторами удобнее работать как в теоретических выкладках, так и при выведении практических методов.\\nОбратите внимание. На ft(w)f_t(w)ft\\u200b(w) мы не хотим накладывать ограничения сильной выпуклости, но сильную выпуклость функции h0:t(w)=f1:t(w)+r0:t(w)h_{0:t}(w) = f_{1:t}(w) + r_{0:t}(w)h0:t\\u200b(w)=f1:t\\u200b(w)+r0:t\\u200b(w) можно обеспечить за счет выбора сильно выпуклых регуляризаторов. В самом деле, сумма выпуклой и сильно выпуклой функций сильно выпукла. Если\\nft(w)≥ft(wt)+(w−wt)T∇ft(wt)f_t(w) \\\\geq f_t(w_t) + (w - w_t)^T\\\\nabla f_t(w_t)\\nft\\u200b(w)≥ft\\u200b(wt\\u200b)+(w−wt\\u200b)T∇ft\\u200b(wt\\u200b)и\\nrt(w)≥rt(wt)+(w−wt)T∇rt(wt)+12∣∣w−wt∣∣2,r_t(w) \\\\geq r_t(w_t) + (w - w_t)^T\\\\nabla r_t(w_t) + \\\\frac{1}{2}\\\\vert\\\\vert w - w_t\\\\vert\\\\vert^2,\\nrt\\u200b(w)≥rt\\u200b(wt\\u200b)+(w−wt\\u200b)T∇rt\\u200b(wt\\u200b)+21\\u200b∣∣w−wt\\u200b∣∣2,то\\nft(w)+rt(w)≥ft(wt)+rt(wt)+(w−wt)T(∇ft(wt)+∇rt(wt))+12∣∣w−wt∣∣2.f_t(w) + r_t(w) \\\\geq f_t(w_t) + r_t(w_t) + (w - w_t)^T(\\\\nabla f_t(w_t) + \\\\nabla r_t(w_t)) + \\\\frac{1}{2}\\\\vert\\\\vert w - w_t\\\\vert\\\\vert^2.\\nft\\u200b(w)+rt\\u200b(w)≥ft\\u200b(wt\\u200b)+rt\\u200b(wt\\u200b)+(w−wt\\u200b)T(∇ft\\u200b(wt\\u200b)+∇rt\\u200b(wt\\u200b))+21\\u200b∣∣w−wt\\u200b∣∣2.Обратите внимание. Норма ∣∣w∣∣t,∗2\\\\vert\\\\vert w\\\\vert\\\\vert_{t,*}^2∣∣w∣∣t,∗2\\u200b является сопряженной к норме, относительно которой 1-сильно выпукла функция h0:t(w)=f1:t(w)+r0:t(w)h_{0:t}(w) = f_{1:t}(w) + r_{0:t}(w)h0:t\\u200b(w)=f1:t\\u200b(w)+r0:t\\u200b(w). Это значит, что норму мы будем выбирать по сумме регуляризаторов r0:t(w)r_{0:t}(w)r0:t\\u200b(w), а не просто по rt(w)r_t(w)rt\\u200b(w).\\nДоказательство на примере теоремы 2\\nНам понадобится следующая чисто техническая лемма, доказательство которой мы опустим. Желающие могут прочитать Appendix B в обзоре.\\nLemma 7. Пусть\\n\\nϕ1\\\\phi_1ϕ1\\u200b — выпуклая функция Rn→R∪{∞}\\\\mathbb{R}^n \\\\rightarrow \\\\mathbb{R} \\\\cup \\\\{\\\\infty\\\\}Rn→R∪{∞}, для которой существует x1=argmin\\u2061xϕ1(x)x_1 = arg\\\\min\\\\limits_x \\\\phi_1(x)x1\\u200b=argxmin\\u200bϕ1\\u200b(x);\\nψ\\\\psiψ — выпуклая функция;\\nϕ2(x)=ϕ1(x)+ψ(x)\\\\phi_2(x) = \\\\phi_1(x) + \\\\psi(x)ϕ2\\u200b(x)=ϕ1\\u200b(x)+ψ(x) — выпуклая функция, для которой существует x2=argmin\\u2061xϕ2(x)x_2 = arg\\\\min\\\\limits_x \\\\phi_2(x)x2\\u200b=argxmin\\u200bϕ2\\u200b(x) и которая, кроме того, 1-сильно выпукла по норме ∣∣⋅∣∣\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert∣∣⋅∣∣.\\n\\nТогда, для любого элемента bbb субдифференциала ∂x1ψ\\\\partial_{x_1} \\\\psi∂x1\\u200b\\u200bψ имеет место неравенство\\n∣∣x1−x2∣∣≤∣∣b∣∣∗\\\\vert\\\\vert x_1 - x_2\\\\vert\\\\vert  \\\\leq \\\\vert\\\\vert b\\\\vert\\\\vert_*\\n∣∣x1\\u200b−x2\\u200b∣∣≤∣∣b∣∣∗\\u200bи для любого x′x'x′ имеет место неравенство\\nϕ2(x1)−ϕ2(x′)≤12∣∣b∣∣∗.\\\\phi_2(x_1) - \\\\phi_2(x') \\\\leq \\\\frac{1}{2}\\\\vert\\\\vert b\\\\vert\\\\vert_*.\\nϕ2\\u200b(x1\\u200b)−ϕ2\\u200b(x′)≤21\\u200b∣∣b∣∣∗\\u200b.Доказательство теоремы 2\\nРассмотрим соседние раунды wtw_twt\\u200b и wt+1w_{t+1}wt+1\\u200b. Имеем\\nwt=argmin\\u2061wh0:t−1=argmin\\u2061w[f1:t−1+r0:t−1]w_{t} = arg\\\\min\\\\limits_w h_{0:t-1} = arg\\\\min\\\\limits_w\\\\left[f_{1:t-1} + r_{0:t-1}\\\\right]\\nwt\\u200b=argwmin\\u200bh0:t−1\\u200b=argwmin\\u200b[f1:t−1\\u200b+r0:t−1\\u200b]Обозначим ϕ1(w)=f1:t−1(w)+r0:t(w)=h0:t−1(w)+rt(w)=h0:t(w)−ft(w)\\\\phi_1(w) = f_{1:t-1}(w) + r_{0:t}(w) = h_{0:t-1}(w) + r_t(w) = h_{0:t}(w) - f_t(w)ϕ1\\u200b(w)=f1:t−1\\u200b(w)+r0:t\\u200b(w)=h0:t−1\\u200b(w)+rt\\u200b(w)=h0:t\\u200b(w)−ft\\u200b(w). Поскольку wtw_twt\\u200b одновременно минимизирует и rt(w)r_t(w)rt\\u200b(w) (т.к. это proximal регуляризатор), и h0:t−1h_{0:t-1}h0:t−1\\u200b, имеем\\nwt=argmin\\u2061w[h0:t−1+rt(w)]=argmin\\u2061wϕ1(w).w_t = arg\\\\min\\\\limits_w \\\\Big[h_{0:t-1} + r_t(w)\\\\Big] = arg\\\\min\\\\limits_w \\\\phi_1(w).\\nwt\\u200b=argwmin\\u200b[h0:t−1\\u200b+rt\\u200b(w)]=argwmin\\u200bϕ1\\u200b(w).Далее,\\nwt+1=argmin\\u2061wh0:t=argmin\\u2061w[ϕ1(w)+ft(w)]w_{t+1} = arg\\\\min\\\\limits_w h_{0:t} = arg\\\\min\\\\limits_w \\\\Big[\\\\phi_1(w) + f_t(w)\\\\Big]\\nwt+1\\u200b=argwmin\\u200bh0:t\\u200b=argwmin\\u200b[ϕ1\\u200b(w)+ft\\u200b(w)]Выпишем оценку из Strong FTRL Lemma и постараемся оценить отмеченные рыжим слагаемые\\n∑t=1Tft(wt)−f1:T(w∗)≤r0:T(w∗)+∑t=1Th0:t(wt)−h0:t(wt+1)−rt(wt)\\\\sum\\\\limits_{t=1}^Tf_t(w_t) - f_{1:T}(w^*) \\\\leq r_{0:T}(w^*) + \\\\color{#E06A27}{\\\\sum\\\\limits_{t=1}^Th_{0:t}(w_t) - h_{0:t}(w_{t+1}) - r_t(w_t)}\\nt=1∑T\\u200bft\\u200b(wt\\u200b)−f1:T\\u200b(w∗)≤r0:T\\u200b(w∗)+t=1∑T\\u200bh0:t\\u200b(wt\\u200b)−h0:t\\u200b(wt+1\\u200b)−rt\\u200b(wt\\u200b)Так как по условию теоремы rt(wt)=0r_t(w_t) = 0rt\\u200b(wt\\u200b)=0, мы можем убрать это слагаемое:\\nh0:t(wt)−h0:t(wt+1)−rt(wt)=h0:t(wt)−h0:t(wt+1)=h_{0:t}(w_t) - h_{0:t}(w_{t+1}) - r_t(w_t) = \\\\color{#C81D6B}{h_{0:t}(w_t)} - \\\\color{#348FEA}{h_{0:t}(w_{t+1})} = \\nh0:t\\u200b(wt\\u200b)−h0:t\\u200b(wt+1\\u200b)−rt\\u200b(wt\\u200b)=h0:t\\u200b(wt\\u200b)−h0:t\\u200b(wt+1\\u200b)==ϕ1(wt)+ft(wt)−(ϕ1(wt+1)+ft(wt+1))= \\\\color{#C81D6B}{\\\\phi_1(w_t) + f_t(w_t)} - \\\\color{#348FEA}{(\\\\phi_1(w_{t+1}) + f_t(w_{t+1}))}\\n=ϕ1\\u200b(wt\\u200b)+ft\\u200b(wt\\u200b)−(ϕ1\\u200b(wt+1\\u200b)+ft\\u200b(wt+1\\u200b))Обозначим ϕ2(w)=ϕ1(w)+ft(w)\\\\phi_2(w) = \\\\phi_1(w) + f_t(w)ϕ2\\u200b(w)=ϕ1\\u200b(w)+ft\\u200b(w). Применив Лемму 7, получаем\\nϕ1(wt)+ft(wt)−ϕ1(wt+1)−ft(wt+1)≤12∣∣gt∣∣t,∗2\\\\phi_1(w_t) + f_t(w_t) - \\\\phi_1(w_{t+1}) - f_t(w_{t+1}) \\\\leq \\\\frac{1}{2}\\\\vert\\\\vert g_t\\\\vert\\\\vert_{t,*}^2\\nϕ1\\u200b(wt\\u200b)+ft\\u200b(wt\\u200b)−ϕ1\\u200b(wt+1\\u200b)−ft\\u200b(wt+1\\u200b)≤21\\u200b∣∣gt\\u200b∣∣t,∗2\\u200bО связи оценок на regret для обычного и линеаризованного FTRL\\nВспомним, что для линеаризованного FTRL имеет место неравенство:\\nRegretT(w)≤LinearizedRegretT(w).Regret_T(w) \\\\leq LinearizedRegret_T(w).\\nRegretT\\u200b(w)≤LinearizedRegretT\\u200b(w).Увы, верхняя оценка на левую часть неравенства не помогает оценить правую. Поэтому рассмотрим линеаризованный алгоритм более подробно. Он работает с последовательностью функций f^t(w)=gtTw\\\\hat{f}_t(w) = g_t^Twf^\\u200bt\\u200b(w)=gtT\\u200bw, где gt∈∂wtftg_t \\\\in \\\\partial_{w_t} f_tgt\\u200b∈∂wt\\u200b\\u200bft\\u200b. Субдифференциал f^t\\\\hat{f}_tf^\\u200bt\\u200b состоит из одного вектора (градиента это функции)\\ng^t=∂f^t(w)∂w=gt\\\\hat{g}_t = \\\\frac{\\\\partial \\\\hat{f}_t(w)}{\\\\partial w} = g_t\\ng^\\u200bt\\u200b=∂w∂f^\\u200bt\\u200b(w)\\u200b=gt\\u200bПрименим приведённые выше оценки на regret для исходного и для линеаризованного алгоритма:\\nRegretT(w∗)≤r0:T(w∗)+12∑t=1T∣∣gt∣∣t,∗2Regret_T(w^*) \\\\leq r_{0:T}(w^*) + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g_t\\\\vert\\\\vert_{t,*}^2\\nRegretT\\u200b(w∗)≤r0:T\\u200b(w∗)+21\\u200bt=1∑T\\u200b∣∣gt\\u200b∣∣t,∗2\\u200bLinearizedRegretT(w∗)≤r0:T(w∗)+12∑t=1T∣∣g^t∣∣t,∗2=r0:T(w∗)+12∑t=1T∣∣gt∣∣t,∗2LinearizedRegret_T(w^*) \\\\leq r_{0:T}(w^*) + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert \\\\hat{g}_t\\\\vert\\\\vert_{t,*}^2 = r_{0:T}(w^*) + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g_t\\\\vert\\\\vert_{t,*}^2\\nLinearizedRegretT\\u200b(w∗)≤r0:T\\u200b(w∗)+21\\u200bt=1∑T\\u200b∣∣g^\\u200bt\\u200b∣∣t,∗2\\u200b=r0:T\\u200b(w∗)+21\\u200bt=1∑T\\u200b∣∣gt\\u200b∣∣t,∗2\\u200bЛегко убедиться, что оценки regret для обычного и линеаризованного FTRL совпадают и выполнено соотношение\\nRegretT(w∗)≤LinearizedRegretT(w∗)≤TheoremRegretT(w∗).Regret_T(w^*) \\\\leq LinearizedRegret_T(w^*) \\\\leq TheoremRegret_T(w^*).\\nRegretT\\u200b(w∗)≤LinearizedRegretT\\u200b(w∗)≤TheoremRegretT\\u200b(w∗).Таким образом, для линеаризованного варианта любого алгоритма FTRL не нужно доказывать собственные оценки. А поскольку линеаризованный FTRL намного эффективнее, в дальнейшем мы всегда будем сразу переходить от исходного алгоритма к линеаризованному.\\nПостроение эффективного адаптивного FTRL\\nТеперь, когда мы получили теоретические оценки на качество работы адаптивного FTRL, настала пора рассмотреть несколько конкретных примеров алгоритмов из этого класса.\\nСемейство квадратичных регуляризаторов rt(w)r_t(w)rt\\u200b(w)\\nВо всех дальнейших выкладках мы сразу ограничим себя семейством квадратичных регуляризаторов:\\n\\nДля FTRL-Centered алгоритмов: rt(w)=wTDtw=∣∣w∣∣Dt2r_t(w) = w^TD_tw = \\\\vert\\\\vert w\\\\vert\\\\vert_{D_t}^2rt\\u200b(w)=wTDt\\u200bw=∣∣w∣∣Dt\\u200b2\\u200b,\\nДля FTRL-Proximal алгоритмов: rt(w)=(w−wt)TDt(w−wt)=∣∣w−wt∣∣Dt2r_t(w) = (w - w_t)^TD_t(w - w_t) = \\\\vert\\\\vert w - w_t\\\\vert\\\\vert_{D_t}^2rt\\u200b(w)=(w−wt\\u200b)TDt\\u200b(w−wt\\u200b)=∣∣w−wt\\u200b∣∣Dt\\u200b2\\u200b,\\n\\nгде DtD_tDt\\u200b — некоторая симметричная положительно определённая матрица (возможно, своя для каждого шага).\\nПомимо того, что они удобны и привычны, таки регуляризаторы позволяют достаточно просто выписывать оценки на regret. Чтобы в этом убедиться, вспомним, какие нетривиальные сущности возникают в теоремах:\\n\\nна каждом шаге нам нужно выбрать норму ∣∣⋅∣∣t\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_{t}∣∣⋅∣∣t\\u200b, по отношение к которой выражение h0:t(w)+ft+1(w)=r0:t(w)+f1:t+1(w)h_{0:t}(w) + f_{t+1}(w) = r_{0:t}(w) + f_{1:t+1}(w)h0:t\\u200b(w)+ft+1\\u200b(w)=r0:t\\u200b(w)+f1:t+1\\u200b(w) было бы 1-сильно выпуклым;\\nво всех оценках участвует r0:T(w∗)r_{0:T}(w^*)r0:T\\u200b(w∗) (или r0:T−1(w∗)r_{0:T-1}(w^*)r0:T−1\\u200b(w∗)), и его хорошо бы уметь оценивать сверху;\\nтакже в оценках фигурирует норма, двойственная к ∣∣⋅∣∣t\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_{t}∣∣⋅∣∣t\\u200b, и её нужно уметь выводить.\\n\\nДавайте разберёмся с каждым из пунктов и поймём, почему для квадратичных регуляризаторов всё довольно хорошо.\\nВыбор нормы ∣∣⋅∣∣t\\\\vert\\\\vert\\\\cdot\\\\vert\\\\vert_{t}∣∣⋅∣∣t\\u200b\\nТут всё просто:\\n\\nРегуляризатор ∣∣w∣∣D\\\\vert\\\\vert w\\\\vert\\\\vert_D∣∣w∣∣D\\u200b является 1-сильно выпуклым относительно нормы ∣∣w∣∣D\\\\vert\\\\vert w\\\\vert\\\\vert_D∣∣w∣∣D\\u200b (т.е. относительно себя же);\\nРегуляризатор ∣∣w−wt∣∣D\\\\vert\\\\vert w - w_t\\\\vert\\\\vert_D∣∣w−wt\\u200b∣∣D\\u200b является 1-сильно выпуклым относительно той же самой нормы ∣∣w∣∣D\\\\vert\\\\vert w\\\\vert\\\\vert_D∣∣w∣∣D\\u200b.\\n\\nНам, впрочем, нужна 1-сильная выпуклость всей суммы r0:t(w)r_{0:t}(w)r0:t\\u200b(w), но легко убедиться, что r0:tr_{0:t}r0:t\\u200b 1-сильно выпукло относительно суммарной нормы ∣∣⋅∣∣D0:t2\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert_{D_{0:t}}^2∣∣⋅∣∣D0:t\\u200b2\\u200b. Поскольку D0:tD_{0:t}D0:t\\u200b — тоже симметричная положительно определенная матрица, мы остаёмся в том же классе норм Махаланобиса.\\nДвойственная норма r0:t(w)r_{0:t}(w)r0:t\\u200b(w)\\nОказывается, что\\n∣∣w∣∣D,∗=∣∣w∣∣D−1\\\\vert\\\\vert w\\\\vert\\\\vert_{D,*} = \\\\vert\\\\vert w\\\\vert\\\\vert_{D^{-1}}\\n∣∣w∣∣D,∗\\u200b=∣∣w∣∣D−1\\u200bДоказательствоПо определению\\n∣∣x∣∣D,∗=sup{xTy:yTDy≤1}\\\\vert\\\\vert x\\\\vert\\\\vert_{D,*} = sup \\\\{ x^Ty: y^TDy \\\\leq 1 \\\\}\\n∣∣x∣∣D,∗\\u200b=sup{xTy:yTDy≤1}Для начала заметим, что ограничение-неравенство можно заменить на ограничение-равенство. А именно, если ∣∣y∣∣D<1\\\\vert\\\\vert y\\\\vert\\\\vert_D < 1∣∣y∣∣D\\u200b<1, то, взяв α>1:∣∣αy∣∣D=1\\\\alpha > 1: \\\\vert\\\\vert \\\\alpha y\\\\vert\\\\vert_D = 1α>1:∣∣αy∣∣D\\u200b=1, мы получим ∣αxTy∣>∣xTy∣\\\\vert\\\\alpha x^Ty\\\\vert > \\\\vert x^Ty\\\\vert∣αxTy∣>∣xTy∣. Значит, супремум можно искать на границе.\\nДалее, заметим, что мы работаем с конечномерными пространствами (вряд ли у вектора весов бесконечное число компонент), поэтому единичный шар ∣∣y∣∣D=1\\\\vert\\\\vert y\\\\vert\\\\vert_D = 1∣∣y∣∣D\\u200b=1 является компактом и, стало быть, супремум на нём достигается. Таким образом, мы можем решать привычную задачу оптимизации с ограничениями и применить для неё метод множителей Лагранжа.\\nВыпишем функцию Лагранжа:\\nL(y,λ)=xTy−λ(yTDy−1)L(y, \\\\lambda) = x^Ty - \\\\lambda(y^TDy - 1)\\nL(y,λ)=xTy−λ(yTDy−1)Продифференцируем её и приравняем градиент к нулю:\\n∇yL(y,λ)=x−2λy(D+DT)=0\\\\nabla_y L(y, \\\\lambda) = x - 2\\\\lambda y(D + D^T) = 0\\n∇y\\u200bL(y,λ)=x−2λy(D+DT)=0Так как матрица DDD симметрична, имеем D+DT=2DD + D^T = 2DD+DT=2D и, следовательно,\\ny=1λD−1xy = \\\\frac{1}{\\\\lambda}D^{-1}x\\ny=λ1\\u200bD−1xПодставим его в граничное условие-равенство и выразим λ\\\\lambdaλ:\\n1=yTDy=(−1λD−1x)TD(−1λD−1x)=1λ2xTD−TDD−1x1 = y^TDy = \\\\left(-\\\\frac{1}{\\\\lambda}D^{-1}x\\\\right)^TD\\\\left(-\\\\frac{1}{\\\\lambda}D^{-1}x\\\\right) =\\\\frac{1}{\\\\lambda^2}x^TD^{-T}DD^{-1}x\\n1=yTDy=(−λ1\\u200bD−1x)TD(−λ1\\u200bD−1x)=λ21\\u200bxTD−TDD−1xОтсюда\\nxTD−Tx=λ2x^TD^{-T}x = \\\\lambda^2\\nxTD−Tx=λ2Транспонировав обе части, получаем\\nxTD−1x=λ2x^TD^{-1}x = \\\\lambda^2\\nxTD−1x=λ2λ=xTD−1x\\\\lambda = \\\\sqrt{x^TD^{-1}x}\\nλ=xTD−1x\\u200bПодставим найденное λ\\\\lambdaλ обратно в выражение y=1λD−1xy = \\\\frac{1}{\\\\lambda}D^{-1}xy=λ1\\u200bD−1x:\\ny=D−1xxTD−1x,y = \\\\frac{D^{-1}x}{\\\\sqrt{x^TD^{-1}x}},\\ny=xTD−1x\\u200bD−1x\\u200b,а полученное решение подставим в исходную функцию xTyx^TyxTy:\\nxTy=xTD−1xxTD−1x=xTD−1x=∣∣x∣∣D−1.x^Ty = \\\\frac{x^TD^{-1}x}{\\\\sqrt{x^TD^{-1}x}} = \\\\sqrt{x^TD^{-1}x} = \\\\vert\\\\vert x\\\\vert\\\\vert_{D^{-1}}.\\nxTy=xTD−1x\\u200bxTD−1x\\u200b=xTD−1x\\u200b=∣∣x∣∣D−1\\u200b.Ограничение сверху для r0:t(w∗)r_{0:t}(w^*)r0:t\\u200b(w∗)\\nСтрого говоря, здесь никаких гарантий нет, и, например, очень плохая инициализация может всё сильно испортить. На практике, впрочем, всё работает нормально, но авторы статей не могут себе позволить надеяться на благосклонность судьбы. Поэтому в статьях часто встречается следующий костыль. Для вывода оценок на regret вводится регуляризатор r0(w)=IR(w)r_0(w) = I_{R}(w)r0\\u200b(w)=IR\\u200b(w), где\\nIR(w)={∞∣∣w∣∣>R0∣∣w∣∣≤RI_{R}(w) = \\\\begin{cases}\\n      \\\\infty & \\\\vert\\\\vert w \\\\vert\\\\vert > R \\\\\\\\\\n      0 & \\\\vert\\\\vert w \\\\vert\\\\vert \\\\leq R\\n   \\\\end{cases}\\nIR\\u200b(w)={∞0\\u200b∣∣w∣∣>R∣∣w∣∣≤R\\u200bэто проекция на шар. Тогда можно доказать, что ∣∣w∗∣∣≤R\\\\vert\\\\vert w^* \\\\vert\\\\vert \\\\leq R∣∣w∗∣∣≤R.\\nСемейство логарифмических регуляризаторов\\nДля ряда частных задач вроде expert advice problem и оптимизаций по вероятностным распределениям используется также семейство энтропийных регуляризаторов\\nrt(w)=∑i=1Nwilog\\u2061wir_t(w) = \\\\sum\\\\limits_{i=1}^Nw_i\\\\log{w_i}\\nrt\\u200b(w)=i=1∑N\\u200bwi\\u200blogwi\\u200bБолее подробно о нём можно почитать в обзоре Shai-Shalev Schwartz, пример 2.5.\\nConstant learning rate FTRL\\nПростейший пример — это константный регуляризатор\\nrs(w)={12η∣∣w∣∣22,\\xa0s=0,0,\\xa0s>0r_s(w) = \\\\begin{cases}\\n\\\\frac{1}{2\\\\eta}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2,\\\\ s=0,\\\\\\\\ \\n0,\\\\ s > 0\\n\\\\end{cases}\\nrs\\u200b(w)={2η1\\u200b∣∣w∣∣22\\u200b,\\xa0s=0,0,\\xa0s>0\\u200bЛегко показать, что 12η∣∣w∣∣22=∣∣w∣∣12ηI2\\\\frac{1}{2\\\\eta}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2 = \\\\vert\\\\vert w\\\\vert\\\\vert_{\\\\frac{1}{2\\\\eta}I}^22η1\\u200b∣∣w∣∣22\\u200b=∣∣w∣∣2η1\\u200bI2\\u200b.\\nСоответствующий итерационный процесс оптимизации имеет вид\\nwt+1=argmin\\u2061w[g1:tTw+12η∣∣w∣∣22]w_{t+1} = arg\\\\min\\\\limits_w\\\\Big[ g_{1:t}^Tw + \\\\frac{1}{2\\\\eta}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2\\\\Big]\\nwt+1\\u200b=argwmin\\u200b[g1:tT\\u200bw+2η1\\u200b∣∣w∣∣22\\u200b]Как мы уже наблюдали ранее, этот метод эквивалентен методу стохастического градиентного спуска с константным learning rate. А именно, шаг обновления весов можно сформулировать двумя способами:\\n\\n\\nна языке FTRL: wt+1=−ηg1:tTw_{t+1} = -\\\\eta g_{1:t}^Twt+1\\u200b=−ηg1:tT\\u200b;\\n\\n\\nна языке градиентного спуска: wt+1=wt−ηgtw_{t+1} = w_t - \\\\eta g_twt+1\\u200b=wt\\u200b−ηgt\\u200b.\\n\\n\\nОценка на Regret (3.1 Constant Learning Rate Online Gradient Descent). Пусть\\n\\n∣∣gt∣∣≤G\\\\vert\\\\vert g_t\\\\vert\\\\vert  \\\\leq G∣∣gt\\u200b∣∣≤G;\\n∣∣w∗∣∣≤R\\\\vert\\\\vert w^*\\\\vert\\\\vert  \\\\leq R∣∣w∗∣∣≤R.\\n\\nТогда, если взять η=RGT′\\\\eta = \\\\frac{R}{G\\\\sqrt{T'}}η=GT′\\u200bR\\u200b, то для любого T′≤TT' \\\\leq TT′≤T\\nRegretT(w∗)≤RGT′Regret_T(w^*) \\\\leq RG\\\\sqrt{T'}\\nRegretT\\u200b(w∗)≤RGT′\\u200bВ целом, такая стратегия регуляризации не самая оптимальная. Интуитивно, наш регуляризатор фиксирован вне зависимости от того, сколько мы уже сыграли раундов, и со временем может перестать компенсировать член g1:tTwg_{1:t}^Twg1:tT\\u200bw, и тогда стабильность алгоритма может падать.\\nFTRL с learning rate scheduling\\nЧтобы исправить нестабильность алгоритма, возьмём L2L_2L2\\u200b-регуляризатор, не равный нулю на каждом шаге.\\nПроцесс оптимизации примет вид:\\n\\nДля FTRL-Proximal: wt+1=argmin\\u2061w[g1:tTw+∑s=0tσs2∣∣w−ws∣∣22]w_{t+1} = arg\\\\min\\\\limits_w \\\\Big[ g_{1:t}^Tw + \\\\sum\\\\limits_{s=0}^t\\\\frac{\\\\sigma_s}{2}\\\\vert\\\\vert w-w_s\\\\vert\\\\vert_2^2 \\\\Big]wt+1\\u200b=argwmin\\u200b[g1:tT\\u200bw+s=0∑t\\u200b2σs\\u200b\\u200b∣∣w−ws\\u200b∣∣22\\u200b];\\nДля FTRL-Centered: wt+1=argmin\\u2061w[g1:tTw+∑s=0tσs2∣∣w∣∣22]w_{t+1} = arg\\\\min\\\\limits_w \\\\Big[ g_{1:t}^Tw + \\\\sum\\\\limits_{s=0}^t\\\\frac{\\\\sigma_s}{2}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2 \\\\Big]wt+1\\u200b=argwmin\\u200b[g1:tT\\u200bw+s=0∑t\\u200b2σs\\u200b\\u200b∣∣w∣∣22\\u200b].\\n\\nПосмотрим, какое обличье примет алгоритм FTRL-Proximal, если его изложить на языке градиентного спуска. Для этого продифференцируем и приравниваем нулю выражение, которое мы минимизируем:\\n0=g1:t+∑s=0tσs(w−ws)0 = g_{1:t} + \\\\sum\\\\limits_{s=0}^t\\\\sigma_s (w - w_s)\\n0=g1:t\\u200b+s=0∑t\\u200bσs\\u200b(w−ws\\u200b)∑s=0tσsws−g1:t=σ0:tw\\\\sum\\\\limits_{s=0}^t\\\\sigma_s w_s - g_{1:t} = \\\\sigma_{0:t} w\\ns=0∑t\\u200bσs\\u200bws\\u200b−g1:t\\u200b=σ0:t\\u200bwwt+1=1σ0:t∑s=0tσsws−1σ0:tg1:tw_{t+1} = \\\\frac{1}{\\\\sigma_{0:t}} \\\\sum\\\\limits_{s=0}^t\\\\sigma_s w_s -\\\\frac{1}{\\\\sigma_{0:t}} g_{1:t}\\nwt+1\\u200b=σ0:t\\u200b1\\u200bs=0∑t\\u200bσs\\u200bws\\u200b−σ0:t\\u200b1\\u200bg1:t\\u200bПопробуем получить рекуррентную формулу для выражения wt+1w_{t+1}wt+1\\u200b через wtw_twt\\u200b:\\nwt+1=1σ0:t(∑s=0tσsws−g1:t)w_{t+1} = \\\\frac{1}{\\\\sigma_{0:t}} \\\\Big(\\\\sum\\\\limits_{s=0}^t\\\\sigma_s w_s - g_{1:t}\\\\Big)\\nwt+1\\u200b=σ0:t\\u200b1\\u200b(s=0∑t\\u200bσs\\u200bws\\u200b−g1:t\\u200b)wt=1σ0:t−1(∑s=0t−1σsws−g1:t−1)w_t = \\\\frac{1}{\\\\sigma_{0:t-1}} \\\\color{#E06A27}{\\\\Big(\\\\sum\\\\limits_{s=0}^{t-1}\\\\sigma_s w_s - g_{1:t-1}\\\\Big)}\\nwt\\u200b=σ0:t−1\\u200b1\\u200b(s=0∑t−1\\u200bσs\\u200bws\\u200b−g1:t−1\\u200b)wt+1=1σ0:t(∑s=0t−1σsws+σtwt−g1:t−1−gt)w_{t+1} = \\\\frac{1}{\\\\sigma_{0:t}} \\\\Big(\\\\color{#E06A27}{\\\\sum\\\\limits_{s=0}^{t-1}\\\\sigma_s w_s} + \\\\sigma_t w_t - \\\\color{#E06A27}{g_{1:t-1}} - g_t \\\\Big)\\nwt+1\\u200b=σ0:t\\u200b1\\u200b(s=0∑t−1\\u200bσs\\u200bws\\u200b+σt\\u200bwt\\u200b−g1:t−1\\u200b−gt\\u200b)wt+1=1σ0:t(σ0:t−1wt+σtwt−gt)=1σ0:t(σ0:twt−gt)=wt−1σ0:tgtw_{t+1} = \\\\frac{1}{\\\\sigma_{0:t}} \\\\Big(\\\\sigma_{0:t-1} w_t + \\\\sigma_t w_t - g_t\\\\Big) = \\\\frac{1}{\\\\sigma_{0:t}} \\\\Big( \\\\sigma_{0:t} w_t - g_t\\\\Big) = w_t - \\\\frac{1}{\\\\sigma_{0:t}}g_t\\nwt+1\\u200b=σ0:t\\u200b1\\u200b(σ0:t−1\\u200bwt\\u200b+σt\\u200bwt\\u200b−gt\\u200b)=σ0:t\\u200b1\\u200b(σ0:t\\u200bwt\\u200b−gt\\u200b)=wt\\u200b−σ0:t\\u200b1\\u200bgt\\u200bЕсли теперь положить ηt=1σ0:t\\\\eta_t = \\\\frac{1}{\\\\sigma_{0:t}}ηt\\u200b=σ0:t\\u200b1\\u200b, мы получаем формулу градиентного спуска:\\nwt+1=wt−ηtgtw_{t+1} = w_t - \\\\eta_t g_t\\nwt+1\\u200b=wt\\u200b−ηt\\u200bgt\\u200bТаким образом, темп обучения градиентного спуска равен обратной сумме коэффициентов регуляризации ftrl. Точно так же можно выразить\\nσt=1ηt−1ηt−1\\\\sigma_t = \\\\frac{1}{\\\\eta_t} - \\\\frac{1}{\\\\eta_{t-1}}\\nσt\\u200b=ηt\\u200b1\\u200b−ηt−1\\u200b1\\u200bВ качестве классической непокоординатной последовательности learning rate обычно берут\\nηt=αt\\\\eta_t = \\\\frac{\\\\alpha}{\\\\sqrt{t}}\\nηt\\u200b=t\\u200bα\\u200bσt=t+1−tα\\\\sigma_t = \\\\frac{\\\\sqrt{t + 1} - \\\\sqrt{t}}{\\\\alpha}\\nσt\\u200b=αt+1\\u200b−t\\u200b\\u200bОценка на Regret (3.2, Dual Averaging) Пусть\\n\\n∣∣gt∣∣≤G\\\\vert\\\\vert g_t\\\\vert\\\\vert  \\\\leq G∣∣gt\\u200b∣∣≤G,\\n∣∣w∗∣∣≤R\\\\vert\\\\vert w^*\\\\vert\\\\vert  \\\\leq R∣∣w∗∣∣≤R.\\n\\nТогда, если выбрать ηt=R2Gt+1\\\\eta_t = \\\\frac{R}{\\\\sqrt{2}G\\\\sqrt{t+1}}ηt\\u200b=2\\u200bGt+1\\u200bR\\u200b, то\\nRegretT(w∗)≤2RGTRegret_T(w^*) \\\\leq \\\\sqrt{2}RG\\\\sqrt{T}\\nRegretT\\u200b(w∗)≤2\\u200bRGT\\u200bКак и в случае с константным learning rate, константа R2G\\\\frac{R}{\\\\sqrt{2}G}2\\u200bGR\\u200b в ηt\\\\eta_tηt\\u200b на практике никому не известна, так что ее подменяют на α\\\\alphaα и перебирают руками с learning rate, равным αt+1\\\\frac{\\\\alpha}{\\\\sqrt{t+1}}t+1\\u200bα\\u200b.\\nData-Adaptive FTRL\\nДо сих пор мы рассматривали в качестве нормы ∣∣⋅∣∣\\\\vert\\\\vert \\\\cdot\\\\vert\\\\vert∣∣⋅∣∣ стандартное скалярное произведение, в которое различные компоненты вектора весов (которые, грубо говоря, соответствуют различным признакам) вносят равный вклад. Такой подход может быть слишком наивным для «боевых» задач, где геометрия оптимизации имеет форму, например, вытянутого эллипса.\\nНетрудно обобщить предыдущие рассуждения на случай произвольного скалярного произведения\\nwt+1=argmin\\u2061w[g1:tTw+12∑s=0T∣∣w−ws∣∣Ds2]w_{t+1} = arg\\\\min\\\\limits_w\\\\Big[ g_{1:t}^Tw + \\\\frac{1}{2}\\\\sum\\\\limits_{s=0}^T\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{D_s}^2\\\\Big]\\nwt+1\\u200b=argwmin\\u200b[g1:tT\\u200bw+21\\u200bs=0∑T\\u200b∣∣w−ws\\u200b∣∣Ds\\u200b2\\u200b]Коэффициенты σs\\\\sigma_sσs\\u200b в этом выражении теперь спрятались в DsD_sDs\\u200b. Найдем точку минимума:\\n0=g1:t+12∑s=0t(w−ws)(Ds+DsT)=g1:t+∑s=0t(w−ws)Ds0 = g_{1:t} + \\\\frac{1}{2}\\\\sum\\\\limits_{s=0}^t(w - w_s)(D_s + D_s^T) = g_{1:t} + \\\\sum\\\\limits_{s=0}^t(w - w_s)D_s\\n0=g1:t\\u200b+21\\u200bs=0∑t\\u200b(w−ws\\u200b)(Ds\\u200b+DsT\\u200b)=g1:t\\u200b+s=0∑t\\u200b(w−ws\\u200b)Ds\\u200b(∑s=0tDs)w=∑s=0tDsws−g1:t\\\\Big(\\\\sum\\\\limits_{s=0}^tD_s\\\\Big)w = \\\\sum\\\\limits_{s=0}^tD_sw_s - g_{1:t}\\n(s=0∑t\\u200bDs\\u200b)w=s=0∑t\\u200bDs\\u200bws\\u200b−g1:t\\u200bНо сразу возникают проблемы:\\n\\nНужно хранить ∑s=0tDs\\\\sum\\\\limits_{s=0}^tD_ss=0∑t\\u200bDs\\u200b, в общем случае это квадрат по памяти от числа параметров. Ни в какой реальной задаче мы не сможем себе этого позволить;\\nНа каждой итерации метода нужно решать гигантскую систему линейных уравнений для поиска www. Есть все шансы состариться, так и не успев увидеть решение задачи оптимизации.\\n\\nУпростим себе жизнь и предположим, что все матрицы DsD_sDs\\u200b диагональны. Тогда ∑s=1tDs\\\\sum\\\\limits_{s=1}^tD_ss=1∑t\\u200bDs\\u200b можно хранить в виде вектора диагональных элементов того же размера, что и www, а система на каждой итерации будет решаться за линию.\\nAdaGrad: наилучший адаптивный метод\\nРазрешив себе брать нормы ∣∣⋅∣∣Ds\\\\vert\\\\vert\\\\cdot\\\\vert\\\\vert_{D_s}∣∣⋅∣∣Ds\\u200b\\u200b с диагональными матрицами DsD_sDs\\u200b, мы сделали алгоритм более гибким, но при этом приобрели дополнительные степени свободы (выбор диагональных элементов). Попробуем ответить на два вопроса:\\n\\nМожно ли матрицы DsD_sDs\\u200b не угадывать, а настраивать по доступной на очередном шаге информации?\\nКак выбирать матрицы DsD_sDs\\u200b так, чтобы минимизировать оценки на regret?\\n\\nВ процессе поисков ответов на них мы придём к известному методу оптимизации AdaGrad.\\nПомня, что ∣∣.∣∣D,∗=∣∣.∣∣D−1\\\\vert\\\\vert .\\\\vert\\\\vert_{D,*} = \\\\vert\\\\vert .\\\\vert\\\\vert_{D^{-1}}∣∣.∣∣D,∗\\u200b=∣∣.∣∣D−1\\u200b, выпишем общий вид оценки на regret:\\nRegretT(w∗)≤r0:T−1(w∗)+12∑t=1T∣∣g∣∣(t),∗2=∑t=1T∣∣w∗−wt∣∣Dt2+12∑t=1T∣∣gt∣∣(D0:T)−12Regret_T(w^*) \\\\leq r_{0:T-1}(w^*) + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g\\\\vert\\\\vert_{(t),*}^2 = \\\\sum\\\\limits_{t=1}^T \\\\vert\\\\vert w^* - w_t\\\\vert\\\\vert_{D_t}^2 + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g_t\\\\vert\\\\vert_{(D_{0:T})^{-1}}^2\\nRegretT\\u200b(w∗)≤r0:T−1\\u200b(w∗)+21\\u200bt=1∑T\\u200b∣∣g∣∣(t),∗2\\u200b=t=1∑T\\u200b∣∣w∗−wt\\u200b∣∣Dt\\u200b2\\u200b+21\\u200bt=1∑T\\u200b∣∣gt\\u200b∣∣(D0:T\\u200b)−12\\u200bЧтобы упростить выкладки, введем новую симметричную положительно определенную матрицу ST=D0:T−1S_T = D_{0:T}^{-1}ST\\u200b=D0:T−1\\u200b и перепишем формулы\\nRegretT(w∗)≤r0:T−1(w∗)+12∑t=1T∣∣g∣∣t−1,∗2=∑t=1T∣∣w∗−wt∣∣Dt2+12∑t=1T∣∣gt∣∣ST2Regret_T(w^*) \\\\leq r_{0:T-1}(w^*) + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g\\\\vert\\\\vert_{t-1,*}^2 = \\\\sum\\\\limits_{t=1}^T \\\\vert\\\\vert w^* - w_t\\\\vert\\\\vert_{D_t}^2 + \\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g_t\\\\vert\\\\vert_{S_T}^2\\nRegretT\\u200b(w∗)≤r0:T−1\\u200b(w∗)+21\\u200bt=1∑T\\u200b∣∣g∣∣t−1,∗2\\u200b=t=1∑T\\u200b∣∣w∗−wt\\u200b∣∣Dt\\u200b2\\u200b+21\\u200bt=1∑T\\u200b∣∣gt\\u200b∣∣ST\\u200b2\\u200bС членом ∑t=1T∣∣w∗−wt∣∣Dt2\\\\sum\\\\limits_{t=1}^T \\\\vert\\\\vert w^* - w_t\\\\vert\\\\vert_{D_t}^2t=1∑T\\u200b∣∣w∗−wt\\u200b∣∣Dt\\u200b2\\u200b явно будет очень сложно работать: чтобы им пользоваться, нужно иметь на руках оптимальное решение wt∗w_t^*wt∗\\u200b для всей предыдущей выборки. Более перспективным выглядит слагаемое 12∑t=1T∣∣gt∣∣ST2\\\\frac{1}{2}\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g_t\\\\vert\\\\vert_{S_T}^221\\u200bt=1∑T\\u200b∣∣gt\\u200b∣∣ST\\u200b2\\u200b: вычислять их одно удовольствие. Идея метода AdaGrad как раз в том, чтобы не пытаться работать с первым членом и минимизировать второй, надеясь, что итоговые оценки на regret при этом тоже улучшатся.\\nДля начала выведем диагональный AdaGrad как более простой случай. Если все DtD_tDt\\u200b диагональны, то матрица ST=D1:T−1S_T = D_{1:T}^{-1}ST\\u200b=D1:T−1\\u200b тоже диагональна и представляется набором диагональных элементов 1si\\\\frac{1}{s_i}si\\u200b1\\u200b (уберем индекс TTT для сокращения выкладок, так как мы рассматриваем фиксированный раунд).\\nРаспишем второе слагаемое в regret\\n∑t=1T∣∣gt∣∣ST2=∑t=1T∑i=1Ngt,i2si\\\\sum\\\\limits_{t=1}^T\\\\vert\\\\vert g_t\\\\vert\\\\vert_{S_T}^2 = \\\\sum\\\\limits_{t=1}^T\\\\sum\\\\limits_{i=1}^N\\\\frac{g_{t,i}^2}{s_i}\\nt=1∑T\\u200b∣∣gt\\u200b∣∣ST\\u200b2\\u200b=t=1∑T\\u200bi=1∑N\\u200bsi\\u200bgt,i2\\u200b\\u200bПопробуем минимизировать его\\n{∑t=1T∑i=1Ngt,i2si⟶inf\\u2061ssi≥0\\\\begin{cases}\\\\sum\\\\limits_{t=1}^T\\\\sum\\\\limits_{i=1}^N \\\\frac{g_{t,i}^2}{s_i} \\\\longrightarrow \\\\inf\\\\limits_s\\\\\\\\\\ns_i \\\\geq 0\\n\\\\end{cases}\\n⎩⎨⎧\\u200bt=1∑T\\u200bi=1∑N\\u200bsi\\u200bgt,i2\\u200b\\u200b⟶sinf\\u200bsi\\u200b≥0\\u200bУсловие si≥0s_i \\\\geq 0si\\u200b≥0 возникает из неотрицательной определенности матрицы STS_TST\\u200b. Решеним такой задачи, очевидно, является si→+∞s_i \\\\rightarrow +\\\\inftysi\\u200b→+∞. Однако в этом случае член ∑t=1T∣∣w∗−wt∣∣Dt2\\\\sum\\\\limits_{t=1}^T \\\\vert\\\\vert w^* - w_t\\\\vert\\\\vert_{D_t}^2t=1∑T\\u200b∣∣w∗−wt\\u200b∣∣Dt\\u200b2\\u200b из оценки на regret станет, наоборот, бесконечно большим, и нужен какой-то компромисс. Введем довольно слабое ограничение на положительные коэффициенты\\n{∑t=1T∑i=1Ngt,i2si⟶inf\\u2061ssi≥0,∑i=1Nsi≤c\\\\begin{cases}\\\\sum\\\\limits_{t=1}^T\\\\sum\\\\limits_{i=1}^N \\\\frac{g_{t,i}^2}{s_i} \\\\longrightarrow \\\\inf\\\\limits_s\\\\\\\\\\ns_i \\\\geq 0,\\\\\\\\\\n\\\\sum\\\\limits_{i=1}^Ns_i \\\\leq c\\n\\\\end{cases}\\n⎩⎨⎧\\u200bt=1∑T\\u200bi=1∑N\\u200bsi\\u200bgt,i2\\u200b\\u200b⟶sinf\\u200bsi\\u200b≥0,i=1∑N\\u200bsi\\u200b≤c\\u200bи найдём оптимум с помощью метода множителей Лагранжа. Функция Лагранжа имеет вид\\nL(s,λ,θ)=∑t=1T∑i=1Ngt,i2si+λTs+θ(∑i=1Nsi−c)L(s,\\\\lambda,\\\\theta) = \\\\sum\\\\limits_{t=1}^T\\\\sum\\\\limits_{i=1}^N \\\\frac{g_{t,i}^2}{s_i} + \\\\lambda^Ts + \\\\theta\\\\left(\\\\sum\\\\limits_{i=1}^Ns_i - c\\\\right)\\nL(s,λ,θ)=t=1∑T\\u200bi=1∑N\\u200bsi\\u200bgt,i2\\u200b\\u200b+λTs+θ(i=1∑N\\u200bsi\\u200b−c)Отметим, что здесь λ\\\\lambdaλ — это вектор, а θ\\\\thetaθ — число.\\nПриравняем к нулю частные производные:\\n0=∂L(s,λ,θ)∂si=−1si∑t=1Tgt,i2+λi+θ0=\\\\frac{\\\\partial L(s,\\\\lambda,\\\\theta)}{\\\\partial s_i} = -\\\\frac1{s_i}\\\\sum\\\\limits_{t=1}^T g_{t,i}^2 + \\\\lambda_i + \\\\theta\\n0=∂si\\u200b∂L(s,λ,θ)\\u200b=−si\\u200b1\\u200bt=1∑T\\u200bgt,i2\\u200b+λi\\u200b+θВспомним про условия дополняющей нежесткости, требующие, чтобы λisi=0\\\\lambda_i s_i = 0λi\\u200bsi\\u200b=0. Так как sis_isi\\u200b мы нулю приравнять здесь не можем, получаем, что λi=0\\\\lambda_i = 0λi\\u200b=0:\\n1si∑t=1Tgt,i2−θ=0\\\\frac1{s_i}\\\\sum\\\\limits_{t=1}^T g_{t,i}^2 - \\\\theta = 0\\nsi\\u200b1\\u200bt=1∑T\\u200bgt,i2\\u200b−θ=0si=θ−12∑t=1Tgt,i2s_i = \\\\theta^{-\\\\frac{1}{2}}\\\\sqrt{\\\\sum\\\\limits_{t=1}^T g_{t,i}^2}\\nsi\\u200b=θ−21\\u200bt=1∑T\\u200bgt,i2\\u200b\\u200bТеперь вспомним про условие ∑i=1Nsi≤c\\\\sum\\\\limits_{i=1}^Ns_i \\\\leq ci=1∑N\\u200bsi\\u200b≤c. Можно показать, что оптимум достигается на границе (то есть когда неравенство превращается в равенство). Тогда\\nc=∑i=1Nsi=θ−12∑i=1N∑t=1Tgt,i2c = \\\\sum\\\\limits_{i=1}^Ns_i  = \\\\theta^{-\\\\frac{1}{2}}\\\\sum\\\\limits_{i=1}^N\\\\sqrt{\\\\sum\\\\limits_{t=1}^T g_{t,i}^2}\\nc=i=1∑N\\u200bsi\\u200b=θ−21\\u200bi=1∑N\\u200bt=1∑T\\u200bgt,i2\\u200b\\u200bθ−12=c∑i=1N∑t=1Tgt,i2\\\\theta^{-\\\\frac{1}{2}} = \\\\frac{c}{\\\\sum\\\\limits_{i=1}^N\\\\sqrt{\\\\sum\\\\limits_{t=1}^T g_{t,i}^2}}\\nθ−21\\u200b=i=1∑N\\u200bt=1∑T\\u200bgt,i2\\u200b\\u200bc\\u200bsi=c∑t=1Tgt,i2∑i=1N∑t=1Tgt,i2s_i =  \\\\frac{c\\\\sqrt{\\\\sum\\\\limits_{t=1}^T g_{t,i}^2}}{\\\\sum\\\\limits_{i=1}^N\\\\sqrt{\\\\sum\\\\limits_{t=1}^T g_{t,i}^2}}\\nsi\\u200b=i=1∑N\\u200bt=1∑T\\u200bgt,i2\\u200b\\u200bct=1∑T\\u200bgt,i2\\u200b\\u200b\\u200bВернемся к оценке на regret. Чему равно ccc мы не знаем, поэтому мы просто констатируем, что оптимальные коэффициенты sis_isi\\u200b пропорциональны ∑s=1tgs,i2\\\\sqrt{\\\\sum\\\\limits_{s=1}^tg^2_{s,i}}s=1∑t\\u200bgs,i2\\u200b\\u200b:\\nsi=1α∑t=1Tgt,i2s_i = \\\\frac1{\\\\alpha}\\\\sqrt{\\\\sum\\\\limits_{t=1}^T g_{t,i}^2}\\nsi\\u200b=α1\\u200bt=1∑T\\u200bgt,i2\\u200b\\u200bТеперь STS_TST\\u200b — диагональная матрица с диагональными элементами 1si\\\\frac1{s_i}si\\u200b1\\u200b. Следовательно, D0:T=(ST)−1D_{0:T} = (S_T)^{-1}D0:T\\u200b=(ST\\u200b)−1 - тоже диагональная матрица с диагональными элементами sis_isi\\u200b:\\nD0:T,i=1α∑t=1Tgt,i2=∑t=1T(1α∑t=1Tgt,i2−1α∑t=1T−1gt,i2)+0,D0=0,D_{0:T,i} = \\\\frac1{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{t=1}^T g_{t,i}^2}} = \\\\sum\\\\limits_{t=1}^T \\\\left(\\\\frac1{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{t=1}^T g_{t,i}^2}} - \\\\frac1{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{t=1}^{T-1} g_{t,i}^2}}\\\\right) + 0, \\\\quad D_0 = 0,\\nD0:T,i\\u200b=α1\\u200bt=1∑T\\u200bgt,i2\\u200b\\u200b=t=1∑T\\u200b\\u200bα1\\u200bt=1∑T\\u200bgt,i2\\u200b\\u200b−α1\\u200bt=1∑T−1\\u200bgt,i2\\u200b\\u200b\\u200b+0,D0\\u200b=0,и легко убедиться, что\\nDt,i=1α∑t=1Tgt,i2−1α∑t=1Tgt,i2D_{t,i} = \\\\frac1{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{t=1}^T g_{t,i}^2}} - \\\\frac1{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{t=1}^T g_{t,i}^2}}\\nDt,i\\u200b=α1\\u200bt=1∑T\\u200bgt,i2\\u200b\\u200b−α1\\u200bt=1∑T\\u200bgt,i2\\u200b\\u200bТеперь вспомним, что эти формулы в точности повторяют то, что мы получили выше для соотношения σt=1ηt−1ηt−1\\\\sigma_t = \\\\frac{1}{\\\\eta_t} - \\\\frac{1}{\\\\eta_{t-1}}σt\\u200b=ηt\\u200b1\\u200b−ηt−1\\u200b1\\u200b, только вместо общего коэффициента ηt\\\\eta_tηt\\u200b у нас теперь покоординатные коэффициенты ηt,i\\\\eta_{t,i}ηt,i\\u200b:\\nηt,i=α∑s=1tgs,i2\\\\eta_{t,i} = \\\\frac{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{s=1}^t g_{s,i}^2}}\\nηt,i\\u200b=s=1∑t\\u200bgs,i2\\u200b\\u200bα\\u200bПолучаем формулы для метода AdaGrad в градиентной постановке:\\nwt+1,i=wt,i−α∑s=1tgs,i2gt,i,w_{t+1,i} = w_{t,i} - \\\\frac{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{s=1}^t g_{s,i}^2}}g_{t,i},\\nwt+1,i\\u200b=wt,i\\u200b−s=1∑t\\u200bgs,i2\\u200b\\u200bα\\u200bgt,i\\u200b,где коэффициент α\\\\alphaα приобретает значение learning rate.\\nОценка на Regret (3.4, FTRL-Proximal with Diagonal Matrix Learning Rates)\\nЕсли использовать AdaGrad с покоординатными learning rate, то\\nRegretT(w∗)≤22R∑t=1Tgt2Regret_T(w^*) \\\\leq 2 \\\\sqrt{2} R \\\\sqrt{\\\\sum\\\\limits_{t=1}^Tg_t^2}\\nRegretT\\u200b(w∗)≤22\\u200bRt=1∑T\\u200bgt2\\u200b\\u200bОтметим, что это оценка отличается от предыдущей тем, что вместо GTG\\\\sqrt{T}GT\\u200b используется ∑t=1Tgt2\\\\sqrt{\\\\sum\\\\limits_{t=1}^Tg_t^2}t=1∑T\\u200bgt2\\u200b\\u200b. Таким образом, если у градиента на какой-то из позиций стоит что-то большое, это повлияет лишь на одно из слагаемых под корнем вместо того, чтобы умножиться на T\\\\sqrt{T}T\\u200b.\\nЭффективный размер шага. Предположим, что градиенты ограничены по норме ∣∣g∣∣2≤R\\\\vert\\\\vert g\\\\vert\\\\vert_2 \\\\leq R∣∣g∣∣2\\u200b≤R. Перепишем наши формулы в виде\\nα∑s=1Tgs,i2=αT⋅1T∑s=1Tgs,i2≤αRT\\\\frac{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{s=1}^T g_{s,i}^2}} = \\\\frac{\\\\alpha}{\\\\sqrt{T}\\\\cdot\\\\sqrt{\\\\frac{1}{T}\\\\sum\\\\limits_{s=1}^T g_{s,i}^2}} \\\\leq \\\\frac{\\\\alpha}{R\\\\sqrt{T}}\\ns=1∑T\\u200bgs,i2\\u200b\\u200bα\\u200b=T\\u200b⋅T1\\u200bs=1∑T\\u200bgs,i2\\u200b\\u200bα\\u200b≤RT\\u200bα\\u200bИз этих формул следует, что в среднем learning rate в AdaGrad убывает как ηt=O(1T)\\\\eta_t = O\\\\left(\\\\frac{1}{\\\\sqrt{T}}\\\\right)ηt\\u200b=O(T\\u200b1\\u200b), то есть так же, как в предыдущем методе. Отличие состоит лишь в более правильной покоординатной нормировке, которая улучшает сходимость.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф15.1. Введение в онлайн-обучениеСледующий параграф15.3. Регуляризация в онлайн-обученииЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_17.html', 'title': 'Генеративный подход к классификации'}, page_content=\"Генеративный подход к классификацииЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/74.1.Вероятностный подход в ML4.2.Экспоненциальный класс распределений и принцип максимальной энтропии4.3.Обобщённые линейные модели4.4.Как оценивать вероятности4.5.Генеративный подход к классификацииГенеративный и дискриминативный подходы к обучениюМетод наивного байесаОценка одномерного распределенияНаивный байесовский подход и логистическая регрессия4.6.Байесовский подход к оцениванию4.7.Модели с латентными переменными5.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Генеративный подход к классификации4.5. Генеративный подход к классификацииАвторыАртемьев МихаилКак использовать распределение меток классов в\\xa0задаче классификации. LDA, QDA и\\xa0наивный байесКлассификационные модели, которые мы рассматривали в предыдущих параграфах, нацелены непосредственно на оценку P(Y∣X)P(Y \\\\vert X)P(Y∣X). Такие модели называются дискриминативными.\\nК ним относится, например, логистическая регрессия: она предлагает оценку P^(y=1∣x)=σ(wTˆx)\\\\hat P(y=1 \\\\vert x) = \\\\sigma(w\\\\^Tx)P^(y=1∣x)=σ(wTˆx). В процессе обучения дискриминативные модели подбирают разделяющую поверхность (гиперплоскость в случае логистической регрессии). Новые объекты дискриминативная модель классифицирует в зависимости от того, по какую сторону от разделяющей поверхности они лежат.\\nНапример, обучившись на изображениях домашних кошек (y=0) и рысей (y=1), дискриминативная модель будет определять, новое изображение больше похоже на кошку или на рысь. При этом, если на вход такой модели дать изображение собаки (объект класса, которого не было в обучении, выброс), дискриминативная модель заведомо не сможет обнаружить, что это и не кошка, и не рысь, и отнесёт такой объект к одному из «знакомых» ей классов.\\nВ этом параграфе мы поговорим о другой группе моделей, которые нацелены на оценку P(X,Y)=P(X∣Y)P(Y)P(X, Y) = P(X \\\\vert Y)P(Y)P(X,Y)=P(X∣Y)P(Y). Такая модель описала бы, как обычно выглядят кошки, как они могут выглядеть, а каких кошек точно не бывает. Так же она описала бы и рысей. Она также определила бы по обучающим данным, насколько изображения кошек встречаются чаще, чем изображения рысей, т.е. оценила бы P(Y)P(Y)P(Y).\\nГенеративный и дискриминативный подходы к обучению\\nЕсли модель позволила точно оценить распределение P(X∣Y)P(X \\\\vert Y)P(X∣Y), с её помощью можно генерировать объекты из этого условного распределения, в нашем примере — изображения кошек и рысей соответственно.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nА вместе распределение P(X,Y)P(X, Y)P(X,Y) дало бы нам возможность генерировать изображения и кошек, и рысей, причём именно в той пропорции, в которой они встречаются в реальном мире.  Поэтому модели, оценивающие P(X,Y)P(X, Y)P(X,Y), называют генеративными. Ещё одно достоинство генеративных моделей — их способность находить выбросы в данных: объект xxx можно считать выбросом, если P(x∣y)P(x \\\\vert y)P(x∣y) мало для каждого класса yyy.\\nЗаметим, что находить выбросы с помощью генеративной модели можно и когда класс всего один — то есть никакие метки классов не доступны. Такая задача называется одноклассовой классификацией. Например, если у нас есть не размеченный датасет с аудиозаписями речи людей, то, обучив на нём генеративную модель, оценивающую в данном случае P(X∣Y)=P(X)P(X \\\\vert Y)=P(X)P(X∣Y)=P(X), мы сможем для нового аудио xxx определить, похоже ли оно на аудиозапись человеческой речи (значение P(x)P(x)P(x) велико), или это что-то другое: синтезированная речь, посторонний шум и т.п. (значение P(x)P(x)P(x) мало).\\nЕсли мы знаем, что «выбросы», с которыми модели предстоит сталкиваться, — это, как правило, синтезированная речь, то, мы можем дополнить датасет вторым классом, состоящим из синтезированной речи, и смоделировать также распределение этого класса. Это позволит существенно увеличить качество детектирования таких выбросов.\\nЧтобы использовать генеративную модель для классификации, необходимо выразить P(Y∣X)P(Y \\\\vert X)P(Y∣X) через P(X∣Y)P(X \\\\vert Y)P(X∣Y) и P(Y)P(Y)P(Y). Сделать это позволяет формула Байеса:\\nP(y∣x)=P(x,y)∑y′∈YP(y′)P(x∣y′)=P(y)P(x∣y)∑y′∈YP(y′)P(x∣y′)P(y \\\\vert x) = \\\\frac{P(x, y)}{\\\\sum\\\\limits_{y'\\\\in Y} P(y')P(x \\\\vert y')} = \\\\frac{P(y)P(x \\\\vert y)}{\\\\sum\\\\limits_{y'\\\\in Y} P(y')P(x \\\\vert y')}\\nP(y∣x)=y′∈Y∑\\u200bP(y′)P(x∣y′)P(x,y)\\u200b=y′∈Y∑\\u200bP(y′)P(x∣y′)P(y)P(x∣y)\\u200bКлассификация в генеративных моделях осуществляется с помощью байесовского классификатора:\\na(x)=arg\\u2061max\\u2061y∈YP(y∣x)=arg\\u2061max\\u2061y∈YP(y)P(x∣y)∑y′∈YP(y′)P(x∣y′)=arg\\u2061max\\u2061y∈YP(y)P(x∣y)a(x) = \\\\arg\\\\max\\\\limits_{y\\\\in Y} P(y \\\\vert x) = \\\\arg\\\\max\\\\limits_{y\\\\in Y} \\\\frac{P(y)P(x \\\\vert y)}{\\\\sum\\\\limits_{y'\\\\in Y} P(y')P(x \\\\vert y')} = \\\\arg\\\\max\\\\limits_{y\\\\in Y} P(y)P(x \\\\vert y)\\na(x)=argy∈Ymax\\u200bP(y∣x)=argy∈Ymax\\u200by′∈Y∑\\u200bP(y′)P(x∣y′)P(y)P(x∣y)\\u200b=argy∈Ymax\\u200bP(y)P(x∣y)Оценить P(Y)P(Y)P(Y), как правило, несложно. Для этого используют частотные оценки, полученные в обучающей выборке:\\nВыражение (1)\\nP^(Y=y)=#(Y=y)N\\\\hat P(Y=y) = \\\\frac{\\\\#(Y=y)}{N}\\nP^(Y=y)=N#(Y=y)\\u200bОтметим ещё раз, что использование генеративного подхода позволяет внедрять в модель априорные знания о P(y)P(y)P(y). Это не очень впечатляет, когда речь идёт о бинарной классификации, но всё меняется, если рассмотреть задачу ASR (автоматического распознавания речи), в которой по записи голоса восстанавливается произносимый текст.\\nТаргетами здесь могут быть любые предложения или даже более развёрнутые тексты. При этом размеченных данных (запись, текст) обычно намного меньше, чем доступных текстов, и обученная на большом чисто текстовом корпусе языковая модель, которая будет оценивать вероятность того или иного предложения, может стать большим подспорьем, позволив из нескольких фонетически корректных наборов слов выбрать тот, который в большей степени похож на настоящее предложение.\\nНо как смоделировать распределение P(X,Y)P(X, Y)P(X,Y)? Пространство всех возможных функций распределения P(X,Y)P(X, Y)P(X,Y) бесконечномерно, из-за чего оценить произвольное распределение с помощью конечной выборки невозможно. Поэтому перед оценкой P(X,Y)P(X, Y)P(X,Y) на это распределение накладывают дополнительные ограничения. Некоторые простые примеры таких ограничений мы рассмотрим в следующих разделах.\\nGaussian discriminant analysis\\nМодель гауссовского (или квадратичного) дискриминантного анализа (GDA) строится в предположении, что распределение объектов каждого класса yyy подчиняется многомерному нормальному закону со средним μy\\\\mu_yμy\\u200b и ковариационной матрицей Σy\\\\Sigma_yΣy\\u200b:\\np(x∣y)=1(2π)n/2∣Σy∣1/2exp\\u2061(−12(x−μy)TΣy−1(x−μy))p(x \\\\mid y)=\\\\frac{1}{(2 \\\\pi)^{n / 2}\\\\left|\\\\Sigma_y\\\\right|^{1 / 2}} \\\\exp \\\\left(-\\\\frac{1}{2}\\\\left(x-\\\\mu_y\\\\right)^T \\\\Sigma_y^{-1}\\\\left(x-\\\\mu_y\\\\right)\\\\right)\\np(x∣y)=(2π)n/2∣Σy\\u200b∣1/21\\u200bexp(−21\\u200b(x−μy\\u200b)TΣy−1\\u200b(x−μy\\u200b))Тогда функция правдоподобия\\nL(P(Y),μ,Σ)=∏i=1Np(xi∣yi;μyi,Σyi)P(yi)\\\\mathcal L(P(Y), \\\\mu, \\\\Sigma) = \\\\prod_{i=1}^N p(x_i \\\\vert y_i; \\\\mu_{y_i}, \\\\Sigma_{y_i})P(y_i)\\nL(P(Y),μ,Σ)=i=1∏N\\u200bp(xi\\u200b∣yi\\u200b;μyi\\u200b\\u200b,Σyi\\u200b\\u200b)P(yi\\u200b)достигает максимума при\\nμ^y=∑i=1Nxi1yi=y∑i=1N1yi=y,Σ^y=∑i=1N(xi−μ^y)(xi−μ^y)T1yi=y∑i=1N1yi=y\\\\hat\\\\mu_y = \\\\frac{\\\\sum\\\\limits_{i=1}^N{x_i\\\\mathbb{1}_{y_i=y}}}{\\\\sum\\\\limits_{i=1}^N\\\\mathbb{1}_{y_i=y}},\\\\hspace{5mm} \\n\\\\hat\\\\Sigma_y = \\\\frac{\\\\sum\\\\limits_{i=1}^N{(x_i - \\\\hat\\\\mu_{y})(x_i - \\\\hat\\\\mu_{y})^T \\\\mathbb{1}_{y_i=y}}}{\\\\sum\\\\limits_{i=1}^N\\\\mathbb{1}_{y_i=y}}\\nμ^\\u200by\\u200b=i=1∑N\\u200b1yi\\u200b=y\\u200bi=1∑N\\u200bxi\\u200b1yi\\u200b=y\\u200b\\u200b,Σ^y\\u200b=i=1∑N\\u200b1yi\\u200b=y\\u200bi=1∑N\\u200b(xi\\u200b−μ^\\u200by\\u200b)(xi\\u200b−μ^\\u200by\\u200b)T1yi\\u200b=y\\u200b\\u200bИ P^(Y)\\\\hat P(Y)P^(Y), представленной выше см. выражение (1)(1)(1).\\nРассмотрим, как выглядит разделяющая поверхность в модели GDA. На поверхности, разделяющей классы yiy_iyi\\u200b и yjy_jyj\\u200b выполняется\\nP(yi∣x)=P(yj∣x)⇔P(y_i \\\\vert x)=P(y_j \\\\vert x) \\\\Leftrightarrow\\nP(yi\\u200b∣x)=P(yj\\u200b∣x)⇔p(x∣yi)P(yi)=p(x∣yj)P(yj)⇔p(x \\\\vert y_i)P(y_i) = p(x \\\\vert y_j)P(y_j)\\\\Leftrightarrow\\np(x∣yi\\u200b)P(yi\\u200b)=p(x∣yj\\u200b)P(yj\\u200b)⇔log\\u2061p(x∣yi)+log\\u2061P(yi)−log\\u2061p(x∣yj)−log\\u2061P(yj)=0⇔\\\\log p(x \\\\vert y_i) + \\\\log P(y_i) - \\\\log p(x \\\\vert y_j) - \\\\log P(y_j) = 0\\\\Leftrightarrow\\nlogp(x∣yi\\u200b)+logP(yi\\u200b)−logp(x∣yj\\u200b)−logP(yj\\u200b)=0⇔Выражение (2)\\n−12(x−μyi)TΣyi−1(x−μyi)−log\\u2061(2π)n/2∣Σyi∣1/2+log\\u2061P(yi)+12(x−μyj)TΣyj−1(x−μyj)+log\\u2061(2π)n/2∣Σyj∣1/2−log\\u2061P(yj)=0 \\\\begin{equation}\\n -\\\\frac{1}{2}(x-\\\\mu_{y_i})^T\\\\Sigma_{y_i}^{-1} (x-\\\\mu_{y_i})-\\\\log (2\\\\pi)^{n/2}|\\\\Sigma_{y_i}|^{1/2} + \\\\log P(y_i) +\\n\\\\frac{1}{2}(x-\\\\mu_{y_j})^T\\\\Sigma_{y_j}^{-1} (x-\\\\mu_{y_j}) + \\\\log (2\\\\pi)^{n/2}|\\\\Sigma_{y_j}|^{1/2} - \\\\log P(y_j) = 0\\n \\\\end{equation}\\n−21\\u200b(x−μyi\\u200b\\u200b)TΣyi\\u200b−1\\u200b(x−μyi\\u200b\\u200b)−log(2π)n/2∣Σyi\\u200b\\u200b∣1/2+logP(yi\\u200b)+21\\u200b(x−μyj\\u200b\\u200b)TΣyj\\u200b−1\\u200b(x−μyj\\u200b\\u200b)+log(2π)n/2∣Σyj\\u200b\\u200b∣1/2−logP(yj\\u200b)=0\\u200b\\u200bПоскольку левая часть уравнения (2) квадратична по xxx, разделяющая поверхность между двумя классами будет представлять из себя гиперповерхность порядка 2. Пример разделяющей поверхности многоклассовой модели GDA приведён на рис.\\n\\n\\nПлотность классов и разделяющая поверхность в многоклассовой модели LDA см. рисунок.\\n\\n\\nLinear Discriminant Analysis\\nВ выражении (2) член второго порядка xT(Σyj−1−Σyi−1)xx^T (\\\\Sigma_{y_j}^{-1} - \\\\Sigma_{y_i}^{-1})xxT(Σyj\\u200b−1\\u200b−Σyi\\u200b−1\\u200b)x зануляется при Σyi=Σyj\\\\Sigma_{y_i}=\\\\Sigma_{y_j}Σyi\\u200b\\u200b=Σyj\\u200b\\u200b. Таким образом, если дополнительно предположить, что все классы имеют общую ковариационную матрицу Σ\\\\SigmaΣ, разделяющая поверхность между любыми двумя классами будет  линейной (см. рисунок). Поэтому такая модель называется линейным дискриминантным анализом (LDA).\\nНа этапе обучения единственное отличие модели LDA от GDA состоит в оценке ковариационной матрицы:\\nΣ^=1N∑i=1N(xi−μ^yi)(xi−μ^yi)T\\\\hat \\\\Sigma = \\\\frac{1}{N}\\\\sum\\\\limits_{i=1}^N{(x_i - \\\\hat\\\\mu_{y_i})(x_i - \\\\hat\\\\mu_{y_i})^T}\\nΣ^=N1\\u200bi=1∑N\\u200b(xi\\u200b−μ^\\u200byi\\u200b\\u200b)(xi\\u200b−μ^\\u200byi\\u200b\\u200b)TЗаметим, что в модели GDA для каждого класса требовалось оценить порядка d2d^2d2 параметров. Это может привести к переобучению в случае, если размерность пространства признаков велика, а некоторые классы представлены в обучающей выборке малым количеством объектов. В LDA для каждого класса требуется оценить лишь порядка ddd параметров (значение P(y)P(y)P(y) и элементы вектора μy\\\\mu_yμy\\u200b), и ещё d2d^2d2 общих для всех классов параметров (элементы матрицы Σ\\\\SigmaΣ).\\nТаким образом, основное преимущество модели LDA перед GDA — её меньшая склонность к переобучению, недостаток — линейная разделяющая поверхность.\\nМетод наивного байеса\\nПредположим, что признаки XXX объектов каждого класса yyy — независимые случайные величины:\\n∀y∈Y∀U,V:U⊔V={1,...d},∀xu⊂R∣U∣,xv⊂R∣V∣\\\\forall y\\\\in Y \\\\hspace{2mm} \\\\forall U, V: U\\\\sqcup V = \\\\{1, ... d\\\\}, \\\\hspace{2mm} \\\\forall x^u\\\\subset \\\\mathbb R^{|U|}, x^v\\\\subset \\\\mathbb R^{|V|}\\n∀y∈Y∀U,V:U⊔V={1,...d},∀xu⊂R∣U∣,xv⊂R∣V∣P(XU∈xu,XV∈xv∣Y=y)=P(XU∈xu∣Y=y)P(XV∈xu∣Y=y).P(X^U\\\\in x^u, X^V\\\\in x^v|Y=y) = P(X^U\\\\in x^u|Y=y)P(X^V\\\\in x^u|Y=y).\\nP(XU∈xu,XV∈xv∣Y=y)=P(XU∈xu∣Y=y)P(XV∈xu∣Y=y).В таком случае говорят, что величины XXX условно независимы относительно YYY. Тогда справедливо\\nВыражение (3)\\nP(X∣Y)=P(X1,X2,...,Xd∣Y)=P(X1∣Y)P(X2,...,Xd∣Y)=...=P(X1∣Y)P(X2∣Y)...P(Xd∣Y)\\\\begin{equation}\\nP(X \\\\vert Y) = P(X^1, X^2, ..., X^d \\\\vert Y) = P(X^1 \\\\vert Y)P(X^2, ..., X^d \\\\vert Y) = ... = P(X^1 \\\\vert Y)P(X^2 \\\\vert Y)...P(X^d \\\\vert Y)\\n\\\\end{equation}\\nP(X∣Y)=P(X1,X2,...,Xd∣Y)=P(X1∣Y)P(X2,...,Xd∣Y)=...=P(X1∣Y)P(X2∣Y)...P(Xd∣Y)\\u200b\\u200bТо есть для того, чтобы оценить плотность многомерного распределения P(X∣Y)P(X \\\\vert Y)P(X∣Y) достаточно оценить плотности одномерных распределений P(Xi∣Y)P(X^i \\\\vert Y)P(Xi∣Y), см. рисунок.\\n\\n\\nНа рисунке приведён пример условно независимых относительно YYY случайных величин X1,X2X^1, X^2X1,X2. Для оценки плотности двумерных распределений объектов классов достаточно оценить плотности маргинальных распределений, изображённые графиками вдоль осей.\\nРассмотрим пример. Пусть решается задача классификации отзывов об интернет-магазине на 2 категории: Y=0Y=0Y=0 — отрицательный отзыв, клиент остался не доволен, и Y=1Y=1Y=1 — положительный отзыв. Пусть признак XwX^wXw равен 1, если слово www присутствует в отзыве, и 0 иначе. Тогда условие выражения (3)(3)(3) означает, что, в частности, наличие или отсутствие слова «дозвониться» в отрицательном отзыве не влияет на вероятность наличия в этом отзыве слова «телефон».\\nНа практике в процессе feature engineering почти всегда создаётся много похожих признаков, и условно независимые признаки можно встретить очень редко. Поэтому генеративную модель, построенную в предположении условия выражения (3)(3)(3), называют наивным байесовским классификатором (Naive Bayes classifier, NB).\\nОбучение модели NB заключается в оценке распределений P(Y)P(Y)P(Y) и P(Xi∣Y)P(X^i \\\\vert Y)P(Xi∣Y). Для P(Y)P(Y)P(Y) можно использовать частотную оценку выражения (1)(1)(1). P(Xi∣y)P(X^i \\\\vert y)P(Xi∣y) — одномерное распределение. Рассмотрим несколько способов оценки одномерного распределения.\\nОценка одномерного распределения\\nПусть мы хотим оценить одномерное распределение P(X)P(X)P(X).\\nЕсли распределение P(X)P(X)P(X) дискретное, требуется оценить его функцию массы, то есть вероятность того, что величина XXX примет значение xjx_jxj\\u200b. Метод максимума правдоподобия приводит к частотной оценке:\\nВыражение (4)\\nP^(X=xj)=#(X=xj)N\\\\hat P(X = x_j) = \\\\frac{\\\\#(X = x_j)}{N}\\nP^(X=xj\\u200b)=N#(X=xj\\u200b)\\u200bГде NNN — размер выборки, по которой оценивается распределение XXX (количество объектов класса yyy в случае оценки плотности класса yyy).\\nПри этом может оказаться, что некоторое значение xjx_jxj\\u200b ни разу не встречается в обучающей выборке. Например, в случае классификации отзывов методом Наивного Байеса, слово «амбивалентно» не встретилось ни в одном положительном отзыве, но встретилось в отрицательных. Тогда использование оценки выражения (4)(4)(4) приведёт к тому, что все отзывы с этим словом будут определяться NB как отрицательные с вероятностью 1. Чтобы избежать принятия таких радикальных решений при недостатке статистики, используют сглаживание Лапласа:\\nP^(X=xj)=#(X=xj)+αN+mα,\\\\hat P(X = x_j) = \\\\frac{\\\\#(X = x_j) + \\\\alpha}{N + m\\\\alpha},\\nP^(X=xj\\u200b)=N+mα#(X=xj\\u200b)+α\\u200b,где mmm — количество различных значений, принимаемых случайной величиной XXX, α\\\\alphaα — гиперпараметр.\\nДля оценки плотности ppp абсолютно непрерывного распределения в точке aaa можно разделить количество объектов обучающей выборки в окрестности точки aaa на размер этой окрестности:\\np^(a)=∑j1a−h<Xj<a+h2h=∑j1−h<Xj−a<h2h.\\\\hat p(a) = \\\\frac{\\\\sum\\\\limits_{j}\\\\mathbb{1}_{a - h < X_j < a + h}}{2h} = \\\\frac{\\\\sum\\\\limits_{j}\\\\mathbb{1}_{-h < X_j - a < h}}{2h}.\\np^\\u200b(a)=2hj∑\\u200b1a−h<Xj\\u200b<a+h\\u200b\\u200b=2hj∑\\u200b1−h<Xj\\u200b−a<h\\u200b\\u200b.Обычно объекты, лежащие дальше от точки aaa, учитывают с меньшим весом. Таким образом, оценка плотности приобретает вид\\np^(a)=∑jKh(Xj−a)2h,\\\\hat p(a) = \\\\frac{\\\\sum\\\\limits_{j}K_h(X_j - a)}{2h},\\np^\\u200b(a)=2hj∑\\u200bKh\\u200b(Xj\\u200b−a)\\u200b,где функция KhK_hKh\\u200b, называемая ядром, обычно имеет носитель (−h,h)(-h, h)(−h,h) (см. рисунок ниже). Такой способ оценки плотности называют непараметрическим.\\n\\n\\nРезультат оценки плотности с разными ядрами. Использованы изображения из:\\n\\nПри параметрической оценке плотности предполагают, что искомое распределение лежит в параметризованном классе, и подбирают значения параметров при помощи метода максимума правдоподобия. Например, предположим, что искомое распределение нормальное. Тогда функция его плотности имеет вид\\np(x)=1σ2πexp\\u2061(−(x−μ)22σ2)p(x) = \\\\frac{1}{\\\\sigma\\\\sqrt{2\\\\pi}}\\\\exp\\\\left(-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}\\\\right)\\np(x)=σ2π\\u200b1\\u200bexp(−2σ2(x−μ)2\\u200b)Таким образом, чтобы оценить плотность p(x)p(x)p(x), достаточно оценить параметры μ,σ\\\\mu, \\\\sigmaμ,σ. Метод максимума правдоподобия в этом случае даст такие оценки:\\nμ^=X‾\\\\hat\\\\mu = \\\\overline Xμ^\\u200b=X — выборочное среднее, σ^=1N∑j=1N(Xj−X‾)\\\\hat\\\\sigma = \\\\sqrt{\\\\frac{1}{N}\\\\sum_{j=1}^N (X_j - \\\\overline X)}σ^=N1\\u200b∑j=1N\\u200b(Xj\\u200b−X)\\u200b — выборочное стандартное отклонение.\\nЕсли в модели NB распределения всех признаков объектов каждого класса нормальные, оценив параметры этих распределений, мы сможем каждый класс yyy описать нормальным распределением со средним μp\\\\mu_pμp\\u200b и диагональной ковариационной матрицей, значения на диагонали которой обозначим σp\\\\sigma_pσp\\u200b.\\nТаким образом, полученная модель (Gaussian Naive Bayes, GNB) эквивалентна модели GDA с дополнительным ограничением на диагональность ковариационных матриц.\\nНаивный байесовский подход и логистическая регрессия\\nПредположим теперь, что в модели GNB класса всего 2, причём соответствующие им ковариационные матрицы совпадают, как это было в модели LDA. Таким образом σ0=σ1=σ\\\\sigma_0 = \\\\sigma_1 = \\\\sigmaσ0\\u200b=σ1\\u200b=σ.\\nПосмотрим, как будет выглядеть P(Y∣X)P(Y \\\\vert X)P(Y∣X) в этом случае. По теореме Байеса имеем\\nP(Y=1∣X)=P(Y=1)P(X∣Y=1)P(Y=1)P(X∣Y=1)+P(Y=0)P(X∣Y=0)P(Y=1 \\\\vert X) = \\\\frac{P(Y = 1)P(X \\\\vert Y = 1)}{P(Y = 1)P(X \\\\vert Y = 1) + P(Y = 0)P(X \\\\vert Y = 0)}\\nP(Y=1∣X)=P(Y=1)P(X∣Y=1)+P(Y=0)P(X∣Y=0)P(Y=1)P(X∣Y=1)\\u200bРазделим числитель и знаменатель полученного выражения на числитель:\\nP(Y=1∣X)=11+P(Y=0)P(X∣Y=0)P(Y=1)P(X∣Y=1)=11+exp\\u2061(ln\\u2061P(Y=0)P(X∣Y=0)P(Y=1)P(X∣Y=1))P(Y=1 \\\\vert X) = \\\\frac{1}{1 + \\\\frac{P(Y = 0)P(X \\\\vert Y = 0)}{P(Y = 1)P(X \\\\vert Y = 1)}} = \\\\frac{1}{1 + \\\\exp\\\\left(\\\\ln\\\\frac{P(Y=0)P(X \\\\vert Y=0)}{P(Y=1)P(X \\\\vert Y=1)}\\\\right)}\\nP(Y=1∣X)=1+P(Y=1)P(X∣Y=1)P(Y=0)P(X∣Y=0)\\u200b1\\u200b=1+exp(lnP(Y=1)P(X∣Y=1)P(Y=0)P(X∣Y=0)\\u200b)1\\u200bИз условной независимости XiX^iXi относительно YYY получаем\\nФормула (5)\\nP(Y=1∣X)=11+exp\\u2061(ln\\u2061P(Y=0)P(Y=1)+∑i=1dln\\u2061P(Xi∣Y=0)P(Xi∣Y=1))\\\\begin{equation}\\nP(Y=1 \\\\vert X) = \\\\frac{1}{1 + \\\\exp\\\\left(\\\\ln\\\\frac{P(Y=0)}{P(Y=1)} + \\\\sum\\\\limits_{i=1}^d \\\\ln\\\\frac{P(X^i \\\\vert Y=0)}{P(X^i \\\\vert Y=1)}\\\\right)}\\n\\\\end{equation}\\nP(Y=1∣X)=1+exp(lnP(Y=1)P(Y=0)\\u200b+i=1∑d\\u200blnP(Xi∣Y=1)P(Xi∣Y=0)\\u200b)1\\u200b\\u200b\\u200bПерепишем сумму в знаменателе, воспользовавшись формулой плотности нормального распределения\\n∑i=1dln\\u2061P(Xi∣Y=0)P(Xi∣Y=1)=∑i=1dln\\u206112πσi2exp\\u2061(−(Xi−μ0,i)22σi2)12πσi2exp\\u2061(−(Xi−μ1,i)22σi2)\\\\sum\\\\limits_{i=1}^d \\\\ln\\\\frac{P(X^i \\\\vert Y=0)}{P(X^i \\\\vert Y=1)} = \\\\sum\\\\limits_{i=1}^d \\\\ln\\\\frac{\\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma_i^2}}\\\\exp \\\\left(\\\\frac{-(X^i - \\\\mu_{0,i})^2}{2\\\\sigma_i^2}\\\\right)}{\\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma_i^2}}\\\\exp \\\\left(\\\\frac{-(X^i - \\\\mu_{1,i})^2}{2\\\\sigma_i^2}\\\\right)} \\ni=1∑d\\u200blnP(Xi∣Y=1)P(Xi∣Y=0)\\u200b=i=1∑d\\u200bln2πσi2\\u200b\\u200b1\\u200bexp(2σi2\\u200b−(Xi−μ1,i\\u200b)2\\u200b)2πσi2\\u200b\\u200b1\\u200bexp(2σi2\\u200b−(Xi−μ0,i\\u200b)2\\u200b)\\u200b=∑i=1d(Xi−μ1,i)2−(Xi−μ0,i)22σi2=∑i=1d(μ0,i−μ1,iσi2Xi+μ1,i2−μ0,i22σi2)= \\\\sum\\\\limits_{i=1}^d \\\\frac{\\\\left(X^i - \\\\mu_{1, i}\\\\right)^2 - \\\\left(X^i - \\\\mu_{0, i}\\\\right)^2}{2\\\\sigma_i^2}= \\n\\\\sum\\\\limits_{i=1}^d \\\\left(\\\\frac{\\\\mu_{0, i} - \\\\mu_{1, i}}{\\\\sigma_i^2}X^i + \\\\frac{\\\\mu_{1, i} ^ 2 - \\\\mu_{0, i} ^ 2}{2\\\\sigma_i^2}\\\\right)=i=1∑d\\u200b2σi2\\u200b(Xi−μ1,i\\u200b)2−(Xi−μ0,i\\u200b)2\\u200b=i=1∑d\\u200b(σi2\\u200bμ0,i\\u200b−μ1,i\\u200b\\u200bXi+2σi2\\u200bμ1,i2\\u200b−μ0,i2\\u200b\\u200b)Подставляя это выражение в формулу  (5) , получаем\\nP(Y=1∣X)=11+exp\\u2061(ln\\u2061P(Y=0)P(Y=1)+∑i=1d(μ0,i−μ1,iσi2Xi+μ1,i2−μ0,i22σi2))P(Y=1 \\\\vert X) = \\\\frac{1}{1 + \\\\exp\\\\left(\\\\ln\\\\frac{P(Y=0)}{P(Y=1)} + \\\\sum\\\\limits_{i=1}^d \\\\left(\\\\frac{\\\\mu_{0, i} - \\\\mu_{1, i}}{\\\\sigma_i^2}X^i + \\\\frac{\\\\mu_{1, i} ^ 2 - \\\\mu_{0, i} ^ 2}{2\\\\sigma_i^2}\\\\right)\\\\right)}\\nP(Y=1∣X)=1+exp(lnP(Y=1)P(Y=0)\\u200b+i=1∑d\\u200b(σi2\\u200bμ0,i\\u200b−μ1,i\\u200b\\u200bXi+2σi2\\u200bμ1,i2\\u200b−μ0,i2\\u200b\\u200b))1\\u200bТаким образом, P(Y=1∣X)P(Y=1 \\\\vert X)P(Y=1∣X) представляется в GNB с общей ковариационной матрицей в таком же виде, как в модели логистической регрессии:\\nФормула (6)\\nP(Y=1∣X)=11+exp\\u2061(w0+∑i=1dwiXi)\\\\begin{equation}\\nP(Y=1 \\\\vert X) = \\\\frac{1}{1 + \\\\exp\\\\left(w_0 + \\\\sum\\\\limits_{i=1}^d w_i X^i\\\\right)}\\n\\\\end{equation}\\nP(Y=1∣X)=1+exp(w0\\u200b+i=1∑d\\u200bwi\\u200bXi)1\\u200b\\u200b\\u200bгде в случае GNB\\nw0=ln\\u2061P(Y=1)P(Y=0)+∑i=1dμ1,i2−μ0,i22σi2,wi=μ0,i−μ1,iσi2i=1,…,lw_0 = \\\\ln\\\\frac{P(Y=1)}{P(Y=0)} +\\\\sum\\\\limits_{i=1}^d\\\\frac{\\\\mu_{1, i} ^ 2 - \\\\mu_{0, i} ^ 2}{2\\\\sigma_i^2}, \\\\quad\\nw_i = \\\\frac{\\\\mu_{0, i} - \\\\mu_{1, i}}{\\\\sigma_i^2} \\\\hspace{1cm} i=1, \\\\dots, lw0\\u200b=lnP(Y=0)P(Y=1)\\u200b+i=1∑d\\u200b2σi2\\u200bμ1,i2\\u200b−μ0,i2\\u200b\\u200b,wi\\u200b=σi2\\u200bμ0,i\\u200b−μ1,i\\u200b\\u200bi=1,…,lОднако это не значит, что модели эквивалентны: модель логистической регрессии накладывает менее строгие ограничения на распределение P(X,Y)P(X, Y)P(X,Y), чем GNB.\\nТак, XiX^iXi могут не являться условно независимыми относительно YYY, а распределения P(X∣Y=y)P(X \\\\vert Y=y)P(X∣Y=y) могут не удовлетворять нормальному закону, но P(y∣X)P(y \\\\vert X)P(y∣X) может при этом всё равно представляться в виде формулы  (6) .\\nВ этом случае использование метода логистической регрессии предпочтительнее. С другой стороны, если есть основания полагать, что требования GNB выполняются, то от GNB можно ожидать более высокого качества классификации по сравнению с логистической регрессией.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф4.4. Как оценивать вероятностиКак правильно оценить вероятности классов в\\xa0задаче классификацииСледующий параграф4.6. Байесовский подход к оцениваниюБайесовская статистика. Априорные и\\xa0апостериорные распределения на\\xa0параметры моделей. MAP-оценки. Байесовский подход к\\xa0выбору моделей. Байесовский подход для задачи линейной регресииЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_40.html', 'title': 'Хорошие свойства рекомендательных систем'}, page_content='Хорошие свойства рекомендательных системЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/49.1.Введение в рекомендательные системы9.2.Рекомендации на основе матричных разложений9.3.Контентные рекомендации9.4.Хорошие свойства рекомендательных системВведениеПолнота (Coverage)Новизна (Novelty)Разнообразие (Diversity)SerendipityЗаключение10.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Хорошие свойства рекомендательных систем9.4. Хорошие свойства рекомендательных системАвторыИлья БойцовВведение\\nПредположим, выдача нашей рекомендательной системы имеет высокие значения метрик ранжирования. Значит ли это, что система действительно хорошая? Не всегда просто ответить на этот вопрос. Оптимизируя определенные метрики, можно выкрутить кликбейт, и пользователи будут охотно кликать в моменте, но больше не станут пользоваться таким сервисом. Соответственно, нужно как-то измерять «счастье пользователей», попытаться формализовать свойства, которыми должна обладать хорошая рекомендательная система. Однозначного ответа на этот вопрос нет, всё зависит от контекста применения рекомендательной системы. В этом разделе мы поговорим о наиболее распространённых критериях, которые довольно часто оказываются важными.\\nПолнота (Coverage)\\nПод полнотой в данном контексте понимается доля рекомендованных объектов IrecommendedI_{recommended}Irecommended\\u200b среди всех объектов III.\\nCoverage=∣Irecommended∣∣I∣Coverage = \\\\frac{|I_{recommended}|}{|I|}\\nCoverage=∣I∣∣Irecommended\\u200b∣\\u200bЭта метрика была предложена в статье Ge, M., Delgado-Battenfeld, C., Jannach, D. (2010, September). Beyond accuracy: evaluating recommender systems by coverage and serendipity. In Proceedings of the fourth ACM conference on Recommender systems (pp. 257-260).\\nДанную метрику имеет смысл оценивать в разных временных интервалах, при этом принимая во внимание возможные ограничения, связанные с объемом данных. Например, нас может интересовать значение полноты за первый день работы рекомендательной системы, а может – за неделю. Целевое поведение полноты будет различаться в зависимости от доменных областей и бизнес деталей конкретного случая. Например, в рекомендациях музыки может быть полезно периодически повторно рекомендовать треки, которые пользователю в наибольшей степени нравятся, так как пользователь может захотеть послушать их еще раз. В то же время в рекомендациях фильмов это реже оказывается осмысленным: обычно проходит много времени, прежде чем пользователь захочет пересмотреть фильм. Таким образом, во втором случае полнота будет расти быстрее за счет отсутствия повторов.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nЕще одним фактором, влияющим на полноту, является алгоритм холодного старта, который может использоваться для того чтобы найти подходящие объекты для нового пользователя или подходящих пользователей для нового объекта. Часто пользователям на этапе холодного старта показывают самые популярные объекты. Из-за этого свежедобавленные объекты (например, музыкальные треки) могут неявно пессимизироваться алгоритмом. Один из способов решения проблемы – бустить свежие объекты в течение определённого времени, чтобы они показывались чаще. Настройки логики холодного старта могут сильно повлиять на метрику полноты.\\nСреди других актуальных вопросов, которыми стоит задаваться:\\n\\nCколько нужно дней, чтобы полнота достигала заданного значения ppp?\\nВозможно ли достичь такого значения в принципе, используя текущий алгоритм?\\n\\nЧтобы ответить на эти вопросы, нужно принимать во внимание ряд факторов:\\n\\nКакой объём трафика у системы рекомендаций?\\nЕсть ли у бизнеса ограничения, влияющие на конечный список рекомендаций?\\nИмеет ли алгоритм рекомендаций достаточную степень персонализации?\\nМожно ли регулировать режимы exploration и exploitation во время работы рекомендательной системы?\\n\\nКаждый из этих факторов может по-разному влиять на динамику полноты. Бизнес ограничения и слабая степень персонализации могут сдерживать рост полноты. Напротив, если модель высокоперсонализированная и учитывает много пользовательских факторов, то она способна рекомендовать больше уникальных объектов из хвоста распределения, которые тоже могут ему понравиться, тем самым обеспечивая рост полноты.\\nНовизна (Novelty)\\nОдин из способов оценить новизну рекомендательной системы – использовать статистическую меру собственной информации объекта (self information), которая используется в теории информации и тесно связана с понятием энтропии. Значение собственной информации для события XXX равняется логарифму вероятности наступления данного события. Согласно теории, чем меньше вероятность наступления события, тем больше потенциальной информации принесет это событие при его наступлении. Единицей информации при использовании логарифма по основании 222 является бит.\\nТеперь если переносить идею собственной информации в парадигму рекомендательных систем, то получается, что чем менее популярен объект, тем более вероятно, что он будет новым для пользователя. А значит мера информации у такого объекта будет выше. Для каждого рекомендованного объекта iii считаем вероятность, с которой его порекомендуют случайному пользователю: Pi=miNP_{i} = \\\\frac{m_{i}}{N}Pi\\u200b=Nmi\\u200b\\u200b, где mim_{i}mi\\u200b – количество пользователей, которым был показан iii-й объект, а NNN – общее число пользователей. Для заданного пользователя усредняем значение собственной информации по списку его рекомендаций RRR и получаем итоговое значение метрики:\\nNoveltyuser=1∣R∣∑i∈R−log(P(i))Novelty_{user} = \\\\frac{1}{|R|}\\\\sum_{i\\\\in R}^{}-log(P(i))\\nNoveltyuser\\u200b=∣R∣1\\u200bi∈R∑\\u200b−log(P(i))Разнообразие (Diversity)\\nРазнообразие – это способность модели рекомендовать разные по содержанию объекты. Такое свойство очень важно для долгосрочного успеха сервисов, основанных на рекомендательных системах. Действительно, если модель постоянно рекомендует похожие друг на друга объекты, то рано или поздно пользователю наскучат такие рекомендации.\\n\\nРазнообразие можно рассчитывать на основе комбинаций метрик полноты и новизны. Также мерой разнообразия может быть дисперсия рекомендаций за заданный промежуток времени. Помимо этого популярны подходы, использующие эмбединги объектов для оценки попарной похожести объектов и расчёта на основе неё значения разнообразия. Одна из таких метрик – Intra List Similarity (ILS). Чтобы ее посчитать, нужно иметь эмбединги объектов рекомендаций, находящиеся в едином векторном пространстве. Для расчёта разнообразия для одного пользователя нужно усреднить попарную схожесть sim\\\\text{sim}sim между рекомендованными объектами:\\nILSuser=1R∑iϵR∑jϵRsim(i,j),ILS_{user} = \\\\frac{1}{R}\\\\sum_{i\\\\epsilon R}^{}\\\\sum_{j\\\\epsilon R}^{}\\\\text{sim}(i,j),\\nILSuser\\u200b=R1\\u200biϵR∑\\u200bjϵR∑\\u200bsim(i,j),где RRR – это набор рекомендованных пользователю объектов.\\nДля того чтобы добиться большего разнообразия, метрику нужно минимизировать. Мера схожести должна быть больше для более похожих объектов. Чаще всего используется косинусная близость (cosine similarity).\\nSerendipity\\nОдно из самых желанных свойств для любой рекомендательной системы. У слова serendipity нет четкого перевода, в 2008 году оно даже попало в список самых неподдающихся переводу слов в мире. На русский иногда оно переводится как «интуитивная прозорливость».\\nSerendipity – это способность рекомендовать такие объекты, которые не только релевантны для пользователя, но ещё и существенно отличаются от того, с какими объектами пользователь взаимодействовал в прошлом.\\n\\nSerendipity – довольно субъективное свойство и его сложно формализовать. Более того рекомендации, удовлетворяющие этому свойству, встречаются редко, что усложняет интерпретацию и измерение serendipity. Нет консенсуса о том, какой метрикой можно оценить его. Мы расскажем о способе, предложенном в статье T. Murakami, K. Mori, R. Orihara, Metrics for evaluating the serendipity of recommendation lists, in: New Frontiers in Artificial Intelligence, Vol. 4914, Springer Berlin Heidelberg, Berlin, Heidelberg, 2008, pp. 40–46.\\nПусть RuR_{u}Ru\\u200b – список рекомендаций для пользователя, Pru(i)\\\\text{Pr}_{u}(i)Pru\\u200b(i) – предсказание модели, для каждого объекта из списка, а Primu(i)\\\\text{Prim}_{u}(i)Primu\\u200b(i) – предсказание примитивной модели (в качестве примитивной можно брать модель на основе эвристик без машинного обучения или простую неперсональную модель), а rel\\\\text{rel}rel – известная релевантность объекта для пользователя. Тогда Serendipity рассчитывается следующим образом:\\nSerendipityuser=∑iϵRmax(Pru(i)−Primu(i),0)⋅relu(i)Serendipity_{user} = \\\\sum_{i\\\\epsilon R}^{}max(\\\\text{Pr}_{u}(i)-\\\\text{Prim}_{u}(i), 0) \\\\cdot {\\\\text{rel}_{u}(i)}\\nSerendipityuser\\u200b=iϵR∑\\u200bmax(Pru\\u200b(i)−Primu\\u200b(i),0)⋅relu\\u200b(i)Значение метрики можно усреднить по всем пользователям тестовой выборки. Чем больше значение, тем больше модель удовлетворяет свойству Serendipity.\\nКлючевая идея формулы такова: если уверенность персонализированной модели в том, что пользователю понравится iii-ый айтем, больше, чем уверенность неперсональной модели (примитивной), это значит, что данному пользователю может особенно понравиться iii-й айтем.\\nОтдельный вопрос – как оптимизировать Serendipity. Нужно улучшать способность модели к персонализации:\\n\\nдобавлять больше фичей для пар (пользователь, объект);\\nвзвешивать таргеты, чтобы более тонко учитывать необычные клики/просмотры;\\nписать кастомные функции потерь, которые будут поощрять модель за буст неожиданныйх объектов (которые в большей степени удовлетворяют свойству serendipity).\\n\\nКроме того, имеет смысл оптимизировать модель по метрике serendipity на офлайн тестовой выборке.\\nЗаключение\\nВ этом разделе мы рассмотрели ключевые свойства рекомендательных систем и метрики для их оценки. Рекомендательные системы – сложная область, где нет готовых рецептов оценки качества. Ключевые метрики всегда идут от продуктовых деталей применения рекомендательной системы. Полезно смотреть на несколько метрик одновременно, чтобы оценить разные свойства моделей.\\nВ какой момент нужно начинать следить за метриками из данного раздела? Несмотря на их ценность, на начальном этапе стоит концентрироваться на более простых и интуитивно понятных с точки зрения бизнеса метриках: конверсии, среднем времени визита и так далее. А вот как только базовые метрики будут на удовлетворительном уровне, стоит начинать мониторить и оптимизировать метрики, разобранные в этом разделе.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф9.3. Контентные рекомендацииСледующий параграф10.1. КластеризацияМетоды кластеризации: K-Means, агломеративная кластеризация, DBSCAN. Оценка качества кластеризацииЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_56.html', 'title': 'Проксимальные методы'}, page_content='Проксимальные методыЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/414.1.Оптимизация в ML14.2.Проксимальные методыПроксимальная минимизация14.3.Методы второго порядка14.4.Сходимость SGD15.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Проксимальные методы14.2. Проксимальные методыАвторыТяпкин ДаниилКак оптимизировать функции потерь с\\xa0$L_1$-регуляризациейВ этом разделе мы поговорим о том, как оптимизировать негладкие функции в ситуациях, когда «плохую» составляющую удаётся локализовать и она сравнительно несложная.\\nПроксимальная минимизация\\nДля того, чтобы подступиться к проксимальным методам, посмотрим на градиентный спуск с другой стороны. Для простоты рассмотрим константный размер шага α\\\\alphaα. Перепишем шаг градиентного спуска следующим образом:\\nxk+1−xkα=−∇f(xk).    \\\\frac{x_{k+1} - x_k}{\\\\alpha} = - \\\\nabla f(x_k).\\nαxk+1\\u200b−xk\\u200b\\u200b=−∇f(xk\\u200b).Посмотрим на это уравнение по-другому. Рассмотрим функцию x(t)x(t)x(t), равную xkx_kxk\\u200b при (k−1)α<t≤αk(k-1)\\\\alpha < t \\\\leq \\\\alpha k(k−1)α<t≤αk (ttt мы будем воспринимать, как некоторый временной параметр). Тогда при t=αkt = \\\\alpha kt=αk:\\nx(t+α)−x(t)α=−∇f(x(t)).    \\\\frac{x(t + \\\\alpha) - x(t)}{\\\\alpha} = - \\\\nabla f(x(t)).\\nαx(t+α)−x(t)\\u200b=−∇f(x(t)).Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.ВступитьТеперь слева не что иное, как аппроксимация производной! Если мы устремим α\\\\alphaα к нулю, то получится так называемое уравнение градиентного потока:\\nx˙=−∇f(x).    \\\\dot{x} = -\\\\nabla f(x).\\nx˙=−∇f(x).Эта динамика в случае выпуклой функции fff сходится к точке минимума x∗x^*x∗ из любой начальной точки при t→+∞t \\\\to +\\\\inftyt→+∞. Сравнение между динамикой градиентного спуска и градиентного потока можно увидеть на следующем изображении:\\n\\nПервый состоит из дискретных шагов, второй же представляет из себя непрерывный процесс.\\nНетрудно осознать физический смысл динамики x˙=−∇f(x)\\\\dot{x} = -\\\\nabla f(x)x˙=−∇f(x): маленькое тело скатывается по склону графика функции так, что в любой момент её скорость совпадает с антиградиентом, то есть оно катится по направлению наискорейшего спуска.\\nТеперь представим, что мы сейчас занимается не машинным обучением, а численными методами. Перед нами есть обыкновенное дифференциальное уравнение (ОДУ), и его надо решить. Одним из численных методов решения ОДУ (более стабильным, чем обычная схема Эйлера) является обратная схема Эйлера (backward Euler scheme):\\nxk+1−xkαk=−∇f(xk+1).    \\\\frac{x_{k+1} - x_k}{\\\\alpha_k} = -\\\\nabla f(x_{\\\\color{red}{k+1}}).\\nαk\\u200bxk+1\\u200b−xk\\u200b\\u200b=−∇f(xk+1\\u200b).В обратной схеме Эйлера мы делаем градиентный спуск, только градиент смотрим не в текущей точке (как было бы в обычной схеме Эйлера), а буквально в будущей. Занятная идея, только вот напрямую выразить xk+1x_{k+1}xk+1\\u200b из этого уравнения не получится. Нужно поступить чуть хитрее. Заметим, что\\n(xk+1−xk)iαk=12αk(x−xk)i2∣xk+1\\\\frac{(x_{k+1} - x_k)_i}{\\\\alpha_k} = \\\\left.\\\\frac{1}{2\\\\alpha_k} (x - x_k)^2_i \\\\right\\\\vert_{x_{k+1}}\\nαk\\u200b(xk+1\\u200b−xk\\u200b)i\\u200b\\u200b=2αk\\u200b1\\u200b(x−xk\\u200b)i2\\u200b\\u200bxk+1\\u200b\\u200bЭто позволяет нам сказать, что весь вектор xk+1−xkαk\\\\frac{x_{k+1} - x_k}{\\\\alpha_k}αk\\u200bxk+1\\u200b−xk\\u200b\\u200b является градиентом функции g(u)=12αk∥u−xk∥2g(u) = \\\\frac{1}{2\\\\alpha_k} \\\\Vert u - x_{k} \\\\Vert^2g(u)=2αk\\u200b1\\u200b∥u−xk\\u200b∥2, посчитанном в точке xk+1x_{k+1}xk+1\\u200b. Тогда получаем, что xk+1x_{k+1}xk+1\\u200b удовлетворяет следующему условию:\\n∇(g(u)+f(u))(xk+1)=0.    \\\\nabla\\\\left( g(u) + f(u) \\\\right)(x_{k+1}) = 0.\\n∇(g(u)+f(u))(xk+1\\u200b)=0.Если функция f(x)f(x)f(x) выпуклая, то f(x)+g(x)f(x) + g(x)f(x)+g(x) тоже выпуклая, и её стационарная точка будет точкой минимума. Стало быть, xk+1x_{k+1}xk+1\\u200b можно высчитывать по формуле\\nxk+1=arg\\u2061min\\u2061u{f(u)+12αk∥u−xk∥2}.    x_{k+1} = \\\\arg\\\\min_{u}\\\\left\\\\{ f(u) + \\\\frac{1}{2\\\\alpha_k}\\\\Vert u - x_{k} \\\\Vert^2   \\\\right\\\\}.\\nxk+1\\u200b=argumin\\u200b{f(u)+2αk\\u200b1\\u200b∥u−xk\\u200b∥2}.Определим прокс-оператор следующим образом:\\nproxf(x)=arg\\u2061min\\u2061{f(u)+12∥u−x∥2}.    \\\\mathrm{prox}_{f}(x)  = \\\\arg\\\\min\\\\left\\\\{ f(u) + \\\\frac{1}{2}\\\\Vert u - x \\\\Vert^2   \\\\right\\\\}.\\nproxf\\u200b(x)=argmin{f(u)+21\\u200b∥u−x∥2}.Тогда, поскольку умножение на αk>0\\\\alpha_k > 0αk\\u200b>0 внутри арг-минимума не влияет на саму точку минимума, получаем следующую итеративную схему:\\nxk+1=arg\\u2061min\\u2061{αk(f(u)+12αk∥u−x∥2)}=    x_{k+1} = \\\\arg\\\\min\\\\left\\\\{ \\\\alpha_k \\\\left(f(u) + \\\\frac{1}{2\\\\alpha_k}\\\\Vert u - x \\\\Vert^2   \\\\right)\\\\right\\\\} = \\nxk+1\\u200b=argmin{αk\\u200b(f(u)+2αk\\u200b1\\u200b∥u−x∥2)}=arg\\u2061min\\u2061{αkf(u)+12∥u−x∥2}=proxαkf(xk).    \\\\arg\\\\min\\\\left\\\\{ \\\\alpha_k f(u) + \\\\frac{1}{2}\\\\Vert u - x \\\\Vert^2  \\\\right\\\\}= \\\\mathrm{prox}_{\\\\alpha_k f}(x_k).\\nargmin{αk\\u200bf(u)+21\\u200b∥u−x∥2}=proxαk\\u200bf\\u200b(xk\\u200b).Итеративный процесс xk+1=proxαkf(xk)x_{k+1} = \\\\mathrm{prox}_{\\\\alpha_k f}(x_k)xk+1\\u200b=proxαk\\u200bf\\u200b(xk\\u200b) называется методом проксимальной минимизации. Вы можете спросить себя: зачем он нужен? Ведь теперь на каждом шаге мы должны решать задачу оптимизации:\\nmin\\u2061uf(u)+12αk∥u−xk∥2    \\\\min_{u} f(u) + \\\\frac{1}{2\\\\alpha_k}\\\\Vert u - x_k \\\\Vert^2\\numin\\u200bf(u)+2αk\\u200b1\\u200b∥u−xk\\u200b∥2Если fff выпуклая, нам есть, что ответить: наличие второго слагаемого гарантирует сильную выпуклость задачи, то есть она решается достаточно эффективно. Но если fff не является выпуклой, то мы ничего не достигли этой модификацией.\\nКомпозитная оптимизация, проксимальный градиентный метод (PGM)\\nЧтобы понять, зачем нам понадобилась проксимальная оптимизация, рассмотрим оптимизацию функций вида\\nmin\\u2061x{f(x)=g(x)+h(x)},    \\\\min_{x} \\\\{ f(x) = g(x) + h(x)\\\\},\\nxmin\\u200b{f(x)=g(x)+h(x)},где g(x)g(x)g(x) – это гладкая функция, а h(x)h(x)h(x) – это функция, для которой прокс-оператор считается аналитически. Воспользуемся следующим трюком: по ggg мы совершим градиентный шаг, а по hhh – проксимальный. Получаем следующую итеративную процедуру:\\nxk+1=proxαkh(xk−αk∇g(xk));    x_{k+1} = \\\\mathrm{prox}_{\\\\alpha_k h} (x_k - \\\\alpha_k \\\\nabla g(x_k));\\nxk+1\\u200b=proxαk\\u200bh\\u200b(xk\\u200b−αk\\u200b∇g(xk\\u200b));Эта процедура определяет так называемый проксимальный градиентный метод (Proximal Gradient Method, PGM), который может использоваться, например, для решения задачи регрессии с ℓ1\\\\ell_1ℓ1\\u200b-регуляризацией.\\nISTA (Iterative Shrinkage-Thresholding Algorithm)\\nТеперь решим конкретную задачу ℓ1\\\\ell_1ℓ1\\u200b-регрессии. Она выглядит следующим образом:\\n∥y−Xw∥22+λ∥w∥1→min\\u2061w.    \\\\Vert y - Xw \\\\Vert_2^2 + \\\\lambda \\\\Vert w \\\\Vert_1 \\\\to \\\\min_w.\\n∥y−Xw∥22\\u200b+λ∥w∥1\\u200b→wmin\\u200b.Мы хотим применить PGM к этой задаче, для этого нужно научиться вычислять прокс-оператор для ℓ1\\\\ell_1ℓ1\\u200b-нормы. Проделаем эту операцию:\\nproxα∥⋅∥1(x)=arg\\u2061min\\u2061u{∥u∥1+12α∥u−x∥22}=    \\\\mathrm{prox}_{\\\\alpha \\\\Vert \\\\cdot \\\\Vert_1}(x) = \\\\arg\\\\min_{u} \\\\left\\\\{  \\\\Vert u \\\\Vert_1 + \\\\frac{1}{2 \\\\alpha} \\\\Vert u - x \\\\Vert_2^2 \\\\right\\\\} = \\nproxα∥⋅∥1\\u200b\\u200b(x)=argumin\\u200b{∥u∥1\\u200b+2α1\\u200b∥u−x∥22\\u200b}==arg\\u2061min\\u2061u{∑i=1d∣ui∣+(ui−xi)22α}.    = \\\\arg\\\\min_{u} \\\\left\\\\{ \\\\sum_{i=1}^d \\\\vert u_i \\\\vert +  \\\\frac{(u_i - x_i)^2}{2\\\\alpha}  \\\\right\\\\}.\\n=argumin\\u200b{i=1∑d\\u200b∣ui\\u200b∣+2α(ui\\u200b−xi\\u200b)2\\u200b}.Заметим, что каждое слагаемое зависит только от одной координаты. Это значит, что каждую координату мы можем прооптимизировать отдельно и получить ddd одномерных задач минимизации вида\\narg\\u2061min\\u2061ui{∣ui∣+(ui−xi)22α}.    \\\\arg\\\\min_{u_i} \\\\left\\\\{ \\\\vert u_i \\\\vert + \\\\frac{(u_i - x_i)^2}{2\\\\alpha} \\\\right\\\\}.\\nargui\\u200bmin\\u200b{∣ui\\u200b∣+2α(ui\\u200b−xi\\u200b)2\\u200b}.Решение такой одномерной задачи записывается в виде функции soft thresholding:\\nproxα∥⋅∥1(x)i={xi−α,xi≥α0∣xi∣≤αxi+αxi≤−α    \\\\mathrm{prox}_{\\\\alpha \\\\Vert \\\\cdot \\\\Vert_1}(x)_i = \\\\begin{cases}\\n        x_i - \\\\alpha &, x_i \\\\geq \\\\alpha \\\\\\\\\\n        0 & \\\\vert x_i \\\\vert \\\\leq \\\\alpha \\\\\\\\\\n        x_i + \\\\alpha & x_i \\\\leq - \\\\alpha\\n    \\\\end{cases}\\nproxα∥⋅∥1\\u200b\\u200b(x)i\\u200b=⎩⎨⎧\\u200bxi\\u200b−α0xi\\u200b+α\\u200b,xi\\u200b≥α∣xi\\u200b∣≤αxi\\u200b≤−α\\u200bТогда мы получаем следующий алгоритм для ℓ1\\\\ell_1ℓ1\\u200b-регрессии, которые называются Iterative Shrinkage-Thresholding Algorithm (ISTA):\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1w = normal(0, 1)                                            # инициализация\\n2repeat S times:                                             # другой вариант: while abs(err) > tolerance\\n3    f = X.dot(w)                                            # посчитать предсказание\\n4    delta = f - y                                           # посчитать отклонение предсказания\\n5    grad = 2 * X.T.dot(delta) / n                           # посчитать градиент\\n6    w_prime = w - alpha * grad                              # считаем веса, которые отправим в прокс\\n7    for i in range(d):\\n8        w[i] = soft_threshold(w_prime[i], alpha * llambda)  # вычисляем прокс\\n\\n\\nЗаметим одну крутую особенность этого алгоритма -- мы явно видим, что решение получается разреженное, ведь какие-то координаты будут явно зануляться при применении soft threshold! Причем чем больше размер и шага, и параметра регуляризации, тем больше прореживается координат.\\nКонкретно этот метод не применяется на практике, но используются его вариации. Например, статья, которая указана в параграфе про линейные модели о том, как работало предсказание CTR в google в 2012 году, также базируется на вычислении soft threshold как прокс-оператора.\\nОбщие выводы\\nПодытожим все вышесказанное:\\n\\nПроксимальные методы – теоретически интересная идея для выпуклой оптимизации, которая должна давать более численно стабильные алгоритмы.\\nПроксимальные методы позволяют достаточно эффективно решать задачи композитной оптимизации, в частности, ℓ1\\\\ell_1ℓ1\\u200b-регуляризованную задачу регрессии. Более того, используемые на практике решения задачи ℓ1\\\\ell_1ℓ1\\u200b-регуляризованной регрессии так или иначе базируются на идее ISTA.\\nТакже есть попытки использовать проксимальные методы для более сложных моделей. Например, статья о применении их в нейросетях.\\n\\nКроме того, имеются применения проксимальных методов для построения распределенных алгоритмов. Все подробности можно найти в монографии Neal Parikh и Stephen Boyd, мы же только привели применение этих идей в машинном обучении.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф14.1. Оптимизация в MLКак найти оптимум функции потерь: от\\xa0градиентного спуска до\\xa0AdamСледующий параграф14.3. Методы второго порядкаОт\\xa0метода Ньютона до\\xa0LBFGSЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_67.html', 'title': 'Независимость и условные распределения вероятностей'}, page_content='Независимость и условные распределения вероятностейЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/716.1.Матричное дифференцирование16.2.Матричная факторизация16.3.Вероятностные распределения16.4.Многомерные распределения16.5.Независимость и условные распределения вероятностейУсловная вероятностьФормула полной вероятностиФормула БайесаНезависимые событияУсловная независимостьУсловные распределенияУсловные математические ожиданияРегрессия16.6.Параметрические оценки16.7.Энтропия и семейство экспоненциальных распределенийГлавная/Хендбуки/Учебник по машинному обучению/Независимость и условные распределения вероятностей16.5. Независимость и условные распределения вероятностейАвторыСергей ЛыткинВ этом параграфе описываются, пожалуй, главные фичи теории вероятностей: независимые события и условные вероятности. Эти концепции имеют большое прикладное значение, да и с теоретической точки зрения главным образом благодаря им теория вероятностей выделяется в отдельную ветвь математики.\\nУсловная вероятность\\nУсловная вероятность возникает при ответе на вопрос о том, каковы шансы события AAA при условии,что случилось событие BBB, и обозначается P(A∣B)\\\\mathbb P(A\\\\vert B)P(A∣B).\\nПример. Согласно исследованиям, в среднем 5%5\\\\%5% пациентов испытывают приступы кашля в течение дня, однако среди курильщиков доля кашляющих составляет 40%40\\\\%40%. То есть (безусловная) вероятность P(кашляет)=0.05\\\\mathbb P(\\\\text{кашляет}) = 0.05P(кашляет)=0.05 при добавлении обусловливания может существенно измениться: P(кашляет∣курит)=0.4\\\\mathbb P(\\\\text{кашляет}\\\\vert\\\\text{курит}) = 0.4P(кашляет∣курит)=0.4.\\nУпражнение. Известно, что в семье два ребёнка, причём один из них мальчик. Какова вероятность, что другой ребёнок тоже мальчик?\\nОтвет (не открывайте сразу, попробуйте сначала решить самостоятельно)Как ни странно, ответ вовсе не 50%50\\\\%50%. Пол новорождённого ребёнка можно приближённо считать результатом испытания Бернулли с вероятностью успеха 12\\\\frac 1221\\u200b.\\nИз четырёх возможных вариантов ММ, МД, ДМ, ДД условию удовлетворяют только первые три, и лишь в одном случае из этих трёх второй ребёнок тоже мальчик. Поэтому правильный ответ — 13\\\\frac 1331\\u200b.\\nДобавляя формализма, обозначим\\nA={хотя\\xa0бы\\xa0один\\xa0ребёнок\\xa0—\\xa0мальчик}  A = \\\\{\\\\text{хотя бы один ребёнок — мальчик}\\\\}\\nA={хотя\\xa0бы\\xa0один\\xa0ребёнок\\xa0—\\xa0мальчик}B={мальчики\\xa0оба\\xa0ребёнка},  B = \\\\{\\\\text{мальчики оба ребёнка}\\\\},\\nB={мальчики\\xa0оба\\xa0ребёнка},и тогда условная вероятность P(B∣A)\\\\mathbb P(B\\\\vert A)P(B∣A) вычисляется по формуле\\nP(B∣A)=P(A∩B)P(A)=1/43/4=13.  \\\\mathbb P(B\\\\vert A) = \\\\frac{\\\\mathbb{P}(A \\\\cap B)}{\\\\mathbb{P}(A)} = \\\\frac{1/4}{3/4} = \\\\frac 13.\\nP(B∣A)=P(A)P(A∩B)\\u200b=3/41/4\\u200b=31\\u200b.В общем случае условная вероятность P(B∣A)\\\\mathbb P(B\\\\vert A)P(B∣A) при P(A)≠0\\\\mathbb P(A) \\\\ne 0P(A)\\ue020=0 полагается равнойВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nP(B∣A)=P(A∩B)P(A).  \\\\mathbb{P}(B \\\\vert A) = \\\\frac{\\\\mathbb{P}(A \\\\cap B)}{\\\\mathbb{P}(A)}.\\nP(B∣A)=P(A)P(A∩B)\\u200b.В зависимости от соотношения событий AAA и BBB условная вероятность P(B∣A)\\\\mathbb{P}(B \\\\vert A)P(B∣A) может принимать разные значения, например:\\n\\nесли A∩B=∅A\\\\cap B = \\\\varnothingA∩B=∅, то событие AAA исключает реализацию события BBB, и P(B∣A)=0\\\\mathbb{P}(B \\\\vert A) = 0P(B∣A)=0;\\nесли A⊂BA \\\\subset BA⊂B, то событие AAA гарантирует осуществление события BBB, и P(B∣A)=1\\\\mathbb{P}(B \\\\vert A) = 1P(B∣A)=1.\\n\\nРазумеется, чаще всего события AAA и BBB соотносятся между собой более хитрым образом, и значение условной вероятности P(B∣A)\\\\mathbb{P}(B \\\\vert A)P(B∣A) находится строго между 000 и 111.\\nФормула полной вероятности\\nПусть пространство Ω\\\\OmegaΩ разбивается на попарно несовместные события B1,B2,…,BnB_1, B_2, \\\\dots, B_nB1\\u200b,B2\\u200b,…,Bn\\u200b:\\nΩ=B1∪…∪Bn,Bi∩Bj=∅\\xa0при\\xa0i≠j.  \\\\Omega = B_1 \\\\cup \\\\ldots \\\\cup B_n, \\\\quad B_i \\\\cap B_j = \\\\varnothing \\\\text{ при } i\\\\ne j.\\nΩ=B1\\u200b∪…∪Bn\\u200b,Bi\\u200b∩Bj\\u200b=∅\\xa0при\\xa0i\\ue020=j.Тогда\\nA=A∩Ω=(A∩B1)∪…∪(A∩Bn);A = A\\\\cap\\\\Omega = (A\\\\cap B_1) \\\\cup \\\\ldots \\\\cup (A\\\\cap B_n);\\nA=A∩Ω=(A∩B1\\u200b)∪…∪(A∩Bn\\u200b);отсюда по свойству конечной аддитивности находим, что\\nP(A)=P(A∩B1)+…+P(A∩Bn).  \\\\mathbb P(A) = \\\\mathbb{P}(A \\\\cap B_1)  + \\\\ldots + \\\\mathbb{P}(A \\\\cap B_n).\\nP(A)=P(A∩B1\\u200b)+…+P(A∩Bn\\u200b).Переходя к условным вероятностям, получаем формулу полной вероятности:\\nP(A)=∑k=1nP(A∣Bk)P(Bk).  \\\\mathbb{P}(A) = \\\\sum\\\\limits_{k=1}^n \\\\mathbb{P}(A \\\\vert B_k) \\\\mathbb{P}(B_k).\\nP(A)=k=1∑n\\u200bP(A∣Bk\\u200b)P(Bk\\u200b).Пример. Среди населения 33.7%33.7\\\\%33.7% имеют первую группу крови, 37.5%37.5\\\\%37.5% — вторую, 20.9%20.9\\\\%20.9% — третью, 7.9%7.9\\\\%7.9% — четвёртую. При переливании крови надо учитывать группы крови донора и рецепиента:\\n\\nреципиенту с четвёртой группой крови можно перелить кровь любой группы;\\nреципиентам со второй и третьей группами можно перелить кровь той же группы или первой;\\nреципиентам с первой группой крови можно перелить только кровь первой группы.\\n\\nС какой вероятностью допустимо переливание в случайно взятой паре донор—реципиент?\\nРешение. Пусть событие AAA состоит в том, что переливание возможно, а событие BkB_kBk\\u200b — в том, что донор имеет группу kkk. По формуле полной вероятности\\nP(A)=P(A∣B1)P(B1)+P(A∣B2)P(B2)+P(A∣B3)P(B3)+P(A∣B4)P(B4).  \\\\mathbb P(A) = \\\\mathbb P(A\\\\vert B_1) \\\\mathbb P(B_1) + \\\\mathbb P(A\\\\vert B_2) \\\\mathbb P(B_2) +\\n  \\\\mathbb P(A\\\\vert B_3) \\\\mathbb P(B_3) + \\\\mathbb P(A\\\\vert B_4) \\\\mathbb P(B_4).\\nP(A)=P(A∣B1\\u200b)P(B1\\u200b)+P(A∣B2\\u200b)P(B2\\u200b)+P(A∣B3\\u200b)P(B3\\u200b)+P(A∣B4\\u200b)P(B4\\u200b).Вероятности P(Bk)\\\\mathbb P(B_k)P(Bk\\u200b) даны в условии, оттуда же находим, что\\nP(A∣B1)=1,  \\\\mathbb P(A\\\\vert B_1) = 1,\\nP(A∣B1\\u200b)=1,P(A∣B2)=P(B2)+P(B4),  \\\\mathbb P(A\\\\vert B_2) =  \\\\mathbb P(B_2) + \\\\mathbb P(B_4),\\nP(A∣B2\\u200b)=P(B2\\u200b)+P(B4\\u200b),P(A∣B3)=P(B3)+P(B4),  P(A\\\\vert B_3) =  \\\\mathbb P(B_3) + \\\\mathbb P(B_4),\\nP(A∣B3\\u200b)=P(B3\\u200b)+P(B4\\u200b),P(A∣B4)=P(B4).  \\\\mathbb P(A\\\\vert B_4) = \\\\mathbb P(B_4). \\nP(A∣B4\\u200b)=P(B4\\u200b).Подставляя численные значения, получаем\\nP(A)=0.337+(0.375+0.079)⋅0.375+(0.209+0.079)⋅0.209+0.0792=0.573683.  \\\\mathbb P(A) = 0.337 + (0.375+0.079)\\\\cdot 0.375 + (0.209+0.079)\\\\cdot 0.209 + 0.079^2 = 0.573683.\\nP(A)=0.337+(0.375+0.079)⋅0.375+(0.209+0.079)⋅0.209+0.0792=0.573683.Упражнение. Решите предыдущий пример, выбирая в качестве разбиения набор событий CkC_kCk\\u200b, каждое из которых заключается в том, что реципиент имеет группу kkk.\\nОтветПо той же формуле полной вероятности получаем, что\\nP(A)=P(A∣С1)P(С1)+P(A∣С2)P(С2)+P(A∣С3)P(С3)+P(A∣С4)P(С4).  \\\\mathbb P(A) = \\\\mathbb P(A\\\\vert С_1) \\\\mathbb P(С_1) + \\\\mathbb P(A\\\\vert С_2) \\\\mathbb P(С_2) +\\n  \\\\mathbb P(A\\\\vert С_3) \\\\mathbb P(С_3) + \\\\mathbb P(A\\\\vert С_4) \\\\mathbb P(С_4).\\nP(A)=P(A∣С1\\u200b)P(С1\\u200b)+P(A∣С2\\u200b)P(С2\\u200b)+P(A∣С3\\u200b)P(С3\\u200b)+P(A∣С4\\u200b)P(С4\\u200b).Ясно, что P(Ck)=P(Bk)\\\\mathbb P(C_k) = \\\\mathbb P(B_k)P(Ck\\u200b)=P(Bk\\u200b); далее из условия находим, что\\nP(A∣С1)=P(C1),  \\\\mathbb P(A\\\\vert С_1) = \\\\mathbb P(C_1),\\nP(A∣С1\\u200b)=P(C1\\u200b),P(A∣C2)=P(C1)+P(C2),  \\\\mathbb P(A\\\\vert C_2) =  \\\\mathbb P(C_1) + \\\\mathbb P(C_2),\\nP(A∣C2\\u200b)=P(C1\\u200b)+P(C2\\u200b),P(A∣C3)=P(C1)+P(C3),  P(A\\\\vert C_3) =  \\\\mathbb P(C_1) + \\\\mathbb P(C_3),\\nP(A∣C3\\u200b)=P(C1\\u200b)+P(C3\\u200b),P(A∣C4)=1.  \\\\mathbb P(A\\\\vert C_4) = 1. \\nP(A∣C4\\u200b)=1.Подставляя численные значения, получаем тот же ответ P(A)=0.573683\\\\mathbb P(A) = 0.573683P(A)=0.573683.\\nФормула полной вероятности легко обобщается на случай счётного числа попарно несовместных событий BkB_kBk\\u200b, а также на случай обусловливания по некоторому событию CCC, например:\\nP(A∣C)=∑nP(A∣Bn,C)P(Bn∣C).  \\\\mathbb{P}(A\\\\vert C) = \\\\sum\\\\limits_n \\\\mathbb{P}(A \\\\vert B_n, C) \\\\mathbb{P}(B_n \\\\vert C).\\nP(A∣C)=n∑\\u200bP(A∣Bn\\u200b,C)P(Bn\\u200b∣C).Формула Байеса\\nЗаметим, что вероятность P(A∩B)\\\\mathbb{P}(A \\\\cap B)P(A∩B) можно записать двумя способами\\nP(B∣A)P(A)=P(A∩B)=P(A∣B)P(B).  \\\\mathbb{P}(B \\\\vert A)\\\\mathbb{P}(A) = \\\\mathbb{P}(A \\\\cap B) = \\\\mathbb{P}(A \\\\vert B)\\\\mathbb{P}(B).\\nP(B∣A)P(A)=P(A∩B)=P(A∣B)P(B).Оставим P(B∣A)\\\\mathbb{P}(B \\\\vert A)P(B∣A) в левой части и получим формулу Байеса.\\nФормула Байеса. Для любых событий AAA, BBB c положительной вероятностью\\nP(B∣A)=P(A∣B)P(B)P(A).  \\\\mathbb{P}(B \\\\vert A) = \\\\frac{\\\\mathbb{P}(A \\\\vert B)\\\\mathbb{P}(B)}{\\\\mathbb{P}(A)}.\\nP(B∣A)=P(A)P(A∣B)P(B)\\u200b.Для вычисления знаменателя в формуле Байеса часто используется формула полной вероятности.\\nУпражнение. Среди определенной группы людей вероятность некоторой болезни 0.02. Тест, позволяющий выявить болезнь, несовершенен. На больном он дает позитивный результат в 98 случаях из 100, и, кроме того, он дает позитивный результат в 4 случаях из 100 на здоровом. Найдите вероятность того, что человек, на котором тест дал положительный результат, действительно болен.\\nОтвет (не открывайте сразу, попробуйте сначала решить самостоятельно)По формуле Байеса,\\nP(болен∣+)=P(+∣болен)P(болен)P(+).  \\\\mathbb{P}(\\\\text{болен}\\\\vert\\\\text{+}) = \\\\frac{\\\\mathbb{P}(\\\\text{+}\\\\vert\\\\text{болен}) \\\\mathbb{P}(\\\\text{болен})}{\\\\mathbb{P}(\\\\text{+})}.\\nP(болен∣+)=P(+)P(+∣болен)P(болен)\\u200b.По условию, P(+∣болен)=0.98\\\\mathbb{P}(\\\\text{+} \\\\vert \\\\text{болен}) = 0.98P(+∣болен)=0.98, P(болен)=0.02\\\\mathbb{P}(\\\\text{болен}) = 0.02P(болен)=0.02. Чтобы посчитать вероятность теста быть положительньным, применим формулу полной вероятности:\\nP(+)=P(+∣болен)P(болен)+P(+∣здоров)P(здоров)=0.98⋅0.02+0.04⋅0.98=0.98⋅0.06.  \\\\mathbb{P}(\\\\text{+}) = \\\\mathbb{P}(\\\\text{+} \\\\vert \\\\text{болен}) \\\\mathbb{P}(\\\\text{болен}) + \\\\mathbb{P}(\\\\text{+} \\\\vert \\\\text{здоров})\\\\mathbb{P}(\\\\text{здоров}) = 0.98 \\\\cdot 0.02 + 0.04 \\\\cdot 0.98 = 0.98 \\\\cdot 0.06.\\nP(+)=P(+∣болен)P(болен)+P(+∣здоров)P(здоров)=0.98⋅0.02+0.04⋅0.98=0.98⋅0.06.Тогда по формуле Байеса\\nP(болен∣+)=0.98⋅0.020.98⋅0.06=13.  \\\\mathbb{P}(\\\\text{болен} \\\\vert \\\\text{+}) = \\\\frac{0.98 \\\\cdot 0.02}{0.98 \\\\cdot 0.06} = \\\\frac13.\\nP(болен∣+)=0.98⋅0.060.98⋅0.02\\u200b=31\\u200b.Получается, что точность теста очень низка — всего лишь около 1 из 3. Это происходит, потому что больные люди встречаются редко (2 из 100), и эта частота сравнима с долей ошибок I и II рода — 0.02 и 0.04.\\nДля непрерывного случая тоже есть своя формула полной вероятности, см. раздел про условную вероятность.\\nНезависимые события\\nСобытия AAA и BBB называются независимыми, если\\nP(A∣B)=P(A)\\\\mathbb{P}(A \\\\vert B) = \\\\mathbb{P}(A)P(A∣B)=P(A), то есть информация о реализации события BBB никак не влияет на вероятность события AAA.\\nПо определению условной вероятности независимость событий AAA и BBB эквивалентна тому, что\\nP(A∩B)=P(A)P(B).  \\\\mathbb{P}(A \\\\cap B) = \\\\mathbb{P}(A) \\\\mathbb{P}(B).\\nP(A∩B)=P(A)P(B).Последнее равенство годится для определения независмости событий AAA и BBB даже в том случае, если P(A)=0\\\\mathbb{P}(A) = 0P(A)=0 или P(B)=0\\\\mathbb{P}(B) = 0P(B)=0.\\nПример. В полной колоде карт находится 525252 карты: 444 масти от двойки до туза. Вероятность вытащить туза равна P(Ace)=452=113\\\\mathbb P(\\\\mathrm{Ace}) = \\\\frac 4{52} = \\\\frac 1{13}P(Ace)=524\\u200b=131\\u200b, карту пиковой масти — P(♠)=1352=14\\\\mathbb P(\\\\spadesuit) = \\\\frac {13}{52} = \\\\frac 1{4}P(♠)=5213\\u200b=41\\u200b. Эти события независимы, поскольку в пересечении этих событий лежит ровно одна карта — туз пик, вероятность появления которого равна 152=113⋅14=P(Ace)P(♠)\\\\frac 1{52} = \\\\frac 1{13} \\\\cdot \\\\frac 14 = \\\\mathbb P(\\\\mathrm{Ace})\\\\mathbb P(\\\\spadesuit)521\\u200b=131\\u200b⋅41\\u200b=P(Ace)P(♠).\\nПусть теперь вытаскивается сразу две карты. Зависимы ли события «вытащены две карты пиковой масти» и «вытащены туз и король»? Посчитаем:\\nP(♠♠)=(132)(522)=13⋅1252⋅51=117,  \\\\mathbb P(\\\\spadesuit \\\\spadesuit) = \\\\frac{\\\\binom{13}2}{\\\\binom{52}2} = \\\\frac{13\\\\cdot 12}{52\\\\cdot 51} = \\\\frac 1{17},\\nP(♠♠)=(252\\u200b)(213\\u200b)\\u200b=52⋅5113⋅12\\u200b=171\\u200b,P(AK)=16(522)=3252⋅51=8663.  \\\\mathbb P(\\\\mathrm{AK}) = \\\\frac{16}{\\\\binom{52}2} = \\\\frac{32}{52\\\\cdot 51} = \\\\frac 8{663}.\\nP(AK)=(252\\u200b)16\\u200b=52⋅5132\\u200b=6638\\u200b.Вероятность вытащить туза и короля пик равна 1(522)=11326≈0.00075\\\\frac 1{\\\\binom{52}2} = \\\\frac 1{1326}\\\\approx 0.00075(252\\u200b)1\\u200b=13261\\u200b≈0.00075, что отличается от P(♠♠)P(AK)=811271≈0.00071\\\\mathbb P(\\\\spadesuit \\\\spadesuit)\\\\mathbb P(\\\\mathrm{AK})  = \\\\frac 8{11271} \\\\approx 0.00071P(♠♠)P(AK)=112718\\u200b≈0.00071. Таким образом, эти события зависимы.\\nСобытия A1,…,AnA_1, \\\\ldots, A_nA1\\u200b,…,An\\u200b попарно независимы, если P(Ai∩Aj)=P(Ai)P(Aj)\\\\mathbb{P}(A_i \\\\cap A_j) = \\\\mathbb{P}(A_i) \\\\mathbb{P}(A_j)P(Ai\\u200b∩Aj\\u200b)=P(Ai\\u200b)P(Aj\\u200b) при i≠ji \\\\ne ji\\ue020=j. Эти же события независимы в совокупности, если\\nP(Ai1∩…∩Aim)=∏k=1mP(Aik)  \\\\mathbb P\\\\big(A_{i_1}\\\\cap \\\\ldots \\\\cap A_{i_m}\\\\big) = \\\\prod\\\\limits_{k=1}^m \\\\mathbb P(A_{i_k})\\nP(Ai1\\u200b\\u200b∩…∩Aim\\u200b\\u200b)=k=1∏m\\u200bP(Aik\\u200b\\u200b)\\xa0для\\xa0любого\\xa0набора\\xa0индексов\\xa01⩽i1<…<im⩽n.  \\\\text{ для любого набора индексов } 1\\\\leqslant i_1 < \\\\ldots < i_m\\\\leqslant n.\\n\\xa0для\\xa0любого\\xa0набора\\xa0индексов\\xa01⩽i1\\u200b<…<im\\u200b⩽n.Упражнение. Приведите пример попарно независимых событий A1A_1A1\\u200b, A2A_2A2\\u200b, A3A_3A3\\u200b, не являющихся независимыми в совокупности.\\nОтвет (не открывайте сразу, попробуйте сначала решить самостоятельно)Раскрасим тетраэдр в три цвета следующим образом: одна грань красная (R), вторая — зелёная (G), третья — синяя (B), а четвёртая содержит все три цвета. События RRR, GGG, BBB состоят в том, что при случайном броске на нижней грани тетраэдра есть соответствующий цвет.\\nТогда\\nP(R)=P(G)=P(B)=12,  \\\\mathbb P(R) = \\\\mathbb P(G) = \\\\mathbb P(B) = \\\\frac 12,\\nP(R)=P(G)=P(B)=21\\u200b,P(R∩G)=P(R∩B)=P(G∩B)=14,  \\\\mathbb P(R \\\\cap G) = \\\\mathbb P(R \\\\cap B) = \\\\mathbb P(G\\\\cap B) = \\\\frac 14,\\nP(R∩G)=P(R∩B)=P(G∩B)=41\\u200b,что влечёт попарную независимость событий RRR, GGG, BBB. Однако P(R∩G∩B)=14\\\\mathbb P(R \\\\cap G \\\\cap B) = \\\\frac 14P(R∩G∩B)=41\\u200b, что не равно P(R)P(G)P(B)=18\\\\mathbb P(R)\\\\mathbb P(G)\\\\mathbb P(B) = \\\\frac 18P(R)P(G)P(B)=81\\u200b, поэтому эти события не являются независимыми  совокупности.\\nОпределение независимости случайных величин из предыдущего параграфа полностью согласуется с только что введённым определением независимых событий. Например, для случая дискретных случайных величин ξ\\\\xiξ и η\\\\etaη обозначим\\nAi=P(ξ=xi),Bj=P(η=yj);  A_i = \\\\mathbb P(\\\\xi = x_i), \\\\quad B_j = \\\\mathbb P(\\\\eta = y_j);\\nAi\\u200b=P(ξ=xi\\u200b),Bj\\u200b=P(η=yj\\u200b);тогда P(ξ=xi,η=yj)=P(Ai∩Bj)\\\\mathbb P(\\\\xi = x_i, \\\\eta = y_j) = \\\\mathbb P(A_i \\\\cap B_j)P(ξ=xi\\u200b,η=yj\\u200b)=P(Ai\\u200b∩Bj\\u200b), и поэтому независимость случайных величин ξ\\\\xiξ и η\\\\etaη эквивалентна независимости событий AiA_iAi\\u200b и BjB_jBj\\u200b для всевозможных значений iii и jjj.\\nЗамечание о статистической независимостиМатематический термин «независимость» подразумевает статистическую (или стохастическую) независимость, которая может не вполне совпадать по смыслу с интуитивным значением этого термина. Например, если вы два раза подкидываете симметричную монетку, то статистически результат первого броска никак не влияет на результат второго броска. Но так ли это с философской точки зрения? Вот представим две ситуации:\\n\\n\\nвы бросили монетку, быстро подняли с пола, и снова бросили;\\n\\n\\nмонетка при первом броске укатилась далеко под диван, и вы полчаса ворочали мебель, прежде чем произвести второе испытание.\\n\\n\\nВесьма вероятно, что столь досадное происшествие после первого броска могло существенно повлиять на ваше физическое и моральное состояние. И уж точно второй бросок в ситуациях (1) и (2) вы бы совершили совершенно по-разному, что вполне могло отразиться на его результате.\\nОднако в математике подобным метафизическим измышлениям нет места. С абстрактным понятием независимости гораздо проще работать, поскольку оно игнорирует замысловатые причинно-следственные связи и прочие несущественные детали. В модели независимых испытаний Бернулли каждое следующее испытание статистически никак не зависит от предыдущих. Что бы с вами не происходило, шансы во втором броске — 50 на 50, именно об этом говорит нам независимость испытаний Бернулли с вероятностью успеха 12\\\\frac 1221\\u200b, не больше и не меньше.\\nУсловная независимость\\nБывает так, что зависимые события AAA и BBB становятся независимыми при выполнении некоторого третьего события CCC. Более формально, события AAA и BBB условно независимы по отношению к событию CCC, если P(C)>0\\\\mathbb P(C) > 0P(C)>0 и\\nP(A∣B,C)=P(A∣C).  \\\\mathbb P(A \\\\vert B, C) = \\\\mathbb P(A\\\\vert C).\\nP(A∣B,C)=P(A∣C).Поскольку\\nP(A∣B,C)=P(A∩B∩C)P(B∩C),P(A∣C)=P(A∩C)P(C),  \\\\mathbb P(A \\\\vert B, C)  = \\\\frac{\\\\mathbb P(A \\\\cap B \\\\cap C)}{\\\\mathbb P(B \\\\cap C)}, \\\\quad\\n  \\\\mathbb P(A \\\\vert C)  = \\\\frac{\\\\mathbb P(A \\\\cap C)}{\\\\mathbb P(C)},\\nP(A∣B,C)=P(B∩C)P(A∩B∩C)\\u200b,P(A∣C)=P(C)P(A∩C)\\u200b,то условная независимость событий AAA и BBB эквивалетна равенству\\nP(A∩B∩C)P(C)=P(A∩C)P(C)⋅P(B∩C)P(C),  \\\\frac{\\\\mathbb P(A \\\\cap B \\\\cap C)}{\\\\mathbb P(C)} =  \\\\frac{\\\\mathbb P(A \\\\cap C)}{\\\\mathbb P(C)} \\\\cdot  \\\\frac{\\\\mathbb P(B \\\\cap C)}{\\\\mathbb P(C)},\\nP(C)P(A∩B∩C)\\u200b=P(C)P(A∩C)\\u200b⋅P(C)P(B∩C)\\u200b,а это, в свою очередь, означает, что\\nP(A∩B∣C)=P(A∣C)P(B∣C).  \\\\mathbb P(A \\\\cap B\\\\vert C) = \\\\mathbb P(A\\\\vert C) \\\\mathbb P(B\\\\vert C).\\nP(A∩B∣C)=P(A∣C)P(B∣C).Таким образом, вероятность произведения условно независимых событий равна произведению условных вероятностей. Эта формула полностью аналогична формуле P(A∩B)=P(A)P(B)\\\\mathbb P(A\\\\cap B) = \\\\mathbb P(A)\\\\mathbb P(B)P(A∩B)=P(A)P(B) для (безусловно) независимых событий.\\nПример (цепь Маркова). Последовательность событий S0,S1,S2,…,St,…S_0, S_1, S_2, \\\\ldots, S_t, \\\\ldotsS0\\u200b,S1\\u200b,S2\\u200b,…,St\\u200b,… называется марковской цепью, если выполняется марковское свойство\\nP(St+1∣St,St−1,…,S0)=P(St+1∣St),t∈N∪{0}.\\\\mathbb P (S_{t+1} \\\\vert S_t, S_{t-1}, \\\\ldots, S_0) = \\\\mathbb P(S_{t+1} \\\\vert S_t), \\\\quad t \\\\in \\\\mathbb N \\\\cup \\\\{0\\\\}.\\nP(St+1\\u200b∣St\\u200b,St−1\\u200b,…,S0\\u200b)=P(St+1\\u200b∣St\\u200b),t∈N∪{0}.В марковском свойстве заложен следующий смысл: в каждый момент времени ttt «будущее» St+1S_{t+1}St+1\\u200b зависит только от «настоящего» StS_tSt\\u200b, но не зависит от «прошлого»\\nPt=St−1∩…∩S0.P_t = S_{t-1} \\\\cap \\\\ldots \\\\cap S_0.\\nPt\\u200b=St−1\\u200b∩…∩S0\\u200b.Итак, цепь Маркова характеризуется равенством P(St+1∣Pt,St)=P(St+1∣St)\\\\mathbb P(S_{t+1} \\\\vert P_t, S_t) = \\\\mathbb P(S_{t+1} \\\\vert S_t)P(St+1\\u200b∣Pt\\u200b,St\\u200b)=P(St+1\\u200b∣St\\u200b), которое означает, что события St+1S_{t+1}St+1\\u200b и PtP_tPt\\u200b условно независимы по отношению к событию StS_tSt\\u200b.\\nУсловные распределения\\nПусть ξ\\\\xiξ и η\\\\etaη — дискретные случайные величины и P(η=y)>0\\\\mathbb P(\\\\eta = y) > 0P(η=y)>0. По аналогии с условными вероятностями условное распределение случайной величины ξ\\\\xiξ при условии, что значение случайной величины η\\\\etaη равно yyy, определяется по формуле\\nP(ξ=xi∣η=y)=P(ξ=xi,η=y)P(η=y).  \\\\mathbb P(\\\\xi = x_i \\\\vert \\\\eta = y) = \\\\frac{\\\\mathbb P(\\\\xi = x_i , \\\\eta = y)}{\\\\mathbb P(\\\\eta = y)}.\\nP(ξ=xi\\u200b∣η=y)=P(η=y)P(ξ=xi\\u200b,η=y)\\u200b.Это действительно распределение вероятностей, поскольку P(ξ=xi∣η=y)⩾0\\\\mathbb P(\\\\xi = x_i \\\\vert \\\\eta = y) \\\\geqslant 0P(ξ=xi\\u200b∣η=y)⩾0 и\\n∑iP(ξ=xi∣η=y)=1P(η=y)∑iP(ξ=xi,η=y)=1.\\\\sum\\\\limits_{i}\\\\mathbb P(\\\\xi = x_i \\\\vert \\\\eta = y) = \\\\frac 1{\\\\mathbb P(\\\\eta = y)} \\\\sum\\\\limits_{i}\\\\mathbb P(\\\\xi = x_i , \\\\eta = y) = 1.\\ni∑\\u200bP(ξ=xi\\u200b∣η=y)=P(η=y)1\\u200bi∑\\u200bP(ξ=xi\\u200b,η=y)=1.В непрерывном случае условное распределение задаётся условной плотностью\\npξ∣η(x∣y)=p(x,y)pη(y),  p_{\\\\xi\\\\vert \\\\eta}(x\\\\vert y) = \\\\frac{p(x, y)}{p_\\\\eta(y)},\\npξ∣η\\u200b(x∣y)=pη\\u200b(y)p(x,y)\\u200b,где p(x,y)p(x, y)p(x,y) — совместная плотность случайных величин ξ\\\\xiξ и η\\\\etaη. И снова проведением маргинализации по xxx убеждаемся в том, что с нормировкой всё в порядке:\\n∫−∞+∞pξ∣η(x∣y)\\u2009dx=1pη(y)∫−∞+∞p(x,y)\\u2009dx=pη(y)pη(y)=1.  \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p_{\\\\xi\\\\vert \\\\eta}(x\\\\vert y)\\\\,dx = \\\\frac 1{p_\\\\eta(y)}\\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p(x, y)\\\\,dx = \\\\frac {p_\\\\eta(y)}{p_\\\\eta(y)} = 1.\\n−∞∫+∞\\u200bpξ∣η\\u200b(x∣y)dx=pη\\u200b(y)1\\u200b−∞∫+∞\\u200bp(x,y)dx=pη\\u200b(y)pη\\u200b(y)\\u200b=1.Поскольку ∫−∞+∞p(x,y)\\u2009dy=pξ(x)\\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p(x, y)\\\\,dy = p_\\\\xi(x)−∞∫+∞\\u200bp(x,y)dy=pξ\\u200b(x), из формулы условной плотности получаем непрерывный аналог формулы полной вероятности:\\npξ(x)=∫Rpξ∣η(x∣y)pη(y)dy.  p_\\\\xi(x) = \\\\int\\\\limits_{\\\\mathbb{R}} p_{\\\\xi \\\\mid \\\\eta}(x\\\\vert y) p_\\\\eta(y) dy.\\npξ\\u200b(x)=R∫\\u200bpξ∣η\\u200b(x∣y)pη\\u200b(y)dy.Пример. Выберем случайное число x∈[12,1]x\\\\in \\\\big[\\\\tfrac12, 1\\\\big]x∈[21\\u200b,1], а затем — случайное число y∈[0,x]y \\\\in [0, x]y∈[0,x]. Как распределена случайная величина yyy?\\nПереформулируем задачу: известно, что ξ∼U[12,1]\\\\xi \\\\sim U\\\\big[\\\\tfrac12, 1\\\\big]ξ∼U[21\\u200b,1] и η∣ξ∼U[0,x]\\\\eta \\\\vert \\\\xi \\\\sim U[0, x]η∣ξ∼U[0,x]. Требуется найти плотность случайной величины η\\\\etaη. Имеем\\npξ(x)=2I[12,1](x),pη∣ξ(y∣x)=1xI[0,x](y).  p_\\\\xi(x) = 2\\\\mathbb I_{\\\\big[\\\\tfrac12, 1\\\\big]}(x), \\\\quad\\n  p_{\\\\eta\\\\mid\\\\xi}(y\\\\vert x) = \\\\frac 1x\\\\mathbb I_{[0, x]}(y).\\npξ\\u200b(x)=2I[21\\u200b,1]\\u200b(x),pη∣ξ\\u200b(y∣x)=x1\\u200bI[0,x]\\u200b(y).Применяя формулу полной вероятности, находим\\npη(y)=∫1/212xI[y⩽x]\\u2009dx={2ln\\u20612,0⩽y<12,−2ln\\u2061y,12⩽y⩽1.  p_\\\\eta(y) = \\\\int\\\\limits_{1/2}^1 \\\\frac 2x \\\\mathbb I[y \\\\leqslant x]\\\\, dx =\\n    \\\\begin{cases}\\n      2 \\\\ln2, & 0 \\\\leqslant y < \\\\tfrac12, \\\\\\\\\\n      -2 \\\\ln{y}, & \\\\tfrac12 \\\\leqslant y \\\\leqslant 1.\\n    \\\\end{cases}\\npη\\u200b(y)=1/2∫1\\u200bx2\\u200bI[y⩽x]dx={2ln2,−2lny,\\u200b0⩽y<21\\u200b,21\\u200b⩽y⩽1.\\u200bУпражнение. Пусть случайные величины ξk∼Exp(λk)\\\\xi_k \\\\sim \\\\mathrm{Exp}(\\\\lambda_k)ξk\\u200b∼Exp(λk\\u200b), k=1,…,nk=1, \\\\ldots, nk=1,…,n, независимы в совокупности. Чему равна вероятность P(ξk=min\\u2061{ξ1,…,ξn})\\\\mathbb P\\\\big(\\\\xi_k = \\\\min \\\\{\\\\xi_1, \\\\ldots, \\\\xi_n \\\\}\\\\big)P(ξk\\u200b=min{ξ1\\u200b,…,ξn\\u200b})?\\n\\nОтвет (не открывайте сразу, попробуйте сначала решить самостоятельно)Обозначим η=argmin\\u20611⩽k⩽n{ξk}\\\\eta = \\\\operatorname*{argmin}\\\\limits_{1\\\\leqslant k \\\\leqslant n} \\\\{\\\\xi_k\\\\}η=1⩽k⩽nargmin\\u200b{ξk\\u200b}. Требуется найти P(η=k)\\\\mathbb P(\\\\eta = k)P(η=k). По формуле полной вероятности имеем\\nP(η=k)=∫0+∞pη∣ξk(η=k∣x)pξk(x)\\u2009dx.\\\\mathbb P(\\\\eta = k) = \\\\int\\\\limits_0^{+\\\\infty} p_{\\\\eta\\\\vert \\\\xi_k}(\\\\eta = k \\\\vert x) p_{\\\\xi_k}(x)\\\\,dx.\\nP(η=k)=0∫+∞\\u200bpη∣ξk\\u200b\\u200b(η=k∣x)pξk\\u200b\\u200b(x)dx.Далее, pξk(x)=λke−λkxp_{\\\\xi_k}(x) = \\\\lambda_k e^{-\\\\lambda_k x}pξk\\u200b\\u200b(x)=λk\\u200be−λk\\u200bx, x⩾0x\\\\geqslant 0x⩾0,\\npη∣ξk(η=k∣x)=P(ξi>x,i≠k)=∏i≠ke−λix.  p_{\\\\eta\\\\vert \\\\xi_k}(\\\\eta = k \\\\vert x) = \\\\mathbb P(\\\\xi_i > x,  i \\\\ne k) = \\\\prod\\\\limits_{i\\\\ne k} e^{-\\\\lambda_i x}.\\npη∣ξk\\u200b\\u200b(η=k∣x)=P(ξi\\u200b>x,i\\ue020=k)=i\\ue020=k∏\\u200be−λi\\u200bx.Таким образом,\\nP(η=k)=∫0+∞λke−λkx∏i≠ke−λix\\u2009dx=λk∫0+∞exp\\u2061(−∑i=1nλix)\\u2009dx=λkλ1+…+λn.\\\\mathbb P(\\\\eta = k) = \\\\int\\\\limits_0^{+\\\\infty} \\\\lambda_k e^{-\\\\lambda_k x}\\\\prod\\\\limits_{i\\\\ne k} e^{-\\\\lambda_i x} \\\\, dx = \\\\lambda_k \\\\int\\\\limits_0^{+\\\\infty} \\\\exp\\\\Big(-\\\\sum\\\\limits_{i=1}^n \\\\lambda_i x\\\\Big)\\\\,dx= \\\\frac{\\\\lambda_k}{\\\\lambda_1 + \\\\ldots + \\\\lambda_n}.\\nP(η=k)=0∫+∞\\u200bλk\\u200be−λk\\u200bxi\\ue020=k∏\\u200be−λi\\u200bxdx=λk\\u200b0∫+∞\\u200bexp(−i=1∑n\\u200bλi\\u200bx)dx=λ1\\u200b+…+λn\\u200bλk\\u200b\\u200b.Условные распределения случайных векторов определяется аналогично с поправкой на возросшее число аргументов: в этом случае xxx и yyy уже не числа, а вектора тех же размерностей, что и сами случайные вектора.\\nУсловные математические ожидания\\nУсловное математическое ожидание E(ξ∣η=y)\\\\mathbb E(\\\\xi\\\\vert\\\\eta = y)E(ξ∣η=y) отвечает на вопрос «чему равно среднее значение случайной величины ξ\\\\xiξ при условии, что η=y\\\\eta = yη=y?».\\nИмея в распоряжении матрицу условного дискретного распределения P(ξ=xi∣η=yj)\\\\mathbb P(\\\\xi = x_i\\\\vert \\\\eta = y_j)P(ξ=xi\\u200b∣η=yj\\u200b) или условную плотность pξ∣η(x∣y)p_{\\\\xi\\\\vert \\\\eta}(x\\\\vert y)pξ∣η\\u200b(x∣y), условное математическое ожидание можно вычислить следующим образом:\\n\\nE(ξ∣η)≡E(ξ∣η=y)=∑ixiP(ξ=xi∣η=y)\\\\mathbb E(\\\\xi\\\\vert\\\\eta)\\\\equiv \\\\mathbb E(\\\\xi\\\\vert\\\\eta=y) = \\\\sum\\\\limits_i x_i \\\\mathbb P(\\\\xi = x_i\\\\vert \\\\eta = y)E(ξ∣η)≡E(ξ∣η=y)=i∑\\u200bxi\\u200bP(ξ=xi\\u200b∣η=y) в дискретном случае;\\nE(ξ∣η)≡E(ξ∣η=y)=∫Rxpξ∣η(x∣y)\\u2009dx\\\\mathbb E(\\\\xi\\\\vert\\\\eta) \\\\equiv \\\\mathbb E(\\\\xi\\\\vert\\\\eta=y) = \\\\int\\\\limits_{\\\\mathbb R} x p_{\\\\xi\\\\vert \\\\eta}(x\\\\vert y)\\\\,dxE(ξ∣η)≡E(ξ∣η=y)=R∫\\u200bxpξ∣η\\u200b(x∣y)dx для непрерывных ξ\\\\xiξ и η\\\\etaη.\\n\\nВажно отметить, что после суммирования или интегрирования по переменной xxx в формуле условного математического ожидания остаются зависимость от yyy. Таким образом, в отличие от обычного среднего, которое является просто числом, условное ожидание представляет собой случайную величину ζ=E(ξ∣η=y)\\\\zeta = \\\\mathbb E(\\\\xi\\\\vert\\\\eta=y)ζ=E(ξ∣η=y), поскольку его значение зависит от случайного значения η=y\\\\eta = yη=y.\\nСвойства условного математического ожидания\\n\\n\\nE(aξ1+bξ2∣η)=aE(ξ1∣η)+bE(ξ2∣η)\\\\mathbb E(a\\\\xi_1 + b \\\\xi_2 \\\\vert \\\\eta) = a\\\\mathbb E (\\\\xi_1\\\\vert \\\\eta) + b \\\\mathbb E (\\\\xi_2 \\\\vert\\\\eta)E(aξ1\\u200b+bξ2\\u200b∣η)=aE(ξ1\\u200b∣η)+bE(ξ2\\u200b∣η) (линейность).\\n\\n\\nЕсли ξ1⩽ξ2\\\\xi_1 \\\\leqslant \\\\xi_2ξ1\\u200b⩽ξ2\\u200b, то E(ξ1∣η)⩽E(ξ2∣η)\\\\mathbb E (\\\\xi_1\\\\vert \\\\eta) \\\\leqslant \\\\mathbb E (\\\\xi_2\\\\vert \\\\eta)E(ξ1\\u200b∣η)⩽E(ξ2\\u200b∣η) (монотонность).\\n\\n\\nЕсли случайные величины ξ\\\\xiξ и η\\\\etaη независимы, то E(ξ∣η)=Eξ\\\\mathbb E(\\\\xi\\\\vert\\\\eta) = \\\\mathbb E\\\\xiE(ξ∣η)=Eξ.\\n\\n\\nE(g(η)ξ∣η)=g(η)E(ξ∣η)\\\\mathbb E(g(\\\\eta) \\\\xi\\\\vert\\\\eta) = g(\\\\eta) \\\\mathbb E(\\\\xi\\\\vert \\\\eta)E(g(η)ξ∣η)=g(η)E(ξ∣η).\\n\\n\\nE(E(ξ∣η))=Eξ\\\\mathbb E\\\\big(\\\\mathbb E(\\\\xi\\\\vert \\\\eta)\\\\big) = \\\\mathbb E\\\\xiE(E(ξ∣η))=Eξ (law of total expectation).\\n\\n\\nУпражнение. Prove the law of total expectation.\\nОтвет (не открывайте сразу, попробуйте сначала решить самостоятельно)Пусть ζ=E(ξ∣η)\\\\zeta = \\\\mathbb E(\\\\xi \\\\vert \\\\eta)ζ=E(ξ∣η). Начнём с дискретного случая:\\nEζ=∑jE(ξ∣η=yj)P(η=yj)=∑j∑ixiP(ξ=xi∣η=yj)P(η=yj)=  \\\\mathbb E\\\\zeta = \\\\sum\\\\limits_j \\\\mathbb E(\\\\xi\\\\vert\\\\eta = y_j)\\\\mathbb P(\\\\eta = y_j) = \\n  \\\\sum\\\\limits_j\\\\sum\\\\limits_i x_i \\\\mathbb P(\\\\xi = x_i\\\\vert \\\\eta = y_j)\\\\mathbb P(\\\\eta = y_j)=\\nEζ=j∑\\u200bE(ξ∣η=yj\\u200b)P(η=yj\\u200b)=j∑\\u200bi∑\\u200bxi\\u200bP(ξ=xi\\u200b∣η=yj\\u200b)P(η=yj\\u200b)==∑ixi∑jP(ξ=xi,η=yj)=∑ixiP(ξ=xi)=Eξ.  =\\\\sum\\\\limits_i x_i \\\\sum\\\\limits_j\\\\mathbb P(\\\\xi = x_i, \\\\eta = y_j) \\n  =\\\\sum\\\\limits_i x_i \\\\mathbb P(\\\\xi = x_i) = \\\\mathbb E\\\\xi.\\n=i∑\\u200bxi\\u200bj∑\\u200bP(ξ=xi\\u200b,η=yj\\u200b)=i∑\\u200bxi\\u200bP(ξ=xi\\u200b)=Eξ.В непрерывном случае вместо сумм потребуется переставить местами интегралы. Это позволяет сделать теорема Фубини о сведении двойного интеграла к повторному:\\nEζ=∫−∞+∞E(ξ∣η=y)pη(y)\\u2009dy=∫−∞+∞pη(y)\\u2009dy∫−∞+∞xpξ∣η(x∣y)\\u2009dx=  \\\\mathbb E\\\\zeta = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty}\\\\mathbb E(\\\\xi\\\\vert\\\\eta = y) p_\\\\eta(y)\\\\,dy =\\n  \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p_\\\\eta(y)\\\\,dy \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty}x p_{\\\\xi\\\\vert\\\\eta}(x\\\\vert y)\\\\,dx =\\nEζ=−∞∫+∞\\u200bE(ξ∣η=y)pη\\u200b(y)dy=−∞∫+∞\\u200bpη\\u200b(y)dy−∞∫+∞\\u200bxpξ∣η\\u200b(x∣y)dx==∫−∞+∞x\\u2009dx∫−∞+∞p(x,y)\\u2009dy=∫−∞+∞xpξ(x)\\u2009dx=Eξ.  = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} x\\\\,dx \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p(x, y)\\\\,dy  = \\n  \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} xp_\\\\xi(x) \\\\,dx = \\\\mathbb E\\\\xi.\\n=−∞∫+∞\\u200bxdx−∞∫+∞\\u200bp(x,y)dy=−∞∫+∞\\u200bxpξ\\u200b(x)dx=Eξ.Условная дисперсия определяется по формуле\\nV(ξ∣η)=E((ξ−E(ξ∣η))2∣η)=E(ξ2∣η)−(E(ξ∣η))2.  \\\\mathbb V(\\\\xi \\\\vert \\\\eta) = \\\\mathbb E\\\\big((\\\\xi - \\\\mathbb E(\\\\xi\\\\vert \\\\eta))^2 \\\\vert \\\\eta\\\\big) = \\\\mathbb E(\\\\xi^2 \\\\vert \\\\eta) - \\\\big(\\\\mathbb E(\\\\xi \\\\vert \\\\eta))^2.\\nV(ξ∣η)=E((ξ−E(ξ∣η))2∣η)=E(ξ2∣η)−(E(ξ∣η))2.Справедливо равенство Vξ=E(V(ξ∣η))+V(E(ξ∣η))\\\\mathbb V \\\\xi = \\\\mathbb E\\\\big(\\\\mathbb V(\\\\xi\\\\vert \\\\eta)\\\\big) + \\\\mathbb V\\\\big(\\\\mathbb E(\\\\xi\\\\vert \\\\eta)\\\\big)Vξ=E(V(ξ∣η))+V(E(ξ∣η)) (law of total variance).\\nРегрессия\\nВ машинном обучении часто встречается задача регрессии, в которой требуется восстановить зависимость Y=f(X)Y = f(X)Y=f(X) при наличии выборки\\n(X1,Y1),…,(Xn,Yn)  (X_1, Y_1), \\\\ldots, (X_n, Y_n) \\n(X1\\u200b,Y1\\u200b),…,(Xn\\u200b,Yn\\u200b)из некоторого неизвестного распределения с совместной плотностью p(x,y)p(x, y)p(x,y). Стандартный способ решения задачи регресии — минимизация среднего значения функции потерь L(Y,f(X))\\\\mathcal L(Y, f(X))L(Y,f(X)):\\nE[L(Y,f(X))]=∬R2L(y,f(x))p(x,y)\\u2009dxdy→min\\u2061.  \\\\mathbb E \\\\big[\\\\mathcal L(Y, f(X))\\\\big] = \\\\iint\\\\limits_{\\\\mathbb R^2} \\\\mathcal L(y, f(x)) p(x, y) \\\\,dxdy \\\\to \\\\min.\\nE[L(Y,f(X))]=R2∬\\u200bL(y,f(x))p(x,y)dxdy→min.В качестве функции потерь на одном объекте (x,y)(x, y)(x,y) в задаче регрессии обычно выбирают квадратичную функцию: L(y,f(x))=(y−f(x))2\\\\mathcal L(y, f(x)) = (y-f(x))^2L(y,f(x))=(y−f(x))2. Тогда\\nE[L(Y,f(X))]=∬R2(y−f(x))2p(x,y)\\u2009dxdy;  \\\\mathbb E \\\\big[\\\\mathcal L(Y, f(X))\\\\big] = \\\\iint\\\\limits_{\\\\mathbb R^2} \\\\mathcal (y-f(x))^2 p(x, y) \\\\,dxdy;\\nE[L(Y,f(X))]=R2∬\\u200b(y−f(x))2p(x,y)dxdy;для минимизации этого функционала применим немножко вариационного исчисления и продифференцируем по функции f(x)f(x)f(x). Получим\\n2∬R2(f(x)−y)p(x,y)\\u2009dxdy=0,2\\\\iint\\\\limits_{\\\\mathbb R^2} (f(x)-y) p(x, y) \\\\,dxdy = 0,\\n2R2∬\\u200b(f(x)−y)p(x,y)dxdy=0,откуда\\nf(x)=1p(x)∫−∞+∞yp(x,y)\\u2009dy=∫−∞+∞ypY∣X(y∣x)\\u2009dy=E(Y∣X=x).  f(x) = \\\\frac 1{p(x)} \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} yp(x, y)\\\\,dy = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} yp_{Y\\\\vert X}(y \\\\vert x)\\\\,dy = \\\\mathbb E(Y\\\\vert X = x).\\nf(x)=p(x)1\\u200b−∞∫+∞\\u200byp(x,y)dy=−∞∫+∞\\u200bypY∣X\\u200b(y∣x)dy=E(Y∣X=x).Полученное условное математическое ожидание, называемое функцией регрессии, показывает, чему в среднем равно значение зависимой переменной YYY при условии, что X=xX=xX=x.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф16.4. Многомерные распределенияСледующий параграф16.6. Параметрические оценкиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_30.html', 'title': 'Дистилляция знаний'}, page_content='Дистилляция знанийЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/27.1.Обучение представлений7.2.Дистилляция знанийСжатие моделейХинтоновская дистилляция знанийДополнительные источники знаний для дистилляцииИерархия методов дистилляции знаний8.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Дистилляция знаний7.2. Дистилляция знанийАвторыВадим ШияновВ этом параграфе вы познакомитесь с продвинутой техникой машинного обучения, получившей название дистилляции знаний. Дистилляция знаний (knowledge distillation) — это способ обучения в первую очередь нейросетевых моделей машинного обучения, направленный на передачу знаний от модели-учителя к модели-ученику.\\n\\n\\n\\nИсточник\\n\\n\\nСлишком абстрактное определение? Соглашусь, но в последние годы дистилляция знаний как поле исследований сильно разрослась и стала включать в себя множество новых и, возможно, даже неожиданных сценариев применения. Так, авторы статьи 2020 года утверждают, что смогли добиться примерной инвариантности выходов полносвязной сети к сдвигу входа-картинки с помощью дистилляции в неё знаний из сверточной сети.\\nТаким образом получается, что дистилляция знаний может применяться для того, чтобы передавать так называемые inductive biases от одной сети к другой. Схожие доводы встречаются и в статьях безумно популярного на момент написания данного параграфа направления трансформеров для компьютерного зрения.\\nТак, использование дистилляции знаний оказалась важным компонентом для получения хорошего качества предсказания на ImageNet от ViT без использования дополнительных данных. Впоследствии данный подход использовался и в других трансформерах для компьютерного зрения, например, в LeViT.\\nТем не менее, среди всего разнообразия применений дистилляции знаний наиболее ярко выделяется одно — сжатие моделей.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nСжатие моделей\\nЗадача сжатия моделей проистекает из следующего наблюдения. Неоднократно было замечено, что в широком диапазоне практически значимых задач машинного обучения точность предсказания модели существенно зависит от её размера. При этом зачастую данная зависимость выглядит достаточно тривиально: последовательное увеличение размеров модели позволяет последовательно улучшать точность её предсказаний.\\nОднако такой безграничный рост приводит к ряду проблем, связанных с практическим применением итоговых моделей. Сюда относятся рост времени обучения больших моделей и повышенные аппетиты таких моделей к размерам и качеству обучающей выборки. Кроме того, большие модели нередко требуют более дорогостоящего вычислительного оборудования для эффективного применения, особенно если мы говорим об обработке большого количества запросов в сжатые сроки. А для некоторых сценариев, таких как предсказание в реальном времени и/или на мобильных устройствах, применение большой модели может оказаться вовсе невозможным.\\nЭти проблемы породили каждая свою ветвь исследований. Так в последние годы де-факто стандартным способом обучения даже относительно компактных моделей стало использование mixed-precision training, которое позволяет ускорить обучение более или менее любых сетей на современных графических процессорах, при этом практически без потерь в итоговом качестве. Для борьбы с недостатком обучающих данных была предложена целая плеяда методов self-supervised pretraining, и новые появляются до сих пор. Сжатие моделей же концентрируется на решении проблем, связанных с этапом применения уже обученных моделей.\\nКак можно догадаться из названия, задача сжатия моделей заключается в том, чтобы взять большую модель, и сжать её в как можно более компактную модель при этом по возможности минимально пожертвовав качеством предсказания.\\nИсторически задачу сжатия моделей пытались решать множеством разных способов. Классическим примером здесь может служить прунинг, где модель обучается специальным образом (например, с использованием L2 регуляризации) так, чтобы часть весов в итоге можно было занулить и исключить из итоговой модели.\\nОднако методы данного семейства, как правило, страдают от двух основных проблем.\\n\\nВо-первых, простое удаление части весов каждого из слоёв обычно показывает лишь незначительное ускорение в применении итоговой модели за исключением случаев использования специализированной аппаратуры\\nВо-вторых, наивный прунинг нередко приводит к существенной просадке в качестве предсказания сжатой модели, причём соотношение степени сжатия и качества итоговой модели едва ли возможно контролировать. Чтобы обойти данные ограничения, и была предложена техника дистилляции знаний.\\n\\nХинтоновская дистилляция знаний\\nПервой статьёй, в которой можно встретить дистилляцию знаний в современном виде является статья  Хинтона и др. 2014 года.\\nВ ней авторы рассматривают задачу классификации картинок и предлагают использовать предсказания большой заранее обученной модели-учителя в качестве новой, мягкой, разметки, которую будет пытаться повторить компактный ученик.\\nФормулировка\\nАвторы предлагают использовать дивергенцию Кульбака-Лейблера между предсказаниями учителя и ученика в качестве дополнительного слагаемого к стандартной функции потерь — кросс-энтропии между предсказанием ученика и жёсткой разметкой:\\nLKD=1N∑i=1N(−∑j=1Kyijlog\\u2061pij+λDKL(pi∥qi))=\\\\mathcal{L}_{KD} = \\\\frac1N \\\\sum_{i=1}^N \\\\left( - \\\\sum_{j=1}^K y_{ij} \\\\log p_{ij} + \\\\lambda D_{KL}(\\\\mathbf{p}_i \\\\| \\\\mathbf{q}_i) \\\\right) =\\nLKD\\u200b=N1\\u200bi=1∑N\\u200b(−j=1∑K\\u200byij\\u200blogpij\\u200b+λDKL\\u200b(pi\\u200b∥qi\\u200b))==1N∑i=1N(−∑j=1Kyijlog\\u2061pij+λ∑j=1Kqijlog\\u2061qijpij)∼= \\\\frac1N \\\\sum_{i=1}^N \\\\left( - \\\\sum_{j=1}^K y_{ij} \\\\log p_{ij} + \\\\lambda \\\\sum_{j=1}^K q_{ij} \\\\log \\\\frac{q_{ij}}{p_{ij}} \\\\right) \\\\sim\\n=N1\\u200bi=1∑N\\u200b(−j=1∑K\\u200byij\\u200blogpij\\u200b+λj=1∑K\\u200bqij\\u200blogpij\\u200bqij\\u200b\\u200b)∼∼−1N∑i=1N(∑j=1Kyijlog\\u2061pij+λ∑j=1Kqijlog\\u2061pij)\\\\sim - \\\\frac1N \\\\sum_{i=1}^N \\\\left( \\\\sum_{j=1}^K y_{ij} \\\\log p_{ij} + \\\\lambda \\\\sum_{j=1}^K q_{ij} \\\\log p_{ij} \\\\right)\\n∼−N1\\u200bi=1∑N\\u200b(j=1∑K\\u200byij\\u200blogpij\\u200b+λj=1∑K\\u200bqij\\u200blogpij\\u200b)Здесь через LKD\\\\mathcal{L}_{KD}LKD\\u200b обозначена функция потерь для дистилляции знаний. Под NNN подразумевается число объектов, а под KKK — классов, представленных в обучающей выборке. Через yijy_{ij}yij\\u200b обозначена жёсткая разметка:\\nyij={1,если\\xa0i-ый\\xa0объект\\xa0принадлежит\\xa0j-ому\\xa0классу,0,в\\xa0противном\\xa0случае.y_{ij} = \\\\begin{cases}\\n1, & \\\\text{если $i$-ый объект принадлежит $j$-ому классу}, \\\\\\\\\\n0, & \\\\text{в противном случае}.\\\\end{cases}\\nyij\\u200b={1,0,\\u200bесли\\xa0i-ый\\xa0объект\\xa0принадлежит\\xa0j-ому\\xa0классу,в\\xa0противном\\xa0случае.\\u200bЧерез pijp_{ij}pij\\u200b обозначены вероятности классов, предсказанные моделью-учеником, а через qijq_{ij}qij\\u200b — моделью-учителем. Коэффициент λ\\\\lambdaλ позволяет настраивать баланс между решением исходной задачи и повторением предсказаний учителя.\\nВ последнем переходе учтено, что логарифм частного раскладывается в разность логарифмов, после чего один из членов можно исключить, поскольку он не зависит от оптимизируемых значений pijp_{ij}pij\\u200b. В дальнейшем для упрощения выкладок под LKD\\\\mathcal{L}_{KD}LKD\\u200b будет подразумеваться именно последнее выражение.\\nМотивация\\nСвой выбор функции потерь авторы мотивируют следующим образом. Широко известным фактом является то, что при классификации картинок на достаточно больших и разнообразных датасетах большие нейронные сети стабильно показывают лучшие результаты по сравнению с компактными. Однако также хорошо известно, что даже сравнительно небольшие нейронные сети способны приближать очень широкий спектр функций.\\nВ таком случае можно предположить, что проблема обучения компактных сетей заключается не в том, что компактная модель не способна приблизить ту же функцию, что и большая, а в том, что компактная модель не способна самостоятельно выучить данную функцию из исходных данных. В таком случае потенциально мы можем подтолкнуть компактную модель к выучиванию более информативного представления путем модификации функции потерь.\\nКак этого добиться? Давайте возьмем заведомо более информативное представление, выученное большой моделью-учителем, и добавим в функцию потерь слагаемое, которое будет учить модель-ученика повторять его. В случае решения задачи классификации KL-дивергенция является именно таким слагаемым.\\nЕсть и другой способ взглянуть на хинтоновскую дистилляцию знаний. Минимизация LKD\\\\mathcal{L}_{KD}LKD\\u200b отличается от стандартного обучения, тем, что мы дополнительно минимизируем расстояние между предсказаниями ученика и учителя. От стандартной разметки такая целевая переменная отличается наличием так называемых теневых знаний (dark knowledge), которые состоят из вероятностей принадлежности объекта ко всем классам, помимо истинного.\\nБлагодаря теневым знаниям модели-ученику во время обучения доступна дополнительная информация о взаимоотношениях между представленными в обучающей выборке классами, а также схожести отдельных объектов и классов.\\nЧтобы проверить данную гипотезу, авторы проводят следующий эксперимент. Сначала они обучают модель-учителя классифицировать картинки на датасете MNIST. После этого авторы обучают компактную модель-ученика с помощью ранее полученного учителя на тех же данных, но опуская при этом все картинки цифры 333. После этого авторы показывают, что, если исправить коэффициент сдвига для данного класса в последнем слое сети-ученика с помощью небольшой валидационной выборки, сеть способна верно определить 98.6%98.6\\\\%98.6% картинок тройки, несмотря на то, что во время обучения она не видела ни одного примера.\\nТакже косвенным подтверждением данной гипотезы можно считать и тот факт, что при использовании довольно популярной сейчас техники сглаживания разметки (label smoothing), эффективность дистилляции знаний заметно падает. Именно теневые знания на данный момент являются де-факто стандартным объяснением эффекта Хинтоновской дистилляции знаний.\\nИспользование температуры при подсчете KL-дивергенции\\nВ качестве дополнительной эвристики авторы также предлагают перед взятием KL-дивергенции сглаживать распределения учителя и ученика с помощью температуры TTT, то есть вместо pi\\\\mathbf{p}_ipi\\u200b и qi\\\\mathbf{q}_iqi\\u200b считать KL-дивергенцию между piT\\\\mathbf{p}_i^TpiT\\u200b и qiT\\\\mathbf{q}_i^TqiT\\u200b, где:\\npiT=softmax(ziT)\\\\mathbf{p}_i^T = \\\\text{softmax}\\\\left(\\\\frac{\\\\mathbf{z}_i}{T}\\\\right)\\npiT\\u200b=softmax(Tzi\\u200b\\u200b)Здесь с помощью zi\\\\mathbf{z}_izi\\u200b обозначены логиты классов, предсказанные моделью-учеником. Формула для qiT\\\\mathbf{q}_i^TqiT\\u200b выглядит аналогично.\\nЗачем нужна температура? Давайте рассмотрим формулу дополнительного слагаемого функции потерь. Для простоты выкладок я ограничусь функцией потерь для единственного объекта под номером iii, а также опущу постоянный множитель λN\\\\frac{\\\\lambda}{N}Nλ\\u200b, который также не существенен для данного повествования.\\nLi=−∑j=1Kqijlog\\u2061pijL_i = - \\\\sum_{j=1}^K q_{ij} \\\\log p_{ij}\\nLi\\u200b=−j=1∑K\\u200bqij\\u200blogpij\\u200bВспомним, что коэффициенты qijq_{ij}qij\\u200b приходят из предобученной модели-учителя, а значит являются константными с точки зрения процесса оптимизации.\\nВ таком случае несложно видеть, что мы имеем дело с чем-то очень близким к стандартной кросс-энтропийной функции потерь, но таргет yijy_{ij}yij\\u200b — это уже не one-shot закодированные номера классов, а что-то более интересное. В таком случае компоненты предсказания ученика, которые отвечают классам, оценённым учителем, как наиболее вероятные, получат большие веса и сформируют каркас итоговой функции потерь.\\nВ то же время все прочие компоненты получат околонулевые коэффициенты и влияния на функцию потерь практически не окажут. В какой-то степени эффект от этого может быть позитивным. Действительно, так как для преобразования предсказания нейронной сети в распределение вероятностей мы используем softmax\\\\text{softmax}softmax, итоговая модель не может предсказать строго нулевую вероятность. Поэтому типичное предсказание обученной сети содержит в себе множество практически нулевых значений.\\nПри этом порядок между данными значениями определяется в первую очередь не похожестью объекта на представителей данных классов, а конкретным исходом стохастического процесса обучения данной модели. В таком случае нам вовсе не хотелось бы вынуждать ученика воспроизводить данный порядок, если ценой тому будет ухудшение точности предсказания истинного класса.\\nС другой стороны, нейронные сети являются зачастую излишне уверенными в себе классификаторами: их предсказание часто содержит ярко выраженный максимум, вероятность которого близка к единице даже в тех случаях, когда модели стоило бы усомниться. К сожалению, для нас это значит, что при дистилляции знаний из такой модели мы рискуем попасть в ситуацию, что итоговые веса qijq_{ij}qij\\u200b настолько малы для всех классов, кроме истинного, что наше дополнительное слагаемое по сути повторяет стандартную кросс энтропию и не способно внести хоть сколь-нибудь заметный вклад в обучение модели-ученика.\\nЭтот эффект можно нивелировать путем сглаживания предсказания учителя таким образом, чтобы сделать распределение qijq_{ij}qij\\u200b ближе к равномерному, для чего, собственно и используется температура.\\nВ таком случае функция потерь задается следующим образом:\\nLKD=−1N∑i=1N(∑j=1Kyijlog\\u2061pij+λ∑j=1KqijTlog\\u2061pijT)L_{KD} = - \\\\frac1N \\\\sum_{i=1}^N \\\\left( \\\\sum_{j=1}^K y_{ij} \\\\log p_{ij} + \\\\lambda \\\\sum_{j=1}^K q_{ij}^T \\\\log p_{ij}^T \\\\right)\\nLKD\\u200b=−N1\\u200bi=1∑N\\u200b(j=1∑K\\u200byij\\u200blogpij\\u200b+λj=1∑K\\u200bqijT\\u200blogpijT\\u200b)Но в данную формулу незаметно закралась одна неприятная деталь. Давайте рассмотрим градиент второго слагаемого в скобках. Как и в прошлый раз, для простоты выкладок я ограничусь случаем единственного объекта под номером iii и опущу константный множитель λN\\\\frac{\\\\lambda}{N}Nλ\\u200b:\\nLi=−∑j=1KqijTlog\\u2061pijTL_i = - \\\\sum_{j=1}^K q_{ij}^T \\\\log p_{ij}^T\\nLi\\u200b=−j=1∑K\\u200bqijT\\u200blogpijT\\u200bЗдесь легко узнаётся формула кросс-энтропийной функции потерь, градиент которой по логитам считается следующим образом:\\n∂Li∂zik=1T(pikT−qikT)\\\\frac{\\\\partial L_i}{\\\\partial z_{ik}} = \\\\frac{1}{T} (p_{ik}^T - q_{ik}^T)\\n∂zik\\u200b∂Li\\u200b\\u200b=T1\\u200b(pikT\\u200b−qikT\\u200b)Доказательство формулы для градиента кросс-энтропийной функции потерь.Так как мы будем искать частную производную функции потерь по логитам, давайте сначала выразим через них саму функцию потерь:\\nLi=−∑j=1KqijTlog\\u2061pijT=L_i = - \\\\sum_{j=1}^K q_{ij}^T \\\\log p_{ij}^T =\\nLi\\u200b=−j=1∑K\\u200bqijT\\u200blogpijT\\u200b==−∑j=1KqijTlog\\u2061exp\\u2061(zij/T)∑l=1Kexp\\u2061(zil/T)== - \\\\sum_{j=1}^K q_{ij}^T \\\\log \\\\frac{\\\\exp(z_{ij}/T)}{\\\\sum\\\\limits_{l=1}^K \\\\exp(z_{il}/T)} =\\n=−j=1∑K\\u200bqijT\\u200blogl=1∑K\\u200bexp(zil\\u200b/T)exp(zij\\u200b/T)\\u200b==∑j=1K(qijTlog\\u2061∑l=1Kexp\\u2061(zil/T)−qijTlog\\u2061exp\\u2061(zij/T))== \\\\sum_{j=1}^K \\\\left(q_{ij}^T \\\\log \\\\sum_{l=1}^K \\\\exp(z_{il}/T) - q_{ij}^T \\\\log \\\\exp(z_{ij}/T)\\\\right) =\\n=j=1∑K\\u200b(qijT\\u200blogl=1∑K\\u200bexp(zil\\u200b/T)−qijT\\u200blogexp(zij\\u200b/T))==∑j=1K(qijTlog\\u2061∑l=1Kexp\\u2061(zil/T)−qijTzij/T)= \\\\sum_{j=1}^K \\\\left(q_{ij}^T \\\\log \\\\sum_{l=1}^K \\\\exp(z_{il}/T) - q_{ij}^T z_{ij} / T\\\\right)\\n=j=1∑K\\u200b(qijT\\u200blogl=1∑K\\u200bexp(zil\\u200b/T)−qijT\\u200bzij\\u200b/T)Теперь мы готовы брать производную:\\n∂Li∂zik=∑j=1K(qijT1∑l=1Kexp\\u2061(zil/T)1Texp\\u2061(zik/T)−1TqikT)=\\\\frac{\\\\partial L_i}{\\\\partial z_{ik}} = \\\\sum_{j=1}^K \\\\left( q_{ij}^T \\\\frac{1}{\\\\sum\\\\limits_{l=1}^K \\\\exp(z_{il}/T)} \\\\frac{1}{T} \\\\exp(z_{ik}/T) - \\\\frac{1}{T} q_{ik}^T \\\\right) =\\n∂zik\\u200b∂Li\\u200b\\u200b=j=1∑K\\u200b\\u200bqijT\\u200bl=1∑K\\u200bexp(zil\\u200b/T)1\\u200bT1\\u200bexp(zik\\u200b/T)−T1\\u200bqikT\\u200b\\u200b==1T(pik∑j=1KqijT−qik)= \\\\frac{1}{T} \\\\left(p_{ik} \\\\sum_{j=1}^K q_{ij}^T - q_{ik}\\\\right)\\n=T1\\u200b(pik\\u200bj=1∑K\\u200bqijT\\u200b−qik\\u200b)Поскольку qijTq_{ij}^TqijT\\u200b для каждого фиксированного iii является вектором вероятностей, то ∑j=1KqijT=1\\\\sum_{j=1}^K q_{ij}^T = 1∑j=1K\\u200bqijT\\u200b=1, откуда мы и получаем искомую формулу.\\nМожно видеть, что при изменении температуры TTT баланс между слагаемыми функции потерь (качеством решения задачи и качеством повторения предсказания учителя) нарушается. Действительно, если раньше мы настраивали его путём выбора подходящего коэффициента λ\\\\lambdaλ, то теперь мы приходим к тому, что при изменении температуры коэффициент λ\\\\lambdaλ необходимо также менять: иначе при взятии градиента одно слагаемое функции потерь будет разделено на TTT, а другое останется неизменным.\\nРазумным кажется ввести множитель TTT в формулу для LKD\\\\mathcal{L}_{KD}LKD\\u200b явным образом. Однако прежде, чем мы сделаем это, давайте ещё раз внимательно посмотрим на получившийся градиент:\\n∂Li∂zik=1T(pikT−qikT)=1T(exp\\u2061(zik/T)∑l=1Kexp\\u2061(zil/T)−exp\\u2061(vik/T)∑l=1Kexp\\u2061(vil/T)),\\\\frac{\\\\partial L_i}{\\\\partial z_{ik}} = \\\\frac{1}{T} (p_{ik}^T - q_{ik}^T) = \\\\frac{1}{T} \\\\left(\\\\frac{\\\\exp(z_{ik}/T)}{\\\\sum_{l=1}^K \\\\exp(z_{il}/T)} - \\\\frac{\\\\exp(v_{ik}/T)}{\\\\sum_{l=1}^K \\\\exp(v_{il}/T)}\\\\right),\\n∂zik\\u200b∂Li\\u200b\\u200b=T1\\u200b(pikT\\u200b−qikT\\u200b)=T1\\u200b(∑l=1K\\u200bexp(zil\\u200b/T)exp(zik\\u200b/T)\\u200b−∑l=1K\\u200bexp(vil\\u200b/T)exp(vik\\u200b/T)\\u200b),где через vijv_{ij}vij\\u200b обозначены логиты, предсказанные моделью-учителем.\\nДавайте теперь устремим TTT к бесконечности. Раскладывая экспоненты в ряд Тейлора до первого слагаемого, получаем:\\n∂Li∂zik≈1T(1+zik/TN+∑l=1Kzil/T−1+vik/TN+∑l=1Kvil/T)\\\\frac{\\\\partial L_i}{\\\\partial z_{ik}} \\\\approx \\\\frac{1}{T} \\\\left(\\\\frac{1 + z_{ik}/T}{N + \\\\sum_{l=1}^K z_{il}/T} - \\\\frac{1 + v_{ik}/T}{N + \\\\sum_{l=1}^K v_{il}/T}\\\\right)\\n∂zik\\u200b∂Li\\u200b\\u200b≈T1\\u200b(N+∑l=1K\\u200bzil\\u200b/T1+zik\\u200b/T\\u200b−N+∑l=1K\\u200bvil\\u200b/T1+vik\\u200b/T\\u200b)Вспомним теперь, что результат применения преобразования softmax\\\\text{softmax}softmax не зависит от сдвига на константу, поэтому на выходе из нейронной сети мы можем вычитать из логитов среднее значение таким образом, чтобы ∑l=1Kzil=∑l=1Kvil=0\\\\sum_{l=1}^K z_{il} = \\\\sum_{l=1}^K v_{il} = 0∑l=1K\\u200bzil\\u200b=∑l=1K\\u200bvil\\u200b=0. В таком случае, предыдущая формула упрощается до:\\n∂Li∂zik≈1NT2(zik−vik)\\\\frac{\\\\partial L_i}{\\\\partial z_{ik}} \\\\approx \\\\frac{1}{NT^2} (z_{ik} - v_{ik})\\n∂zik\\u200b∂Li\\u200b\\u200b≈NT21\\u200b(zik\\u200b−vik\\u200b)Из этой формулы следует два вывода.\\n\\nВо-первых, можно видеть, что для соблюдения баланса второе слагаемое в LKD\\\\mathcal{L}_{KD}LKD\\u200b правильнее будет домножить не на TTT, а на T2T^2T2.\\nВо-вторых, в данной формуле можно узнать градиент квадратичной функции потерь между векторами логитов.\\n\\nТо есть при стремлении температуры TTT к бесконечности градиент второго слагаемого в LKD\\\\mathcal{L}_{KD}LKD\\u200b стремится к градиенту квадрата нормы разности между логитами модели-ученика и модели-учителя.\\nТаким образом, мы приходим к финальной версии функции потерь:\\nLKD=−1N∑i=1N(∑j=1Kyijlog\\u2061pij+λT2∑j=1KqijTlog\\u2061pijT)L_{KD} = - \\\\frac1N \\\\sum_{i=1}^N \\\\left( \\\\sum_{j=1}^K y_{ij} \\\\log p_{ij} + \\\\lambda T^2 \\\\sum_{j=1}^K q_{ij}^T \\\\log p_{ij}^T \\\\right)\\nLKD\\u200b=−N1\\u200bi=1∑N\\u200b(j=1∑K\\u200byij\\u200blogpij\\u200b+λT2j=1∑K\\u200bqijT\\u200blogpijT\\u200b)Описанная выше статья произвела настоящий фурор в 2014 году. Дистилляция знаний путем минимизации KL-дивергенции между предсказаниями ученика и учителя хорошо зарекомендовала себя на практике и породила целый ряд исследований, направленных на использование и усовершенствование предложенного подхода.\\nВместе с методом прижилось и понятие теневых знаний, и его довольно часто можно встретить в статьях, посвящённых данной тематике. Кроме того, зародилась традиция изучения дистилляции знаний на примере задачи классификации картинок.\\nДальше по ходу параграфа мы ещё не раз столкнёмся с тем, что авторы различных методов часто прилагают результаты экспериментов на таких датасетах, как CIFAR-10, CIFAR-100, ImageNet и так далее.\\nТем не менее, сети для работы с данными других модальностей тоже дистиллируют, и начнем мы с разбора статьи, которая использует предложенный метод для решения задачи языкового моделирования (language modelling).\\nDistilBERT как пример хинтоновской дистилляции\\nОдним из наиболее выдающихся примеров применения Хинтоновской дистилляции можно считать модель DistilBERT, которая сохраняет 97% качества модели BERT (согласно бенчмарку GLUE), используя при этом на 40% меньше параметров и требуя на 60% меньше времени при применении. При этом столь выдающийся результат авторы получают, используя хинтоновский подход практически без изменений.\\nПо аналогии с тем, как это делалось для модели-учителя (в роли которого выступает BERT), авторы обучают свою модель решать задачу маскированного языкового моделирования. В дополнение к хинтоновской функции потерь использовалось ещё косинусное расстояние между итоговыми векторными представлениями токенов, полученными с помощью ученика и учителя, разворачивая представлений ученика в сторону направлений, задаваемых представлениями модели-учителя.\\nЕщё одна интересная деталь в этой статье — способ инициализации модели-ученика. Действительно, в качестве архитектуры для своей сети, авторы решили переиспользовать архитектуру самого BERT, но с уменьшенным вдвое числом слоёв для ускорения.\\nАвторы замечают, что большинство операций, которые используются в трансформерах, уже достаточно хорошо оптимизированы во всех популярных библиотеках, поэтому изменение размера внутренних представлений оказывает существенно меньшее влияние на итоговое время применения сети, нежели изменение количества слоёв. Поэтому в статья фокусировалась на сжатии модели именно в глубину, оставляя ширину неизменной.\\nПоскольку веса слоёв модели-ученика имеют при таком подходе такие же размерности, что и веса слоёв модели-учителя, последние можно использовать при инициализации. Ровно так авторы и поступают, копируя веса каждого второго слоя исходной модели для инициализации DistilBERT.\\nМожет показаться, что умная инициализация не критична и наихудшим следствием использования более примитивной стратегии будет всего лишь увеличение времени, требуемого для обучения модели-ученика. Но авторы провели ablation study и выяснили, что обучение без умной инициализации приводит к потере почти 4.84.84.8 процентных пункта итоговой метрики (обученная без неё модель сохраняет лишь 92.2%92.2\\\\%92.2% качества модели-учителя).\\nДля сравнения, исключение из функции потерь кросс-энтропии между предсказанием ученика и истинной разметки приводит к потере лишь 0.40.40.4 процентных пункта итоговой метрики, а исключение KL-дивергенции приводит к потере 3.83.83.8 процентных пункта.\\nИнтересно, что двумя годами позднее вышла независимая статья, авторы которой показали, что хинтоновская дистилляция — это очень сложная оптимизационная задача со множеством локальных минимумов, которые сильно усложняют поиск глобального.\\nПоскольку статья была написана независимо другими авторами, конкретный пример DistilBERT там не изучается, однако в целом авторы приходят к выводу, что умная инициализация может быть ключевым элементом для успеха дистилляции знаний.\\nДополнительные источники знаний для дистилляции\\nНесмотря на широкий успех хинтоновского подхода, дистилляция знаний им не ограничивается.\\nОдно из наиболее очевидных направлений улучшения предложенного метода — это использование дополнительных способов передачи знаний от модели-учителя к модели-ученику. Действительно, в хинтоновской постановке единственный канал передачи знаний — это выходы с последнего слоя модели-учителя.\\nОднако в случае нейронных сетей это отнюдь не единственный доступный нам источник информации. Например, можно использовать веса модели-учителя для умной инициализации, как при обучении DistilBERT. К сожалению, поскольку дистилляция знаний практически всегда сопряжена со сжатием модели, не всегда получается непосредственно использовать веса учителя, и в каждом отдельном случае приходится изобретать специализированные трюки.\\nПо этой причине DistilBERT — это единственная известная автору этого параграфа модель, в которой удалось добиться улучшения результатов благодаря использованию весов модели-учителя для умной инициализации.\\nТем не менее, в нейронных сетях можно найти и другие источники информации. Хинтоновская дистилляция использует только выходы с последнего слоя сети. Почему бы нам дополнительно не использовать выходы промежуточных слоев? И действительно, исследования показывают, что использование выходов промежуточных слоев позволяет улучшить результаты дистилляции знаний.\\nДля получения прироста качества авторы предлагают выбрать один или несколько промежуточных слоев модели-учителя, сопоставить каждому из них промежуточный слой модели-ученика, после чего использовать квадрат нормы разности выходов итоговых пар слоев в качестве дополнительного слагаемого к хинтоновской функции потерь.\\nК сожалению, несмотря на кажущуюся прямолинейность данного подхода, здесь возникают две сложности.\\nСложность №1\\nМы явным образом предполагаем наличие заранее выбранных пар слоёв, оставляя за бортом вопрос о том, каким образом их собственно стоит выбирать. Поскольку дополнительные слагаемые функции потерь по сути обучают модель-ученика повторять промежуточные представления модели-учителя, разумным кажется сохранять порядок слоёв: слои из середины модели-учителя сопоставлять со слоями из середины модели-ученика, а слои, находящиеся ближе к концу модели-учителя, — со слоями, находящимися ближе к концу модели-ученика.\\nВ частности, авторы оригинальной статьи просто берут средний слой в каждой из моделей и используют их в качестве своей единственной пары, однако это в большей степени связано с тем, что статья была написана в 2014 году и рассматривала достаточно маленькие по современным меркам модели. Более свежие статьи, как правило, работают с более глубокими сетями, а потому используют большее количество пар слоёв.\\nТак, авторы следующей работы рассматривают глубокие сверточные сети с промежуточными связями (residual connections) и предлагают разбивать каждую из моделей на группы блоков с промежуточной связью таким образом, чтобы итоговое количество групп совпало. Пример такой разбивки можно видеть на картинке ниже.\\nЗдесь к каждой группе относится по три блока в модели-учителе и по два блока в модели-ученике. После этого выходы каждой такой группы можно сопоставить друг другу и использовать для дистилляции знаний.\\n\\n\\n\\nИсточник\\n\\n\\nПосле того, как пары слоёв были выбраны, перед нами может возникнуть и второе препятствие.\\nСложность №2\\nЧто, если выходы выбранных слоёв различаются по размерам? Такая ситуация запросто может случиться, ведь мы хотим, чтобы модель-ученик была поменьше, а один из способов сжатия — как раз уменьшение количества нейронов в полносвязных слоях.\\nВ таком случае авторы оригинальной статьи предлагают использовать дополнительное преобразование, чтобы придать выходам модели-ученика нужные размеры (например, линейный слой).\\nТакие слои обучаются совместно с моделью-учеником, а после исключаются из сети при применении. В более поздних работах встречаются и другие, более продвинутые преобразования.\\nНесмотря на кажущуюся интуитивность дистилляции промежуточных выходов, практическое применение это метода, к сожалению, осложняется необходимостью выбора целого ряда гиперпараметров. Скажем, оптимальные тактики выбора пар слоёв для дистилляции или дополнительных преобразований для выравнивания размерностей выходов до сих пор являются предметами активных исследований, точно так же, как и функции потерь для оптимизации.\\nИерархия методов дистилляции знаний\\nВыше мы рассмотрели два подхода к дистилляции знаний: хинтоновскую дистилляцию и дистилляцию промежуточных представлений. Как мы уже упоминали ранее, в последние годы область применения дистилляции знаний сильно разрослась, и новые методы появляются день ото дня.\\nЭто породило довольно естественное желание систематизировать предложенные методы в некоторую иерархию. Мы рассмотрим две классификации методов:\\n\\nпо режиму дистилляции,\\nпо области применения.\\n\\nРежимы дистилляции знаний\\nРазличные подходы к дистилляции знаний принято делить по так называемым режимам. Выделяют три основных режима дистилляции знаний: offline-, online- и самодистилляция.\\nOffline-дистилляция знаний\\nВсе рассмотренные выше статьи так или иначе следуют некоторой общей схеме: в качестве учителя используется большая заранее обученная модель, знания из которой дистиллируются в ученика, в то время как сам учитель остается неизменным. Дистилляция в таком режиме получила название offline-дистилляции знаний.\\nНо что делать, если большой предобученный учитель для вашей задачи не доступен? Что если модель-учитель не помещается на доступную нам видеокарту, из-за чего обучение или вовсе невозможно, или требует в десятки раз больше времени, по сравнению с обучением желаемой модели-ученика? Что, если набор данных, описывающий вашу задачу, невелик, и большая модель может переобучиться на нём, делая дистилляцию знаний как минимум неэффективной, а возможно и вредной для итогового качества ученика?\\nТут на помощь приходит online-дистилляция знаний.\\nOnline дистилляция знаний\\nВ качестве альтернативы авторы этой статьи предлагают брать в качестве учителя модель такой же архитектуры, что и ученик, и обучать обе модели одновременно.\\nТо есть вместо одной модели мы случайно инициализируем две, после чего на каждом шаге обучения для каждой из моделей мы минимизируем LKD\\\\mathcal{L}_{KD}LKD\\u200b, где в качестве учителя выступает другая модель.\\nВ таком случае в начале обучения градиент дистилляционного члена не будет иметь какого-то чёткого направления, а обучение обеих моделей будет происходить преимущественно за счет минимизации обычной функции потерь. На поздних же этапах обучения в дело включится и KL-дивергенция, что позволит дополнительно повысить качество каждой из моделей.\\n\\n\\n\\nИсточник\\n\\n\\nПочему данный подход работает? Широко известно, что в ряде задач ансамблирование нескольких одинаковых нейронных сетей, одинаково обученных на одних и тех же данных, но из разных случайных инициализаций, дает прирост в итоговой метрике.\\nЭтот факт подталкивает нас к выводу о том, что в зависимости от инициализации одна и та же нейронная сеть вычленяет из данных разные закономерности. Опираясь на данный вывод, авторы вышеупомянутой статьи утверждают, что в предложенной постановке каждая из моделей в процессе обучения может воспользоваться информацией, которая иначе была бы доступна только модели, стартовавшей из другой инициализации.\\nАвторы проводят ряд экспериментов с моделями разных размеров, обучая их на датасетах CIFAR-100 и Market-1501, и показывают, что использование даже одной дополнительной модели позволяет добиться заметного улучшения в качестве предсказаний обучаемой модели.\\nТак на датасете CIFAR-100 совместное обучение ансамбля из двух моделей практически во всех экспериментах дает прирост в 1−−21--21−−2 процентных пункта к итоговой точности предсказания, причем метод позволяет достигнуть положительного эффекта даже для самой большой из рассмотренных моделей при её совместном обучении с самой малой моделью. Кроме того, авторы проводят ряд экспериментов, в которых сравнивают offline-дистилляцию большей модели в меньшую с их совместным обучением и показывают, что предложенный метод позволяет добиться лучших результатов.\\nOnline-постановка естественным образом обобщается на случай большего числа моделей в обучаемом ансамбле. В таком случае в качестве дистилляционного слагаемого авторы предлагают минимизировать среднее значение KL-дивергенций от текущей модели до предсказаний каждой из других моделей в ансамбле, поскольку минимизация KL-дивергенции до усредненных вероятностей приводит к худшему результату.\\nПри этом авторы в своих экспериментах показывают, что увеличение числа моделей в ансамбле приводит к улучшению результатов обучения. Кроме того авторы отмечают, что для ускорения обучения можно достаточно эффективно использовать несколько видеокарт, поскольку на каждом шаге между устройствами передавать необходимо только результаты предсказания.\\nСамодистилляция\\nВ качестве отдельного режима дистилляции знаний принято выделять также самодистилляцию (self distillation), при которой учитель и ученик являются одной и той же моделью. Самодистилляция включает в себя две основные группы методов.\\nПервая группа методов направлена на использование информации, которая накапливается в модели во время обучения, для дополнительного улучшения качества предсказаний той же самой модели. Методы данной группы являются как бы продолжением идей online дистилляции знаний, поскольку учитель и ученик обучаются одновременно.\\nХороший пример метода из данной группы можно найти в этой статье, где авторы пытаются заставить представления менее глубоких слоёв быть эквивалентными представлениям более глубоких слоёв. А именно, авторы предлагают разделить сеть на несколько частей (444 в статье) и после каждой такой части добавить небольшую предсказательную голову. Все такие головы обучаются путем минимизации суммы трёх слагаемых:\\n\\nкросс-энтропии с истинной разметкой;\\nKL-дивергенции с предсказаниями полной сети;\\nквадратичной функции потерь между промежуточными представлениями данной головы и выходом последней части сети.\\n\\n\\n\\n\\nИсточник\\n\\n\\nТаким образом авторы добиваются от ResNet50 81.04%81.04\\\\%81.04% точности предсказания на тестовой выборке CIFAR-100 с минимальным замедлением обучения.\\nДля сравнения, стандартное обучение такой же сети позволяет добиться лишь 77.68%77.68\\\\%77.68% точности предсказания, а дистилляция из ResNet152 (которая, в свою очередь, показывает точность предсказания в 79.21%79.21\\\\%79.21%) позволяет улучшить данный показатель лишь до 79.31%79.31\\\\%79.31%.\\nПри этом обучение в предложенном режиме занимает 5.875.875.87 часов (обычное обучение занимает 4.034.034.03 часа), а дистилляция из ResNet152 занимает уже 12.3112.3112.31 часов без учета обучения модели учителя (что требует дополнительных 14.6714.6714.67 часов).\\nВторая группа методов по сути заключается в offline дистилляции из обученной модели в новую модель такой же архитектуры. То есть мы выбираем некоторую архитектуру нейронной сети, обучаем одну модель стандартным образом, а затем обучаем точно такую же модель из новой случайной инициализации с использованием хинтоновской дистилляции из ранее обученной модели.\\nСтоит заметить, что с хинтоновской точки зрения данное действие едва ли способно улучшить итоговое качество модели. Действительно, будучи точно такой же моделью, ученик обладает идентичной способностью к обучению, а значит учитель едва ли может предоставить ему какую-либо дополнительную информацию во время обучения.\\nПоэтому такая самодистилляция изначально была предложена как метод изучения процесса хинтоновской дистилляции знаний, поскольку в такой постановке у задачи минимизации KL-дивергенции гарантированно есть глобальный минимум, причем мы даже знаем точку, в которой он достигается. В частности именно с помощью данного метода авторы ранее упомянутой статьи демонстрируют, что хинтоновская дистилляция знаний является сложной оптимизационной задачей.\\nТем удивительнее, что авторы статьи 2018 года обнаружили, что самодистилляция в предложенной выше постановке позволяет получить прирост в обобщающей способности итоговой модели. Так, они проводят ряд экспериментов с моделями DenseNet и Wide-ResNet на датасете CIFAR-100 и показывают, например, что самодистилляция DenseNet-112-33 позволяет повысить точность предсказания на тестовой выборке с 81.75%81.75\\\\%81.75% до 83.05%83.05\\\\%83.05%.\\nВопрос об источнике прироста качества в данном случае до сих пор в значительной степени открыт. Авторы статьи приписывают данный эффект комбинации умного сглаживания разметки и внесения в обучение информации о взаимоотношении классов в датасете. Но на наш взгляд эксперименты, которые предъявляют в статье в качестве доказательства этих гипотез, едва ли можно назвать убедительными.\\nВозможно, здесь в очередной раз проявляется то, что одинаковые модели могут вычленять из одних и тех же данных разные закономерности в зависимости от случайной инициализации, и дистилляция одной такой модели в другую позволяет ученику увидеть ранее недоступные ему связи.\\nТакже хочется обратить внимание на интересную статью, вышедшую в 2020 году. В ней показывается, что в случае обучения с L2-регуляризацией предложенная выше самодистилляция производит неявный отбор признаков.\\nНу и раз мы проходили мимо самодистилляции, здесь никак нельзя не упомянуть статью 2019 года, которая в течении практически года держала почетный статус SOTA на датасете ImageNet.\\nЕё авторы предлагают подход, который во многом очень близок к описанному выше. Они обучают модель на исходном наборе данных, после чего используют её для разметки новых данных, взятых в данном случае из стороннего обширного набора данных JFT-300M (закрытый набор данных, который нередко упоминается в статьях от Google).\\nПосле этого авторы отбрасывают картинки, для которых модель дает неуверенные предсказания, чтобы избежать данных out-of-domain. Кроме того, они выравнивают размеры классов, чтобы избежать связанных с этим спецэффектов (согласно авторам статьи, модели меньшего размера показали себя более чувствительными к данной оптимизации).\\nТаким образом, авторы получают большое количество дополнительных шумно размеченных данных, на которых, совместно с основным набором, они обучают новую модель. Эту модель, в свою очередь, можно использовать для получения новой разметки для дополнительных данных, с помощью которых обучается следующая модель, и такие итерации можно продолжать произвольное количество раз.\\nВ качестве разметки авторы предлагают использовать мягкую разметку, задаваемую моделью-учителем, но и бинаризованная разметка показывает схожие результаты на данных in-domain. Ключевая деталь здесь — что предсказание на новых данных производится без аугментаций, в то время как ученик учится воспроизводить разметку уже с высоким уровнем аугментации данных, а также с применением других техник регуляризации, таких как dropout и stochastic depth.\\nАвторы утверждают, что ученик обучается лучше переносить свои знания на новые данные. Предложенный метод позволил авторам добиться от модели EfficientNet-L2 точности предсказания в 88.4%88.4\\\\%88.4% на тестовом наборе данных ImageNet, существенно улучшив результат исходной модели в 85.5%85.5\\\\%85.5% и обновив мировой рекорд.\\nОбласти применения дистилляции знаний\\nСжатие генеративных состязательных сетей\\nПодавляющее большинство рассмотренных выше статей так или иначе ограничиваются задачей классификации картинок. Такой выбор, хоть и не является случайным, всё же несёт больше исторический, нежели практический характер.\\nНа самом деле, многие предложенные выше методы достаточно тривиально могут быть обобщены и на другие задачи машинного обучения. Например, метод дистилляции промежуточных представлений по сути вовсе никак не зависит от природы итоговых выходов модели, а потому может использоваться при сжатии практически любой модели.\\nВ частности данный метод может быть использован для сжатия генеративных состязательных сетей. Так авторы довольно популярной статьи в данной области демонстрируют 999-, 101010- и даже 292929-кратное ускорение для ряда популярных pix2pix генеративных сетей, при этом не теряя в качестве генерации.\\nКак уже упоминалось ранее, авторы используют дистилляцию промежуточных представлений: модель-ученик учится минимизировать L2-расстояние между своим промежуточным представлением и промежуточным представлением модели-учителя. Но, так как данные представления имеют различное количество каналов (ученик выучивает более сжатое представление) авторы используют дополнительную свертку 1х1 над представлением ученика для сопоставления тензоров друг с другом.\\nПомимо дистилляции промежуточных представлений, авторы также пользуются наличием модели-учителя для того, чтобы получить парную картинку в случае обучения на неспаренных данных (как это происходит, например, в CycleGAN).\\nПарная картинка используется для минимизации L1 нормы разности с предсказанием модели. Кроме того, авторы во время обучения минимизируют и стандартную для генеративных состязательных сетей функцию потерь, при этом для модели-ученика используется такой же дискриминатор, что и для модели-учителя, что позволяет авторам инициализировать веса с помощью весов оригинального дискриминатора.\\nТаким образом авторы предлагают следующий рецепт для сжатия генеративных состязательных сетей. Сначала необходимо обучить модель-учителя. После этого нужно сконструировать сжатый генератор-ученика, скопировать (вместе с весами) дискриминатор и обучить получившуюся систему с помощью минимизации взвешенной суммы трёх функций потерь:\\n\\nСтандартной функции потерь генеративных состязательных сетей.\\nL1-расстояния между предсказанием генератора и парной картинки. При этом если в данных парной картинки нет, вместо неё используется результат генерации моделью-учителем.\\nL2-расстояния между промежуточными представлениями двух генераторов.\\n\\nЭтот метод хорошо себя зарекомендовал на практике и получил широкое распространение в своей нише.\\nДистилляция знаний при квантизации моделей\\nЕщё одно интересное применение дистилляции знаний — улучшение результатов квантизации моделей. Техника квантизации нейронных сетей заключается в том, чтобы перевести часть весов или даже всю сеть из полной точности (как правило, float32) во float8 или даже float4.\\nПомимо очевидной экономии памяти, такое представление нередко позволяет использовать специальные ядра современных графических ускорителей или специальные регистры процессоров для достижения существенного ускорения при применении квантизованных моделей.\\nК сожалению, бесплатный сыр бывает только в мышеловке. Сжатое представление на то и называется сжатым, что является менее богатым, нежели полная точность. Поэтому большинство весов сети приходится изменять при сжатии, чтобы они попали на более грубую сетку. Разумеется изменения каждого отдельного веса может показаться незначительным, однако когда все веса сети незначительно изменяются, итоговый результат подсчетов может оказаться вовсе неузнаваемым.\\nЧтобы смягчить данный эффект, модель принято доучивать после квантизации. И вот здесь на помощь приходит дистилляция знаний: например, из сети полной точности в квантизованного ученика. Ровно к такой схеме приходят авторы этой статьи.\\nИтоговая схема выглядит следующим образом: мы обучаем сеть в полной точности, квантизуем ее веса и доучиваем ее в квантизованном виде с использованием дистилляции знаний из сети в полной точности.\\nДистилляция знаний за пределами сжатия моделей\\nХочется отметить, что сжатие моделей — это хоть и основное, но всё же не единственное применение дистилляции знаний. Так, раньше в этом параграфе уже упоминалась самодистилляция, которая позволяет получать прирост в обобщающей способности обучаемой модели без использования моделей большего размера.\\nСамодистилляцией, однако, примеры применения дистилляции знаний без сжатия модели не ограничиваются. Так, в 2020 году был предложен метод BYOL-предобучения без учителя, основанный на дистилляции знаний.\\nМетод BYOL направлен на предобучение моделей компьютерного зрения и основан на идее так называемого контрастивного предобучения (contrastive pretraining). Суть методов данного семейства заключается в том, чтобы обучать модель выдавать схожие представления для различных аугментаций одной и той же картинки.\\nДействительно, случайные патчи, вырезанные из фотографии автомобиля, скорее всего также являются фотографиями автомобиля. При этом, если в наших данных присутствует достаточное количество фотографий различных автомобилей, мы можем надеяться на то, что модель выучит некоторое общее понимание концепта автомобиля даже несмотря на то, что мы можем вовсе не знать, на каких конкретно картинках автомобили присутствовали, а на каких - нет.\\nОднако если мы хотим добиться от такой модели осмысленных представлений сначала нам необходимо преодолеть проблему коллапса представлений.\\nДействительно, у предложенной выше задачи есть тривиальное решение, в котором выход обучаемой сети не зависит от ее входа. В таком случае представления для произвольных аугментаций любой картинки будут совпадать, то есть функция потерь окажется нулевой. Тем не менее сами представления при этом окажутся совершенно бесполезными.\\nПоэтому различные методы контрастивного обучения отличаются в первую очередь как раз способами борьбы с проблемой коллапса представлений. Так, авторы метода BYOL часто сравнивают свои результаты с довольно свежим на момент написания статьи методом SimCLR, в котором предлагается обучать модель одновременно минимизируя расстояния между парами представлений для различных аугментаций одной картинки и максимизируя расстояния между представлениями для различных картинок.\\nПри этом, для повышения эффективности такого обучения, во время генерации батча данных авторы сначала выбирают некоторое количество картинок из набора данных, затем для каждой картинки производят две различные случайные аугментации, после чего полученные картинки используются для создания одной позитивной пары, расстояние между представлениями которой будет минимизироваться, а также для создания множества негативных пар с аугментациями других картинок в батче, расстояния между представлениями которых будут максимизироваться.\\nАвторы BYOL подвергают данный подход критике, показывая, что для эффективного обучения SimCLR требует большого размера батча, а также довольно агрессивных аугментаций. В противном же случае качество обучаемых представлений заметно падает. Авторы BYOL объясняют данный эффект тем, что сам подход использования негативных пар является субоптимальным, поскольку требует аккуратного выбора негативных примеров. Поэтому свой метод авторы конструируют таким образом, чтобы модель обучалась только на позитивных парах картинок.\\nВ таком случае каким образом авторам удается решить проблему коллапса представлений? Для этого, вместо минимизации расстояния между представлениями обучаемой сети для двух аугментаций одной картинки, авторы обучают свою модель минимизировать расстояние между представлением обучаемой (online) сети для одной аугментации и представлением для второй аугментации, которое задается уже другой, целевой (target) сетью. То есть в некотором смысле здесь происходит дистилляция знаний из целевой сети в обучаемую.\\nПоследней важной деталью является природа целевой сети. Авторы BYOL замечают, что даже использование произвольно инициализированной сети в качестве целевой для предложенного выше метода обучения приводит к выучиванию обучаемой моделью осмысленных представлений.\\nПодробнее, линейный классификатор, обученный на основе выученных таким образом представлений картинок из набора данных ImageNet показывает 18.8%18.8\\\\%18.8% тестовой точности предсказания, в то время как использование представлений задаваемых самой целевой сетью позволяет добиться лишь 1.4%1.4\\\\%1.4% точности. Мотивированные данным наблюдением, авторы предлагают в качестве целевой использовать такую же сеть, что и обучаемая.\\nПри этом:\\n\\nградиенты не текут через целевую модель, и она не обновляется на шаге градиентного спуска;\\nобучаемая модель заканчивается дополнительным двухслойным перцептроном, который используется для преобразования её финальных представлений в представления целевой модели;\\nвеса целевой модели не меняются на шаге градиентного спуска, а вместо этого они обновляются между шагами с помощью экспоненциального сглаживания весов обучаемой модели:\\n\\nξ↦τξ+(1−τ)θ,\\\\xi \\\\mapsto \\\\tau\\\\xi + (1 - \\\\tau)\\\\theta,\\nξ↦τξ+(1−τ)θ,где, следуя обозначениям из статьи, мы обозначили через ξ\\\\xiξ и τ\\\\tauτ веса целевой и обучаемой моделей соответственно, а τ\\\\tauτ — вещественный параметр.\\n\\n\\n\\nИсточник\\n\\n\\nПредложенный метод позволяет авторам добиться уже 79.6%79.6\\\\%79.6% тестовой точности от линейного классификатора на наборе данных ImageNet, заметно превосходя предложенные ранее методы self-supervised предобучения, и практически преодолевая разрыв между self-supervised и supervised обучением классификаторов на основе ResNet.\\nСтоить заметить, что сценарии применения дистилляции знаний отнюдь не ограничиваются выше рассмотренными. На данный момент уже существует множество различных подходов и алгоритмов, так или иначе связанных с дистилляцией знаний, и их количество растет день ото дня.\\nДанный параграф не ставит своей целью полный обзор таких методов. Вместо этого всем заинтересовавшимся я рекомендую обратить внимание на довольно исчерпывающий обзор от 2020 года. Здесь можно найти множество ссылок на актуальные к тому моменту статьи, в числе которых присутствует и большинство статей, упомянутых в этом параграфе.\\nОткрытые проблемы\\nЗавершим параграф упоминанием открытых проблем в области дистилляции знаний.\\nДействительно, несмотря на впечатляющие результаты, дистилляция знаний всё же не является идеальным методом, и ряд вопросов до сих пор остаются без ответа.\\nНапример, с ростом популярности дистилляции знаний выяснилось, что использование учителя с большей обобщающей способностью не всегда приводит к улучшению обобщающей способности ученика. В какой-то степени данный эффект можно списать на то, что компактная модель-ученик упирается в пределы своего качества предсказания, и тогда использование более умного учителя уже не приносит дополнительной пользы.\\nНо это не объясняет, почему использование более точной модели в качестве учителя может приводить даже к ухудшению итоговой точности модели-ученика. В чём причина данного эффекта и как выбрать оптимального учителя для фиксированного ученика, до сих пор открытый вопрос.\\nИ как уже упоминалось ранее, дистилляция знаний из одной сети в точно такую же нередко приводит к росту обобщающей способности ученика по сравнению со своим учителем. С точки зрения хинтоновской теории, которая является де-факто стандартным способом объяснения дистилляции знаний, это звучит абсурдно.\\nМодель-ученик гарантированно способна приблизить ту же функцию, что и модель-учитель. Тем не менее, этого не происходит, а модель-ученик выучивает свое собственное представление, которое нередко качественно превосходит представление учителя. Данный факт уже сложно объяснить в парадигме передачи знаний от учителя к ученику, потому что здесь ученик оказывается в состоянии получить больше знаний, нежели учитель способен передать. Несмотря на то, что на данную тему написана уже не одна статья, исчерпывающего объяснения пока нет.\\nТак или иначе, дистилляция знаний неоспоримо работает и является основным практическим подходом к сжатию нейросетевых моделей на данный момент.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф7.1. Обучение представленийСледующий параграф8.1. Введение в генеративное моделированиеЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_26.html', 'title': 'Трансформеры'}, page_content='ТрансформерыЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/56.1.Свёрточные нейросети6.2.Нейросети для работы с последовательностями6.3.ТрансформерыЗачем нам вниманиеСлой вниманияПолносвязный слой и нормализацияКодирование позицийПро BERT и GPTТонкости обученияТрансформеры не для текстов6.4.Графовые нейронные сети6.5.Нейросети для облаков точек7.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Трансформеры6.3. ТрансформерыАвторыМаксим РябининНаверное, ни один из рассказов про современные нейросети не обойдётся без упоминания трансформер-моделей: в самом деле, почти все нашумевшие достижения в глубинном обучении последних лет так или иначе опираются на эту архитектуру. Что же в ней такого особенного и почему трансформеры успешно применяются в самых разных задачах?\\nДавайте разбираться.\\nЗачем нам внимание\\nДля начала вспомним, что основным подходом для работы с последовательностями до 2017 года (выхода оригинальной статьи про архитектуру «трансформер») было использование рекуррентных нейронных сетей, или RNN. Однако у такого подхода есть несколько известных минусов:\\n\\nВо-первых, RNN содержат всю информацию о последовательности в скрытом состоянии, которое обновляется с каждым шагом. Если модели необходимо «вспомнить» что-то, что было сотни шагов назад, то эту информацию необходимо хранить внутри скрытого состояния и не заменять чем-то новым. Следовательно, придется иметь либо очень большое скрытое состояние, либо мириться с потерей информации.\\nВо-вторых, обучение рекуррентных сетей сложно распараллелить: чтобы получить скрытое состояние RNN-слоя для шага i+1i+1i+1, вам необходимо вычислить состояние для шага iii. Таким образом, обработка батча примеров длиной 100010001000 должна потребовать 100010001000 последовательных операций, что занимает много времени и не очень эффективно работает на GPU, созданных для параллельных вычислений.\\n\\nОбе этих проблемы затрудняют применение RNN к по-настоящему длинным последовательностям: даже если вы дождетесь конца обучения, ваша модель по своей конструкции будет так или иначе терять информацию о том, что было в начале текста. Хочется иметь способ «читать» последовательность так, чтобы в каждый момент времени можно было обратиться к произвольному моменту из прошлого за константное время и без потерь информации. Таким способом и является лежащий в основе трансформеров механизм self-attention, о котором далее пойдет речь. Как мы узнаем позже, благодаря своей универсальности и масштабируемости этот механизм оказался применим к множеству задач помимо обработки естественного языка.\\nНиже приведено устройство архитектуры «трансформер» из оригинальной статьи:Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\n\\nСлева на схеме представлено устройство энкодера. Он по очереди применяется к исходной последовательности из NNN блоков:\\n\\nКаждый блок выдаёт последовательность такой же длины. В нём есть два важных слоя, multi-head attention и feed-forward. После каждого из них к выходу прибавляется вход (это стандартный подход под названием residual connection) и затем активации проходят через слой layer normalization: на рисунке эта часть обозначена как “Add & Norm”.\\nУ декодера схема похожая, но внутри каждого из NNN блоков два слоя multi-head attention, в одном из которых используются выходы энкодера.\\nДавайте подробнее обсудим каждую из составляющих частей этого механизма.\\nСлой внимания\\nПервая часть transformer-блока — это слой self-attention. От обычного внимания его отличает то, что выходом являются новые представления для элементов той же последовательности, что мы подали на вход, причем каждый элемент этой последовательности напрямую взаимодействует с каждым.\\nЕсли говорить более подробно, то в вычислении внимания для последовательности будет участвовать три обучаемых матрицы WQ,\\xa0WK,\\xa0WVW_Q,\\\\ W_K,\\\\ W_VWQ\\u200b,\\xa0WK\\u200b,\\xa0WV\\u200b. Представление xix_ixi\\u200b каждого элемента входной последовательности мы умножаем на WQ,\\xa0WK,\\xa0WVW_Q,\\\\ W_K,\\\\ W_VWQ\\u200b,\\xa0WK\\u200b,\\xa0WV\\u200b, получая вектор-строки qi,ki,viq_i, k_i, v_iqi\\u200b,ki\\u200b,vi\\u200b (iii — номер элемента), которые соответственно называются запросами, ключами и значениями (query, key и value). Их роли можно условно описать следующим образом:\\n\\nqiq_iqi\\u200b — запрос к базе данных;\\nkik_iki\\u200b — ключи хранящихся в базе значений, по которым будет осуществляться поиск;\\nviv_ivi\\u200b — сами значения.\\n\\n\\nБлизость запроса к ключу можно определять, например, с помощью скалярного произведения:\\nself-attention\\xa0weightsi=softmax(qik1TC,qik2TC,…),\\\\text{self-attention weights}_i=\\\\text{softmax}\\\\left(\\\\frac{q_ik_1^T}{C},\\\\frac{q_ik_2^T}{C},\\\\ldots\\\\right),\\nself-attention\\xa0weightsi\\u200b=softmax(Cqi\\u200bk1T\\u200b\\u200b,Cqi\\u200bk2T\\u200b\\u200b,…),где CCC — некоторая нормировочная константа. Именно так и делали в исходной статье; в качестве нормировочной константы брался корень dk\\\\sqrt{d_k}dk\\u200b\\u200b из размерности ключей и значений. Теперь мы складываем значения viv_ivi\\u200b с полученными коэффициентами. Это и будет выходом слоя self-attention. В векторизованном виде можно записать:\\nself-attention(Q,K,V)=softmax(QKTdk)V,\\\\text{self-attention}(Q, K, V)=\\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V,\\nself-attention(Q,K,V)=softmax(dk\\u200b\\u200bQKT\\u200b)V,где QQQ, KKK, VVV — матрицы запросов, ключей и значений соответственно, в которых по строкам записаны qiq_iqi\\u200b, kik_iki\\u200b, viv_ivi\\u200b, а softmax\\\\text{softmax}softmax берётся построчно.\\nОсобенности слоя внимания в декодере\\nКак мы уже упоминали выше, в декодере один из attention-слоёв является слоем кросс-внимания (cross-attention), в котором запросы берутся из выходной последовательности, а ключи и значения — из входной (то есть из результатов работы энкодера).\\n\\nТакже стоит учитывать, что в описанном выше виде внимания каждый токен будет «смотреть» на всю последовательность, что нежелательно для декодера. Действительно, на этапе генерации мы будем порождать по одному токену за шаг, и доступ к последующим шагам на этапе обучения приведёт к утечке информации в декодере и низкому качеству модели. Чтобы избежать этой проблемы, при обучении к вниманию нужно применять авторегрессивную маску, вручную обращая в −∞-\\\\infty−∞ веса до softmax для токенов из будущего, чтобы после softmax их вероятности стали нулевыми. Как можно увидеть на рисунке внизу, эта маска имеет нижнетреугольный вид.\\n\\n\\n\\nИсточник\\n\\n\\nMulti-head attention\\nОдин набор QQQ, KKK и VVV может отражать только один вид зависимостей между токенами, и матрицы извлекают лишь ограниченный набор информации из входных представлений. Чтобы скомпенсировать эту неоптимальность, авторы архитектуры предложили подход с несколькими «головами» внимания (multi-head attention): по сути вместо одного слоя внимания мы применяем несколько параллельных с разными весами, а потом агрегируем результаты. Рисунок ниже показывает, как выглядит multi-head attention:\\n\\nЭффективность\\nПодход к обработке последовательностей целиком через внимание позволяет избавиться от такого понятия, как скрытое состояние, обновляющееся рекуррентно: каждый токен может напрямую «прочитать» любую часть последовательности, наиболее полезную для предсказания. В частности, отсутствие рекуррентности означает, что мы можем применять слой ко всей последовательности одновременно, так как матричные умножения прекрасно параллелятся.\\nОднако стоит помнить о затратах памяти и времени: поскольку каждый элемент последовательности взаимодействует с каждым, легко показать, что сложность self-attention составляет O(n2)O(n^2)O(n2) по длине последовательности, а простые реализации, формирующие полную матрицу внимания, будут расходовать ещё и O(n2)O(n^2)O(n2) памяти. С оптимизацией вычислительной сложности внимания связано множество работ как инженерного, так и архитектурного плана: в частности, есть подходы, которые позволяют сократить время работы self-attention до линейного или существенно уменьшают константы за счёт учёта иерархии памяти GPU.\\nНапример, на графиках ниже сравнивается время работы и потребление памяти трансформера со стандартным вниманием и с механизмом из статьи Longformer:\\n\\nПолносвязный слой и нормализация\\nВторая часть трансформерного блока называется feed-forward network (FFN) и представляет собой два обычных полносвязных слоя, применяемых независимо к каждому элементу входной последовательности. В последних архитектурах размер промежуточного представления (то есть выхода первого слоя) бывает весьма большим — в 4 раза больше выходов блока.\\nИз-за этого вычислительной стоимостью FFN не стоит пренебрегать: несмотря на квадратичную асимптотику внимания, в больших моделях или на коротких последовательностях FFN может занимать существенно больше времени по сравнению с self-attention. В виде формулы применение FFN можно представить так:\\nFFN(x)=act(xW1+b1)W2+b2,\\\\text{FFN}(x)=\\\\text{act}\\\\left(xW_1+b_1\\\\right)W_2+b_2,\\nFFN(x)=act(xW1\\u200b+b1\\u200b)W2\\u200b+b2\\u200b,Промежуточные активации act\\\\text{act}act в FFN бывают разными: начиналось всё с широко известной ReLU, но в какой-то момент сообщество перешло на GELU (Gaussian Error Linear Unit) с формулой xΦ(x)x\\\\Phi(x)xΦ(x), где Φ\\\\PhiΦ — функция распределения стандартной нормальной случайной величины.\\n\\n\\n\\n    Сравнение ReLU, ELU и GELU. Источник\\n\\n\\nСкажем ещё пару слов о layer normalization: как было показано в ряде работ, их положение внутри residual-ветки довольно важно. В стандартной архитектуре используется формулировка PostLN, где нормализация применяется после остаточной связи. Однако такое применение нормализации оказывается довольно нестабильным при обучении моделей с большим числом слоёв: вместо этого предлагается использовать PreLN (справа на рисунке снизу), где нормализация применяется ко входу residual-ветки.\\n\\nКодирование позиций\\nВнимательный читатель может заметить, что все операции внутри трансформер-блока, строго говоря, инвариантны к порядку элементов в последовательности. Например, результат внимания зависит от скалярных произведений между эмбеддингами токенов, но расположение этих токенов внутри текста значения не имеет. Таким образом, итоговые представления каждого токена на выходе из модели будут одинаковыми вне зависимости от порядка слов, что вряд ли нас устроит. Как с этим справиться?\\nНа помощь приходит такая вещь, как позиционные эмбеддинги. Это вспомогательные представления, которые прибавляются к обычным эмбеддингам токенов входной последовательности и позволяют слоям внимания различать одинаковые токены на разных местах.\\nИсторически первым подходом были фиксированные эмбеддинги, однозначно кодирующие позицию тригонометрическими функциями (ниже pospospos — номер позиции, iii — индекс элемента в векторе, кодирующем эту позицию, ddd — размерность эмбеддинга):\\nPE(pos,2i)=sin\\u2061(pos100002i/d),PE(pos,2i+1)=cos\\u2061(pos100002i/d).\\\\begin{align*}\\n\\\\textrm{PE}_{(pos,2i)}&=\\\\sin\\\\left(\\\\frac{pos}{10000^{2i/d}}\\\\right),\\\\\\\\\\n\\\\textrm{PE}_{(pos,2i+1)}&=\\\\cos\\\\left(\\\\frac{pos}{10000^{2i/d}}\\\\right).\\n\\\\end{align*}\\nPE(pos,2i)\\u200bPE(pos,2i+1)\\u200b\\u200b=sin(100002i/dpos\\u200b),=cos(100002i/dpos\\u200b).\\u200bС момента появления архитектуры «трансформер», однако, появилось множество других способов кодировать позиции токенов. Например, можно просто сделать позиционные эмбеддинги обучаемыми наряду с эмбеддингами токенов. Иной подход — напрямую учесть тот факт, что нам важны не абсолютные позиции токенов, а расстояние между ними, и обучать относительные позиционные представления: подобный подход заметно улучшает качество на чувствительных к порядку слов задачах, а его более современные модификации регулярно используются в самых мощных моделях.\\n\\n\\n\\n  Позиционное кодирование ALiBi: метод добавляет необучаемые константы к весам внимания в зависимости от расстояния между токенами ключа и значения. \\n    Источник\\n\\n\\nПро BERT и GPT\\nНесомненно, трансформер-модели не были бы так интересны, если бы практически все задачи NLP сейчас не решались бы с помощью этой архитектуры. Главными факторами, повлиявшими на бурный рост популярности идеи self-attention, послужили два семейства хорошо всем известных архитектур — BERT и GPT, которые в некотором роде являются энкодером и декодером трансформера, которые зажили своей жизнью.\\nМодель GPT (Generative Pretrained Transformer) хронологически появилась раньше. Она представляет собой обычную языковую модель, реализованную в виде последовательности слоев декодера трансформера.\\nВ качестве задачи при обучении выступает обычное предсказание следующего токена (то есть многоклассовая классификация по словарю). Важно, что в качестве маски внимания как раз выступает нижнетреугольная матрица: в противном случае возникла бы утечка в данных из-за того, что токены из «прошлого» будут видеть «будущее». Полученную модель можно использовать для генерации текстов и всех задач, которые на это опираются. Даже ChatGPT, обученная на специальных инструкциях, по своей сути незначительно отличается от базовой модели.\\n\\n\\n\\n    Иллюстрация задачи при обучении GPT. Источник\\n\\n\\nКак понятно из названия, модель Bidirectional Encoder Representations from Transformers (или BERT) отличается от GPT двунаправленностью внимания: это значит, что при обработке входной последовательности все токены могут использовать информацию друг о друге.\\nЭто делает такую архитектуру более удобной для задач, где нужно сделать предсказание относительно всего входа целиком без генерации, например, при классификации предложений или поиске пар похожих документов. Важно, что при этом BERT не учится генерировать тексты с нуля: одна из его задач при обучении — это masked language modeling (предсказание случайно замаскированных слов по оставшимся, изображено на рисунке ниже), а вторая — next sentence prediction (предсказание по паре текстовых фрагментов, следуют они друг за другом или нет).\\n\\n\\n\\n  Пример masked language modeling.\\n  \\n\\nЗаметим, что самое ключевое отличие в моделях BERT и GPT (а не в задачах для обучения или применениях) можно свести к использованию разных видов внимания, изображенных на рисунке снизу.\\n\\n\\n\\n  Отличия между вниманием в BERT и GPT. Источник\\n\\n\\nТонкости обучения\\nК сожалению, если вы просто напишете код Transformer-нейросети и попробуете сразу обучить что-то содержательное, используя привычные для других архитектур гиперпараметры, то вас с большой вероятностью постигнет неудача. Оптимизационный процесс для таких моделей зачастую требуется изменить, и недостаточное внимание к этому может повлечь за собой существенные потери в итоговом качестве или вообще привести к нестабильному обучению.\\nПервый момент, на который стоит обратить внимание, — размер батча для обучения. Практически все современные Transformer-модели обучаются на больших батчах, которые для самых больших языковых моделей могут достигать миллионов токенов. Разумеется, ни одна современная GPU не может обработать столько данных за один шаг: на помощь приходят распределенное обучение и чуть более универсальный трюк с аккумуляцией градиентов по микробатчам.\\nТакже в последних статьях зачастую прибегают к увеличению размера батча по ходу обучения: идея заключается в том, что на ранних этапах важнее быстрее совершить много шагов градиентного спуска, а на поздних становится важнее иметь точную оценку градиента.\\n\\n\\n\\n  Размер батча может играть большую роль даже для сравнительно маленьких моделей. Источник\\n\\n\\nВторой немаловажный фактор — выбор оптимизатора и расписания для learning rate. Обучить трансформер стандартным SGD, скорее всего, не выйдет: в оригинальной статье в качестве оптимизатора использовался Adam, и де-факто он остаётся стандартом до сих пор.\\nОднако стоит заметить, что для больших размеров батча Adam порой работает плохо: из-за этого порой приходится прибегать к алгоритмам наподобие LAMB, нормализующим обновления весов для каждого слоя.\\nТрансформеры не для текстов\\nРазумеется, успех этого семейства архитектур на множестве текстовых задач не мог остаться незамеченным для исследователей в других доменах. Одним из наиболее ярких примеров областей, в которой Transformer-модели нашли новое приложение, несомненно, является компьютерное зрение.\\nК примеру, архитектура ViT (Vision Transformer) в свое время побила рекорды качества по классификации изображений, задействуя идею self-attention для картинок, разделенных на множество «лоскутных» (patches) сегментов квадратной формы.\\nКак пишут авторы статьи, идея использовать Transformer-архитектуру в зрении пришла к ним после наблюдения за успехами таких моделей в NLP: использование такого общего подхода, как self-attention, позволяет избежать необходимости явно закладывать в архитектуру особенности задачи (это ещё называют inductive bias) при достаточном времени обучения, числе параметров и размере выборки.\\n\\nТакже именно на трансформерах базируется генеративная часть DALL-E — модели, положившей начало активным исследованиям последних лет в генерации изображений по тексту. Концептуально DALL-E довольно проста: её можно рассматривать как авторегрессивную «языковую модель», генерирующую изображение по одному «визуальному токену» за шаг.\\nПрименяют трансформеры и к обучению с подкреплением: ярким примером является работа Decision Transformer, в которой предлагают использовать авторегрессивное моделирование с использованием этой архитектуры для построения агента.\\nАвторы показали, что такой же подход, который используют для генерации текстов, можно использовать для предсказания действий в динамической среде: как показано на рисунке ниже, модель последовательно принимает стандартные тройки из закодированных состояний, текущих действий и наград и в качестве ответа на каждом шаге выдаёт следующее действие.\\n\\n\\n\\n  Архитектура Decision Transformer. Источник\\n\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиСообщить об ошибкеПредыдущий параграф6.2. Нейросети для работы с последовательностямиСледующий параграф6.4. Графовые нейронные сетиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_51.html', 'title': 'PAC-байесовские оценки риска'}, page_content='PAC-байесовские оценки рискаЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/613.1.Введение в теорию глубокого обучения13.2.Обобщающая способность – классическая теория13.3.PAC-байесовские оценки рискаПрименение пак-байесовских оценок к детерминированным алгоритмам обучения13.4.Сети бесконечной ширины13.5.Ландшафт функции потерь13.6.Implicit bias14.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/PAC-байесовские оценки риска13.3. PAC-байесовские оценки рискаАвторыГоликов ЕвгенийВ предыдущем параграфе рассматривались равномерные оценки разницы истинного и эмпирического рисков. Если в рассматриваемом классе моделей есть «плохие», то равномерные оценки становятся слишком пессимистичными. Часто нельзя гарантировать, что что наш алгоритм обучения их никогда не выбирает, поэтому класс моделей F\\\\mathcal{F}F для равномерной оценки не получится сузить до класса только «хороших» моделей. Но можно надеяться, что плохие выучиваются не слишком часто. Например, известно, что градиентный спуск обычно сходится к хорошим моделям (об этом мы ещё поговорим в параграфе про implicit bias). В этом параграфе мы разберём элегантный способ учесть «предпочтения» алгоритма обучения в оценке разницы рисков.\\nВспомним равномерную оценку для конечного F\\\\mathcal{F}F:\\nP(sup\\u2061f∈F(R(f)−R^m(f))≥ϵ)=P(∃f∈F:\\u2005\\u200a(R(f)−R^m(f))≥ϵ)≤\\\\mathbb{P}\\\\left(\\\\sup_{f\\\\in\\\\mathcal{F}} (R(f) - \\\\hat R_m(f)) \\\\geq \\\\epsilon\\\\right) =\\n\\\\mathbb{P}\\\\left(\\\\exists f\\\\in\\\\mathcal{F}: \\\\; (R(f) - \\\\hat R_m(f)) \\\\geq \\\\epsilon\\\\right) \\\\leqP(f∈Fsup\\u200b(R(f)−R^m\\u200b(f))≥ϵ)=P(∃f∈F:(R(f)−R^m\\u200b(f))≥ϵ)≤≤∑f∈FP(R(f)−R^m(f)≥ϵ)≤∣F∣e−2mϵ2∀ϵ>0,\\\\leq\\n\\\\sum_{f\\\\in\\\\mathcal{F}} \\\\mathbb{P}(R(f) - \\\\hat R_m(f) \\\\geq \\\\epsilon) \\\\leq\\n\\\\vert\\\\mathcal{F}\\\\vert e^{-2 m \\\\epsilon^2}\\n\\\\quad\\n\\\\forall \\\\epsilon > 0,\\n≤f∈F∑\\u200bP(R(f)−R^m\\u200b(f)≥ϵ)≤∣F∣e−2mϵ2∀ϵ>0,где ∣F∣\\\\vert\\\\mathcal{F}\\\\vert∣F∣ – мощность класса F\\\\mathcal{F}F. Эта оценка формально верна и для бесконечного F\\\\mathcal{F}F, но смысл её теряется. Давайте попробуем исправить это.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nПусть F\\\\mathcal{F}F не более, чем счётно. Для каждого f∈Ff \\\\in \\\\mathcal{F}f∈F возьмём своё ϵ(f)\\\\epsilon(f)ϵ(f). Если взять ϵ(f)\\\\epsilon(f)ϵ(f) таким, чтобы ∑f∈Fe−2mϵ2(f)\\\\sum_{f \\\\in \\\\mathcal{F}} e^{-2 m \\\\epsilon^2(f)}∑f∈F\\u200be−2mϵ2(f) было конечным, то приходим к осмысленной оценке:\\nP(∃f∈F:\\u2005\\u200a(R(f)−R^m(f))≥ϵ(f))≤\\\\mathbb{P}\\\\left(\\\\exists f\\\\in\\\\mathcal{F}: \\\\; (R(f) - \\\\hat R_m(f)) \\\\geq \\\\epsilon(f)\\\\right) \\\\leq\\nP(∃f∈F:(R(f)−R^m\\u200b(f))≥ϵ(f))≤∑f∈FP(R(f)−R^m(f)≥ϵ(f))≤∑f∈Fe−2mϵ2(f).\\\\sum_{f\\\\in\\\\mathcal{F}} \\\\mathbb{P}\\\\left(R(f) - \\\\hat R_m(f) \\\\geq \\\\epsilon(f)\\\\right) \\\\leq\\n\\\\sum_{f \\\\in \\\\mathcal{F}} e^{-2 m \\\\epsilon^2(f)}. \\nf∈F∑\\u200bP(R(f)−R^m\\u200b(f)≥ϵ(f))≤f∈F∑\\u200be−2mϵ2(f).Рассмотрим теперь некоторое вероятностное распределение P(f)P(f)P(f) на F\\\\mathcal{F}F. В качестве ϵ(f)\\\\epsilon(f)ϵ(f) возьмём\\ne−2mϵ2(f)=P(f)e−2mϵ~2,e^{-2 m \\\\epsilon^2(f)} = P(f) e^{-2 m \\\\tilde\\\\epsilon^2},\\ne−2mϵ2(f)=P(f)e−2mϵ~2,где ϵ~∈R+\\\\tilde\\\\epsilon \\\\in \\\\mathbb{R}_+ϵ~∈R+\\u200b. Из этого уравнения получаем следующее выражение для ϵ(f)\\\\epsilon(f)ϵ(f):\\nϵ(f)=ϵ~2+12mlog\\u20611P(f).\\\\epsilon(f) =\\n\\\\sqrt{\\\\tilde\\\\epsilon^2 + \\\\frac{1}{2m} \\\\log\\\\frac{1}{P(f)}}.\\nϵ(f)=ϵ~2+2m1\\u200blogP(f)1\\u200b\\u200b.В итоге, для любого ϵ~>0\\\\tilde\\\\epsilon > 0ϵ~>0 получаем оценку:\\nP(∃f∈F:\\u2005\\u200a(R(f)−R^m(f))≥ϵ~2+12mlog\\u20611P(f))≤e−2mϵ~2.\\\\mathbb{P}\\\\left(\\\\exists f\\\\in\\\\mathcal{F}: \\\\; (R(f) - \\\\hat R_m(f)) \\\\geq \\\\sqrt{\\\\tilde\\\\epsilon^2 + \\\\frac{1}{2m} \\\\log\\\\frac{1}{P(f)}}\\\\right) \\\\leq\\ne^{-2 m \\\\tilde\\\\epsilon^2}.\\nP(∃f∈F:(R(f)−R^m\\u200b(f))≥ϵ~2+2m1\\u200blogP(f)1\\u200b\\u200b)≤e−2mϵ~2.Или, что то же самое, с вероятностью ≥1−δ\\\\geq 1 - \\\\delta≥1−δ по SmS_mSm\\u200b для любого f∈Ff \\\\in \\\\mathcal{F}f∈F:\\nR(f)−R^m(f)≤12m(log\\u20611δ+log\\u20611P(f))R(f)-\\\\hat{R}_m(f)\\\\leq\\\\sqrt{\\\\frac{1}{2m}\\\\left(\\\\log\\\\frac{1}{\\\\delta}+\\\\log\\\\frac{1}{P(f)}\\\\right)}\\nR(f)−R^m\\u200b(f)≤2m1\\u200b(logδ1\\u200b+logP(f)1\\u200b)\\u200bЗаметим, что если F\\\\mathcal{F}F конечно, а P(f)P(f)P(f) – равномерное распределение, то оценка выше совпадает с равномерной оценкой. Если же наш алгоритм обучения предпочитает выбирать модели, для которых P(f)P(f)P(f) велико, то оценка улучшается по сравнению с равномерной. Таким образом, распределение P(f)P(f)P(f) «кодирует» наши представления о предпочтениях алгоритма. Будем называть P(f)P(f)P(f) «априорным распределением».\\nКак обобщить оценку выше на несчётные классы моделей? В первую очередь, предположим, что наш алгоритм обучения A\\\\mathcal{A}A стохастичен, а значит, на выходе даёт не одну модель, а распределение:\\nf^m∼Q^m=A(Sm).\\\\hat f_m \\\\sim \\n\\\\hat Q_m =\\n\\\\mathcal{A}(S_m).\\nf^\\u200bm\\u200b∼Q^\\u200bm\\u200b=A(Sm\\u200b).Будем называть это распределение «апостериорным». Такое рассуждение осмысленно, например, для стохастического градиентного спуска: очевидно, что результат его работы на невыпуклой функции потерь недетерминирован (он может сходиться в разные локальные минимумы).\\nЗаметим, что главное отличие апостериорного распределения от априорного в том, что первое зависит от данных, а второе – нет. Важно понимать при этом, что, несмотря на названия, эти два распределения не связаны между собой никаким вариантом формулой Байеса. Сходство с байесовским подходом скорее внешнее. Поэтому слова «априорное» и «апостериорное» имеет смысл писать в кавычках, но для экономии места мы их будем в дальнейшем опускать.\\nОценки разности рисков, о которых речь пойдёт ниже, называются PAC-байесовскими (PAC-bayesian, где PAC – probably approximately correct).\\nСформулируем одну из классических оценок из этого класса:\\nТеорема Макаллестера. Пусть F\\\\mathcal{F}F – множество моделей и PPP – распределение на F\\\\mathcal{F}F. Тогда для любого δ∈(0,1)\\\\delta \\\\in (0,1)δ∈(0,1) с вероятностью ≥1−δ\\\\geq 1 - \\\\delta≥1−δ по SmS_mSm\\u200b имеем:\\nR(Q^m)≤R^m(Q^m)+12m−1(log\\u20614mδ+KL(Q^m\\u2005\\u200a∣∣P)),R(\\\\hat Q_m) \\\\leq\\n\\\\hat R_m(\\\\hat Q_m) + \\\\sqrt{\\\\frac{1}{2m-1} \\\\left(\\\\log \\\\frac{4m}{\\\\delta} + \\\\mathrm{KL}(\\\\hat Q_m\\\\;\\\\vert\\\\vert P) \\\\right)},\\nR(Q^\\u200bm\\u200b)≤R^m\\u200b(Q^\\u200bm\\u200b)+2m−11\\u200b(logδ4m\\u200b+KL(Q^\\u200bm\\u200b∣∣P))\\u200b,где R(Q)=Ef∼QR(f)R(Q) = \\\\mathbb{E}_{f \\\\sim Q} R(f)R(Q)=Ef∼Q\\u200bR(f) и R^m(Q)=Ef∼QR^m(f)\\\\hat R_m(Q) = \\\\mathbb{E}_{f \\\\sim Q} \\\\hat R_m(f)R^m\\u200b(Q)=Ef∼Q\\u200bR^m\\u200b(f).\\nВидим, что оценка тем лучше, чем ближе апостериорное распределение к априорному. Здесь работает следующая интуиция. Если для большинства обучающих наборов данных апостериорное распределение близко к априорному, то оно почти не зависит от данных, а значит, истинный риск и риск на обучающей выборке должны быть близки с высокой вероятностью. Если же апостериорное зависит от данных сильно, то, скорее всего, модель сильно переобучается, а значит, оценка не может быть хорошей; в нашем случае она велика из-за большой KL-дивергенции.\\nДля доказательства теоремы нам понадобятся две леммы:\\nЛемма 1. Для любого распределения PPP на F\\\\mathcal{F}F и для любого δ∈(0,1)\\\\delta \\\\in (0,1)δ∈(0,1) с вероятностью ≥1−δ\\\\geq 1 - \\\\delta≥1−δ по SmS_mSm\\u200b имеем:\\nEf∼Pe(2m−1)(Δm(f))2≤4mδ,\\\\mathbb{E}_{f \\\\sim P} e^{(2m-1) (\\\\Delta_m(f))^2} \\\\leq\\n\\\\frac{4m}{\\\\delta},\\nEf∼P\\u200be(2m−1)(Δm\\u200b(f))2≤δ4m\\u200b,где Δm(f)=∣R(f)−R^m(f)∣\\\\Delta_m(f) = \\\\vert R(f) - \\\\hat R_m(f)\\\\vertΔm\\u200b(f)=∣R(f)−R^m\\u200b(f)∣.\\nЛемма 2 (лемма Донскера-Вередана, Donsker-Varadhan). Пусть PPP и QQQ – вероятностные распределения на множестве XXX. Тогда для любого h:\\u2005\\u200aX→Rh: \\\\; X \\\\to \\\\mathbb{R}h:X→R\\nEx∼Qh(x)≤log\\u2061Ex∼Peh(x)+KL(Q∣∣P).\\\\mathbb{E}_{x\\\\sim Q} h(x) \\\\leq\\n\\\\log\\\\mathbb{E}_{x \\\\sim P} e^{h(x)} + \\\\mathrm{KL}(Q\\\\vert\\\\vert P).\\nEx∼Q\\u200bh(x)≤logEx∼P\\u200beh(x)+KL(Q∣∣P).Доказательство теоремыПрименим лемму 2 к X=FX = \\\\mathcal{F}X=F, h=(2m−1)Δm2h = (2m-1) \\\\Delta_m^2h=(2m−1)Δm2\\u200b и Q=Q^mQ = \\\\hat Q_mQ=Q^\\u200bm\\u200b:\\nEf∼Q^m(2m−1)(Δm(f))2≤log\\u2061Ef∼Pe(2m−1)(Δm(f))2+KL(Q^m\\u2005\\u200a∣∣\\u2005\\u200aP).        \\\\mathbb{E}_{f \\\\sim \\\\hat Q_m} (2m-1) (\\\\Delta_m(f))^2 \\\\leq\\n        \\\\log\\\\mathbb{E}_{f \\\\sim P} e^{(2m-1) (\\\\Delta_m(f))^2} + \\\\mathrm{KL}(\\\\hat Q_m\\\\;\\\\vert\\\\vert\\\\;P).\\nEf∼Q^\\u200bm\\u200b\\u200b(2m−1)(Δm\\u200b(f))2≤logEf∼P\\u200be(2m−1)(Δm\\u200b(f))2+KL(Q^\\u200bm\\u200b∣∣P).Тогда по лемме 1 с вероятностью ≥1−δ\\\\geq 1 - \\\\delta≥1−δ по SmS_mSm\\u200b имеем:\\nEf∼Q^m(2m−1)(Δm(f))2≤log\\u20614mδ+KL(Q^m\\u2005\\u200a∣∣\\u2005\\u200aP).        \\\\mathbb{E}_{f \\\\sim \\\\hat Q_m} (2m-1) (\\\\Delta_m(f))^2 \\\\leq\\n        \\\\log\\\\frac{4m}{\\\\delta} + \\\\mathrm{KL}(\\\\hat Q_m\\\\;\\\\vert\\\\vert\\\\;P).\\nEf∼Q^\\u200bm\\u200b\\u200b(2m−1)(Δm\\u200b(f))2≤logδ4m\\u200b+KL(Q^\\u200bm\\u200b∣∣P).Доказательство, таким образом, легко завершается:\\nR(Q^m)−R^m(Q^m)≤∣Ef∼Q^m(R(f)−R^m(f))∣≤        R(\\\\hat Q_m) - \\\\hat R_m(\\\\hat Q_m) \\\\leq\\n        \\\\vert\\\\mathbb{E}_{f\\\\sim \\\\hat Q_m} (R(f) - \\\\hat R_m(f))\\\\vert \\\\leq\\nR(Q^\\u200bm\\u200b)−R^m\\u200b(Q^\\u200bm\\u200b)≤∣Ef∼Q^\\u200bm\\u200b\\u200b(R(f)−R^m\\u200b(f))∣≤≤Ef∼Q^m∣R(f)−R^m(f)∣=Ef∼Q^mΔm(f)≤\\\\leq\\\\mathbb{E}_{f\\\\sim \\\\hat Q_m} |R(f) - \\\\hat R_m(f)| =\\n\\\\mathbb{E}_{f\\\\sim \\\\hat Q_m} \\\\Delta_m(f) \\\\leq\\n≤Ef∼Q^\\u200bm\\u200b\\u200b∣R(f)−R^m\\u200b(f)∣=Ef∼Q^\\u200bm\\u200b\\u200bΔm\\u200b(f)≤≤Ef∼Q^m(Δm(f))2≤        \\\\leq\\\\sqrt{\\\\mathbb{E}_{f\\\\sim \\\\hat Q_m} (\\\\Delta_m(f))^2} \\\\leq\\n≤Ef∼Q^\\u200bm\\u200b\\u200b(Δm\\u200b(f))2\\u200b≤12m−1(log\\u20614mδ+KL(Q^m\\u2005\\u200a∣∣\\u2005\\u200aP)).        \\\\sqrt{\\\\frac{1}{2m-1} \\\\left(\\\\log \\\\frac{4m}{\\\\delta} + \\\\mathrm{KL}(\\\\hat Q_m\\\\;\\\\vert\\\\vert\\\\;P) \\\\right)}.\\n2m−11\\u200b(logδ4m\\u200b+KL(Q^\\u200bm\\u200b∣∣P))\\u200b.Доказательство леммы 2Мы рассмотрим лишь простой случай, когда и у PPP, и у QQQ есть плотности, и они нигде не обращаются в ноль.\\nEx∼Qh(x)−KL(Q∣∣P)=Ex∼Q(h(x)−log\\u2061(q(x)p(x)))=        \\\\mathbb{E}_{x \\\\sim Q} h(x) - \\\\mathrm{KL}(Q\\\\vert\\\\vert P) =\\n        \\\\mathbb{E}_{x \\\\sim Q} \\\\left(h(x) - \\\\log\\\\left(\\\\frac{q(x)}{p(x)}\\\\right)\\\\right) =\\nEx∼Q\\u200bh(x)−KL(Q∣∣P)=Ex∼Q\\u200b(h(x)−log(p(x)q(x)\\u200b))==Ex∼Qlog\\u2061(eh(x)p(x)q(x))≤log\\u2061Ex∼Q(eh(x)p(x)q(x))=log\\u2061Ex∼Peh(x).        =\\\\mathbb{E}_{x \\\\sim Q} \\\\log\\\\left(e^{h(x)} \\\\frac{p(x)}{q(x)}\\\\right) \\\\leq\\n        \\\\log \\\\mathbb{E}_{x \\\\sim Q} \\\\left(e^{h(x)} \\\\frac{p(x)}{q(x)}\\\\right) =\\n        \\\\log \\\\mathbb{E}_{x \\\\sim P} e^{h(x)}.\\n=Ex∼Q\\u200blog(eh(x)q(x)p(x)\\u200b)≤logEx∼Q\\u200b(eh(x)q(x)p(x)\\u200b)=logEx∼P\\u200beh(x).Доказательство леммы 1Нам понадобится\\nНеравенство Маркова. Пусть XXX – неотрицательная случайная величина. Тогда для любого a>0a > 0a>0 имеем\\nP(X≥a)≤EXa.        \\\\mathbb{P}(X \\\\geq a) \\\\leq\\n        \\\\frac{\\\\mathbb{E} X}{a}.\\nP(X≥a)≤aEX\\u200b.В качестве XXX и aaa из неравенства Маркова возьмём\\nX=Ef∼Pe(2m−1)(Δm(f))2,a=4m/δX = \\\\mathbb{E}_{f \\\\sim P} e^{(2m-1) (\\\\Delta_m(f))^2},\\\\qquad a = 4m / \\\\delta\\nX=Ef∼P\\u200be(2m−1)(Δm\\u200b(f))2,a=4m/δТогда\\nP(14Ef∼Pe(2m−1)(Δm(f))2≤4mδ)≤\\\\mathbb{P}\\\\left(\\\\vphantom{\\\\frac14}\\\\mathbb{E}_{f \\\\sim P} e^{(2m-1) (\\\\Delta_m(f))^2} \\\\leq\\n        \\\\frac{4m}{\\\\delta}\\\\right) \\\\leq\\nP(41\\u200bEf∼P\\u200be(2m−1)(Δm\\u200b(f))2≤δ4m\\u200b)≤≤δ4mESmEf∼Pe(2m−1)(Δm(f))2\\\\leq\\\\frac{\\\\delta}{4m}\\\\mathbb{E}_{S_m} \\\\mathbb{E}_{f \\\\sim P} e^{(2m-1) (\\\\Delta_m(f))^2}\\n≤4mδ\\u200bESm\\u200b\\u200bEf∼P\\u200be(2m−1)(Δm\\u200b(f))2Значит, нам достаточно доказать, что\\nESmEf∼Pe(2m−1)(Δm(f))2≤4m.        \\\\mathbb{E}_{S_m} \\\\mathbb{E}_{f \\\\sim P} e^{(2m-1) (\\\\Delta_m(f))^2} \\\\leq\\n        4m.\\nESm\\u200b\\u200bEf∼P\\u200be(2m−1)(Δm\\u200b(f))2≤4m.Мы докажем даже более сильное соотношение:\\nESme(2m−1)(Δm(f))2≤4m∀f∈F\\\\mathbb{E}_{S_m}e^{(2m-1)(\\\\Delta_m(f))^2}\\\\leq4m\\\\quad\\\\forall f\\\\in\\\\mathcal{F}\\nESm\\u200b\\u200be(2m−1)(Δm\\u200b(f))2≤4m∀f∈FЗаметим, что из неравенства Хёффдинга будет следовать\\nPSm(Δm(f)≥ϵ)≤2e−2mϵ2∀ϵ>0∀f∈F\\\\mathbb{P}_{S_m}(\\\\Delta_m(f)\\\\geq\\\\epsilon)\\\\leq2e^{-2m\\\\epsilon^2}\\\\quad\\\\forall\\\\epsilon>0\\\\quad\\\\forall f\\\\in\\\\mathcal{F}\\nPSm\\u200b\\u200b(Δm\\u200b(f)≥ϵ)≤2e−2mϵ2∀ϵ>0∀f∈FДля простоты предположим, что распределение Δm(f)\\\\Delta_m(f)Δm\\u200b(f) имеет плотность для любого f∈Ff \\\\in \\\\mathcal{F}f∈F; обозначим её pf(Δ)p_f(\\\\Delta)pf\\u200b(Δ). В этом случае мы можем ограничить матожидания по SmS_mSm\\u200b напрямую:\\nESme(2m−1)(Δm(f))2=∫0∞e(2m−1)ϵ2pf(ϵ)\\u2009dϵ=        \\\\mathbb{E}_{S_m} e^{(2m-1) (\\\\Delta_m(f))^2} =\\n        \\\\int_0^\\\\infty e^{(2m-1) \\\\epsilon^2} p_f(\\\\epsilon) \\\\, d\\\\epsilon =\\nESm\\u200b\\u200be(2m−1)(Δm\\u200b(f))2=∫0∞\\u200be(2m−1)ϵ2pf\\u200b(ϵ)dϵ=∫0∞e(2m−1)ϵ2ddϵ(−∫ϵ∞pf(Δ)\\u2009dΔ)\\u2009dϵ=        \\\\int_0^\\\\infty e^{(2m-1) \\\\epsilon^2} \\\\frac{d}{d\\\\epsilon} \\\\left(-\\\\int_\\\\epsilon^\\\\infty p_f(\\\\Delta) \\\\, d\\\\Delta\\\\right) \\\\, d\\\\epsilon =\\n∫0∞\\u200be(2m−1)ϵ2dϵd\\u200b(−∫ϵ∞\\u200bpf\\u200b(Δ)dΔ)dϵ==−(e(2m−1)ϵ2∫ϵ∞pf(Δ)\\u2009dΔ)∣ϵ=0∞+2(2m−1)∫0∞ϵ\\u2009e(2m−1)ϵ2∫ϵ∞pf(Δ)\\u2009dΔ\\u2009dϵ≤        =\\n        \\\\left.-\\\\left(e^{(2m-1) \\\\epsilon^2} \\\\int_\\\\epsilon^\\\\infty p_f(\\\\Delta) \\\\, d\\\\Delta\\\\right) \\\\right|_{\\\\epsilon=0}^\\\\infty +\\n        2 (2m-1) \\\\int_0^\\\\infty \\\\epsilon\\\\,e^{(2m-1) \\\\epsilon^2} \\\\int_\\\\epsilon^\\\\infty p_f(\\\\Delta) \\\\, d\\\\Delta \\\\, d\\\\epsilon \\\\leq\\n=−(e(2m−1)ϵ2∫ϵ∞\\u200bpf\\u200b(Δ)dΔ)\\u200bϵ=0∞\\u200b+2(2m−1)∫0∞\\u200bϵe(2m−1)ϵ2∫ϵ∞\\u200bpf\\u200b(Δ)dΔdϵ≤≤∫0∞pf(Δ)\\u2009dΔ+2(2m−1)∫0∞ϵ\\u2009e(2m−1)ϵ2∫ϵ∞pf(Δ)\\u2009dΔ\\u2009dϵ≤        \\\\leq\\n        \\\\int_0^\\\\infty p_f(\\\\Delta) \\\\, d\\\\Delta +\\n        2 (2m-1) \\\\int_0^\\\\infty \\\\epsilon\\\\,e^{(2m-1) \\\\epsilon^2} \\\\int_\\\\epsilon^\\\\infty p_f(\\\\Delta) \\\\, d\\\\Delta \\\\, d\\\\epsilon \\\\leq\\n≤∫0∞\\u200bpf\\u200b(Δ)dΔ+2(2m−1)∫0∞\\u200bϵe(2m−1)ϵ2∫ϵ∞\\u200bpf\\u200b(Δ)dΔdϵ≤≤2+4(2m−1)∫0∞ϵ\\u2009e(2m−1)ϵ2e−2mϵ2\\u2009dϵ=        \\\\leq\\n        2 + 4 (2m-1) \\\\int_0^\\\\infty \\\\epsilon\\\\,e^{(2m-1) \\\\epsilon^2} e^{-2m \\\\epsilon^2} \\\\, d\\\\epsilon =\\n≤2+4(2m−1)∫0∞\\u200bϵe(2m−1)ϵ2e−2mϵ2dϵ=2+4(2m−1)∫0∞ϵ\\u2009e−ϵ2\\u2009dϵ=2+2(2m−1)=4m.        2 + 4 (2m-1) \\\\int_0^\\\\infty \\\\epsilon\\\\,e^{-\\\\epsilon^2} \\\\, d\\\\epsilon =\\n        2 + 2 (2m-1) =\\n        4m.\\n2+4(2m−1)∫0∞\\u200bϵe−ϵ2dϵ=2+2(2m−1)=4m.В общем случае, мы не можем предполагать наличие плотности у Δm(f)\\\\Delta_m(f)Δm\\u200b(f). Доказательство в этом случае можно найти в оригинальной работе D. A. McAllester Some pac-bayesian theorems, а также в конспекте лекций автора этого параграфа.\\nТеорема Макаллестера – не единственная из возможных пак-байесовских оценок. Например, несколько улучшенную версию той же оценки можно найти в работе Bounds for averaging classifiers. Другие оценки подобного типа можно найти в монографии PAC-Bayesian supervised classification: the thermodynamics of statistical learning.\\nПрименение пак-байесовских оценок к детерминированным алгоритмам обучения\\nВыше были рассмотрены две PAC-байесовские оценки: одна для не более, чем счётного множества моделей, другая – для произвольного. За возможность использования несчётных классов моделей мы заплатили тем, что алгоритм обучения должен быть недетерминированным (для детерминированных алгоритмов KL-дивергенция в Теореме Макаллестера может вырождаться в бесконечность; например, это так, если априорное распределение гауссово). Чаще всего класс моделей F\\\\mathcal{F}F всё-таки несчетён: например, если это класс всех сетей фиксированной архитектуры, то он индексируется весами, которых несчётное множество. При этом, хотя используемый алгоритм обучения и в самом деле недетерминирован (стохастический градиентный спуск зависит от случайного выбора батчей и от инициализации весов) и теорема Макаллестера выполняется, финальное распределение моделей очень сложно охарактеризовать, и из-за этого непонятно, как считать KL-дивергенцию.\\nПредположим, что алгоритм обучения всё-таки детерминирован; этого можно добиться, зафиксировав сид генератора случайных чисел при обучении. Как получить осмысленную PAC-байесовскую оценку для детерминированного алгоритма на несчётном множестве моделей?\\nМы рассмотрим два способа.\\nПервый способ – добавить известный шум в финальную модель, выданную детерминированным алгоритмом. Так, для нейронных сетей, результатом работы алгоритма обучения является набор весов. Если добавить в этот набор гауссовский шум, а также в качестве априорного распределения взять гауссовское, то KL-дивергенцию в теореме Макаллестера можно будет посчитать аналитически.\\nДисперсию шума в апостериорном распределении тоже можно обучить с помощью градиентного спуска одновременно с весами, тем самым минимизируя правую часть оценки из вышеупомянутой теоремы.   Если в найденную модель удастся добавить шум так, чтобы KL-дивергенция значительно уменьшилась, но при этом риск на обучающей выборке не сильно вырос, то оценка на истинный риск получится хорошей.\\nЭто рассуждение связывает PAC-байесовские оценки и гипотезу о том, что «плоские» («широкие») минимумы хорошо обобщают. В самом деле, если минимум «плоский», то в модель из него можно добавить много шума, не испортив качество на обучении. Оценки, основанные на этом принципе, можно найти в работах Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data и A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks.\\nВторой способ состоит в том, чтобы взять дискретное кодирование ccc и применить дискретную PAC-байесовскую оценку к закодированной модели вместо оригинальной. Обозначим закодированную модель fff через fcf_cfc\\u200b. Следуя работе Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach, возьмём априорное распределение с массой, убывающей с ростом длины кода:\\nPc(fc)=1Zm(∣fc∣)2−∣fc∣.P_c(f_c) =\\n\\\\frac{1}{Z} m(\\\\vert f_c\\\\vert ) 2^{-|f_c|}.\\nPc\\u200b(fc\\u200b)=Z1\\u200bm(∣fc\\u200b∣)2−∣fc\\u200b∣.Здесь ∣fc∣\\\\vert f_c\\\\vert∣fc\\u200b∣ – длина кода модели fff, m(k)m(k)m(k) – некоторое вероятностное распределение на N\\\\mathbb{N}N, а ZZZ – нормализующая константа. Тогда KL-дивергенция примет следующий вид:\\nKL(δfc∣∣Pc)=log\\u2061Z+∣fc∣log\\u20612−log\\u2061(m(∣fc∣)).\\\\mathrm{KL}(\\\\delta_{f_c}\\\\vert\\\\vert {P_c}) =\\n\\\\log Z + \\\\vert f_c\\\\vert  \\\\log 2 - \\\\log(m(\\\\vert f_c\\\\vert )).\\nKL(δfc\\u200b\\u200b∣∣Pc\\u200b)=logZ+∣fc\\u200b∣log2−log(m(∣fc\\u200b∣)).Для того, чтобы KL-дивергенция выше была как можно меньше, необходимо, чтобы наш алгоритм обучения на реалистичных данных сходился в модели с маленькой длиной кода. Для этого будем применять наше кодирование не к оригинальной модели, а к сжатой с помощью некоторого алгоритма сжатия. Здесь мы предполагаем, что модели, к которым сходится наш алгоритм обучения, можно сжать с малыми потерями до моделей с малой длиной кода. Другими словами, мы опираемся на предположение, что обученные модели в некоторым смысле «простые».\\nЕсли модель параметризована весами θ\\\\thetaθ, типичный алгоритм сжатия выдаст набор (S,Q,C)(S,Q,C)(S,Q,C), где\\n\\nS=s1:k⊂[dim\\u2061θ]S = s_{1:k} \\\\subset [\\\\dim\\\\theta]S=s1:k\\u200b⊂[dimθ] – позиции ненулевых весов;\\nC=c1:r⊂RC = c_{1:r} \\\\subset \\\\mathbb{R}C=c1:r\\u200b⊂R – «словарь» весов;\\nQ=q1:kQ = q_{1:k}Q=q1:k\\u200b, qi∈[r]q_i \\\\in [r]qi\\u200b∈[r] ∀i∈[k]\\\\forall i \\\\in [k]∀i∈[k] – квантизованные значения весов.\\n\\nВыход алгоритма будет выглядеть как C(θ)i=cqj\\\\mathcal{C}(\\\\theta)_i = c_{q_j}C(θ)i\\u200b=cqj\\u200b\\u200b, если i=sji = s_ji=sj\\u200b, иначе 000.\\nТогда наивное 32-битное кодирование даст следующую длину:\\n∣C(θ)∣c=∣S∣c+∣Q∣c+∣C∣c≤k(log\\u2061dim\\u2061θ+log\\u2061r)+32r.\\\\vert\\\\mathcal{C}(\\\\theta)\\\\vert_c =\\n\\\\vert S\\\\vert_c + \\\\vert Q\\\\vert_c + \\\\vert C\\\\vert_c \\\\leq\\nk (\\\\log\\\\dim\\\\theta + \\\\log r) + 32 r.\\n∣C(θ)∣c\\u200b=∣S∣c\\u200b+∣Q∣c\\u200b+∣C∣c\\u200b≤k(logdimθ+logr)+32r.В работе Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach описанный выше способ применяется к модели MobileNet (свёрточной сети, сконструированной специально для мобильных устройств), обученной на наборе данных ImageNet, и получают верхнюю оценку на истинный риск, равную 96.5%96.5\\\\%96.5% (риск случайного угадывания – 99.9%99.9\\\\%99.9%). Хотя такой результат и выглядит очень скромным, но это первая осмысленная оценка обобщающей способности реально используемой нейронной сети на реалистичном наборе данных.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанЗагрузка...Сообщить об ошибкеПредыдущий параграф13.2. Обобщающая способность – классическая теорияСледующий параграф13.4. Сети бесконечной шириныЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_47.html', 'title': 'Краудсорсинг'}, page_content=\"КраудсорсингЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/211.1.Обучение с подкреплением11.2.КраудсорсингВступлениеЧто такое краудсорсинг в ML?Ключевые принципы краудсорсинга в MLML-задачи, где используется разметкаРазметка данныхСбор данныхКраудсорсинговые платформыГраницы применимости краудсорсингаЭтапы создания краудсорсингового проектаДекомпозицияИнструкцияАгрегация результатов12.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Краудсорсинг11.2. КраудсорсингВступление\\nДля обучения и проверки качества ML-модели необходимы данные, размеченные человеком. Студенты обычно получают эти данные уже в готовом виде, но в работе над реальными продуктами задачи по сбору и разметке приходится решать самостоятельно, учитывая специфику конкретного продукта.\\nГотовые наборы зачастую однообразны, а иногда и вовсе мешают достичь требуемых результатов: так, модели компьютерного зрения для беспилотного транспорта необходимо обучать на данных, собранных в той же среде, где используется модель. Кроме того, высокие темпы развития нейросетевых технологий провоцируют все большую необходимость в крупных объемах данных: чем лучше текущее качество модели, тем больше новых данных требуется, чтобы поднять это качество на новый уровень. Как следствие, сбор и разметка данных становится неотъемлемой частью почти любого ML-производства, а качество и количество этих данных напрямую влияет на качество конечного продукта.\\nКраудсорсинг зарекомендовал себя, как один из эффективных способов сбора и разметки данных в больших масштабах. Его используют в разработке новых технологий, чтобы создавать обучающие датасеты для ML-моделей беспилотных автомобилей, голосовых помощников, чат-ботов, поисковых систем и других разработок.\\nКачественные данные удается собрать благодаря краудсорсинговым платформам: они помогают снизить количество ошибок с помощью специальных настроек, которые можно найти на платформе, а также дают доступ к огромному количеству исполнителей, способных в любое время присоединиться к работе.\\nТакже секрет успеха кроется в той последовательности действий, которые нужно соблюдать, создавая проект на карудсорсинговой платформе. Участникам краудсорсинговых платформ под силу выполнить не все задания, а только простые: сложные задания нужно разбивать на несколько небольших. Качество выполняемой ими работы нужно проверять с помощью доступных на платформе инструментов контроля качества. Полученные результаты в некоторых случаях нужно правильно обрабатывать (здесь полезно разобраться в способах агрегации данных). Чтобы исполнители получили оплату только за правильно выполненные задания, нужно сформулировать подходящую модель ценообразования и т. д.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nНюансов в работе с краудсорсингом достаточно много, поэтому мы подготовили этот параграф в учебник. Стоит отметить, что в ней мы уделим внимание нетехническим аспектам краудсорсинга для ML. Такой уклон связан с тем, что использование краудсорсинга в качестве инструмента работы с данными требует не только знания технических и математических методов (они пригодятся в финальной части, когда полученные данные необходимо будет обработать), но и умения правильно организовать процесс сбора данных, понимания самого феномена краудсорсинга, который сегодня используется в разных сферах для решения разных задач.\\nПо этой причине структура этого параграфа будет выглядеть следующим образом:\\n\\n В первой части мы сделаем общий обзор краудсорсинга в ML и объясним, для каких задач он применим. \\n Во второй части мы остановимся на основных этапах запуска краудсорсингового проекта: от деления проекта на небольшие задачи до обработки полученных от исполнителей данных. \\n Кроме того, в этом параграфе мы разберем примеры некоторых ML-задач, которые встречаются в проектах в сфере AI и ML. Это сбор данных для поисковых сетей, разметка изображений для беспилотных автомобилей и сбор аудиозаписей для голосовых помощников. \\n\\nНадеемся, что вам будет интересно погружаться в мир краудсорсинга для ML. Будем рады, если мы поможем вам разложить все по полочкам, чтобы вы смогли дальше наращивать свои знания и изучать отдельные темы более глубоко.\\nЧто такое краудсорсинг в ML?\\nСуществует довольно много определений краудсорсинга, а также близких к нему по значению терминов (например, «человеческие вычисления», «мудрость толпы» и «коллективный разум»). Это связано с тем, что этот метод используется в разных сферах и применяется для решения разного рода задач, в том числе поиска креативных идей, создания контента, сбора денежных средств. Например, автор термина «краудсорсинг», Джефф Хоу, в 2006 году предложил следующее определение этого метода: Краудсорсинг (от англ. crowd — «толпа», source — «использование ресурсов») — это процесс, в котором компания переносит определенные функции, ранее возлагавшиеся на сотрудников, аутсорсинговые предприятия и поставщиков, на неопределенное, достаточно большое количество людей в формате открытого запроса.\\nЭто определение отражает основную идею краудсорсинга. Однако его недостаточно в контексте рассматриваемой нами темы. Мы говорим о краудсорсинге в машинном обучении. Это значит, что мы передаем облаку исполнителей те задачи, которые связаны со сбором и разметкой данных, а также с оценкой этих данных для разного рода проектов. Разработчики используют эти данные, чтобы обучать машины, а именно модели этих машин, выполнять требуемые задачи. Поэтому в машинном обучении краудсорсинг — это дополнительный вычислительный кластер, который помогает командам создавать и улучшать их продукты.\\nОдин из первых проектов, который задействовал краудсорсинг в получении данных для обучения модели, — проект Distributed Proofreaders (с англ.\\u2009—\\u2009«Распределенные корректоры»). Его главная цель — цифровизация печатных книг с помощью программы для оптического распознавания символов (OCR). Вовлекая тысячи волонтёров, проект Distributed Proofreaders оцифровывает печатные книги и улучшает программу для распознавания текстов. Чем больше данных модель этой программы получает от волонтеров, тем лучше она считывает текст с отсканированных страниц книг. Соответственно, чем лучше становится эта модель, тем меньше времени и усилий человека требуется для того, чтобы находить и исправлять ее ошибки.\\nРассмотрим этот проект подробнее:\\n\\nДобровольцам предлагают сравнить отсканированное изображение страницы и текст этой страницы, распознанный с помощью программного обеспечения для оптического распознавания символов (OCR).\\nПоскольку программа оптического распознавания текста не справляется с задачей в полном объеме, в тексте часто появляются ошибки. Задача добровольца — исправить ошибки OCR и загрузить файл обратно на сайт.\\nВыполненная работа передается второму добровольцу, он проверяет ее, исправляет ошибки.\\nКнига аналогичным образом проходит третий этап корректуры и два этапа форматирования с использованием одного и того же веб-интерфейса.\\nПосле того, как все страницы книги прошли через несколько этапов проверки, постпроцессор собирает их в электронную книгу и отправляет в архив проекта «Гутенберг».\\nОтредактированные страницы книг в дальнейшем используются разработчиками, как данные для обучения OCR. Модель программы обучается на данных и в дальнейшем совершает меньше ошибок при распознавании текста на изображениях.\\n\\nДругой пример использования краудсорсинга в ML — сервис reCaptcha. Он был запущен учеными Университета Карнеги-Меллона в 2007 году и стал продолжением проекта Captcha, появившегося в 2000 году. Напомним, что Captcha — это программа, которая защищает сайты от интернет-ботов. Посещая сайт и совершая на нем определенные действия, пользователь получает просьбу заполнить веб-форму. Его задача — вписать в эту форму буквы и цифры, которые он видит на изображении. Люди с хорошим зрением могут легко распознать эти символы, а боты не могут. Так сервис определяет, кто из посетителей сайта человек, а кто — бот. Ботам доступ к сайтам закрывается, так как они наносят вред сайтам.\\nСоздатели проекта Captcha пошли дальше. Они подсчитали, что у каждого человека уходит примерно 10 секунд на ввод одной капчи. А у человечества (10 умножаем на 200 млн) — 500 000 часов. Тогда появилась идея о том, что время, потраченное на ввод капчи, можно использовать с пользой для людей. Это стало началом проекта reCaptcha. Отличие этого проекта от проекта Captcha состоит в том, что вы не только печатаете капчу и подтверждаете, что вы человек, но и одновременно делаете минимальное полезное усилие. В 2007 году таким усилием была оцифровка книг, а с 2012 года reCaptcha стали использовать для распознавания изображений из онлайн-карт. Мы расскажем про инициативу, вошедшую в историю под девизом Stop Spam, Read Books. В чем она заключалась?\\n\\nКаждая страница книги сканируется.\\nКомпьютер расшифровывает слова на каждом отсканированном изображении. Для этого используется технология OCR — та, же технология, что и в первом проекте.\\nПри распознавании текста OCR допускает ошибки. Их особенно много в распознанных текстах старых книг, поскольку в некоторых местах чернила выцвели и страницы пожелтели. Например, в книгах, написанных более 50 лет назад, компьютер не может распознать более 30% слов.\\nВсе нераспознанные слова направляются людям, чтобы они их распознали, когда вводят капчу в интернете. Задача добровольцев — ввести слова, взятые из отсканированных книг, которые компьютер не смог распознать.\\nДобровольцу необходимо распознать два слова из книги. Почему именно два? Одно из слов взято из книги, и оно неизвестно компьютеру. Соответственно, проверить ответ добровольца компьютер не может. Поэтому волонтер получает второе слово — его компьютер знает. Мы не говорим, какое из слов известно компьютеру, и просим добровольца ввести оба. Если доброволец вводит известное слово правильно, система получает подтверждение, что он — человек, а также получает уверенность в правильности ввода другого слова.\\nОдно и то же слово, которое неизвестно компьютеру, направляется десяти участникам проекта. Если все они вводят его одинаково, то есть их ответы совпадают, то это слово отправляется в книгу.\\nКак и в случае с первым проектом, данные, полученные от добровольцев, используются для обучения технологии OCR.\\n\\nИнициативой проекта reCaptcha впечатлилось множество владельцев сайтов. Новый сервис взамен традиционной Captcha установили такие сайты, как Tiketmaster, Facebook, Twitter и примерно 350 000 других сайтов. Каждый день на этих сайтах вплоть до 2012 года люди оцифровывали примерно 100 млн слов в день. Это 2,5 млн книг в год. В результате, в течение пяти лет с момента его запуска в проекте по оцифровке книг поучаствовали минимум 750 млн людей (это 10% всего населения). Книги, оцифрованные в рамках этого проекта сегодня представлены на сайте books.google.com.\\nПодводя итоги вышесказанного, сформулируем определение краудсорсинга в ML. Краудсорсинг в ML — это способ сбора данных, которые необходимы разработчикам, чтобы обучать машины выполнять необходимые действия. С помощью краудсорсинга разработчики вовлекают в процесс выполнения задач обычных людей, которые не владеют определенными навыками и экспертизой. В рамках четко заданных инструкций они выполняют нужное количество заданий. Результаты этих заданий — собранные, размеченные или оцененные данные — входят в те датасеты, которые используются для обучения машин.\\nКлючевые принципы краудсорсинга в ML\\nПрименение краудсорсинга в машинном обучении значительно ускорило процесс развития AI продуктов. Беспилотные автомобили, голосовые помощники, поисковые системы, онлайн-карты, машинный перевод появились и развиваются во многом благодаря данным, полученным с помощью краудсорсинга. Например, чтобы поисковая система смогла точно отвечать на вопросы пользователей, нужно проделать большую работу по разметке данных: проанализировать запросы и поведение пользователя, оценить возможные результаты на соответствие запросу, сравнить разные варианты поисковых выдач и выбрать лучший. Все эти данные ложатся в основу моделей, которые учатся искать лучшие ответы, опираясь на размеченные людьми образцы. Такие задачи, на первый взгляд, кажутся трудозатратными и продолжительными по времени. Но если воспользоваться возможностями краудсорсинга и подойти к ним, как к инженерной проблеме, эти сложности будут преодолены.\\nВ этом тезисе содержится основная идея краудсорсинга для AI и машинного обучения: чтобы решить задачу по разметке данных для обучения или оценки качества модели, нужно подойти к ней как к инженерной проблеме. Это значит, что нужно организовать выполнение задачи таким образом, чтобы конечный результат зависел от качества самого процесса, а не от добросовестности или экспертности отдельных исполнителей.\\nТакой подход требует соблюдения ряда правил. Прежде всего, чтобы проект был доступен максимальному количеству исполнителей и не зависел от редких компетенций, его необходимо разделить на сценарии или небольшие задачи. Принцип деления сложной задачи на несколько микрозадач называется декомпозицией. Это основополагающий принцип для каждого краудсорсингового проекта, создаваемого для задач машинного обучения.\\nКаждую микрозадачу необходимо детально продумать. Определить элементы, которые будут ее сопровождать. Некоторые из них (например, инструкции или интерфейсы) обязательно должны присутствовать в проекте. Другие — такие как предварительная фильтрация исполнителей или отслеживание их поведения в проекте — используются в случае необходимости. Все эти элементы решают вопрос качества данных: чем лучше продуман проект, чем эффективнее он «сопровождает» исполнителя во время разметки, тем меньше остается пространства для ошибок или недобросовестного поведения.\\nДетальную схему проекта, состоящую из цепочки микрозадач и сопровождающих их элементов, называют пайплайном (от англ. pipeline — «линия, очередь»). Его создают на этапе планирования проекта и обращаются к нему как к «дорожной карте».\\nML-задачи, где используется разметка\\nКраудсорсинг помогает решить разнообразный спектр ML-задач. Разделим их на две основные группы — разметка и сбор данных.\\nРазметка данных\\nК этой группе относится целый ряд задач, в рамках которых пользователю краудсорсинговой платформы необходимо выполнить некоторое действие с уже полученными данными. Например, его могут попросить перевести записи из аудио в текст (транскрипция аудио) или выделить в запросе пользователя в поисковой системе определенные смысловые части, такие как тип продукта, цвет, бренд (NLP-задания). Также в эту группу входят задачи по проверке автоматического перевода, модерации контента, разметке видео или сегментации объектов на изображениях.\\nВ качестве примера рассмотрим задачи по сегментации изображений. Как правило, они нужны для обучения алгоритмов компьютерного зрения. Они используются, например, для создания беспилотного транспорта, который должен распознавать всевозможные препятствия на дорогах: людей, светофоры, разметку, дорожные знаки, дома, заборы, искусственные неровности и т. д. Чтобы эти модели были качественными и могли без труда распознавать любые объекты на своем пути, им нужно показать большое количество изображений и в, более сложных случаях, видео с выделенными на них объектами разных классов.\\nВыделением этих объектов занимаются пользователи краудсорсинговых платформ. На 2D и 3D изображениях, а также видео, снятых во время движения с помощью камер, радаров и лидаров, они находят нужные объекты и обводят их. Изображения и видео, размеченные по требованиям инструкции, используются для обучения моделей компьютерного зрения.\\nСамый простой пайплайн задачи по сегментации изображений для беспилотных автомобилей состоит из трех проектов (рис. 1). В первом проекте исполнители отвечают на вопрос, есть ли на фото нужные объекты (например, дорожные знаки). Те изображения, на которых эти объекты есть, перенаправляются в проект номер два. В нем вторая группа исполнителей обводит дорожные знаки с помощью прямоугольников. Эту разметку проверяет еще одна группа исполнителей в следующем проекте, третьем по счету. Далее включается схема так называемой отложенной приёмки заданий. В случае отклонения задание отправляется на повторную разметку. Верно выполненная работа включается в итоговый датасет.\\n\\n\\n\\n    Пайплайн проекта по разметке данных для обучения модели компьютерного зрения\\n  \\n\\nПодобные пайплайны, но еще более многоступенчатые, используются для обучения моделей компьютерного зрения Яндекса. В январе 2020 года инженерам компании удалось продемонстрировать одну из моделей на конференции Consumer Electronics в Лас-Вегасе. Беспилотные автомобили со встроенной моделью проследовали по маршруту с разными дорожными сценариями: нерегулируемыми перекрестками, сложными поворотами со встречным разъездом, пешеходными переходами и многополосными участками. Всего эти автомобили преодолели более 7\\u202fтысяч км.\\nСбор данных\\nСуть задач, связанных со сбором контента, заключается в поиске материалов (изображений, фотографий, фактов), необходимых для решения проблемы. Например, используя краудсорсинг, инженеры собирают фразы для обучения голосового помощника (рис. 2).\\nПайплайн такого проекта выглядит довольно просто: исполнители записывают необходимую фразу, например, «Привет, Алиса», и загружают ее в интерфейс задания на краудсорсинговой платформе. Далее другая группа исполнителей проверяет эти записи на предмет ошибок и других требований: если запись соответствует инструкции, вторая группа подтверждает ее, а если в записи допущены ошибки, отклоняет. В следующем проекте еще одна группа исполнителей записывает недостающие фразы, затем они вновь проходят проверку. Этот процесс повторяется по кругу, пока не будет собрано достаточное количество фраз нужного качества.\\n\\n\\n\\n    Пайплайн проекта по сбору данных для обучения модели распознавания голоса\\n  \\n\\nКраудсорсинговые платформы\\nМасштабируемость и скорость выполнения задач по разметке данных напрямую зависят от доступа заказчика к большому облаку исполнителей. Залог успеха здесь — использование открытых краудсорсинговых платформ, которые позволяют постоянно пополнять это облако и, следовательно, масштабировать процессы сбора или разметки данных.\\nОткрытые краудсорсинговые платформы — например, Amazon Mechanical Turk или Толока — работают по принципу маркетплейсов. Заказчик может создать на такой платформе свой проект, найти для него нужных исполнителей, обучить их и поручить им выполнение задания, контролируя качество результата. Пользователи открытой платформы, в свою очередь, могут выбрать интересующий их проект, выполнить задания и получить за проделанную работу вознаграждение. Свой выбор проекта они могут сделать как на основе рейтинга проекта, так и с учетом итогового вознаграждения — либо просто потому, что какая-то задача им интересна больше других.\\nОткрытые краудсорсинговые платформы — инструмент для тех, кто планирует самостоятельно контролировать разметку данных. А это, как правило, большинство проектов в сфере AI и машинного обучения. Для ML-разработчиков крайне важно, чтобы кропотливая работа по написанию инструкций, проектированию интерфейсов, отбору и обучению участников, настройке контроля качества была выполнена в точности так, как это запланировано в пайплайне проекта. Все эти шаги напрямую влияют на качество тренировочных данных, а от них в немалой степени зависит успех продукта.\\nПри выборе краудсорсинговой платформы важно учесть и то, какими инструментами они располагают. Например, с готовыми шаблонами можно быстрее спроектировать интерфейс задания, а инструменты контроля качества помогут отсеять роботов и недобросовестных исполнителей. Кроме того, выбор платформы во многом определит то, с какими исполнителями будет вестись работа. Изучение их характеристик даст понимание, в каких странах они проживают, на каких языках разговаривают и, что немаловажно, сталкивались ли они с проектами, подобными тому, над которым планируется работа.\\nАльтернативой платформам-маркетплейсам могут стать проекты, которые предлагают готовые датасеты и помощь в разметке данных для проекта. Это, например, Scale AI, Hive Data, Alegion. Такие платформы подойдут не всем — выше уже шла речь о том, что некоторые проекты (как, например, обучение алгоритмов компьютерного зрения) нуждаются в специфическом контексте для сбора датасета.\\nКроме того, построенные по общим принципам краудсорсинга проекты могут запускаться и на внутреннее облако исполнителей, связанных с компанией какими-либо договорными отношениями. Это важно в случаях, если речь идет о разметке чувствительных данных. Однако такой процесс тяжело поддается масштабированию, потому что требует больших ресурсов для сопровождения сотрудников.\\nГраницы применимости краудсорсинга\\nНесмотря на все многообразие задач, которые можно решить с помощью краудсорсинга, есть случаи, когда его применение затруднено либо просто нецелесообразно.\\nВо-первых, необходимо оценить затраты, сопутствующие запуску проекта. Создание и настройка эффективного пайплайна для сбора или обработки данных требуют времени и квалификации высокоуровневого специалиста. Потраченный им ресурс может не окупиться, если требуется лишь один раз разметить небольшое количество данных.\\nОблаку исполнителей с трудом поддаются задачи, требующие серьезного включения и поддержания контекста. Секрет краудсорсинга — в создании небольших автономных заданий, каждое из которых может быть решено согласно несложной инструкции. Если исполнителю требуется учитывать большой объем сопутствующей информации, чтобы выполнить задачу верно — скорее всего, ее лучше выполнять без использования краудсорсинга. Например, облако исполнителей вряд ли сможет осуществить перевод книги: ее не стоит разбивать на отдельные предложения, ведь перевод должен быть последовательным и согласованным. В то же время, краудсорсинг может помочь при переводе отдельных фраз в конечном контексте, например, отдельных реплик для голосового ассистента.\\nНаконец, если задача требует крайне специфических навыков, то поиск или обучение подходящего исполнителя на краудсорсинговой платформе сравнится с наймом эксперта. В таких случаях стоит оценить возможность декомпозиции задачи так, чтобы она оказалась разбита на ряд менее сложных действий. Если сделать это невозможно (например, для выполнения задания требуется знание редкого языка), оптимальным способом поиска исполнителя могут стать профессиональные сообщества.\\nЭтапы создания краудсорсингового проекта\\nТипичная краудсорсинговая задача состоит из шести этапов:\\n\\nДекомпозиция;\\nИнструкция и интерфейс;\\nКонтроль качества;\\nОтбор и обучение исполнителей;\\nВыбор схемы оплаты и бонусирования;\\nАгрегация ответов.\\n\\nРазберем каждый из этапов на примере уже упомянутого проекта по сбору данных для обучения беспилотных автомобилей. Мы запустим этот проект на краудсорсинговой платформе «Толока».\\nДекомпозиция\\nВ качестве исходных данных возьмем объемный набор фотографий с изображением улиц. После запуска краудсорсингового проекта мы должны получить те же изображения, но с выделенными на них дорожными знаками. Наша задача — выделить прямоугольниками дорожные знаки на каждой фотографии.\\nПример того, как должен выглядеть итоговый датасет с выделенными на них объектами приведен на рисунке 3.\\n\\n\\n\\n    Изображение с выделенными на нем дорожными знаками с помощью полигонов\\n  \\n\\nМожем ли мы поручить нашу задачу участникам краудсорсинговой платформы напрямую? В данном случае — нет. Изображения для разметки могут полностью не соответствовать нашему запросу. Например, на изображениях может не быть нужных объектов. Некоторые фотографии могут не загрузиться в интерфейсе (появится ошибка). Чтобы избежать подобных ситуаций, нам нужно отобрать фотографии с подходящими объектами. Отбор фото или их фильтрация станет первой микрозадачей или первым пулом (так называется набор заданий в рамках проекта на платформе «Толока») нашего проекта.\\nЧто дальше? Когда мы получили фотографии с дорожными знаками, мы сможем запустить проект по выделению объектов на изображениях. Наша задача — выделить на фотографиях все дорожные знаки прямоугольниками. Чтобы создать подобное задание на краудсорсинговой платформе «Толока», можно воспользоваться готовым шаблоном. Он предусматривает специальный инструмент, «полигон», который с легкостью позволяет выполнять подобные задания.\\nНа этом мы могли бы остановиться. Получили изображения с выделенными объектами — задача выполнена. Однако для данного проекта потребуется запустить еще одно микрозадание. Фотографии с выделенными объектами необходимо проверить. Кто-то из исполнителей может пропустить некоторые знаки или выделить их неверно. Таким образом, проверка размеченных изображений в конкретном проекте необходима. Но специфика задачи такова, что мы не можем просто сравнить работу отдельного исполнителя с заведомо верным примером: выделенные области могут отличаться на несколько пикселей, но это не будет означать, что ответ неверен.\\nИтак, что мы делаем? Мы создаем новый пул заданий, в котором спрашиваем «Верно ли выделены объекты на фото?». Участники отвечают на вопрос, после чего фото с верно отмеченными объектами отправляются в итоговый датасет и оплачиваются. Фото с неверно выделенными объектами отклоняются и не оплачиваются. Все фотографии, которые не проходят проверку, отправляются на переразметку (т. е. размечаются повторно).\\nКакие выводы мы можем сделать по итогу разбора декомпозиции проекта? Самый главный вывод — решение о декомпозиции задачи следует принимать, исходя из типа задачи и данных, которые есть на входе — это могут быть изображения, видео, ссылки, точки на карте, координаты этих точек. Также следует различать типичные случаи, в которых декомпозиция особенно рекомендована для проекта. Речь идет об объемных проектах, многослойных задачах, задачах со множеством вариантов ответов и объемных процессах:\\n\\nОбъемные проекты. Если в рамках проекта нужно ответить на несколько вопросов, то лучше сделать это поочередно или в выбранной последовательности.\\nМногослойные задачи. Если в рамках одной задачи нужно выполнить более одного действия (например, отнести объект к определенной группе и ответить на вопрос, предназначен ли он только для взрослых), то лучше сделать это поочередно или в выбранной последовательности.\\nЗадачи со множеством вариантов ответов. Если в задании есть один вопрос и 10 и более вариантов ответа, то лучшим решением будет группировка ответов по темам, а затем создание отдельного проекта для каждой группы ответов.\\nОбъемные процессы. Если задача включает сложные механизмы контроля качества и отложенную проверку, необходимо создать отдельный проект, в котором одна группа исполнителей будет проверять другую.\\n\\nЕсть ли случаи, когда декомпозировать задачу не нужно? Да. Нет необходимости разбивать задачу на части, если соблюдаются два критерия: инструкции к задаче помещаются на половине листа бумаги формата А4, или задача выполнена с помощью одного действия, например, выбора из нескольких категорий.\\nИнструкция\\nПосле декомпозиции нашего проекта нам необходимо создать для него инструкцию. Инструкция потребуется для каждой микрозадачи. В нашем случае нам необходимо создать три инструкции.\\nКакие пункты мы обязательно в них укажем?\\nПервым пунктом инструкции станет описание задачи. В нем мы объясним участнику, что предстоит сделать и где будет использован результат этой работы. Например:\\n\\n    Вашему вниманию представлен проект, результаты которого помогут сделать беспилотные автомобили безопасным транспортом. Ваша задача — определить, есть ли дорожные знаки на изображении. Выберите ответ «Да», если изображение содержит дорожные знаки. Выберите ответ «Нет», если на изображении дорожных знаков нет. На изображении, представленном ниже, есть несколько дорожных знаков. Значит, правильный ответ — «Да». \\n\\nДалее, мы подробно опишем условия входа в задание: расскажем, будет ли обучение и экзамен, с каким качеством его нужно пройти, есть ли в проекте повторный экзамен для тех, кто не прошел испытание с первого раза. Также опишем ценообразование. Например:\\n\\n    Чтобы выполнить это задание, вам потребуется пройти обучение на тренировочном пуле. В тренировочный пул войдут задания аналогичные тем, что будут в основном проекте. После обучения мы предложим вам пройти экзамен. В экзамен войдут 5 изображений.\\nСледующий элемент инструкции — технические нюансы. Здесь мы расскажем, с какого устройства потребуется выполнить задание — со смартфона или с компьютера — и какие дополнительные настройки браузера будут необходимы. Этот пункт в особенности важен для второго задания в рамках нашего проекта. Разметить дорожные знаки прямоугольниками участники смогут только с компьютера:\\n\\n    Мы рекомендуем выполнять это задание с персонального компьютера. Это необходимо, чтобы вы смогли корректно выделить все необходимые объекты на изображении.\\nКраткое описание интерфейса задания — еще один важный пункт в инструкции. Для большей наглядности мы сделаем скриншот с комментариями о том, для чего нужны те или иные блоки и кнопки. Если в задании простой интерфейс, эту часть можно пропустить. Например:\\n\\n    Используйте желтый квадрат («полигон») в левой части экрана, чтобы выделять дорожные знаки на изображении.\\nТеперь о самом задании. Чтобы избежать ошибок, мы пошагово опишем все частые сценарии, которые могут случиться при выполнении наших задач. Также мы укажем, что делать с нестандартными случаями. Добавим примеры: несколько кейсов сделают теорию намного понятнее. Справочные материалы — глоссарий, faq — важное дополнение к этим сценариям. Наконец, мы расскажем, куда направлять вопросы по заданию или проекту в целом.\\nНа что мы обратим внимание при написании текста?\\nПервое, за чем стоит проследить — сам язык, которым написана инструкция. Мы откажемся от профессионального сленга и не будем использовать терминологию. Некоторые термины, например, «полигоны», мы объясним или заменим синонимами — «прямоугольники». Наша задача — сделать инструкцию простой и понятной для большого числа участников. Следуя этой же задаче, мы упростим стиль и синтаксис (одна мысль = одно предложение; одна тема = один абзац), не будем использовать пояснения в скобках и сделаем форматирование единообразным.\\nГотовый текст инструкции мы обязательно проверим, выполнив некоторое количество заданий. Такое упражнение быстро покажет, какие случаи еще не описаны в инструкции, а какие описаны мало. Кроме того, оно позволит проверить как выглядит наше задание на разных устройствах: умещаются ли все картинки и скриншоты на экранах мобильного телефона, планшета и компьютера.\\nВ итоге каждая инструкция не займет больше двух экранов. Это максимальное количество пространства для инструкции, за пределы которого лучше не выходить. Если инструкция все же не вписывается в такой объем, вероятно, задача слишком многосоставная и ее нужно декомпозировать.\\nАгрегация результатов\\nПредставим, что мы запустили наш проект и получили необходимые данные. В краудсорсинговых проектах данные обычно собираются в перекрытии (мнения большинства) — это один из распространенных механизмов контроля качества исполнителей и улучшения качества итогового набора данных. Но как выбрать из нескольких оценок финальную?\\nВ данном случае нам помогут механизмы агрегации данных. Что они делают? Они обрабатывают файлы с ответами исполнителей и выбирают из нескольких ответов тот, который с наибольшей вероятностью окажется верным. Рассмотрим принцип работы механизмов агрегации данных на примере первого пула с заданиями (см. рис. 4).\\nУ нас есть набор изображений, и наша цель — отнести каждое изображение к группе «изображения с дорожными знаками» или к группе «изображения без дорожных знаков». В соответствии с принципом краудсорсинга задание должно быть распределено между несколькими исполнителями, каждый из которых размечает некое подмножество изображений. В результате для каждого изображения у нас есть несколько результатов разметки. Цель метода агрегации — объединить эти результаты в один качественный ответ.\\n\\n\\n\\n    Агрегация данных, полученных с помощью краудсорсинга\\n  \\n\\nМнение большинства\\nАлгоритм агрегации данных «Мнение большинства» основан на предположении, что правильный ответ — этот тот, который выбирают большинство исполнителей (рис. 5). Самый популярный ответ становится финальным ответом.\\n\\n\\n\\n    Агрегация данных по методу, основанному на мнении большинства\\n  \\n\\nПрактика показывает, что при помощи метода, основанного на мнении большинства, можно получить достойные результаты. Поэтому этот метод с успехом применяется во многих проектах. Также одно из преимуществ этого метода заключается в том, что он весьма нагляден и логика его работы понятна. Однако в проектах краудсорсинга существуют определенные временные и бюджетные ограничения. Наша цель в том, чтобы собрать минимальный объем данных, необходимый для достижения желаемой точности. С этой точки зрения, метод, основанный на мнении большинства, далеко не всегда будет оптимальным выбором. Чтобы осознать слабые стороны метода, рассмотрим его модель.\\nМодель\\nМодель, лежащая в основе метода, проста. Есть N изображений и M исполнителей. Каждое изображение j∈1,...,Nj \\\\in {1, ..., N}j∈1,...,N подразумевает некий неизвестный ответ («изображения с дорожными знаками» или «изображения без дорожных знаков» в нашем случае). При использовании модели, основанной на мнении большинства, предполагается, что если исполнитель iii разметил изображение jjj, его ответ является правильным с некоторой вероятностью p>12p > \\\\frac{1}{2}p>21\\u200b\\nP(Исполнитель\\u2009i\\u2009отвечает\\u2009на\\u2009вопрос\\u2009j\\u2009верно)=pP(Исполнитель\\\\,i\\\\,отвечает\\\\,на\\\\,вопрос\\\\,j\\\\,верно) = p\\nP(Исполнительiотвечаетнавопросjверно)=pПри этом вероятность правильного ответа полагается одинаковой для каждого исполнителя и вопроса. Допущение, что p>1/2p > 1/2p>1/2 учитывает, что для каждого исполнителя вероятность правильного ответа выше, чем неправильного. В таком случае, поскольку число разметок для каждого изображения достаточно велико, мнение большинства с высокой вероятностью даст истинные ответы.\\nОграничения\\nВ силу своей простоты, метод основанный на мнении большинства имеет ряд ограничений:\\n\\n**Однородность исполнителей.**Во-первых, данный метод предполагает, что все исполнители обладают одинаковыми способностями. Иными словами,для каждого конкретного вопроса вероятность того, что исполнитель правильно ответит на вопрос, одинакова для всех исполнителей. Однако на практике пул исполнителей на краудсорсинговых платформах чрезвычайно разнообразен: кто-то из них очень аккуратно и скрупулезно выполняет задачи, а кто-то небрежен и чаще допускает ошибки. Таким образом, одно из направлений совершенствования модели, основанной на мнении большинства, — это учет различия в способностях исполнителей в рамках модели.\\n**Однородность вопросов.**Во-вторых, модель, основанная на мнении большинства, предполагает, что вопросы имеют одинаковую сложность. Другими словами, вероятность того, что исполнитель правильно ответит на вопрос, одинакова для всех вопросов. Однако некоторые вопросы в рамках проекта могут быть сложнее других. Таким образом, еще одно направление по улучшению модели на основании мнения большинства — это учесть в модели разную степень сложности вопросов.\\n\\nДалее мы рассмотрим оба направления развития модели и расскажем о других алгоритмах, учитывающих особенности краудсорсинговых заданий.\\nАгрегация с учетом способностей исполнителей\\nРассмотрим модель, которая учитывает неоднородность исполнителей при агрегации ответов.\\nМодель\\nЕстественный способ учесть различия в способностях исполнителей — ввести параметр качества для каждого исполнителя. Если есть MMM исполнителей, то мы можем связать каждого исполнителя i∈1,...,Mi \\\\in {1, ..., M}i∈1,...,M с неизвестным параметром качества pi∈[0,1]p_i \\\\in [0, 1]pi\\u200b∈[0,1]. Чем выше параметр качества исполнителя, тем больше вероятность того, что исполнитель ответит на вопрос правильно:\\nP(Исполнитель\\u2009i\\u2009отвечает\\u2009на\\u2009вопрос\\u2009j\\u2009верно)=piP(Исполнитель\\\\,i\\\\,отвечает\\\\,на\\\\,вопрос\\\\,j\\\\,верно) = p_i\\nP(Исполнительiотвечаетнавопросjверно)=pi\\u200bДругими словами, вероятность того, что исполнитель правильно ответит на вопрос, своя для каждого исполнителя (но от вопроса она все еще не зависит).\\nМетоды\\nВ ситуации, когда у исполнителей разные способности, логично присваивать больший вес ответам более сильных исполнителей и меньший вес — ответам более слабых. Однако проблема в том, что параметры качества для исполнителей априори нам не известны. Основная идея двух методов модели агрегации данных с учетом способностей исполнителей заключается том, чтобы одновременно оценить параметры качества для исполнителей и ответы на поставленные вопросы. Рассмотрим каждый их них.\\nИспользование большого объема контрольных заданий\\nКонтрольные вопросы (также honeypots, golden sets) — это задания, на которые заказчик заранее знает правильные ответы. На практике мы часто добавляем в набор данных определенное количество контрольных вопросов, чтобы контролировать качество работы исполнителей. Когда этих вопросов достаточно много, мы можем использовать их для оценки качества работы. Предположим, что у нас есть GGG контрольных вопросов и некий исполнитель iii, который правильно ответил на kik_iki\\u200b вопросов из GGG контрольных вопросов. Тогда мы можем оценить параметр качества для исполнителя следующим образом:\\npi^=kiG\\\\hat{p_i} = \\\\frac{k_i}{G}\\npi\\u200b^\\u200b=Gki\\u200b\\u200bТеперь, когда у нас есть оценка параметра качества, мы можем оценить ответ каждого исполнителя по-разному. Эта идея подводит нас к концепции взвешенного мнения большинства (от англ. Weighted majority vote).\\nИдея этого метода проиллюстрирована на рисунке ниже (рис. 6). Предположим, что у нас есть нестандартное изображение, на котором столб похож на дорожный знак. В этом случае модель, основанная на простом мнении большинства, не делает отличия между ответами исполнителей с меньшими способностями (первых двух исполнителей) и ответами исполнителя-эксперта (последнего исполнителя) и допускает ошибку. Напротив, модель взвешенного мнения большинства дополнительно взвешивает каждый ответ полученным коэффициентом качества исполнителя. Такая модель приводит к правильному ответу, поскольку мнение исполнителя-эксперта в таком случае перевешивает мнения двух других исполнителей.\\n\\n\\n\\n    Агрегация данных по методу, основанному на взвешенном мнении большинства\\n  \\n\\nКогда контрольных вопросов не так много\\nМетод взвешенного мнения большинства подходит для тех случаев, когда в проекте есть достаточное количество контрольных заданий, необходимых для оценки качества работы исполнителя. Однако зачастую контрольных заданий в проекте не хватает, в связи с чем оценки могут быть довольно неточными. Кроме того, исполнители могут коллективно выявить контрольные вопросы и начать обманывать систему, давая правильные ответы на контрольные вопросы и случайные ответы на другие. В этом случае, чтобы оценить параметры качества исполнителей при ответе на неизвестные вопросы, мы можем использовать метод Дэвида — Скина:\\nМетод Дэвида-Скина (Dawid, Skene, 1979)\\nМетод Дэвида-Скина одновременно находит значения качества исполнителей и ответы на вопросы, которые согласуются с наблюдаемыми данными в наибольшей степени.\\nМы имеем в качестве данных nikun_{ik}^uniku\\u200b — количество раз, при которых разметчик u∈Uu \\\\in Uu∈U поставил класс k∈Kk \\\\in Kk∈K объекту i∈Ii \\\\in Ii∈I (возможно, разметчик видел этот объект несколько раз). Обозначим через\\nYik=I{объект\\xa0i\\xa0класса\\xa0k},Y_{ik} = I\\\\{\\\\text{объект } i \\\\text{ класса } k\\\\},\\nYik\\u200b=I{объект\\xa0i\\xa0класса\\xa0k},это наши латентные величины.\\nВ качестве параметров имеем\\n\\nπkℓu\\\\pi_{k\\\\ell}^uπkℓu\\u200b — вероятность того, что разметчик uuu поставил класс ℓ\\\\ellℓ вместо правильного класса kkk.\\nρk\\\\rho_kρk\\u200b — вероятность класса kkk.\\n\\nПримем также обозначения:\\n\\nNi={niku\\xa0по\\xa0всем\\xa0u\\xa0и\\xa0k\\xa0для\\xa0объекта\\xa0i}N_i = \\\\{n^u_{ik}\\\\text{ по всем } u \\\\text{ и } k \\\\text{ для объекта } i \\\\}Ni\\u200b={niku\\u200b\\xa0по\\xa0всем\\xa0u\\xa0и\\xa0k\\xa0для\\xa0объекта\\xa0i},\\nNiu={niku\\xa0для\\xa0разметчика\\xa0u\\xa0и\\xa0объекта\\xa0i}N_i ^ u = \\\\{n^u_{ik} \\\\text{ для разметчика } u \\\\text{ и объекта i}\\\\}Niu\\u200b={niku\\u200b\\xa0для\\xa0разметчика\\xa0u\\xa0и\\xa0объекта\\xa0i},\\nYi={Yik\\xa0по\\xa0всем\\xa0k\\xa0для\\xa0объекта\\xa0i}Y_i = \\\\{Y_{ik} \\\\text{ по всем } k \\\\text{ для объекта } i\\\\}Yi\\u200b={Yik\\u200b\\xa0по\\xa0всем\\xa0k\\xa0для\\xa0объекта\\xa0i}.\\n\\nПоймём, какой будет функция неполного правдоподобия в этой задаче. Прежде всего,\\npπ,p(N,Y)=∏i∈Ip(Ni,Yi),p_{\\\\pi,p}(N, Y) = \\\\prod_{i\\\\in I}p(N_i, Y_i),\\npπ,p\\u200b(N,Y)=i∈I∏\\u200bp(Ni\\u200b,Yi\\u200b),Если kkk – номер класса iii-го объекта, то\\np(Ni,Yi)=p(объект\\xa0i\\xa0класса\\xa0k)⏟=ρkp(Ni∣объект\\xa0i\\xa0класса\\xa0k)p(N_i, Y_i)=\\\\underbrace{p(\\\\text{объект $i$ класса $k$})}_{=\\\\rho_k}p(N_i\\\\mid\\\\text{объект $i$ класса $k$})\\np(Ni\\u200b,Yi\\u200b)==ρk\\u200bp(объект\\xa0i\\xa0класса\\xa0k)\\u200b\\u200bp(Ni\\u200b∣объект\\xa0i\\xa0класса\\xa0k)(значения YitY_{it}Yit\\u200b однозначно определяются номером истинного класса, поэтому справа YiY_iYi\\u200b пропадает). Далее, мы считаем, что разметчики действуют независимо, поэтому\\np(Ni∣объект\\xa0i\\xa0класса\\xa0k)=∏u∈Up(Niu∣объект\\xa0i\\xa0класса\\xa0k).p(N_i\\\\mid\\\\text{объект $i$ класса $k$}) = \\\\prod_{u\\\\in U}p(N_i^u\\\\mid\\\\text{объект $i$ класса $k$}).\\np(Ni\\u200b∣объект\\xa0i\\xa0класса\\xa0k)=u∈U∏\\u200bp(Niu\\u200b∣объект\\xa0i\\xa0класса\\xa0k).Разберёмся с величиной p(Ni∣объект\\xa0i\\xa0класса\\xa0k)p(N_i\\\\mid\\\\text{объект i класса k})p(Ni\\u200b∣объект\\xa0i\\xa0класса\\xa0k). Она отвечает за то, какие классы uuu-й разметчик ставил iii-му объекту. Мы считаем, что встречи разметчика с объектом упорядочены по времени, тогда\\np(u-й\\xa0разметчик\\xa0отнёс\\xa0i-й\\xa0объект\\xa0к\\xa0классам\\xa0k1′,…,kr′∣объект\\xa0i\\xa0класса\\xa0k)=p(\\\\text{$u$-й разметчик отнёс $i$-й объект к классам $k'_1,\\\\ldots,k'_r$}\\\\mid\\\\text{объект $i$ класса $k$}) =\\np(u-й\\xa0разметчик\\xa0отнёс\\xa0i-й\\xa0объект\\xa0к\\xa0классам\\xa0k1′\\u200b,…,kr′\\u200b∣объект\\xa0i\\xa0класса\\xa0k)==∏sp(в\\xa0s-ю\\xa0встречу\\xa0с\\xa0i-м\\xa0объектом\\xa0u-й\\xa0разметчик\\xa0отнёс\\xa0его\\xa0к\\xa0классу\\xa0ks′∣объект\\xa0i\\xa0класса\\xa0k)\\\\begin{matrix}=\\\\prod_{s}p(\\\\text{в $s$-ю встречу с $i$-м объектом $u$-й разметчик отнёс его к классу $k'_s$}\\\\mid \\\\\\\\ \\\\text{объект $i$ класса $k$})\\\\end{matrix}\\n=∏s\\u200bp(в\\xa0s-ю\\xa0встречу\\xa0с\\xa0i-м\\xa0объектом\\xa0u-й\\xa0разметчик\\xa0отнёс\\xa0его\\xa0к\\xa0классу\\xa0ks′\\u200b∣объект\\xa0i\\xa0класса\\xa0k)\\u200bЭту вероятность можно переписать в виде\\n∏ℓ∈K(πkℓu)niℓu,\\\\prod_{\\\\ell \\\\in K} \\\\left( \\\\pi_{k\\\\ell}^u \\\\right)^{n_{i\\\\ell}^u},\\nℓ∈K∏\\u200b(πkℓu\\u200b)niℓu\\u200b,а итоговое неполное правдоподобие предстаёт в виде\\npπ,p(N,Y)=∏i∈I∏k∈K(ρk∏u∈U∏ℓ∈K(πkℓu)niℓu)Yikp_{\\\\pi,p}(N, Y) = \\\\prod_{i\\\\in I}\\\\prod_{k \\\\in K} \\\\left( \\\\rho_k \\\\prod_{u\\\\in U} \\\\prod_{\\\\ell \\\\in K} \\\\left( \\\\pi_{k\\\\ell}^u \\\\right)^{n_{i\\\\ell}^u} \\\\right)^{Y_{ik}}\\npπ,p\\u200b(N,Y)=i∈I∏\\u200bk∈K∏\\u200b(ρk\\u200bu∈U∏\\u200bℓ∈K∏\\u200b(πkℓu\\u200b)niℓu\\u200b)Yik\\u200bЕго нам нужно максимизировать по π\\\\piπ и ρ\\\\rhoρ\\nПояснение к формуле:\\nВне больших скобок фиксируются объект и его класс, сама скобка возводится в степень 1, если рассматривается правильный класс объекта, и в степень 0 иначе. Внутри сначала записана вероятность того, что объект имеет данный класс, а затем — перебор по всем пользователям и всем классам, которые мог поставить данный пользователь. Наконец, записывается вероятность того, что пользователь нашему объекту поставил некоторый класс, которая возводится в степень того, сколько раз он поставил этот класс. Например, если пользователь видел изображение котика 5 раз, при этом 3 раза он сказал, что котик, а два раза — песик, то вероятность πcat,catu\\\\pi_{cat,cat}^uπcat,catu\\u200b для данного котика учтется 3 раза, а вероятность πcat,dogu\\\\pi_{cat,dog}^uπcat,dogu\\u200b — 2 раза.\\nРассмотрим концепцию метода Дэвида-Скина на простом примере (рис. 7).\\nПредположим, что у нас есть только N=4N = 4N=4 вопросов и M=3M = 3M=3 исполнителей. Каждый исполнитель отвечает на все вопросы. В этом случае наблюдаемые данные — это ответы исполнителей на вопросы.\\n\\n\\n\\n    Агрегация данных по методу Дэвида-Скина\\n  \\n\\nДавайте разберемся в том, каким образом метод Дэвида — Скина позволяет найти параметры качества для исполнителей и те ответы на вопросы, которые лучше всего соответствуют наблюдаемым данным. Для этого рассмотрим два варианта, показанные на картинках ниже (см. рис. 7.1). Каждая картинка предполагает свой набор параметров. Посмотрим, какой из предложенных вариантов лучше соответствует наблюдаемым данным.\\n\\n\\n\\n\\n    Агрегация данных по методу Дэвида-Скина\\n  \\n\\nВо-первых, обратите внимание, что на обоих изображениях предложенные ответы согласуются с ответами исполнителя, у которого, по оценкам, высокий параметр качества. Но какой выбор параметров подходит данным лучше всего? Чтобы ответить на этот вопрос, обратите внимание, что ответы второго и третьего исполнителей полностью совпадают. Если параметры качества для этих исполнителей соответствуют первой картинке p2^=p3^=0.5\\\\hat{p_2} = \\\\hat{p_3} = 0.5p2\\u200b^\\u200b=p3\\u200b^\\u200b=0.5, тогда, если верить этой модели, эти два исполнителя отвечают наугад. В таком случае высокая степень согласия между исполнителями нас бы скорее удивила, поскольку отвечая наугад, они должны время от времени расходиться в своих ответах. Напротив, если исполнители 2 и 3 — эксперты, как на втором изображении p2^=p3^=1\\\\hat{p_2} = \\\\hat{p_3} = 1p2\\u200b^\\u200b=p3\\u200b^\\u200b=1, тогда мы ожидаем, что у них будет высокая степень согласия, и это то, что мы видим в данных. Интуитивно, второй набор параметров лучше согласуется с наблюдаемыми данными. Приведенный простой пример показывает, что концепция согласованности между потенциальными параметрами и наблюдаемыми данными позволяет нам исключить те варианты, которые плохо согласуются с наблюдаемыми данными.\\nОба метода — взвешенное мнение большинства и агрегация по методу Дэвида — Скина — входят в стандартный функционал Толоки. В двух наших пулах, в первом и третьем, мы будем использовать метод Дэвида — Скина. Он позволит нам получить наиболее точные данные для нашего проекта. Подробнее узнать о том, как получить агрегированные результаты из размеченного пула, можно в документации.\\nАгрегация с учетом сложности вопросов\\nМетод Дэвида-Скина и метод, основанный на мнении взвешенного большинства, — основа современного краудсорсинга. Многие создатели проектов повышают качество данных, используя эти методы агрегации. Однако существуют и другие современные подходы. Например, есть группа подходов, которые учитывают сложность вопроса при агрегировании ответов.\\nПараметрический подход\\nАналогично тому, как мы замеряли качество для каждого исполнителя, вводя параметр качества pip_ipi\\u200b, точно так же для каждого исполнителя мы можем ввести параметр сложности djd_jdj\\u200b для каждого вопроса. Тем не менее, главная проблема заключается в том, как описать взаимодействие между качеством исполнителя и сложностью вопроса, и в результате рассчитать вероятность того, что конкретный исполнитель правильно ответит на выбранный вопрос. В работе Уайтхилла с соавторами (2009) предлагается следующее решение.\\n\\nВо-первых, параметр качества для исполнителя, который раньше мерился в диапазоне [0,1][0, 1][0,1], теперь задается в интервале (−∞,∞)(-\\\\infty, \\\\infty)(−∞,∞). В частности, возможно нулевое качество p=0p = 0p=0, которое соответствует ситуации, когда исполнитель отвечает на все вопросы наугад. Положительные значения качества подразумевают, что работник с большей вероятностью даст правильный ответ, а отрицательные значения означают, что исполнитель настроен враждебно и с большей вероятностью даст неправильный ответ.\\nВо-вторых, для параметра сложности каждого вопроса d∈(0,∞)d \\\\in (0, \\\\infty)d∈(0,∞) также может быть дана интуитивная интерпретация: низкая сложность вопроса (d≈0)(d \\\\approx 0)(d≈0) означает что вопрос настолько прост, что любой исполнитель ответит на него правильно с вероятностью, близкой к 1. Чем выше уровень сложности, тем меньше вероятность того, что конкретный исполнитель ответит на вопрос правильно.\\n\\nОбъединив эти параметры, модель предполагает, что вероятность для конкретного исполнителя iii при ответе на конкретный вопрос jjj может быть корректно описана следующим параметрическим выражением:\\nP(Исполнитель\\u2009i\\u2009отвечает\\u2009на\\u2009вопрос\\u2009j\\u2009верно)=1exp\\u2061(−pidj)P(Исполнитель\\\\,i\\\\,отвечает\\\\,на\\\\,вопрос\\\\,j\\\\,верно) = \\\\frac{1}{\\\\exp{\\\\left(-\\\\frac{p_i}{d_j}\\\\right)}}\\nP(Исполнительiотвечаетнавопросjверно)=exp(−dj\\u200bpi\\u200b\\u200b)1\\u200bСледует заметить, что в таком случае вероятность является функцией и самого исполнителя, и вопроса, на который исполнитель отвечает. Как только мы выбрали параметрическое уравнение для описания взаимосвязи между уровнем качества исполнителя и сложностью вопроса, с одной стороны, и вероятностью правильного ответа, с другой, мы можем применять все те же принципы, что и для расчета параметров по модели Дэвида – Скина. Таким образом мы можем оценить не только параметры модели, но и полученные ответы на вопросы. Более подробно об этом можно почитать в статье.\\nНесмотря на то, что параметрические модели позволяют делать весьма эффективные выводы, в них неизбежно заложены сильные допущения о когнитивных процессах, присущих исполнителям при ответе на вопросы. Эти допущения обычно невозможно проверить, поэтому неясно, насколько хорошо они согласуются с реальностью. Соответственно, если допущения параметрической модели неверны, то и методы, используемые такой моделью, могут дать неожиданные результаты. Это подводит нас к идее непараметрического подхода, где можно попробовать избежать сильных допущений о мыслительных процессах.\\nНепараметрический подход\\nНепараметрический подход предложил Нихар Б. Шах с коллегами в 2016 году. Вместо моделирования вероятностей, что исполнитель iii верно ответит на вопрос jjj, считается, что между этими вероятностями есть взаимосвязь. При этом модель использует два ключевых допущения:\\n\\nВо-первых, предполагается, что исполнителей можно выстроить в ряд в порядке возрастания способностей. Если исполнитель i1i_1i1\\u200b занимает в этом ряду более высокую позицию, чем исполнитель i2i_2i2\\u200b, то при ответе на каждый вопрос исполнитель i1i_1i1\\u200b с большей вероятностью даст правильный ответ, чем исполнитель i2i_2i2\\u200b.\\nВо-вторых, предполагается, что вопросы можно выстроить в\\xa0ряд в\\xa0зависимости от\\xa0их\\xa0сложности. Если вопрос j1j_1j1\\u200b сложнее вопроса j2j_2j2\\u200b, то\\xa0любой исполнитель совершит ошибку при ответе на\\xa0вопрос j1j_1j1\\u200b с\\xa0не\\xa0меньшей вероятностью, что и\\xa0отвечая на\\xa0вопрос j2j_2j2\\u200b.\\n\\nСтоит заметить, что эти допущения гораздо слабее, чем в параметрической модели. В самом деле, параметрическая модель не только предполагает существование таких упорядоченных рядов, но и задает все вероятности. С другой стороны, непараметрический подход делает всего лишь естественное предположение о существовании последовательных рядов, но не ограничивает набор когнитивных механизмов, характерных для исполнителей. Было показано, что в некоторых случаях непараметрическая модель позволяет лучше делать выводы. Более подробно об этом можно почитать в полном тексте статьи.\\nКак мы уже говорили, эти подходы еще достаточно новые и не успели стать классикой краудсорсинга. Если сложность вопросов в вашем проекте существенно варьируется, мы рекомендуем более основательно изучить упомянутые методы и лежащие в их основе допущения, а затем опробовать их на практике.\\nИспользованная литература\\n\\nJeff Howe, The Rise of Crowdsourcing, The Wired, 2006.\\nДжефф Хау, Краудсорсинг: Коллективный разум как инструмент развития бизнеса, Альпина Паблишер, 2012.\\nOmar Alonso, The Practice of Crowdsourcing, 2019.\\n«Cамая богатая часть планеты работает бесплатно во время перерывов на кофе»: редактор Wired Джефф Хау о краудсорсинге, T&P, 2012.\\nР. А. Долженко, А. В. Бакаленко, Краудсорсинг как инструмент мобилизации интеллектуальных ресурсов: опыт использования в Сбербанке России, Российский журнал менеджмента, Том 14, №3, 2016, С. 77–102.\\nБеспилотные автомобили Яндекса на CES 2020: 7 тысяч км без водителя за рулём по улицам Лас-Вегаса, Новости Яндекса, 2020.\\nМетод Дэвида и Скина\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф11.1. Обучение с подкреплениемСледующий параграф12.1. Bias-variance decompositionКлассический взгляд на\\xa0то, почему слишком сложные модели переобучаютсяЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_10.html', 'title': 'Метрики классификации и регрессии'}, page_content='Метрики классификации и регрессииЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/33.1.Метрики классификации и регрессииВыбор метрик в реальных задачахФункция потерь  метрика качестваБинарная классификация: метки классовБинарная классификация: вероятности классовМногоклассовая классификацияКак оптимизировать метрики классификации?РегрессияКак оптимизировать метрики регрессии?3.2.Кросс-валидация3.3.Подбор гиперпараметров4.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Метрики классификации и регрессии3.1. Метрики классификации и регрессииАвторыБуркина МарияГубко ПавелГорчаков АлексейКак оценить качество модели для классификации или регрессии и\\xa0почему для разных задач нужны разные метрики\\nГораздо легче что-то измерить, чем понять, что именно вы измеряете\\n\\nДжон Уильям Салливан\\n\\n\\nЗадачи машинного обучения с учителем, как правило, состоят в восстановлении зависимости между парами (признаковое описание, целевая переменная) по данным, доступным нам для анализа. Алгоритмы машинного обучения (learning algorithm), со многими из которых вы уже успели познакомиться, позволяют построить модель, аппроксимирующую эту зависимость. Но как понять, насколько качественной получилась аппроксимация?\\nПочти наверняка наша модель будет ошибаться на некоторых объектах: будь она даже идеальной, шум или выбросы в тестовых данных всё испортят. При этом разные модели будут ошибаться на разных объектах и в разной степени. Задача специалиста по машинному обучению — подобрать подходящий критерий, который позволит сравнивать различные модели.\\nВажно: качество модели нельзя оценивать на обучающей выборке. Как минимум, это стоит делать на отложенной (тестовой) выборке, но если вам это позволяют время и вычислительные ресурсы, стоит прибегнуть и к более надёжным способам проверки — например, кросс-валидации (о ней мы поговорим в следующем параграфе).\\nВыбор метрик в реальных задачах\\nВозможно, вы уже участвовали в соревнованиях по анализу данных. На таких соревнованиях метрику (критерий качества модели) организатор выбирает за вас, и она, как правило, довольно понятным образом связана с результатами предсказаний. Но на практике всё бывает намного сложнее.\\nНапример, мы хотим:\\n\\nрешить, сколько коробок с бананами нужно завтра привезти в конкретный магазин — чтобы предложение соответствовало спросу, и не пришлось выбрасывать излишки;\\nувеличить счастье пользователя от работы с сервисом, чтобы он стал лояльным и приносил стабильный прогнозируемый доход;\\nрешить, нужно ли направить человека на дополнительное обследование.\\nВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nВ каждом конкретном случае может возникать целая иерархия метрик. Представим, например, что речь идёт о стриминговом музыкальном сервисе, пользователей которого мы решили порадовать сгенерированными самодельной нейросетью треками — не защищёнными авторским правом, а потому совершенно бесплатными.\\nИерархия метрик могла бы иметь такой вид:\\n\\nСамый верхний уровень: будущий доход сервиса — невозможно измерить в моменте, сложным образом зависит от совокупности всех наших усилий;\\nМедианная длина сессии, возможно, служащая оценкой радости пользователей, которая, как мы надеемся, повлияет на их желание продолжать платить за подписку — её нам придётся измерять в продакшене, ведь нас интересует реакция настоящих пользователей на новшество;\\nДоля удовлетворённых качеством сгенерированной музыки асессоров, на которых мы потестируем её до того, как выставить на суд пользователей;\\nФункция потерь, на которую мы будем обучать генеративную сеть.\\n\\nНа этом примере мы можем заметить сразу несколько общих закономерностей. Во-первых, метрики бывают offline и online (оффлайновыми и онлайновыми). Online-метрики вычисляются по данным, собираемым с работающей системы (например, медианная длина сессии). Offline-метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным или с привлечением специальных людей, асессоров.\\nПоследнее часто применяется, когда метрика — это реакция живого человека: скажем, так поступают поисковые компании, которые предлагают людям оценить качество ранжирования экспериментальной системы ещё до того, как рядовые пользователи увидят эти результаты в обычном порядке. На самом же нижнем этаже иерархии лежат оптимизируемые в ходе обучения функции потерь.\\nВ данном разделе нас будут интересовать offline-метрики, которые могут быть измерены без привлечения людей.\\nФункция потерь ≠\\\\neq\\ue020= метрика качества\\nКак мы узнали ранее, методы обучения реализуют разные подходы к обучению:\\n\\nобучение на основе прироста информации (как в деревьях решений);\\nобучение на основе сходства (как в методах ближайших соседей);\\nобучение на основе вероятностной модели данных (например, максимизацией правдоподобия);\\nобучение на основе ошибок (минимизация эмпирического риска).\\n\\nИ в рамках обучения на основе минимизации ошибок мы уже отвечали на вопрос: как можно штрафовать модель за предсказание на обучающем объекте.\\nВо время сведения задачи о построении решающего правила к задаче численной оптимизации, мы вводили понятие функции потерь и, обычно, объявляли целевой функцией сумму потерь от предсказаний на всех объектах обучающей выборки.\\nВажно понимать разницу между функцией потерь и метрикой качества. Её можно сформулировать следующим образом:\\n\\n\\nФункция потерь возникает в тот момент, когда мы сводим задачу построения модели к задаче оптимизации. Обычно требуется, чтобы она обладала хорошими свойствами (например, дифференцируемостью).\\n\\n\\nМетрика — внешний, объективный критерий качества, обычно зависящий не от параметров модели, а только от предсказанных меток.\\n\\n\\nВ некоторых случаях метрика может совпадать с функцией потерь. Например, в задаче регрессии MSE играют роль как функции потерь, так и метрики. Но, скажем, в задаче бинарной классификации они почти всегда различаются: в качестве функции потерь может выступать кросс-энтропия, а в качестве метрики — число верно угаданных меток (accuracy). Отметим, что в последнем примере у них различные аргументы: на вход кросс-энтропии нужно подавать логиты, а на вход accuracy — предсказанные метки (то есть по сути argmax логитов).\\nБинарная классификация: метки классов\\nПерейдём к обзору метрик и начнём с самой простой разновидности классификации — бинарной, а затем постепенно будем наращивать сложность.\\nНапомним постановку задачи бинарной классификации: нам нужно по обучающей выборке {(xi,yi)}i=1N\\\\{(x_i, y_i)\\\\}_{i=1}^N{(xi\\u200b,yi\\u200b)}i=1N\\u200b, где yi∈{0,1}y_i \\\\in \\\\{0, 1\\\\}yi\\u200b∈{0,1} построить модель, которая по объекту xxx предсказывает метку класса f(x)∈{0,1}f(x) \\\\in \\\\{0, 1\\\\}f(x)∈{0,1}.\\nПервый критерий качества, который приходит в голову, — accuracy, то есть доля объектов, для которых мы правильно предсказали класс:\\nAccuracy(y,ypred)=1N∑i=1NI[yi=f(xi)]\\\\color{#348FEA}{\\\\text{Accuracy}(y, y^{pred}) = \\\\frac{1}{N} \\\\sum_{i=1}^N \\\\mathbb{I}[y_i = f(x_i)]}\\nAccuracy(y,ypred)=N1\\u200bi=1∑N\\u200bI[yi\\u200b=f(xi\\u200b)]Или же сопряженная ей метрика — доля ошибочных классификаций (error rate):\\nError\\xa0rate=1−Accuracy\\\\text{Error rate} = 1 - \\\\text{Accuracy}\\nError\\xa0rate=1−AccuracyПознакомившись чуть внимательнее с этой метрикой, можно заметить, что у неё есть несколько недостатков:\\n\\nона не учитывает дисбаланс классов. Например, в задаче диагностики редких заболеваний классификатор, предсказывающий всем пациентам отсутствие болезни будет иметь достаточно высокую accuracy просто потому, что больных людей в выборке намного меньше;\\nона также не учитывает цену ошибки на объектах разных классов. Для примера снова можно привести задачу медицинской диагностики: если ошибочный положительный диагноз для здорового больного обернётся лишь ещё одним обследованием, то ошибочно отрицательный вердикт может повлечь роковые последствия.\\n\\nConfusion matrix (матрица ошибок)\\nИсторически задача бинарной классификации — это задача об обнаружении чего-то редкого в большом потоке объектов, например, поиск человека, больного туберкулёзом, по флюорографии. Или задача признания пятна на экране приёмника радиолокационной станции бомбардировщиком, представляющем угрозу охраняемому объекту (в противовес стае гусей).\\nПоэтому класс, который представляет для нас интерес, называется «положительным», а оставшийся — «отрицательным».\\nЗаметим, что для каждого объекта в выборке возможно 4 ситуации:\\n\\nмы предсказали положительную метку и угадали.  Будет относить такие объекты к true positive (TP) группе. True — потому что предсказали мы правильно, а positive — потому что предсказали положительную метку;\\nмы предсказали положительную метку, но ошиблись в своём предсказании — false positive (FP). False, потому что предсказание было неправильным;\\nмы предсказали отрицательную метку и угадали — true negative (TN);\\nи наконец, мы предсказали отрицательную метку, но ошиблись — false negative (FN).\\nДля удобства все эти 4 числа изображают в виде таблицы, которую называют confusion matrix (матрицей ошибок):\\n\\n\\nНе волнуйтесь, если первое время эти обозначения будут сводить вас с ума (будем откровенны, даже профи со стажем в них порой путаются), однако логика за ними достаточно простая: первая часть названия группы показывает угадали ли мы с классом, а вторая — какой класс мы предсказали.\\n\\nПример\\nПопробуем воспользоваться введёнными метриками в боевом примере: сравним работу нескольких моделей классификации на Breast cancer wisconsin (diagnostic) dataset.\\nОбъекты выборки — фотографии биопсии грудных опухолей. С их помощью было сформировано признаковое описание, которое заключается в характеристиках ядер клеток (таких как радиус ядра, его текстура, симметричность). Положительным классом в такой постановке будут злокачественные опухоли, а отрицательным — доброкачественные.\\nМодель 1. Константное предсказание\\nРешение задачи начнём с самого простого классификатора, который выдаёт на каждом объекте константное предсказание — самый часто встречающийся класс.\\nЗачем вообще замерять качество на такой модели?При разработке модели машинного обучения для проекта всегда желательно иметь некоторую baseline модель. Так нам будет легче проконтролировать, что наша более сложная модель действительно даёт нам прирост качества.\\n\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1from sklearn.datasets \\n2import load_breast_cancer \\n3the_data = load_breast_cancer()    \\n4\\n5# 0 — «доброкачественный» \\n6# 1 — «злокачественный» \\n7relabeled_target = 1 - the_data[\"target\"] \\n8\\n9from sklearn.model_selection import train_test_split \\n10X = the_data[\"data\"] \\n11y = relabeled_target \\n12X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) \\n13\\n14from sklearn.dummy import DummyClassifier \\n15dc_mf = DummyClassifier(strategy=\"most_frequent\") \\n16dc_mf.fit(X_train, y_train) \\n17\\n18from sklearn.metrics import confusion_matrix \\n19y_true = y_test y_pred = dc_mf.predict(X_test) \\n20dc_mf_tn, dc_mf_fp, dc_mf_fn, dc_mf_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() \\n\\n\\n\\n\\n\\n\\n\\nПрогнозируемый    класс +\\n\\n\\nПрогнозируемый класс -\\n\\n\\n\\n\\nИстинный класс +\\n\\n\\nTP = 0\\n\\n\\nFN = 53\\n\\n\\n\\n\\nИстинный класс -\\n\\n\\nFP = 0\\n\\n\\nTN = 90\\n\\n\\n\\n\\nОбучающие данные таковы, что наш dummy-классификатор все объекты записывает в отрицательный класс, то есть признаёт все опухоли доброкачественными. Такой наивный подход позволяет нам получить минимальный штраф за FP (действительно, нельзя ошибиться в предсказании, если положительный класс вообще не предсказывается), но и максимальный штраф за FN (в эту группу попадут все злокачественные опухоли).\\nМодель 2. Случайный лес.\\nНастало время воспользоваться всем арсеналом моделей машинного обучения, и начнём мы со случайного леса.\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1from sklearn.ensemble import RandomForestClassifier \\n2rfc = RandomForestClassifier()       \\n3rfc.fit(X_train, y_train)       \\n4y_true = y_test       \\n5y_pred = rfc.predict(X_test)       \\n6rfc_tn, rfc_fp, rfc_fn, rfc_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel()\\n\\n\\n\\n\\n\\n\\n\\nПрогнозируемый класс +\\n\\n\\nПрогнозируемый класс -\\n\\n\\n\\n\\nИстинный класс +\\n\\n\\nTP = 52\\n\\n\\nFN = 1\\n\\n\\n\\n\\nИстинный класс -\\n\\n\\nFP = 4\\n\\n\\nTN = 86\\n\\n\\n\\n\\nМожно сказать, что этот классификатор чему-то научился, так как главная диагональ матрицы стала содержать все объекты из отложенной выборки, за исключением 4 + 1 = 5 объектов (сравните с 0 + 53 объектами dummy-классификатора, все опухоли объявляющего доброкачественными).\\nОтметим, что вычисляя долю недиагональных элементов, мы приходим к метрике error rate, о которой мы говорили в самом начале:\\nError\\xa0rate=FP+FNTP+TN+FP+FN\\\\text{Error rate} = \\\\frac{FP + FN}{ TP + TN + FP + FN}\\nError\\xa0rate=TP+TN+FP+FNFP+FN\\u200bтогда как доля объектов, попавших на главную диагональ — это как раз таки accuracy:\\nAccuracy=TP+TNTP+TN+FP+FN\\\\text{Accuracy} = \\\\frac{TP + TN}{ TP + TN + FP + FN}\\nAccuracy=TP+TN+FP+FNTP+TN\\u200bМодель 3. Метод опорных векторов.\\nДавайте построим еще один классификатор на основе линейного метода опорных векторов.\\nВажно: Не забудьте привести признаки к единому масштабу, иначе численный алгоритм не сойдется к решению и мы получим гораздо более плохо работающее решающее правило. Попробуйте проделать это упражнение.\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1from sklearn.svm import LinearSVC\\n2from sklearn.preprocessing import StandardScaler \\n3ss = StandardScaler() ss.fit(X_train) \\n4scaled_linsvc = LinearSVC(C=0.01,random_state=42) \\n5scaled_linsvc.fit(ss.transform(X_train), y_train) \\n6y_true = y_test \\n7y_pred = scaled_linsvc.predict(ss.transform(X_test)) \\n8tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() \\n\\n\\n\\n\\n\\n\\n\\nПрогнозируемый класс +\\n\\n\\nПрогнозируемый класс -\\n\\n\\n\\n\\nИстинный класс +\\n\\n\\nTP = 50\\n\\n\\nFN = 3\\n\\n\\n\\n\\nИстинный класс -\\n\\n\\nFP = 1\\n\\n\\nTN = 89\\n\\n\\n\\n\\nСравним результаты\\nЛегко заметить, что каждая из двух моделей лучше классификатора-пустышки, однако давайте попробуем сравнить их между собой. С точки зрения error rate модели практически одинаковы: 5/143 для леса против 4/143 для SVM.\\nПосмотрим на структуру ошибок чуть более внимательно: лес — (FP = 4, FN = 1), SVM — (FP = 1, FN = 3). Какая из моделей предпочтительнее?\\nЗамечание: Мы сравниваем несколько классификаторов на основании их предсказаний на отложенной выборке. Насколько ошибки данных классификаторов зависят от разбиения исходного набора данных? Иногда в процессе оценки качества мы будем получать модели, чьи показатели эффективности будут статистически неразличимыми.\\nПусть мы учли предыдущее замечание и эти модели действительно статистически значимо ошибаются в разную сторону. Мы встретились с очевидной вещью: на матрицах нет отношения порядка. Когда мы сравнивали dummy-классификатор и случайный лес с помощью Accuracy, мы всю сложную структуру ошибок свели к одному числу, так как на вещественных числах отношение порядка есть. Сводить оценку модели к одному числу очень удобно, однако не стоит забывать, что у вашей модели есть много аспектов качества.\\nЧто же всё-таки важнее уменьшить: FP или FN? Вернёмся к задаче:\\n\\nFP — доля доброкачественных опухолей, которым ошибочно присваивается метка злокачественной;\\nFN — доля злокачественных опухолей, которые классификатор пропускает.\\n\\nВ такой постановке становится понятно, что при сравнении выиграет модель с меньшим FN (то есть лес в нашем примере), ведь каждая не обнаруженная опухоль может стоить человеческой жизни.\\nРассмотрим теперь другую задачу: по данным о погоде предсказать, будет ли успешным запуск спутника. FN в такой постановке — это ошибочное предсказание неуспеха, то есть не более, чем упущенный шанс (если вас, конечно не уволят за срыв сроков). С FP всё серьёзней: если вы предскажете удачный запуск спутника, а на деле он потерпит крушение из-за погодных условий, то ваши потери будут в разы существеннее.\\nИтак, из примеров мы видим, что в текущем виде введенная нами доля ошибочных классификаций не даст нам возможности учесть неравную важность FP и FN. Поэтому введем две новые метрики: точность и полноту.\\nТочность и полнота\\nAccuracy - это метрика, которая характеризует качество модели, агрегированное по всем классам. Это полезно, когда классы для нас имеют одинаковое значение. В случае, если это не так, accuracy может быть обманчивой.\\nРассмотрим ситуацию, когда положительный класс это событие редкое. Возьмем в качестве примера поисковую систему - в нашем хранилище хранятся миллиарды документов, а релевантных к конкретному поисковому запросу на несколько порядков меньше.\\nПусть мы хотим решить задачу бинарной классификации «документ d релевантен по запросу q». Благодаря большому дисбалансу, Accuracy dummy-классификатора, объявляющего все документы нерелевантными, будет близка к единице. Напомним, что Accuracy=TP+TNTP+TN+FP+FN\\\\text{Accuracy} = \\\\frac{TP + TN}{TP + TN + FP + FN}Accuracy=TP+TN+FP+FNTP+TN\\u200b, и в нашем случае высокое значение метрики будет обеспечено членом TN, в то время для пользователей более важен высокий TP.\\nПоэтому в случае ассиметрии классов, можно использовать метрики, которые не учитывают TN и ориентируются на TP.\\nЕсли мы рассмотрим долю правильно предсказанных положительных объектов среди всех объектов, предсказанных положительным классом, то мы получим метрику, которая называется точностью (precision)\\nPrecision=TPTP+FP\\\\color{#348FEA}{\\\\text{Precision} = \\\\frac{TP}{TP + FP}}\\nPrecision=TP+FPTP\\u200bИнтуитивно метрика показывает долю релевантных документов среди всех найденных классификатором. Чем меньше ложноположительных срабатываний будет допускать модель, тем больше будет её Precision.\\nЕсли же мы рассмотрим долю правильно найденных положительных объектов среди всех объектов положительного класса, то мы получим метрику, которая называется полнотой (recall)\\nRecall=TPTP+FN\\\\color{#348FEA}{\\\\text{Recall} = \\\\frac{TP}{TP + FN}}\\nRecall=TP+FNTP\\u200bИнтуитивно метрика показывает долю найденных документов из всех релевантных. Чем меньше ложно отрицательных срабатываний, тем выше recall модели.\\nНапример, в задаче предсказания злокачественности опухоли точность показывает, сколько из определённых нами как злокачественные опухолей действительно злокачественные, а полнота — какую долю злокачественных опухолей нам удалось выявить.\\nХорошее понимание происходящего даёт следующая картинка:\\n\\n\\n\\nИсточник\\n\\n\\nRecall@k, Precision@k\\nМетрики Recall и Precision хорошо подходят для задачи поиска «документ d релевантен запросу q», когда из списка рекомендованных алгоритмом документов нас интересует только первый. Но не всегда алгоритм машинного обучения вынужден работать в таких жестких условиях. Может быть такое, что вполне достаточно, что релевантный документ попал в первые k рекомендованных.\\nНапример, в интерфейсе выдачи первые три подсказки видны всегда одновременно и вообще не очень понятно, какой у них порядок. Тогда более честной оценкой качества алгоритма будет «в выдаче D размера k по запросу q нашлись релевантные документы». Для расчёта метрики по всей выборке объединим все выдачи и рассчитаем precision, recall как обычно подокументно.\\nF1-мера\\nКак мы уже отмечали ранее, модели очень удобно сравнивать, когда их качество выражено одним числом. В случае пары Precision-Recall существует популярный способ скомпоновать их в одну метрику - взять их среднее гармоническое. Данный показатель эффективности исторически носит название F1-меры (F1-measure).\\nF1=21Recall+1Precision=\\\\color{#348FEA}{F_1 = \\\\frac{2}{\\\\frac{1}{Recall} + \\\\frac{1}{Precision}}} = \\nF1\\u200b=Recall1\\u200b+Precision1\\u200b2\\u200b==2Recall⋅PrecisionRecall+Precision=TPTP+FP+FN2 = 2 \\\\frac{Recall \\\\cdot Precision }{Recall + Precision} = \\\\frac\\n{TP} {TP + \\\\frac{FP + FN}{2}}\\n=2Recall+PrecisionRecall⋅Precision\\u200b=TP+2FP+FN\\u200bTP\\u200bСтоит иметь в виду, что F1-мера предполагает одинаковую важность Precision и Recall, если одна из этих метрик для вас приоритетнее, то можно воспользоваться FβF_{\\\\beta}Fβ\\u200b мерой:\\nFβ=(β2+1)Recall⋅PrecisionRecall+β2PrecisionF_{\\\\beta} = (\\\\beta^2 + 1) \\\\frac{Recall \\\\cdot Precision }{Recall + \\\\beta^2Precision}\\nFβ\\u200b=(β2+1)Recall+β2PrecisionRecall⋅Precision\\u200bБинарная классификация: вероятности классов\\nМногие модели бинарной классификации устроены так, что класс объекта получается бинаризацией выхода классификатора по некоторому фиксированному порогу:\\nf(x;w,w0)=I[g(x,w)>w0].f\\\\left(x ; w, w_{0}\\\\right)=\\\\mathbb{I}\\\\left[g(x, w) > w_{0}\\\\right].\\nf(x;w,w0\\u200b)=I[g(x,w)>w0\\u200b].Например, модель логистической регрессии возвращает оценку вероятности принадлежности примера к положительному классу. Другие модели бинарной классификации обычно возвращают произвольные вещественные значения, но существуют техники, называемые калибровкой классификатора, которые позволяют преобразовать предсказания в более или менее корректную оценку вероятности принадлежности к положительному классу.\\nКак оценить качество предсказываемых вероятностей, если именно они являются нашей конечной целью? Общепринятой мерой является логистическая функция потерь, которую мы изучали раньше, когда говорили об устройстве некоторых методов классификации (например уже упоминавшейся логистической регрессии).\\nЕсли же нашей целью является построение прогноза в терминах метки класса, то нам нужно учесть, что в зависимости от порога мы будем получать разные предсказания и разное качество на отложенной выборке. Так, чем ниже порог отсечения, тем больше объектов модель будет относить к положительному классу. Как в этом случае оценить качество модели?\\nAUC\\nПусть мы хотим учитывать ошибки на объектах обоих классов. При уменьшении порога отсечения мы будем находить (правильно предсказывать) всё большее число положительных объектов, но также и неправильно предсказывать положительную метку на всё большем числе отрицательных объектов. Естественным кажется ввести две метрики TPR и FPR:\\nTPR (true positive rate) — это полнота, доля положительных объектов, правильно предсказанных положительными:\\nTPR=TPP=TPTP+FNTPR = \\\\frac{TP}{P} = \\\\frac{TP}{TP + FN} \\nTPR=PTP\\u200b=TP+FNTP\\u200bFPR (false positive rate) — это доля отрицательных объектов, неправильно предсказанных положительными:\\nFPR=FPN=FPFP+TNFPR = \\\\frac{FP}{N} = \\\\frac{FP}{FP + TN}\\nFPR=NFP\\u200b=FP+TNFP\\u200bОбе эти величины растут при уменьшении порога. Кривая в осях TPR/FPR, которая получается при варьировании порога, исторически называется ROC-кривой (receiver operating characteristics curve, сокращённо ROC curve). Следующий интерактивный график поможет вам понять поведение ROC-кривой.\\nЖелтая и синяя кривые показывают распределение предсказаний классификатора на объектах положительного и отрицательного классов соответственно. То есть значения на оси X (на графике с двумя гауссианами) мы получаем из классификатора.\\n\\n\\nЕсли классификатор идеальный, — две кривые разделимы по оси X, — то на правом графике мы получаем ROC-кривую (0,0)->(0,1)->(1,1), площадь под которой равна 1.\\n\\n\\nЕсли классификатор случайный (предсказывает одинаковые метки положительным и отрицательным объектам), то мы получаем ROC-кривую (0,0)->(1,1), площадь под которой равна 0.5.\\n\\n\\nПоэкспериментируйте с разными вариантами распределения предсказаний по классам и посмотрите, как меняется ROC-кривая.\\nЧем лучше классификатор разделяет два класса, тем больше площадь (area under curve) под ROC-кривой — и мы можем использовать её в качестве метрики. Эта метрика называется AUC и она работает благодаря следующему свойству ROC-кривой:\\nAUC равен доле пар объектов вида (объект класса 1, объект класса 0), которые алгоритм верно упорядочил, то есть предсказание классификатора на первом объекте больше:\\nAUC\\u2061=∑i=1N∑j=1NI[yi<yj]I′[f(xi)<f(xj)]∑i=1N∑j=1NI[yi<yj]\\\\color{#348FEA}{\\\\operatorname{AUC} = \\\\frac{\\\\sum\\\\limits_{i = 1}^{N} \\\\sum\\\\limits_{j = 1}^{N}\\\\mathbb{I}[y_i < y_j] I^{\\\\prime}[f(x_{i}) < f(x_{j})]}{\\\\sum\\\\limits_{i = 1}^{N} \\\\sum\\\\limits_{j = 1}^{N}\\\\mathbb{I}[y_i < y_j]}}\\nAUC=i=1∑N\\u200bj=1∑N\\u200bI[yi\\u200b<yj\\u200b]i=1∑N\\u200bj=1∑N\\u200bI[yi\\u200b<yj\\u200b]I′[f(xi\\u200b)<f(xj\\u200b)]\\u200bI′[f(xi)<f(xj)]={0,f(xi)>f(xj)0.5f(xi)=f(xj)1,f(xi)<f(xj)I^{\\\\prime}\\\\left[f(x_{i}) < f(x_{j})\\\\right]=\\n\\\\left\\\\{\\n  \\\\begin{array}{ll}\\n    0, & f(x_{i}) > f(x_{j}) \\\\\\\\\\n    0.5 & f(x_{i}) = f(x_{j}) \\\\\\\\\\n    1, & f(x_{i}) < f(x_{j})\\n  \\\\end{array}\\n\\\\right.\\nI′[f(xi\\u200b)<f(xj\\u200b)]=⎩⎨⎧\\u200b0,0.51,\\u200bf(xi\\u200b)>f(xj\\u200b)f(xi\\u200b)=f(xj\\u200b)f(xi\\u200b)<f(xj\\u200b)\\u200bI[yi<yj]={0,yi≥yj1,yi<yjI\\\\left[y_{i}< y_{j}\\\\right]=\\n\\\\left\\\\{\\n  \\\\begin{array}{ll}\\n    0, & y_{i} \\\\geq y_{j} \\\\\\\\\\n    1, & y_{i} < y_{j}\\n  \\\\end{array}\\n\\\\right.\\nI[yi\\u200b<yj\\u200b]={0,1,\\u200byi\\u200b≥yj\\u200byi\\u200b<yj\\u200b\\u200bЧтобы детальнее разобраться, почему это так, советуем вам обратиться к материалам А.Г.Дьяконова.\\nВ каких случаях лучше отдать предпочтение этой метрике? Рассмотрим следующую задачу: некоторый сотовый оператор хочет научиться предсказывать, будет ли клиент пользоваться его услугами через месяц. На первый взгляд кажется, что задача сводится к бинарной классификации с метками 1, если клиент останется с компанией и 000 — иначе.\\nОднако если копнуть глубже в процессы компании, то окажется, что такие метки практически бесполезны. Компании скорее интересно упорядочить клиентов по вероятности прекращения обслуживания и в зависимости от этого применять разные варианты удержания: кому-то прислать скидочный купон от партнёра, кому-то предложить скидку на следующий месяц, а кому-то и новый тариф на особых условиях.\\nТаким образом, в любой задаче, где нам важна не метка сама по себе, а правильный порядок на объектах, имеет смысл применять AUC.\\nУтверждение выше может вызывать у вас желание использовать AUC в качестве метрики в задачах ранжирования, но мы призываем вас быть аккуратными.\\nПодробнееПродемонстрируем это на следующем примере: пусть наша выборка состоит из 910091009100 объектов класса 000 и 101010 объектов класса 111, и модель расположила их следующим образом:\\n0…0⏟9000\\xa01…1⏟10\\xa00…0⏟100\\\\underbrace{0 \\\\dots 0}_{9000} ~ \\\\underbrace{1 \\\\dots 1}_{10} ~ \\\\underbrace{0 \\\\dots 0}_{100}\\n90000…0\\u200b\\u200b\\xa0101…1\\u200b\\u200b\\xa01000…0\\u200b\\u200bТогда AUC будет близка к единице: количество пар правильно расположенных объектов будет порядка 900009000090000, в то время как общее количество пар порядка 910009100091000.\\nОднако самыми высокими по вероятности положительного класса будут совсем не те объекты, которые мы ожидаем.\\nAverage Precision\\nБудем постепенно уменьшать порог бинаризации. При этом полнота будет расти от 000 до 111, так как будет увеличиваться количество объектов, которым мы приписываем положительный класс (а количество объектов, на самом деле относящихся к положительному классу, очевидно, меняться не будет).\\nПро точность же нельзя сказать ничего определённого, но мы понимаем, что скорее всего она будет выше при более высоком пороге отсечения (мы оставим только объекты, в которых модель «уверена» больше всего). Варьируя порог и пересчитывая значения Precision и Recall на каждом пороге, мы получим некоторую кривую примерно следующего вида:\\n\\n\\n\\nИсточник\\n\\n\\nРассмотрим среднее значение точности (оно равно площади под кривой точность-полнота):\\n\\xa0AP\\xa0=∫01p(r)dr\\\\text { AP }=\\\\int_{0}^{1} p(r) d r\\n\\xa0AP\\xa0=∫01\\u200bp(r)drПолучим показатель эффективности, который называется average precision. Как в случае матрицы ошибок мы переходили к скалярным показателям эффективности, так и в случае с кривой точность-полнота мы охарактеризовали ее в виде числа.\\nМногоклассовая классификация\\nЕсли классов становится больше двух, расчёт метрик усложняется. Если задача классификации на KKK классов ставится как KKK задач об отделении класса iii от остальных (i=1,…,Ki=1,\\\\ldots,Ki=1,…,K), то для каждой из них можно посчитать свою матрицу ошибок. Затем есть два варианта получения итогового значения метрики из KKK матриц ошибок:\\n\\nУсредняем элементы матрицы ошибок (TP, FP, TN, FN) между бинарными классификаторами, например TP=1K∑i=1KTPiTP = \\\\frac{1}{K}\\\\sum_{i=1}^{K}TP_iTP=K1\\u200b∑i=1K\\u200bTPi\\u200b. Затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру. Это называют микроусреднением.\\nСчитаем Precision, Recall для каждого классификатора отдельно, а потом усредняем. Это называют макроусреднением.\\n\\nПорядок усреднения влияет на результат в случае дисбаланса классов. Показатели TP, FP, FN — это счётчики объектов. Пусть некоторый класс обладает маленькой мощностью (обозначим её MMM). Тогда значения TP и FN при классификации этого класса против остальных будут не больше MMM, то есть тоже маленькие. Про FP мы ничего уверенно сказать не можем, но скорее всего при дисбалансе классов классификатор не будет предсказывать редкий класс слишком часто, потому что есть большая вероятность ошибиться. Так что FP тоже мало. Поэтому усреднение первым способом сделает вклад маленького класса в общую метрику незаметным. А при усреднении вторым способом среднее считается уже для нормированных величин, так что вклад каждого класса будет одинаковым.\\nРассмотрим пример. Пусть есть датасет из объектов трёх цветов: желтого, зелёного и синего. Желтого и зелёного цветов почти поровну — 21 и 20 объектов соответственно, а синих объектов всего 4.\\n\\nМодель по очереди для каждого цвета пытается отделить объекты этого цвета от объектов оставшихся двух цветов. Результаты классификации проиллюстрированы матрицей ошибок. Модель «покрасила» в жёлтый 25 объектов, 20 из которых были действительно жёлтыми (левый столбец матрицы). В синий был «покрашен» только один объект, который на самом деле жёлтый (средний столбец матрицы). В зелёный — 19 объектов, все на самом деле зелёные (правый столбец матрицы).\\n\\nПосчитаем Precision классификации двумя способами:\\n\\nС помощью микроусреднения получаем\\n\\nPrecision=13(20+0+19)13(20+0+19)+13(5+1+0)=0.87\\\\text{Precision} = \\\\frac{\\\\dfrac{1}{3}\\\\left(20 + 0 + 19\\\\right)}{\\\\dfrac{1}{3}\\\\left(20 + 0 + 19\\\\right) + \\\\dfrac{1}{3}\\\\left(5 + 1 + 0\\\\right)} = 0.87\\nPrecision=31\\u200b(20+0+19)+31\\u200b(5+1+0)31\\u200b(20+0+19)\\u200b=0.87\\nС помощью макроусреднения получаем\\n\\nPrecision=13(2020+5+00+1+1919+0)=0.6\\\\text{Precision} = \\\\dfrac{1}{3}\\\\left( \\\\frac{20}{20 + 5} + \\\\frac{0}{0 + 1} + \\\\frac{19}{19 + 0}\\\\right) = 0.6\\nPrecision=31\\u200b(20+520\\u200b+0+10\\u200b+19+019\\u200b)=0.6Видим, что макроусреднение лучше отражает тот факт, что синий цвет, которого в датасете было совсем мало, модель практически игнорирует.\\nКак оптимизировать метрики классификации?\\nПусть мы выбрали, что метрика качества алгоритма будет F(a(X),Y)F(a(X), Y)F(a(X),Y). Тогда мы хотим обучить модель так, чтобы FFF на валидационной выборке была минимальная/максимальная. Лучший способ добиться минимизации метрики FFF — оптимизировать её напрямую, то есть выбрать в качестве функции потерь ту же F(a(X),Y)F(a(X), Y)F(a(X),Y). К сожалению, это не всегда возможно. Рассмотрим, как оптимизировать метрики иначе.\\nМетрики precision и recall невозможно оптимизировать напрямую, потому что эти метрики нельзя рассчитать на одном объекте, а затем усреднить. Они зависят от того, какими были правильная метка класса и ответ алгоритма на всех объектах. Чтобы понять, как оптимизировать precision, recall, рассмотрим, как расчитать эти метрики на отложенной выборке.\\nПусть модель обучена на стандартную для классификации функцию потерь (LogLoss).\\nДля получения меток класса специалист по машинному обучению сначала применяет на объектах модель и получает вещественные предсказания модели (pi∈(0,1)p_i \\\\in \\\\left(0, 1\\\\right)pi\\u200b∈(0,1)). Затем предсказания бинаризуются по порогу, выбранному специалистом: если предсказание на объекте больше порога, то метка класса 1 (или «положительная»), если меньше — 0 (или «отрицательная»). Рассмотрим, что будет с метриками precision, recall в крайних положениях порога.\\nПусть порог равен нулю\\nТогда всем объектам будет присвоена положительная метка. Следовательно, все объекты будут либо TP, либо FP, потому что отрицательных предсказаний нет, TP+FP=NTP + FP = NTP+FP=N, где NNN — размер выборки. Также все объекты, у которых метка на самом деле 1, попадут в TP.\\nПо формуле точность Precision=TPTP+FP=1N∑i=1NI[yi=1]\\\\text{Precision} = \\\\frac{TP}{TP + FP} = \\\\frac1N \\\\sum_{i = 1}^N \\\\mathbb{I} \\\\left[ y_i = 1 \\\\right]Precision=TP+FPTP\\u200b=N1\\u200b∑i=1N\\u200bI[yi\\u200b=1] равна среднему таргету в выборке. А полнота Recall=TPTP+FN=TPTP+0=1\\\\text{Recall} = \\\\frac{TP}{TP + FN} = \\\\frac{TP}{TP + 0} = 1Recall=TP+FNTP\\u200b=TP+0TP\\u200b=1 равна единице.\\nПусть теперь порог равен единице\\nТогда ни один объект не будет назван положительным, TP=FP=0TP = FP = 0TP=FP=0. Все объекты с меткой класса 1 попадут в FN. Если есть хотя бы один такой объект, то есть FN≠0FN \\\\ne 0FN\\ue020=0, будет верна формула Recall=TPTP+FN=00+FN=0\\\\text{Recall} = \\\\frac{TP}{TP + FN} = \\\\frac{0}{0+ FN} = 0Recall=TP+FNTP\\u200b=0+FN0\\u200b=0.\\nТо есть при пороге единица, полнота равна нулю. Теперь посмотрим на точность. Формула для Precision состоит только из счётчиков положительных ответов модели (TP, FP). При единичном пороге они оба равны нулю, Precision=TPTP+FP=00+0\\\\text{Precision} = \\\\frac{TP}{TP + FP} = \\\\frac{0}{0 + 0}Precision=TP+FPTP\\u200b=0+00\\u200bто есть при единичном пороге точность неопределена. Пусть мы отступили чуть-чуть назад по порогу, чтобы хотя бы несколько объектов были названы моделью положительными.\\nСкорее всего это будут самые «простые» объекты, которые модель распознает хорошо, потому что её предсказание близко к единице. В этом предположении FP≈0FP \\\\approx 0FP≈0. Тогда точность Precision=TPTP+FP≈TPTP+0≈1\\\\text{Precision} = \\\\frac{TP}{TP + FP} \\\\approx \\\\frac{TP}{TP + 0} \\\\approx 1Precision=TP+FPTP\\u200b≈TP+0TP\\u200b≈1 будет близка к единице.\\nИзменяя порог, между крайними положениями, получим графики Precision и Recall, которые выглядят как-то так:\\n\\nRecall меняется от единицы до нуля, а Precision от среднего тагрета до какого-то другого значения (нет гарантий, что график монотонный).\\nИтого оптимизация precision и recall происходит так:\\n\\nМодель обучается на стандартную функцию потерь (например, LogLoss).\\nИспользуя вещественные предсказания на валидационной выборке, перебирая разные пороги от 0 до 1, получаем графики метрик в зависимости от порога.\\nВыбираем нужное сочетание точности и полноты.\\n\\nПусть теперь мы хотим максимизировать метрику AUC. Стандартный метод оптимизации, градиентный спуск, предполагает, что функция потерь дифференцируема. AUC этим качеством не обладает, то есть мы не можем оптимизировать её напрямую. Поэтому для метрики AUC приходится изменять оптимизационную задачу.\\nМетрика AUC считает долю верно упорядоченных пар. Значит от исходной выборки можно перейти к выборке упорядоченных пар объектов. На этой выборке ставится задача классификации: метка класса 1 соответствует правильно упорядоченной паре, 0 — неправильно.\\nНовой метрикой становится accuracy — доля правильно классифицированных объектов, то есть доля правильно упорядоченных пар. Оптимизировать accuracy можно по той же схеме, что и precision, recall: обучаем модель на LogLoss и предсказываем вероятности положительной метки у объекта выборки, считаем accuracy для разных порогов по вероятности и выбираем понравившийся.\\nРегрессия\\nВ задачах регрессии целевая метка у нас имеет потенциально бесконечное число значений. И природа этих значений, обычно, связана с каким-то процессом измерений:\\n\\nвеличина температуры в определенный момент времени на метеостанции\\nколичество прочтений статьи на сайте\\nколичество проданных бананов в конкретном магазине, сети магазинов или стране\\nдебит добывающей скважины на нефтегазовом месторождении за месяц и т.п.\\n\\nМы видим, что иногда метка это целое число, а иногда произвольное вещественное число. Обычно случаи целочисленных меток моделируют так, словно это просто обычное вещественное число. При таком подходе может оказаться так, что модель A лучше модели B по некоторой метрике, но при этом предсказания у модели A могут быть не целыми. Если в бизнес-задаче ожидается именно целочисленный ответ, то и оценивать нужно огрубление.\\nОбщая рекомендация такова: оценивайте весь каскад решающих правил: и те «внутренние», которые вы получаете в результате обучения, и те «итоговые», которые вы отдаёте бизнес-заказчику.\\nНапример, вы можете быть удовлетворены, что стали ошибаться не во втором, а только в третьем знаке после запятой при предсказании погоды. Но сами погодные данные измеряются с точностью до десятых долей градуса, а пользователь и вовсе может интересоваться лишь целым числом градусов.\\nИтак, напомним постановку задачи регрессии: нам нужно по обучающей выборке {(xi,yi)}i=1N\\\\{(x_i, y_i)\\\\}_{i=1}^N{(xi\\u200b,yi\\u200b)}i=1N\\u200b, где yi∈Ry_i \\\\in \\\\mathbb{R}yi\\u200b∈R построить модель f(x).\\nВеличину ei=f(xi)−yie_i = f(x_i) - y_iei\\u200b=f(xi\\u200b)−yi\\u200b называют ошибкой на объекте i или регрессионным остатком.\\nВесь набор ошибок на отложенной выборке может служить аналогом матрицы ошибок из задачи классификации. А именно, когда мы рассматриваем две разные модели, то, глядя на то, как и на каких объектах они ошиблись, мы можем прийти к выводу, что для решения бизнес-задачи нам выгоднее взять ту или иную модель. И, аналогично со случаем бинарной классификации, мы можем начать строить агрегаты от вектора ошибок, получая тем самым разные метрики.\\nMSE, RMSE, R2R^2R2\\nMSE — одна из самых популярных метрик в задаче регрессии. Она уже знакома вам, так как применяется в качестве функции потерь (или входит в ее состав) во многих ранее рассмотренных методах.\\nMSE(ytrue,ypred)=1N∑i=1N(yi−f(xi))2MSE(y^{true}, y^{pred}) = \\\\frac1N\\\\sum_{i=1}^{N} (y_i - f(x_i))^2 \\nMSE(ytrue,ypred)=N1\\u200bi=1∑N\\u200b(yi\\u200b−f(xi\\u200b))2Иногда для того, чтобы показатель эффективности MSE имел размерность исходных данных, из него извлекают квадратный корень и получают показатель эффективности RMSE.\\nMSE неограничен сверху, и может быть нелегко понять, насколько «хорошим» или «плохим» является то или иное его значение. Чтобы появились какие-то ориентиры, делают следующее:\\n\\n\\nБерут наилучшее константное предсказание с точки зрения MSE — среднее арифметическое меток yˉ\\\\bar{y}yˉ\\u200b. При этом чтобы не было подглядывания в test, среднее нужно вычислять по обучающей выборке\\n\\n\\nРассматривают в качестве показателя ошибки:\\nR2=1−∑i=1N(yi−f(xi))2∑i=1N(yi−yˉ)2.R^2 = 1 - \\\\frac{\\\\sum_{i=1}^{N} (y_i - f(x_i))^2}{\\\\sum_{i=1}^{N} (y_i - \\\\bar{y})^2}.\\nR2=1−∑i=1N\\u200b(yi\\u200b−yˉ\\u200b)2∑i=1N\\u200b(yi\\u200b−f(xi\\u200b))2\\u200b.У идеального решающего правила R2R^2R2 равен 111, у наилучшего константного предсказания он равен 000 на обучающей выборке. Можно заметить, что R2R^2R2 показывает, какая доля дисперсии таргетов (знаменатель) объяснена моделью.\\n\\n\\nMSE квадратично штрафует за большие ошибки на объектах. Мы уже видели проявление этого при обучении моделей методом минимизации квадратичных ошибок — там это проявлялось в том, что модель старалась хорошо подстроиться под выбросы.\\nПусть теперь мы хотим использовать MSE для оценки наших регрессионных моделей. Если большие ошибки для нас действительно неприемлемы, то квадратичный штраф за них — очень полезное свойство (и его даже можно усиливать, повышая степень, в которую мы возводим ошибку на объекте). Однако если в наших тестовых данных присутствуют выбросы, то нам будет сложно объективно сравнить модели между собой: ошибки на выбросах будет маскировать различия в ошибках на основном множестве объектов.\\nТаким образом, если мы будем сравнивать две модели при помощи MSE, у нас будет выигрывать та модель, у которой меньше ошибка на объектах-выбросах, а это, скорее всего, не то, чего требует от нас наша бизнес-задача.\\nИстория из жизни про бананы и квадратичный штраф за ошибкуИз-за неверно введенных данных метка одного из объектов оказалась в 100 раз больше реального значения. Моделировалась величина при помощи градиентного бустинга над деревьями решений. Функция потерь была MSE.\\nОднажды уже во время эксплуатации случилось ЧП: у нас появились предсказания, в 100 раз превышающие допустимые из соображений физического смысла значения. Представьте себе, например, что вместо обычных 4 ящиков бананов система предлагала поставить в магазин 400. Были распечатаны все деревья из ансамбля, и мы увидели, что постепенно число ящиков действительно увеличивалось до прогнозных 400.\\nБыло решено проверить гипотезу, что был выброс в данных для обучения. Так оно и оказалось: всего одна точка давала такую потерю на объекте, что алгоритм обучения решил, что лучше переобучиться под этот выброс, чем смириться с большим штрафом на этом объекте. А в эксплуатации у нас возникли точки, которые плюс-минус попадали в такие же листья ансамбля, что и объект-выброс.\\nИзбежать такого рода проблем можно двумя способами: внимательнее контролируя качество данных или адаптировав функцию потерь.\\nАналогично, можно поступать и в случае, когда мы разрабатываем метрику качества: менее жёстко штрафовать за большие отклонения от истинного таргета.\\nMAE\\nИспользовать RMSE для сравнения моделей на выборках с большим количеством выбросов может быть неудобно. В таких случаях прибегают к также знакомой вам в качестве функции потери метрике MAE (mean absolute error):\\nMAE(ytrue,ypred)=1N∑i=1N∣yi−f(xi)∣MAE(y^{true}, y^{pred}) = \\\\frac{1}{N}\\\\sum_{i=1}^{N} \\\\left|y_i - f(x_i)\\\\right| \\nMAE(ytrue,ypred)=N1\\u200bi=1∑N\\u200b∣yi\\u200b−f(xi\\u200b)∣Метрики, учитывающие относительные ошибки\\nИ MSE и MAE считаются как сумма абсолютных ошибок на объектах.\\nРассмотрим следующую задачу: мы хотим спрогнозировать спрос товаров на следующий месяц. Пусть у нас есть два продукта: продукт A продаётся в количестве 100 штук, а продукт В в количестве 10 штук. И пусть базовая модель предсказывает количество продаж продукта A как 98 штук, а продукта B как 8 штук. Ошибки на этих объектах добавляют 4 штрафных единицы в MAE.\\nИ есть 2 модели-кандидата на улучшение. Первая предсказывает товар А 99 штук, а товар B 8 штук. Вторая предсказывает товар А 98 штук, а товар B 9 штук.\\nОбе модели улучшают MAE базовой модели на 1 единицу. Однако, с точки зрения бизнес-заказчика вторая модель может оказаться предпочтительнее, так как предсказание продажи редких товаров может быть приоритетнее. Один из способов учесть такое требование — рассматривать не абсолютную, а относительную ошибку на объектах.\\nMAPE, SMAPE\\nКогда речь заходит об относительных ошибках, сразу возникает вопрос: что мы будем ставить в знаменатель?\\nВ метрике MAPE (mean absolute percentage error) в знаменатель помещают целевое значение:\\nMAPE(ytrue,ypred)=1N∑i=1N∣yi−f(xi)∣∣yi∣MAPE(y^{true}, y^{pred}) = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\frac{ \\\\left|y_i - f(x_i)\\\\right|}{\\\\left|y_i\\\\right|} \\nMAPE(ytrue,ypred)=N1\\u200bi=1∑N\\u200b∣yi\\u200b∣∣yi\\u200b−f(xi\\u200b)∣\\u200bС особым случаем, когда в знаменателе оказывается 000, обычно поступают «инженерным» способом: или выдают за непредсказание 000 на таком объекте большой, но фиксированный штраф, или пытаются застраховаться от подобного на уровне формулы и переходят к метрике SMAPE (symmetric mean absolute percentage error):\\nSMAPE(ytrue,ypred)=1N∑i=1N2∣yi−f(xi)∣yi+f(xi)SMAPE(y^{true}, y^{pred}) = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\frac{ 2 \\\\left|y_i - f(x_i)\\\\right|}{y_i + f(x_i)} \\nSMAPE(ytrue,ypred)=N1\\u200bi=1∑N\\u200byi\\u200b+f(xi\\u200b)2∣yi\\u200b−f(xi\\u200b)∣\\u200bЕсли же предсказывается ноль, штраф считаем нулевым.\\nТаким переходом от абсолютных ошибок на объекте к относительным мы сделали объекты в тестовой выборке равнозначными: даже если мы делаем абсурдно большое предсказание, на фоне которого истинная метка теряется, мы получаем штраф за этот объект порядка 1 в случае MAPE и 2 в случае SMAPE.\\nWAPE\\nКак и любая другая метрика, MAPE имеет свои границы применимости: например, она плохо справляется с прогнозом спроса на товары с прерывистыми продажами. Рассмотрим такой пример:\\n\\n\\n\\n\\n\\nПонедельник\\n\\n\\nВторник\\n\\n\\nСреда\\n\\n\\n\\n\\nПрогноз\\n\\n\\n55\\n\\n\\n2\\n\\n\\n50\\n\\n\\n\\n\\nПродажи\\n\\n\\n50\\n\\n\\n1\\n\\n\\n50\\n\\n\\n\\n\\nMAPE\\n\\n\\n10%\\n\\n\\n100%\\n\\n\\n0%\\n\\n\\n\\n\\nСреднее MAPE — 36.7%, что не очень отражает реальную ситуацию, ведь два дня мы предсказывали с хорошей точностью. В таких ситуациях помогает WAPE (weighted average percentage error):\\nWAPE(ytrue,ypred)=∑i=1N∣yi−f(xi)∣∑i=1N∣yi∣WAPE(y^{true}, y^{pred}) = \\\\frac{\\\\sum_{i=1}^{N} \\\\left|y_i - f(x_i)\\\\right|}{\\\\sum_{i=1}^{N} \\\\left|y_i\\\\right|} \\nWAPE(ytrue,ypred)=∑i=1N\\u200b∣yi\\u200b∣∑i=1N\\u200b∣yi\\u200b−f(xi\\u200b)∣\\u200bЕсли мы предсказываем идеально, то WAPE = 0, если все предсказания отдаём нулевыми, то WAPE = 1.\\nВ нашем примере получим WAPE = 5.9%\\nRMSLE\\nАльтернативный способ уйти от абсолютных ошибок к относительным предлагает метрика RMSLE (root mean squared logarithmic error):\\nRMSLE(ytrue,ypred∣c)=1N∑i=1N(12log\\u2061(yi+c)−log\\u2061(f(xi)+c))2RMSLE(y^{true}, y^{pred}| c) = \\\\sqrt{ \\\\frac{1}{N} \\\\sum_{i=1}^N \\\\left(\\\\vphantom{\\\\frac12}\\\\log{\\\\left(y_i + c \\\\right)} - \\\\log{\\\\left(f(x_i) + c \\\\right)}\\\\right)^2 } \\nRMSLE(ytrue,ypred∣c)=N1\\u200bi=1∑N\\u200b(21\\u200blog(yi\\u200b+c)−log(f(xi\\u200b)+c))2\\u200bгде нормировочная константа ccc вводится искусственно, чтобы не брать логарифм от нуля. Также по построению видно, что метрика пригодна лишь для неотрицательных меток.\\nВеса в метриках\\nВсе вышеописанные метрики легко допускают введение весов для объектов. Если мы из каких-то соображений можем определить стоимость ошибки на объекте, можно брать эту величину в качестве веса. Например, в задаче предсказания спроса в качестве веса можно использовать стоимость объекта.\\nДоля предсказаний с абсолютными ошибками больше, чем d\\nЕще одним способом охарактеризовать качество модели в задаче регрессии является доля предсказаний с абсолютными ошибками больше заданного порога ddd:\\n1N∑i=1NI[∣yi−f(xi)∣>d]\\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\mathbb{I}\\\\left[ \\\\left| y_i - f(x_i) \\\\right| > d \\\\right] \\nN1\\u200bi=1∑N\\u200bI[∣yi\\u200b−f(xi\\u200b)∣>d]Например, можно считать, что прогноз погоды сбылся, если ошибка предсказания составила меньше 1/2/3 градусов. Тогда рассматриваемая метрика покажет, в какой доле случаев прогноз не сбылся.\\nКак оптимизировать метрики регрессии?\\nПусть мы выбрали, что метрика качества алгоритма будет F(a(X),Y)F(a(X), Y)F(a(X),Y). Тогда мы хотим обучить модель так, чтобы F на валидационной выборке была минимальная/максимальная. Аналогично задачам классификации лучший способ добиться минимизации метрики FFF — выбрать в качестве функции потерь ту же F(a(X),Y)F(a(X), Y)F(a(X),Y). К счастью, основные метрики для регрессии: MSE, RMSE, MAE можно оптимизировать напрямую. С формальной точки зрения MAE не дифференцируема, так как там присутствует модуль, чья производная не определена в нуле. На практике для этого выколотого случая в коде можно возвращать ноль.\\nДля оптимизации MAPE придётся изменять оптимизационную задачу. Оптимизацию MAPE можно представить как оптимизацию MAE, где объектам выборки присвоен вес 1∣yi∣\\\\frac{1}{\\\\vert y_i\\\\vert}∣yi\\u200b∣1\\u200b.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф2.5. Градиентный бустингКак устроено самое мощное семейство не-нейросетевых моделей: градиентный бустинг над решающими деревьямиСледующий параграф3.2. Кросс-валидацияКак строить надёжные оценки качества моделей и\\xa0никогда не\\xa0смешивать train и\\xa0testЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_11.html', 'title': 'Кросс-валидация'}, page_content='Кросс-валидацияЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/33.1.Метрики классификации и регрессии3.2.Кросс-валидацияHold-outk-FoldКогда стоит заподозрить, что оценка качества модели завышена?Почитать по теме3.3.Подбор гиперпараметров4.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Кросс-валидация3.2. Кросс-валидацияАвторы Елистратова ЕвгенияКак строить надёжные оценки качества моделей и\\xa0никогда не\\xa0смешивать train и\\xa0testКросс-валидация — это процедура для оценки качества работы модели, которая широко применяется в машинном обучении. Она помогает сравнить между собой различные модели и выбрать наилучшую для конкретной задачи.\\nВ этом разделе мы рассмотрим наиболее распространённые методы кросс-валидации, а также обсудим возможные проблемы, которые могут возникнуть в процессе их применения.\\nHold-out\\nМетод hold-out представляет из себя простое разделение на train и test:\\n\\n\\n\\nИсточник\\n\\n\\nТакое разделение очень легко реализовать с помощью библиотеки sklearn:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1import numpy as np\\n2from sklearn.model_selection import train_test_split\\n3 \\n4X, y = np.arange(1000).reshape((500, 2)), np.arange(500)\\n5X_train, X_test, y_train, y_test = train_test_split(\\n6    X, y, \\n7    test_size=0.2, \\n8    random_state=42\\n9)\\n\\n\\nЧтобы оценить модель, вы обучаете её на тренировочном множестве, а результаты измеряете на тестовом. У sklearn по дефолту выставлен параметр shuffle=True, то есть перед разделением на тренировочное и тестовое множества происходит перемешивание семплов (и для воспроизводимости такого разбиения нужно фиксировать random_state).Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nА что будет, если не перемешать данные?\\nЕсли обучение модели не зависит от порядка подачи в неё примеров (что верно, например, для k-NN или решающего дерева), то перемешивание данных влияет только на то, кто в итоге окажется в train и test. Если данные шли какими-то группами, например сначала 800 картинок с кошками, а за ними 200 картинок с собаками, а train_test_split был совершён в пропорции 0.8, то модель просто не увидит собак в трейне.\\nА в случае когда модель обучается с помощью градиентного спуска или его вариации (про различные модификации SGD подробно рассказывается в параграфе о нейросетях), отсутствие перемешивания данных может влиять более интересным образом.\\nВот пример из практики Yandex.Research —  как вы думаете, что не так с графиком обучения данной модели?\\n\\n\\n\\nИсточник: курс Лены Войты по NLP\\n\\n\\nОтвет (не открывайте сразу; сначала подумайте сами!)На графике видна периодичность по числу итераций! По большим пикам можно вычислить места, где проход по данным начался заново. Кроме того, график в конце ползёт вниз, что означает, что модель уже начала переобучаться, выучив последовательность данных на трейне и используя эту информацию больше, чем сами данные.\\nЕсли данные перемешать, то график обучения станет таким:\\n\\n\\n\\nИсточник: курс Лены Войты по NLP\\n\\n\\nМожно привести даже более простой пример, когда отсутствие перемешивания данных может вас сильно подвести. Допустим, у вас большой датасет из миллиона кошек и собак и вам нужно научить модель их различать.\\nПусть изначальный порядок тренировочных данных такой: сначала подряд идёт полмиллиона картинок с кошками, а затем так же подряд идут картинки с собаками. Тогда модель на первой половине обучения выучит, что на картинке всегда кошка, а за вторую забудет, что учила на первой, и будет всегда предсказывать собак. При этом на сами данные при предсказании она опираться не будет вообще.\\nПродолжим. Если у вас достаточно данных, лучше всегда предусматривать также валидационное множество:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1import numpy as np\\n2from sklearn.model_selection import train_test_split\\n3 \\n4X, y = np.arange(1000).reshape((500, 2)), np.arange(500)\\n5X_train, X_test, y_train, y_test = train_test_split(\\n6    X, y, \\n7    test_size=0.2, \\n8    random_state=42\\n9)\\n10X_train, X_val, y_train, y_val = train_test_split(\\n11    X_train, y_train, \\n12    test_size=0.1, \\n13    random_state=42\\n14)\\n\\n\\nЕсли вы перебираете какие-то модели для вашей задачи, то оптимизировать их качества стоит на валидационном множестве, а окончательное сравнение моделей проводить на тестовом множестве.\\nОптимизация качеств модели может включать в себя подбор гиперпараметров, подбор архитектуры (в случае нейросетей) или подбор оптимального трешхолда для максимизации значений целевой метрики (например, вы делаете двуклассовую классификацию, а модель выдаёт непрерывные значения от 0 до 1, которые нужно бинаризовать так, чтобы получить максимальный скор по F1) и так далее.\\nЕсли же оптимизировать качества моделей и проводить их сравнение на одном и том же множестве, то можно неявно заложить в модели информацию о тестовом множестве и получить результаты хуже ожидаемых на новых данных.\\nНемного прервёмся на пример — к чему может привести неявное использование моделью тестового множества\\nПредставьте, что вы хотите обучить модель одномерной линейной регрессии для предсказания ваших данных:\\ny=mx+b,  y = mx + b,\\ny=mx+b,где mmm и bbb — искомые параметры вашей модели.\\nОднако представьте, что параметр bbb вам кто-то запретил обучать на тренировочном множестве и для вас у этой модели всего один параметр. Пусть на первой итерации у вас задано какое-то фиксированное b=b0b = b_0b=b0\\u200b, вы с ним подобрали на трейне лучшее mmm при данном b0b_0b0\\u200b и замерили качество получившейся модели на тестовом множестве.\\nНа следующей итерации вы взяли новое значение b=b1b = b_1b=b1\\u200b, повторили с ним предыдущий шаг и так далее. Теперь пришло время выбирать модель, и из всех них вы выбрали ту, которая показала лучший результат на тестовом множестве. Вам может показаться, что ваша модель с одним параметром обучена на трейне и всё хорошо, но на самом деле вы использовали оба множества, чтобы обучить модель с двумя параметрами, и теперь ваша тестовая оценка качества модели завышена.\\nМожет показаться, что этот пример довольно искусственный, но он на самом деле легко переносится на модели любой сложности. Просто представьте себе, что часть обучаемых весов вашей сложной модели вам запретили обучать на трейне и вы начинаете так же, как и выше, оценивать их на тесте, то есть по факту учить на тесте.\\nА чем такая ситуация отличается от подбора гиперпараметров модели (которые вы уже действительно не можете обучить на трейне) сразу на тестовом множестве? Вообще говоря, ничем.\\nПродолжим. Для окончательного применения найденную лучшую модель можно обучить на всех имеющихся данных. Правда, вы не сможете оценить качество получившейся модели, так как у вас уже не будет тестового множества. Чтобы примерно оценить, как будет вести себя модель при добавлении новых данных, вы можете построить кривые обучения: графики качества модели на трейне и на тесте в зависимости от числа поданных семплов на вход.\\nКривые обучения могут выглядеть следующим образом (код для отрисовки таких кривых можно найти в документации библиотеки sklearn):\\n\\n\\n\\nИсточник\\n\\n\\nЕсли графики подсказывают, что качество модели по валидационным метрикам продолжает расти, имеет смысл добавить новые данные.\\nНа картинке выше приведены кривые обучения двух моделей на одном и том же датасете. Модель слева показала итоговые результаты явно хуже модели справа — плюс график качества на валидации у неё близок к плато, хотя и продолжает расти, — а качество модели справа могло бы ещё вырасти при добавлении дополнительных семплов (качество на трейне константно высокое, а на валидации возрастает).\\nСтратификация (stratification)\\nПри простом случайном разделении на тренировочное и тестовое множества (как в примерах выше) может случиться так, что их распределения окажутся не такими, как у всего исходного множества. Проиллюстрируем такую ситуацию на примере случайного разбиения датасета Iris на трейн и тест. Распределение классов в данном датасете равномерное:\\n\\n33.3%33.3\\\\%33.3% Setosa\\n33.3%33.3\\\\%33.3% Versicolor\\n33.3%33.3\\\\%33.3% Virginica\\n\\nСлучайное разбиение, в котором две трети цветов (100) отправились в трейн, а оставшаяся треть (50) отправилась в тест, может выглядеть, например, так:\\n\\nтрейн: 38 ×\\\\times× Setosa, 28 ×\\\\times× Versicolor, 34 ×\\\\times× Virginica (распределение 38%:28%:34%38\\\\%:28\\\\%:34\\\\%38%:28%:34%)\\nтест: 12 ×\\\\times× Setosa, 22 ×\\\\times× Versicolor, 16 ×\\\\times× Virginica (распределение 24%:44%:32%24\\\\%:44\\\\%:32\\\\%24%:44%:32%)\\n\\nЕсли распределение цветов в исходном датасете отражает то, что в природе они встречаются одинаково часто, то мы только что получили два новых датасета, не соответствующих распределению цветов в природе. Распределения обоих датасетов вышли не только несбалансированными, но ещё и разными: самый частый класс в трейне соответствует наименее частому классу в тесте.\\nНа помощь в такой ситуации может прийти стратификация: разбиение на трейн и тест, сохраняющее соотношение классов, представленное в исходном датасете. В библиотеке sklearn такое разбиение можно получить с помощью параметра stratify:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1import numpy as np\\n2from sklearn.model_selection import train_test_split\\n3 \\n4X, y = np.arange(1000).reshape((500, 2)), np.random.choice(4, size=500, p=[0.1, 0.2, 0.3, 0.4])\\n5X_train, X_test, y_train, y_test = train_test_split(\\n6    X, y, \\n7    test_size=0.2, \\n8    random_state=42,\\n9    stratify=y\\n10)\\n\\n\\nВ целом на достаточно больших датасетах (порядка хотя бы 10 тысяч семплов) со сбалансированными классами можно не очень сильно беспокоиться об описанной выше проблеме и использовать обычный random split.\\nНо если у вас очень несбалансированные данные, в которых один класс встречается сильно чаще другого (как, например, в задачах фильтрации спама или сегментации осадков на спутниковых снимках), стратификация может довольно сильно помочь.\\nk-Fold\\nМетод k-Fold чаще всего имеют в виду, когда говорят о кросс-валидации. Он является обобщением метода hold-out и представляет из себя следующий алгоритм:\\n\\nФиксируется некоторое целое число kkk (обычно от 5 до 10), меньшее числа семплов в датасете.\\nДатасет разбивается на kkk одинаковых частей (в последней части может быть меньше семплов, чем в остальных). Эти части называются фолдами.\\nДалее происходит kkk итераций, во время каждой из которых один фолд выступает в роли тестового множества, а объединение остальных — в роли тренировочного. Модель учится на k−1k - 1k−1 фолде и тестируется на оставшемся.\\nФинальный скор модели получается либо усреднением kkk получившихся тестовых результатов, либо измеряется на отложенном тестовом множестве, не участвовавшем в кросс-валидации.\\n\\n\\n\\n\\nИсточник\\n\\n\\nЭтот метод есть в sklearn:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1import numpy as np\\n2from sklearn.model_selection import KFold\\n3 \\n4X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\\n5y = np.array([1, 2, 3, 4])\\n6kf = KFold(n_splits=2)\\n7 \\n8for train_index, test_index in kf.split(X):\\n9    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\\n10    X_train, X_test = X[train_index], X[test_index]\\n11    y_train, y_test = y[train_index], y[test_index]\\n12\\'\\'\\'\\n13result:\\n14TRAIN: [2 3] TEST: [0 1]\\n15TRAIN: [0 1] TEST: [2 3]\\n16\\'\\'\\'\\n\\n\\nВ коде выше получилось два фолда: в первый вошли объекты с индексами 2 и 3, во второй — объекты с индексами 0 и 1. На первой итерации алгоритма фолд с индексами 2 и 3 будет тренировочным, а на второй — фолд с индексами 0 и 1. В sklearn есть также метод cross_val_score, принимающий на вход классификатор, данные и способ разбиения данных (либо число фолдов) и возвращающий результаты кросс-валидации:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1from sklearn.model_selection import cross_val_score\\n2 \\n3clf = svm.SVC(kernel=\\'linear\\', C=1, random_state=42)\\n4scores = cross_val_score(clf, X, y, cv=5)\\n5print(scores)\\n6\\'\\'\\'\\n7result:\\n8array([0.96..., 1. , 0.96..., 0.96..., 1. ])\\n9\\'\\'\\'\\n\\n\\nИнтересный вопрос состоит в том, какую модель брать для сравнения с остальными на отложенном тестовом множестве (если оно у вас есть) либо для окончательного применения в задаче. После применения k-Fold для одной модели у вас на руках останется kkk экземпляров (инстансов) этой модели, обученных на разных подмножествах трейна. Возможные варианты:\\n\\nделать предсказание с помощью усреднения предсказаний этих kkk инстансов;\\nиз этих kkk инстансов выбрать тот, который набрал лучший скор на своём тестовом фолде, и применять дальше его;\\nзаново обучить модель уже на всех kkk фолдах и делать предсказания уже этой моделью.\\n\\nВыбирать, какой способ лучше, нужно в зависимости от конкретной задачи и имеющихся вычислительных возможностей.\\nМетод k-Fold даёт более надёжную оценку качества модели, чем hold-out, так как обучение и тест модели происходят на разных подмножествах исходного датасета. Однако проведение kkk итераций обучения и теста может быть вычислительно затратным, и поэтому метод обычно применяют либо когда данных достаточно мало, либо при наличии большого количества вычислительных ресурсов, позволяющих проводить все kkk итераций параллельно.\\nВ реальных задачах данных зачастую достаточно много для того, чтобы hold-out давал хорошую оценку качества модели, поэтому k-Fold в больших задачах применяется не очень часто.\\nLeave-one-out\\nМетод leave-one-out (LOO) — частный случай метода k-Fold: в нём каждый фолд состоит ровно из одного семпла. LOO тоже есть в библиотеке sklearn:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1import numpy as np\\n2from sklearn.model_selection import LeaveOneOut\\n3 \\n4X = np.array([[1, 2], [3, 4], [5, 6]])\\n5y = np.array([1, 2, 3])\\n6loo = LeaveOneOut()\\n7 \\n8for train_index, test_index in loo.split(X):\\n9    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\\n10    X_train, X_test = X[train_index], X[test_index]\\n11    y_train, y_test = y[train_index], y[test_index]\\n12\\'\\'\\'\\n13result:\\n14TRAIN: [1 2] TEST: [0]\\n15TRAIN: [0 2] TEST: [1]\\n16TRAIN: [0 1] TEST: [2]\\n17\\'\\'\\'\\n\\n\\nЭтот метод может понадобиться в случае, если у вас очень мало данных, — например, в задаче сегментации клеток на изображениях с оптического микроскопа, — и вы хотите использовать максимальное их количество для обучения модели.\\nДля валидации на каждой итерации методу требуется всего один семпл, однако и итераций будет столько, сколько семплов в данных, поэтому метод неприменим для средних и больших задач.\\nStratified k-Fold\\nМетод stratified k-Fold — это метод k-Fold, использующий стратификацию при разбиении на фолды: каждый фолд содержит примерно такое же соотношение классов, как и всё исходное множество. Такой подход может потребоваться в случае, например, очень несбалансированного соотношения классов, когда при обычном random split некоторые фолды могут либо вообще не содержать семплов каких-то классов, либо содержать их слишком мало. Этот метод также представлен в sklearn:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1import numpy as np\\n2from sklearn.model_selection import StratifiedKFold\\n3 \\n4X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\\n5y = np.array([0, 0, 1, 1])\\n6skf = StratifiedKFold(n_splits=2)\\n7 \\n8for train_index, test_index in skf.split(X, y):\\n9    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\\n10    X_train, X_test = X[train_index], X[test_index]\\n11    y_train, y_test = y[train_index], y[test_index]\\n12\\'\\'\\'\\n13result:\\n14TRAIN: [1 3] TEST: [0 2]\\n15TRAIN: [0 2] TEST: [1 3]\\n16\\'\\'\\'\\n\\n\\nКросс-валидация на временных рядах\\nСуществует такая задача, как прогнозирование временных рядов. На практике она часто возникает в форме «Что будет с показателями нашего продукта в ближайший день / месяц / год?». При этом имеются какие-то исторические данные этих показателей за предыдущее время, которые можно визуализировать в виде некоторого графика по времени:\\n\\n\\n\\nИсточник\\n\\n\\nЭтот график — пример графика временного ряда, и наша задача — спрогнозировать, как будет выглядеть данный график в будущие моменты времени. Кросс-валидация моделей для такой задачи осложняется тем, что данные не должны пересекаться по времени: тренировочные данные должны идти до валидационных, а валидационные — до тестовых. С учётом этих особенностей фолды в кросс-валидации для временных рядов располагаются вдоль временной оси так, как показано на следующей картинке:\\n\\n\\n\\nИсточник\\n\\n\\nВ sklearn реализована такая схема кросс-валидации:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1import numpy as np\\n2from sklearn.model_selection import TimeSeriesSplit\\n3X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\\n4y = np.array([1, 2, 3, 4, 5, 6])\\n5tscv = TimeSeriesSplit()\\n6print(tscv)\\n7 \\n8for train_index, test_index in tscv.split(X):\\n9    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\\n10    X_train, X_test = X[train_index], X[test_index]\\n11    y_train, y_test = y[train_index], y[test_index]\\n12 \\n13\\'\\'\\'\\n14result:\\n15TRAIN: [0] TEST: [1]\\n16TRAIN: [0 1] TEST: [2]\\n17TRAIN: [0 1 2] TEST: [3]\\n18TRAIN: [0 1 2 3] TEST: [4]\\n19TRAIN: [0 1 2 3 4] TEST: [5]\\n20\\'\\'\\'\\n\\n\\nКогда стоит заподозрить, что оценка качества модели завышена?\\nВаша модель показала очень высокое качество на тестовых данных, вы радостно откидываетесь на спинку кресла и достаёте шампанское... Или пока рано? Перед тем как информировать коллег о своих высоких результатах, проверьте, что вы не допустили какую-то из следующих ошибок:\\n\\nваши данные не были перемешаны (вспоминаем пример выше с тензорбордом курильщика);\\nвы подбирали гиперпараметры на тестовом множестве и на нём же оценивали качество модели;\\nу вас в данных есть фича, которая в некотором смысле является «прокси» к таргету (proxy for the target). Это такая фича, которая почти равна таргету, хотя формально им не является и так же, как и таргет, не будет доступна на момент реального применения модели;\\n\\nПримерПусть вы хотите предсказывать, сколько будут зарабатывать выпускники разных вузов с разных факультетов через 10 лет после выпуска. Допустим, что у вас есть разнообразные исторические данные о прошлых выпускниках (какие вуз / школу оканчивали, какие факультеты, в каком городе и так далее), где много колонок, и есть искушение особенно не вглядываться в каждую отдельную колонку, а просто разбить данные на трейн и тест и отправить в модель. Но потом вдруг обнаруживается, что у вас всё это время имелась колонка «Доход через пять лет после выпуска», которая явно скоррелирована с таргетом и является важной для вашей модели, но на момент реального применения модели этой информации у вас не будет. Соответственно, наличием этой колонки во многом и объяснялся высокий скор вашей модели. Мораль: всегда внимательно изучайте свои данные перед обучением моделей.\\n\\nвы проводили feature engineering на всём датасете, а не только на трейне. Например, вы строили tf-idf фичи или bag-of-words на всех данных, а не только на трейне, тем самым заложив в свои тренировочные данные информацию о тестовых данных;\\nвы применяли стандартизацию данных на всём датасете, а не только на трейне. Например, в случае StandardScaler тестовое множество повлияет на используемые этим методом оценки среднего и стандартного отклонения;\\nвы смешали трейн с тестом.\\n\\nПоследний пункт может звучать очень банально, но на практике часто оказывается, что правильно разделить данные на тренировочные и тестовые не так просто даже с учётом всех описанных выше техник. Об этом в следующих разделах.\\nПримеры подмешивания тестовых данных в тренировочные\\n\\nВаши данные зависят от времени, а вы при разбиении на трейн и тест это не учли. Например, вы применили обычный random split при работе с временными рядами, передав тем самым вашей модели информацию из будущего. Или вы предсказываете погоду на несколько часов вперёд, а у вас данные из одного и того же дня находятся и в трейне, и в тесте.\\nУ вас есть датасет с картинками, и вы решили увеличить количество семплов в нём с помощью аугментаций (примерами аугментаций могут служить симметричные отражения, повороты, растяжения). При этом вы взяли весь датасет, применили к нему аугментации и только после этого разделили на трейн и тест. В таком случае преобразования какой-то одной картинки могут попасть в оба множества, и вы получите пересечение трейна и теста.\\nВы решаете задачу рекомендации статей или постов пользователям на основании их комментариев и прочтений, при этом в трейне и тесте у вас одни и те же пользователи.\\nВы решаете какую-то задачу, где происходит работа с видеоданными. Например, распознаёте движение по видео или предсказываете фамилию актёра, попавшего в кадр. При этом в трейн и тест у вас попадают различные кадры из одного и того же видео.\\nУ вас есть спутниковые снимки, и вы хотите по ним предсказывать рельеф местности. При этом у вас в трейне и тесте есть кропы снимков над одними и теми же географическими координатами (хоть и в разное время).\\nВы обучаете голосового ассистента в звуковом потоке распознавать момент, когда к нему обращаются (например, «Слушай, Алиса», «Ok, Google»). При этом у вас в трейне и тесте одни и те же люди. Это, на первый взгляд, не очень страшная проблема, но на самом деле достаточно большая нейронка может запомнить интонации и манеру речи конкретного человека и будет использовать эти сведения для тестовых записей с этим человеком. При этом на новых людях распознавание будет работать сильно хуже.\\nВы хотите расширить тренировочный датасет какими-то дополнительными данными из другого датасета, но при этом оказывается, что другой датасет содержит в себе часть тестового множества вашего исходного датасета. Например, есть два публичных датасета: ImageNet LSVRC 2015, в котором 1000 классов и чуть больше миллиона изображений, и ImageNet, в котором 21 тысяча классов и чуть больше 14 миллионов изображений. При этом первый полностью содержится во втором, поэтому использование ImageNet для расширения обучающей выборки из ImageNet LSVRC 2015 может закончиться тем, что в трейне окажутся примеры из тестового множества, сформированного из ImageNet LSVRC 2015.\\n\\nЕщё один интересный пример, когда что-то пошло не так\\nПример заимствован отсюда. Допустим, что вы должны обучить модель, предсказывающую тему новостной статьи по её тексту. Если отсортировать статьи по дате их публикации, то ваши данные могут выглядеть, например, так:\\n\\n\\n\\nИсточник\\n\\n\\nЗдесь форма и цвет фигуры соответствуют новости, которой посвящена статья. Почему случайное разбиение данных на трейн и тест может привести к проблемам в этой задаче?\\nНа самом деле новостные статьи с одной и той же тематикой появляются кластерами во времени, так как статьи о новом событии выходят, как правило, порциями в то же время, когда произошло событие. Если разбить данные случайно, то тренировочное и тестовое множества с большой вероятностью будут содержать статьи на одни и те же наборы тем:\\n\\n\\n\\nИсточник\\n\\n\\nТакое разбиение не соответствует тому, как потом модель будет применяться в реальной задаче: при нём модель будет ожидать равномерного распределения тем, предложенных ей в трейне, тогда как в реальности ей на вход будут приходить всё те же кластеры, и они, вообще говоря, не обязаны были быть в её тренировочном множестве. Простым решением будет при разбиении на трейн и тест учитывать время, когда была опубликована статья:\\n\\n\\n\\nИсточник\\n\\n\\nТут нужно, однако, учитывать, что в реальности кластеры историй по времени выражены не столь чётко и могут пересекаться. Поэтому если трейн и тест расположены слишком близко друг от друга по времени, то они могут пересечься. В принципе, это не так плохо с учётом того, что новости о каких-то событиях могут продолжать выходить в течение некоторого растянутого промежутка времени. Но если хочется избежать такой ситуации, то можно оставить между трейном и тестом некоторый временной зазор: тренироваться, например, на апрельских публикациях, а тестироваться на второй неделе мая, оставив, таким образом, недельный промежуток между двумя множествами.\\nПочитать по теме\\n\\nОригинальный текст описанного выше примера.\\nЕщё один классный пример, когда случайное разбиение данных может испортить ML-модель.\\nОтличный блог-пост от Neptune про различные методы кросс-валидации.\\nРаздел нашего учебника, посвящённый сравнению и оценке качества моделей.\\nБольшая статья-обзор про методы сравнения моделей и оценки их качества.\\nСекция Model Selection от sklearn.\\nБлог-пост про различные «умные» способы получить завышенные оценки качества моделей.\\nОтличный гайд о том, как читать графики обучающих кривых в разных случаях.\\nСтатья про временные ряды из курса «Открытый курс машинного обучения» от ODS\\nБиблиотека Prophet от Facebook для прогнозирования временных рядов, у которой есть своя имплементация кросс-валидации с дополнительными фичами (таблицы с результатами кросс-валидации, красивые графики).\\nЗдесь можно почитать статью с теоретическим обоснованием метода Prophet.\\nОтличное видео про лики в данных от DataRobot.\\nБлог-пост на эту же тему от них же.\\nСтатья про методики разбиения данных в рекомендательных системах.\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф3.1. Метрики классификации и регрессииКак оценить качество модели для классификации или регрессии и\\xa0почему для разных задач нужны разные метрикиСледующий параграф3.3. Подбор гиперпараметровКак эффективно подбирать значения гиперпараметров модели и\\xa0не\\xa0переобучиться при этомЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_46.html', 'title': 'Обучение с подкреплением'}, page_content=\"Обучение с подкреплениемЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/211.1.Обучение с подкреплениемПостановка задачиОкей, и как такое решать?А где же метод проб и ошибок?Дилемма Exploration-exploitationДобавим нейросетокExperience ReplayА если пространство действий непрерывно?Policy Gradient алгоритмыЧто там ещё?11.2.Краудсорсинг12.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Обучение с подкреплением11.1. Обучение с подкреплениемАвторыИванов СергейДо сих пор опыт, благодаря которому было возможно обучение в наших алгоритмах, был задан в виде обучающей выборки. Насколько такая модель обучения соотносится с тем, как учится, например, человек? Чтобы научиться кататься на велосипеде, печь тортики или играть в теннис, нам не нужны огромные датасеты с примерами того, что нужно делать в каждый момент; вместо этого мы способны обучаться методом проб и ошибок (trial and error), предпринимая попытки решить задачу, взаимодействуя с окружающим миром, и как-то улучшая своё поведение на основе полученного в ходе этого взаимодействия опыта.\\nВ обучении с подкреплением (reinforcement learning, RL) мы хотим построить алгоритм, моделирующий обучение методом проб и ошибок. Вместо получения обучающей выборки на вход такой алгоритм будет взаимодействовать с некоторой средой (environment), окружающим миром, а в роли «разметки» будет выступать награда (reward) — скалярная величина, которая выдаётся после каждого шага взаимодействия со средой и показывает, насколько хорошо алгоритм справляется с поставленной ему задачей. Например, если вы печёте тортики, то за каждый испечённый тортик вы получаете +1, а если вы пытаетесь кататься на велосипеде, то за каждое падение с велосипеда вам прилетает -1.\\n\\nНаграда не подсказывает, как именно нужно решать задачу и что вообще нужно делать;\\nНаграда может быть отложенной во времени (вы нашли в пустыне сокровища, но чтобы получить заслуженные тортики, вам ещё понадобится куча времени, чтобы выбраться из пустыни; а награда приходит только за тортики) или сильно разреженной (большую часть времени давать агенту +0). Всё это сильно отличает задачу от обучения с учителем;\\nНаграда предоставляет какой-то «сигнал» для обучения (хорошо/плохо), которого нет, например, в обучении без учителя.\\n\\n\\n\\n\\nисточник картинки — курс UC Berkeley AI\\n\\n\\nПостановка задачи\\nТеперь попробуем формализовать всю эту концепцию и познакомиться с местной терминологией. Задача обучения с подкреплением задаётся Марковским Процессом Принятия Решений (Markov Decision Process или сокращённо MDP) это четвёрка (S,A,P,r)(\\\\mathcal{S}, \\\\mathcal{A}, \\\\mathcal{P}, r)(S,A,P,r), где:\\n\\nS\\\\mathcal{S}S — пространство состояний (state space), множество состояний, в которых в каждый момент времени может находиться среда.\\nA\\\\mathcal{A}A — пространство действий (action space), множество вариантов, из которых нужно производить выбор на каждом шаге своего взаимодействия со средой.\\nP\\\\mathcal{P}P — функция переходов (transition function), которая задаёт изменение среды после того, как в состоянии s∈Ss \\\\in \\\\mathcal{S}s∈S было выбрано действие a∈Aa \\\\in \\\\mathcal{A}a∈A. В общем случае функция переходов может быть стохастична, и тогда такая функция переходов моделируется распределением p(s′∣s,a)p(s' \\\\mid s, a)p(s′∣s,a): с какой вероятностью в какое состояние перейдёт среда после выбора действия aaa в состоянии sss.\\nr\\u2009\\u2063:S×A→Rr \\\\colon \\\\mathcal{S} \\\\times \\\\mathcal{A} \\\\to \\\\mathbb{R}r:S×A→R — функция награды (reward function), выдающая скалярную величину за выбор действия aaa в состоянии sss. Это наш «обучающий сигнал».\\n\\nТрадиционно субъект, взаимодействующий со средой и влияющий на неё, называется в обучении с подкреплением агентом (agent). Агент руководствуется некоторым правилом, возможно, тоже стохастичным, как выбирать действия в зависимости от текущего состояния среды, которое называется стратегией (policy; термин часто транслитерируют и говорят политика) и моделируется распределением π(a∣s)\\\\pi(a \\\\mid s)π(a∣s). Стратегия и будет нашим объектом поиска, поэтому, как и в классическом машинном обучении, мы ищем какую-то функцию.\\nВзаимодействие со средой агента со стратегией π(a∣s)\\\\pi(a \\\\mid s)π(a∣s) моделируется так. Изначально среда находится в некотором состоянии s0s_0s0\\u200b. Агент сэмплирует действие из своей стратегии a0∼π(a0∣s0)a_0 \\\\sim \\\\pi(a_0 \\\\mid s_0)a0\\u200b∼π(a0\\u200b∣s0\\u200b). Среда отвечает на это, сэмплируя своё следующее состояние s1∼p(s1∣s0,a0)s_1 \\\\sim p(s_1 \\\\mid s_0, a_0)s1\\u200b∼p(s1\\u200b∣s0\\u200b,a0\\u200b) из функции переходов, а также выдаёт агенту награду в размере r(s0,a0)r(s_0, a_0)r(s0\\u200b,a0\\u200b). Процесс повторяется: агент снова сэмплирует a1a_1a1\\u200b, а среда отвечает генерацией s2s_2s2\\u200b и скалярной наградой r(s1,a1)r(s_1, a_1)r(s1\\u200b,a1\\u200b). Так продолжается до бесконечности или пока среда не перейдёт в терминальное состояние, после попадания в которое взаимодействие прерывается, и сбор агентом награды заканчивается. Если в среде есть терминальные состояния, одна итерация взаимодействия от начального состояния до попадания в терминальное состояние называется эпизодом (episode). Цепочка генерируемых в ходе взаимодействия случайных величин s0,a0,s1,a1,s2,a2,…s_0, a_0, s_1, a_1, s_2, a_2, \\\\dotss0\\u200b,a0\\u200b,s1\\u200b,a1\\u200b,s2\\u200b,a2\\u200b,… называется траекторией (trajectory). Примечание: функция награды тоже может быть стохастичной, и тогда награды за шаг тоже будут случайными величинами и частью траекторий, но без ограничения общности мы будем рассматривать детерминированные функции награды.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\n\\nИтак, фактически среда для нас — это управляемая марковская цепь: на каждом шаге мы выбором aaa определяем то распределение, из которого будет генерироваться следующее состояние. Мы предполагаем, во-первых, марковское свойство: что переход в следующее состояние определяется лишь текущим состоянием и не зависит от всей предыдущей истории:\\np(st+1∣st,at,st−1,at−1,…,s0,a0)=p(st+1∣st,at)p(s_{t+1} \\\\mid s_{t}, a_{t}, s_{t-1}, a_{t-1}, \\\\dots, s_0, a_0) = p(s_{t+1} \\\\mid s_{t}, a_{t})\\np(st+1\\u200b∣st\\u200b,at\\u200b,st−1\\u200b,at−1\\u200b,…,s0\\u200b,a0\\u200b)=p(st+1\\u200b∣st\\u200b,at\\u200b)Во-вторых, мы предполагаем стационарность: функция переходов p(s′∣s,a)p(s' \\\\mid s, a)p(s′∣s,a) не зависит от времени, от того, сколько шагов прошло с начала взаимодействия. Это довольно реалистичные предположения: законы мира не изменяются со временем (стационарность), а состояние — описывает мир целиком (марковость). В этой модели взаимодействия есть только одно нереалистичное допущение: полная наблюдаемость (full observability), которая гласит, что агент в своей стратегии π(a∣s)\\\\pi(a \\\\mid s)π(a∣s) наблюдает всё состояние sss полностью и может выбирать действия, зная об окружающем мире абсолютно всё; в реальности нам же доступны лишь какие-то частичные наблюдения состояния. Такая более реалистичная ситуация моделируется в частично наблюдаемых MDP (Partially observable MDP, PoMDP), но мы далее ограничимся полностью наблюдаемыми средами.\\nИтак, мы научились на математическом языке моделировать среду, агента и их взаимодействие между собой. Осталось понять, чего же мы хотим. Во время взаимодействия на каждом шаге агенту приходит награда rt=r(st,at)r_t = r(s_t, a_t)rt\\u200b=r(st\\u200b,at\\u200b), однако, состояния и действия st,ats_t, a_tst\\u200b,at\\u200b в рамках такой постановки — случайные величины. Один и тот же агент может в силу стохастики как внутренней (в силу случайности выбора действий в его стратегии), так и внешней (в силу стохастики в функции переходов) набирать очень разную суммарную награду ∑t≥0rt\\\\sum_{t \\\\ge 0} r_t∑t≥0\\u200brt\\u200b в зависимости от везения. Мы скажем, что хотим научиться выбирать действия так, чтобы собирать в среднем как можно больше награды.\\nЧто значит в среднем, в среднем по чему? По всей стохастике, которая заложена в нашем процессе взаимодействия со средой. Каждая стратегия π\\\\piπ задаёт распределение в пространстве траекторий — с какой вероятностью нам может встретится траектория T=(s0,a0,s1,a1,…\\u2009)\\\\mathcal{T} = (s_0, a_0, s_1, a_1, \\\\dots)T=(s0\\u200b,a0\\u200b,s1\\u200b,a1\\u200b,…):\\np(T∣π)=p(s0,a0,s1,a1,⋯∣π)=∏t≥0p(st+1∣st,at)π(at∣st)p(\\\\mathcal{T} \\\\mid \\\\pi) = p(s_0, a_0, s_1, a_1, \\\\dots \\\\mid \\\\pi) = \\\\prod_{t \\\\ge 0} p(s_{t + 1} \\\\mid s_t, a_t)\\\\pi(a_t \\\\mid s_t)\\np(T∣π)=p(s0\\u200b,a0\\u200b,s1\\u200b,a1\\u200b,⋯∣π)=t≥0∏\\u200bp(st+1\\u200b∣st\\u200b,at\\u200b)π(at\\u200b∣st\\u200b)Вот по такому распределению мы и хотим взять среднее получаемой агентом награды. Записывают это обычно как-нибудь так:\\nET∼π∑t≥0rt→max\\u2061π\\\\mathbb{E}_{\\\\mathcal{T} \\\\sim \\\\pi} \\\\sum_{t \\\\ge 0} r_t \\\\to \\\\max_{\\\\pi}\\nET∼π\\u200bt≥0∑\\u200brt\\u200b→πmax\\u200bЗдесь мат.ожидание по траекториям — это бесконечная цепочка вложенных мат.ожиданий:\\nET∼π(⋅)=Ea0∼π(a0∣s0)Es1∼p(s1∣s0,a0)Ea1∼π(a1∣s1)…(⋅)\\\\mathbb{E}_{\\\\mathcal{T} \\\\sim \\\\pi} ( \\\\cdot ) = \\\\mathbb{E}_{a_0 \\\\sim \\\\pi(a_0 \\\\mid s_0)} \\\\mathbb{E}_{s_1 \\\\sim p(s_1 \\\\mid s_0, a_0)} \\\\mathbb{E}_{a_1 \\\\sim \\\\pi(a_1 \\\\mid s_1)} \\\\dots ( \\\\cdot )\\nET∼π\\u200b(⋅)=Ea0\\u200b∼π(a0\\u200b∣s0\\u200b)\\u200bEs1\\u200b∼p(s1\\u200b∣s0\\u200b,a0\\u200b)\\u200bEa1\\u200b∼π(a1\\u200b∣s1\\u200b)\\u200b…(⋅)Вот такую конструкцию мы и хотим оптимизировать выбором стратегии π\\\\piπ. На практике, однако, вносят ещё одну маленькую корректировку. В средах, где взаимодействие может продолжаться бесконечно долго, агент может научиться набирать бесконечную награду, с чем могут быть связаны разные парадоксы (например, получать +1 на каждом втором шаге становится также хорошо, как получать +1 на каждом сотом шаге). Поэтому вводят дисконтирование (discounting) награды, которое гласит: тортик сейчас лучше, чем тот же самый тортик завтра. Награду, которую мы получим в будущем, агент будет дисконтировать на некоторое число γ\\\\gammaγ, меньшее единицы. Тогда наш функционал примет такой вид:\\nET∼π∑t≥0γtrt→max\\u2061π\\\\mathbb{E}_{\\\\mathcal{T} \\\\sim \\\\pi} \\\\sum_{t \\\\ge 0} \\\\gamma^t r_t \\\\to \\\\max_{\\\\pi}\\nET∼π\\u200bt≥0∑\\u200bγtrt\\u200b→πmax\\u200b\\n\\n\\nисточник картинки — курс UC Berkeley AI\\n\\n\\nЗаметим, что обучение с подкреплением - это в первую очередь задача оптимизации, оптимизации функционалов определённого вида. Если в классическом машинном обучении подбор функции потерь можно считать элементом инженерной части решения, то здесь функция награды задана нам готовая и определяет тот функционал, который мы хотим оптимизировать.\\nПримерыФормализм MDP очень общий, и под него попадает практически всё, что можно назвать «интеллектуальной задачей» (с той оговоркой, что не всегда очевидно, какая функция награды задаёт ту или иную задачу).\\nСамые простые примеры MDP можно нарисовать «на бумажке». Например, часто рассматривают «клетчатые миры» (GridWorlds): агент находится в некоторой позиции клетчатой доски и может в качестве действий выбирать одно из четырёх направлений. Такие миры могут по-разному реагировать агента за выбор действия «пойти в стену», с некоторой вероятностью перемещать агента не в том направлении, которое он выбрал, содержать предметы в некоторых клетках и так далее. Пространство состояний, в которых может оказаться агент, в таких примерах конечно, как и пространство действий. Такие MDP называют табличными: все состояния и действия можно перечислить.\\n\\n\\n\\nисточник картинки — курс UC Berkeley AI\\n\\n\\nОгромное разнообразие MDP предоставляют видеоигры. Можно считать, что на вход агенту подаётся изображение экрана видеоигры, и несколько раз в секунду агент выбирает, какие кнопки на контроллере он хочет нажать. Тогда пространство состояний - множество всевозможных картинок, которые вам может показать видеоигра. Множество, в общем-то, конечное (конечное количество пикселей экрана с тремя цветовыми каналами, каждый из который показывает целочисленное значение от 0 до 255), но только очень большое; например, их уже нельзя перечислить или сохранить все возможные варианты в памяти. Но на каждом шаге нужно выбирать действие из конечного набора: какие кнопки нажать, поэтому это задачи дискретного управления.\\n\\n\\n\\nисточник картинки — курс UC Berkeley AI\\n\\n\\nНаконец, естественный способ создавать среды — использование физических симуляций. В качестве бенчмарка часто используют locomotion — задачу научить какое-нибудь «существо» ходить в рамках той или иной физической модели (примеры можно посмотреть, например, здесь). Причём концептуально, в рамках задачи обучения с подкреплением, нам даже неважно, как именно устроена симуляция или как задана функция награды: мы хотим построить общий алгоритм оптимизации этой самой награды. Если награда поощряет перемещение центра масс «существа» вдоль некоторого направления, агент постепенно научится выбирать действия так, чтобы существо перемещалось и не падало, если последнее приводит к завершению эпизода и мешает дальнейшему получению награды.\\n\\n\\n\\nисточник картинки — статья DeepMind Producing Flexible Behaviours in Simulated Environments\\n\\n\\nВ таких задачах агент на каждом шаге выбирает несколько вещественных чисел в диапазоне [−1,1][-1, 1][−1,1], где -1 — «максимально расслабить» сустав, а +1 — «максимально напрячь». Такое пространство действий возникает во многих задачах робототехники, где нужно научиться поворачивать какой-нибудь руль, и у него есть крайнее правое и крайнее левое положение, но можно выбрать и любое промежуточное. Такие задачи называются задачами непрерывного управления (continuous control).\\nОкей, и как такое решать?\\nВыглядит сложновато, но у человечества есть уже довольно много наработок, как подойти к этой на вид очень общей задаче, причём с основной идеей вы скорее всего уже сталкивались. Называется она динамическим программированием.\\nДело в том, что мы оптимизируем не абы какой функционал, а среднюю дисконтированную кумулятивную награду. Чтобы придумать более эффективное решение, чем какой-нибудь подход, не использующий этот факт (например, эволюционные алгоритмы), нам нужно воспользоваться структурой поставленной задачи. Эта структура задана в формализме MDP и определении процесса взаимодействия агента со средой. Интуитивно она выражается так: вот мы сидим в некотором состоянии sss и хотим выбрать действие aaa как можно оптимальнее. Мы знаем, что после выбора этого действия мы получим награду за этот шаг r=r(s,a)r = r(s, a)r=r(s,a), среда перекинет нас в состояние s′s's′ и, внимание, дальше нас ждёт подзадача эквивалентной структуры: в точности та же задача выбора оптимального действия, только в другом состоянии. Действительно: когда мы будем принимать решение на следующем шаге, на прошлое мы повлиять уже не способны; стационарность означает, что законы, по которым ведёт себя среда, не поменялись, а марковость говорит, что история не влияет на дальнейший процесс нашего взаимодействия. Это наводит на мысль, что задача максимизации награды из текущего состояния тесно связана с задачей максимизации награды из следующего состояния s′s's′, каким бы оно ни было.\\nЧтобы сформулировать это на языке математики, вводятся «дополнительные переменные», вспомогательные величины, называемые оценочными функциями. Познакомимся с одной такой оценочной функцией - оптимальной Q-функцией, которую будем обозначать Q∗(s,a)Q^{*}(s,a)Q∗(s,a). Скажем, что Q∗(s,a)Q^{*}(s,a)Q∗(s,a) - это то, сколько максимально награды можно (в среднем) набрать после выбора действия aaa из состояния sss. Итак:\\nQ∗(s,a)=max\\u2061πET∼π∣s0=s,a0=a∑t≥0γtrtQ^{*}(s, a) = \\\\max_{\\\\pi} \\\\mathbb{E}_{\\\\mathcal{T} \\\\sim \\\\pi \\\\mid s_0 = s, a_0 = a} \\\\sum_{t \\\\ge 0} \\\\gamma^t r_t\\nQ∗(s,a)=πmax\\u200bET∼π∣s0\\u200b=s,a0\\u200b=a\\u200bt≥0∑\\u200bγtrt\\u200bЗапись T∼π∣s0=s,a0=a\\\\mathcal{T} \\\\sim \\\\pi \\\\mid s_0 = s, a_0 = aT∼π∣s0\\u200b=s,a0\\u200b=a здесь означает, что мы садимся в состояние s0=ss_0 = ss0\\u200b=s; выбираем действие a0=aa_0 = aa0\\u200b=a, а затем продолжаем взаимодействие со средой при помощи стратегии π\\\\piπ, порождая таким образом траекторию T\\\\mathcal{T}T. По определению, чтобы посчитать Q∗(s,a)Q^{*}(s,a)Q∗(s,a), нужно перебрать все стратегии, посмотреть, сколько каждая из них набирает награды после выбора aaa из состояния sss, и взять наилучшую стратегию. Поэтому эта оценочная функция называется оптимальной: она предполагает, что в будущем после выбора действия aaa из состояния sss агент будет вести себя оптимально.\\nОпределение неконструктивное, конечно, поскольку в реальности мы так сделать не можем, зато обладает интересным свойством. Если мы каким-то чудом узнали Q∗(s,a)Q^{*}(s,a)Q∗(s,a), то мы знаем оптимальную стратегию. Действительно: представьте, что вы находитесь в состоянии sss, вам нужно сделать выбор из трёх действий, и вы знаете значения Q∗(s,a)Q^{*}(s,a)Q∗(s,a). Вы знаете, что если выберете первое действие a=0a = 0a=0, то в будущем сможете набрать не более чем, допустим, Q∗(s,a=0)=+3Q^{*}(s, a = 0) = +3Q∗(s,a=0)=+3 награды. При этом вы знаете, что существует какая-то стратегия π\\\\piπ, на которой достигается максимум в определении оптимальной Q-функции, то есть которая действительно позволяет набрать эти +3. Вы знаете, что если выберете второе действие, то в будущем сможете набрать, допустим, Q∗(s,a=1)=+10Q^{*}(s, a = 1) = +10Q∗(s,a=1)=+10, а для третьего действия Q∗(s,a=2)=−1Q^{*}(s, a = 2) = -1Q∗(s,a=2)=−1. Вопрос: так как нужно действовать? Интуиция подсказывает, что надо просто выбирать действие a=1a = 1a=1, что позволит набрать +10, ведь по определению больше набрать никак не получится. Значит, выбор в этом состоянии действия a=1a = 1a=1 оптимален. Эта интуиция нас не обманывает, и принцип такого выбора называется принципом оптимальности Беллмана.\\nВыбор того действия, на котором достигается максимум по действиям Q-функции, называется жадным (greedy) по отношению к ней. Таким образом, принцип оптимальности Беллмана гласит:\\nжадный выбор по отношению к оптимальной Q-функции оптимален:\\nπ∗(s)=argmax\\u2061aQ∗(s,a)\\\\pi^*(s) = \\\\operatorname{argmax}\\\\limits_a Q^{*}(s, a)\\nπ∗(s)=argmaxa\\u200bQ∗(s,a)Примечание: если Q-функция достигает максимума на нескольких действиях, то можно выбирать любое из них.\\nЗаметим, что эта оптимальная стратегия детерминирована. Этот интересный факт означает, что нам, в общем-то, необязательно искать стохастичную стратегию. Наше рассуждение пока даже показывает, что мы можем просто пытаться найти Q∗(s,a)Q^{*}(s,a)Q∗(s,a), а дальше выводить из неё оптимальную стратегию, выбирая действие жадно.\\nНо как искать Q∗(s,a)Q^{*}(s,a)Q∗(s,a)? Тут на сцене и появляется наше наблюдение про структуру задачи. Оказывается, Q∗(s,a)Q^{*}(s,a)Q∗(s,a) выражается через саму себя. Действительно: рассмотрим некоторую пару состояние-действие s,as, as,a. С одной стороны, по определению, мы в будущем сможем при условии оптимального поведения получить Q∗(s,a)Q^{*}(s,a)Q∗(s,a) награды. С другой стороны, после того, как мы выберем действие aaa в состоянии sss, мы получим награду за один шаг r(s,a)r(s, a)r(s,a), вся дальнейшая награда будет дисконтирована на γ\\\\gammaγ, среда ответит нам сэмплированием s′∼p(s′∣s,a)s' \\\\sim p(s' \\\\mid s, a)s′∼p(s′∣s,a) (на результат этого сэмплирования мы уже никак повлиять не можем и по этой стохастике нашу будущую награду надо будет усреднять), а затем в состоянии s′s's′ мы, в предположении оптимальности поведения, выберем то действие a′a'a′, на котором достигается максимум Q∗(s′,a′)Q^{*}(s', a')Q∗(s′,a′). Другими словами, в дальнейшем после попадания в s′s's′ мы сможем получить max\\u2061a′Q∗(s′,a′)\\\\max\\\\limits_{a'} Q^{*}(s', a')a′max\\u200bQ∗(s′,a′) награды. А значит, верно следующее рекурсивное соотношение, называемое уравнением оптимальности Беллмана для Q-функции:\\nQ∗(s,a)=r(s,a)+γEs′∼p(s′∣s,a)max\\u2061a′Q∗(s′,a′)Q^{*}(s, a) = r(s, a) + \\\\gamma \\\\mathbb{E}_{s' \\\\sim p(s' \\\\mid s, a)} \\\\max_{a'} Q^{*}(s', a')\\nQ∗(s,a)=r(s,a)+γEs′∼p(s′∣s,a)\\u200ba′max\\u200bQ∗(s′,a′)Мы получили систему уравнений, связывающую значения Q∗(s,a)Q^{*}(s,a)Q∗(s,a) с самой собой. Это нелинейная система уравнений, но оказывается, что она в некотором смысле «хорошая». У неё единственное решение - и, значит, решение этого уравнения можно считать эквивалентным определением Q∗(s,a)Q^{*}(s,a)Q∗(s,a), - и его можно искать методом простой итерации. Метод простой итерации решения систем уравнений позволяет улучшать своё текущее приближение xxx решения некоторого уравнения вида x=f(x)x = f(x)x=f(x) его подстановкой в правую часть. То есть: инициализируем произвольную функцию Q0∗(s,a)\\u2009\\u2063:S×A→RQ_0^*(s, a) \\\\colon \\\\mathcal{S} \\\\times \\\\mathcal{A} \\\\to \\\\mathbb{R}Q0∗\\u200b(s,a):S×A→R, которая будет приближать Q∗(s,a)Q^{*}(s,a)Q∗(s,a), затем итеративно будем подставлять её в правую часть уравнений оптимальности Беллмана и полученным значением обновлять наше приближение:\\nQk+1∗(s,a)←r(s,a)+γEs′∼p(s′∣s,a)max\\u2061a′Qk∗(s′,a′)Q^{*}_{k+1}(s, a) \\\\leftarrow r(s, a) + \\\\gamma \\\\mathbb{E}_{s' \\\\sim p(s' \\\\mid s, a)} \\\\max_{a'} Q^{*}_k(s', a')\\nQk+1∗\\u200b(s,a)←r(s,a)+γEs′∼p(s′∣s,a)\\u200ba′max\\u200bQk∗\\u200b(s′,a′)Такая процедура в пределе приведёт нас к истинной Q∗(s,a)Q^{*}(s,a)Q∗(s,a), а значит и оптимальной стратегии. Кстати, когда вы в прошлом встречались с динамическим программированием, вы скорее всего неявно использовали именно эту идею, разве что часто в задачах для решения уравнений оптимальности Беллмана можно просто последовательно исключать неизвестные переменные; но метод простой итерации даёт более общую схему, применимую всегда. А сейчас для нас принципиально следующее: если у нас есть какое-то приближение Q∗Q^{*}Q∗, то вычисление правой части уравнения оптимальности Беллмана позволит получить приближение лучше.\\nА где же метод проб и ошибок?\\nРешать методом простой итерации уравнения оптимальности Беллмана и таким образом получать Q∗(s,a)Q^{*}(s,a)Q∗(s,a) в реальности можно только при двух очень существенных ограничивающих условиях. Нужно, чтобы, во-первых, мы могли хранить как-то текущее приближение Qk∗(s,a)Q^{*}_k(s, a)Qk∗\\u200b(s,a) в памяти. Это возможно только если пространства состояний и действий конечные и не очень большие, то есть, например, в вашем MDP всего 10 состояний и 5 действий, тогда Q∗(s,a)Q^{*}(s,a)Q∗(s,a) — это табличка 10x5. Но что, если вы хотите научиться играть в видеоигру, и состояние — это входное изображение? Тогда множество картинок, которые вам может показать видеоигра, сохранить в памяти уже не получится. Ну, допустим пока, что число состояний и число действий не очень большое, и мы всё-таки можем хранить таблицу в памяти, а позже мы снимем это ограничение, моделируя Q∗(s,a)Q^{*}(s,a)Q∗(s,a) при помощи нейросети.\\nВо-вторых, нам необходимо уметь считать выражение, стоящее справа в уравнение оптимальности Беллмана:\\nr(s,a)+γEs′∼p(s′∣s,a)max\\u2061a′Qk∗(s′,a′)r(s, a) + \\\\gamma \\\\mathbb{E}_{s' \\\\sim p(s' \\\\mid s, a)} \\\\max_{a'} Q^{*}_k(s', a')\\nr(s,a)+γEs′∼p(s′∣s,a)\\u200ba′max\\u200bQk∗\\u200b(s′,a′)Мало того, что в сложных средах взять мат.ожидание по функции переходов Es′∼p(s′∣s,a)\\\\mathbb{E}_{s' \\\\sim p(s' \\\\mid s, a)}Es′∼p(s′∣s,a)\\u200b в реальности мы не сможем, так ещё и обычно мы эту функцию переходов на самом деле не знаем. Представьте, что вы катаетесь на велосипеде: можете ли вы по текущему состоянию окружающего мира, например, положению всех атомов во вселенной, рассказать, с какими вероятностями в каком состоянии мир окажется в следующий момент времени? Это соображение также подсказывает, что было бы здорово, если б мы смогли решать задачу, избегая даже попыток выучить эту сложную функцию переходов.\\nЧто нам доступно? Мы можем взять какую-нибудь стратегию π\\\\piπ (важный момент: мы должны сами выбрать какую) и повзаимодействовать ею со средой. «Попробовать решить задачу». Мы можем сгенерировать при помощи π\\\\piπ целую траекторию или даже сделать всего один шаг в среде. Таким образом мы соберём данные: допустим, мы были в состоянии sss и сделали выбор действия aaa, тогда мы узнаем, какую награду r=r(s,a)r = r(s, a)r=r(s,a) мы получаем за такой шаг и, самое главное, в какое состояние s′s's′ нас перевела среда. Полученный s′s's′ — сэмпл из функции переходов s′∼p(s′∣s,a)s' \\\\sim p(s' \\\\mid s, a)s′∼p(s′∣s,a). Собранная так информация — четвёрка (s,a,r,s′)(s, a, r, s')(s,a,r,s′) — называется переходом (transition), и может быть как-то использована для оптимизации нашей стратегии.\\nМожем ли мы, используя лишь переходы (s,a,r,s′)(s, a, r, s')(s,a,r,s′), то есть имея на руках лишь сэмплы s′∼p(s′∣s,a)s' \\\\sim p(s' \\\\mid s, a)s′∼p(s′∣s,a), как-то пользоваться схемой динамического программирования? Что, если мы будем заменять значение Qk∗(s,a)Q_{k}^{*}(s, a)Qk∗\\u200b(s,a) не на\\nr(s,a)+γEs′∼p(s′∣s,a)max\\u2061a′Qk∗(s′,a′),r(s, a) + \\\\gamma \\\\mathbb{E}_{s' \\\\sim p(s' \\\\mid s, a)} \\\\max_{a'} Q_{k}^{*}(s', a'),\\nr(s,a)+γEs′∼p(s′∣s,a)\\u200ba′max\\u200bQk∗\\u200b(s′,a′),которое мы не можем посчитать, а на его Монте Карло оценку:\\nr(s,a)+γmax\\u2061a′Qk∗(s′,a′),r(s, a) + \\\\gamma \\\\max_{a'} Q^{*}_k(s', a'),\\nr(s,a)+γa′max\\u200bQk∗\\u200b(s′,a′),где s′s's′ — сэмпл из функции переходов из собранного нами опыта? В среднем-то такая замена верная. Такая Монте-Карло оценка правой части для заданного переходика (s,a,r,s′)(s, a, r, s')(s,a,r,s′) называется Беллмановским таргетом, то есть «целевой переменной». Почему такое название — мы увидим чуть позже.\\nЧтобы понять, как нам нужно действовать, рассмотрим какую-нибудь типичную ситуацию. Допустим, после выполнения действия aaa из некоторого состояния sss среда награждает нас r(s,a)=0r(s, a) = 0r(s,a)=0 и перекидывает нас с равными вероятностями то в состояние s′s's′, для которого max\\u2061a′Qk∗(s′,a′)=+1\\\\max_{a'} Q^{*}_{k}(s', a') = +1maxa′\\u200bQk∗\\u200b(s′,a′)=+1, то в состояние s′s's′, для которого max\\u2061a′Qk∗(s′,a′)=−1\\\\max_{a'} Q^{*}_{k}(s', a') = -1maxa′\\u200bQk∗\\u200b(s′,a′)=−1. Метод простой итерации говорит, что на очередной итерации нужно заменить Qk∗(s,a)Q^{*}_{k}(s, a)Qk∗\\u200b(s,a) на 0.5γ⋅(+1)+0.5γ⋅(−1)=00.5 \\\\gamma \\\\cdot (+1) + 0.5 \\\\gamma \\\\cdot (-1) = 00.5γ⋅(+1)+0.5γ⋅(−1)=0, но в реальности мы встретимся лишь с одним исходом, и таргет — Монте-Карло оценка правой части уравнения оптимальности Беллмана — будет с вероятностью 0.5 равен +γ+\\\\gamma+γ, а с вероятностью 0.5 равен −γ-\\\\gamma−γ. Ясно, что нельзя просто взять и жёстко заменять наше текущее приближение Qk∗(s,a)Q^{*}_k(s, a)Qk∗\\u200b(s,a) на посчитанный Беллмановский таргет по некоторому одному переходу, поскольку нам могло повезти (мы увидели +γ+\\\\gamma+γ) или не повезти (мы увидели −γ-\\\\gamma−γ). Давайте вместо этого поступать также, как учат среднее по выборке: не сдвигать «жёстко» наше текущее приближение в значение очередного сэмпла, а смешивать текущее приближение с очередным сэмплом. То есть: берём переходик (s,a,r,s′)(s, a, r, s')(s,a,r,s′), и не заменяем Qk∗(s,a)Q^{*}_k(s, a)Qk∗\\u200b(s,a) на стохастичную оценку правой части уравнения оптимальности Беллмана, а только сдвигаемся в его сторону:\\nQk+1∗(s,a)←(1−α)Qk∗(s,a)+α(r+γmax\\u2061a′Qk∗(s′,a′))Q^{*}_{k+1}(s, a) \\\\leftarrow (1 - \\\\alpha)Q^{*}_k(s, a) + \\\\alpha (r + \\\\gamma \\\\max_{a'} Q^{*}_k(s', a'))\\nQk+1∗\\u200b(s,a)←(1−α)Qk∗\\u200b(s,a)+α(r+γa′max\\u200bQk∗\\u200b(s′,a′))Таким образом, мы проводим экспоненциальное сглаживание старого приближения Qk∗(s,a)Q^{*}_k(s, a)Qk∗\\u200b(s,a) и новой оценки правой части уравнения оптимальности Беллмана со свежим сэмплом s′s's′. Выбор α\\\\alphaα здесь определяет, насколько сильно мы обращаем внимание на последние сэмплы, и имеет тот же физический смысл, что и learning rate. В среднем по стохастике (а стохастика в этой формуле обновления заложена в случайном s′s's′) мы будем сдвигаться в сторону\\nr(s,a)+γEs′∼p(s′∣s,a)max\\u2061a′Qk∗(s′,a′),r(s, a) + \\\\gamma \\\\mathbb{E}_{s' \\\\sim p(s' \\\\mid s, a)} \\\\max_{a'} Q_{k}^{*}(s', a'),\\nr(s,a)+γEs′∼p(s′∣s,a)\\u200ba′max\\u200bQk∗\\u200b(s′,a′),и значит применять этакий «зашумлённый» метод простой итерации.\\nИтак, возникает следующая идея. Будем как-то взаимодействовать со средой и собирать переходики (s,a,r,s′)(s, a, r, s')(s,a,r,s′). Для каждого перехода будем обновлять одну ячейку нашей Q-таблицы размера число состояний на число действий по вышеуказанной формуле. Таким образом мы получим как бы «зашумлённый» метод простой итерации, где мы на каждом шаге обновляем только одну ячейку таблицы, и не заменяем жёстко значение на правую часть уравнений оптимальности, а лишь сдвигаемся по некоторому в среднем верному стохастичному направлению.\\nОчень похоже на стохастическую оптимизацию вроде стохастического градиентного спуска, и поэтому гарантии сходимости выглядят схожим образом. Оказывается, такой алгоритм сходится к истинной Q∗(s,a)Q^{*}(s,a)Q∗(s,a), если для любой пары s,as, as,a мы в ходе всего процесса проводим бесконечное количество обновлений, а learning rate (гиперпараметр α\\\\alphaα) в них ведёт себя как learning rate из условий сходимости стохастического градиентного спуска:\\n∑iαi=+∞,∑iαi2<+∞\\\\sum_i \\\\alpha_i = +\\\\infty, \\\\qquad \\\\sum_i \\\\alpha^2_i < +\\\\infty\\ni∑\\u200bαi\\u200b=+∞,i∑\\u200bαi2\\u200b<+∞ПримерКолобок Колабулька любит играть в лотереи. Допустим, в некотором состоянии sss он выполнил действие aaa «купить билет» и предполагает, что в будущем сможет набрать Q∗(s,a)=+100Q^{*}(s, a) = +100Q∗(s,a)=+100 награды. Однако, за покупку билета он платит 10 долларов, и таким образом теряет 10 награды на данном шаге r(s,a)=−10r(s, a) = -10r(s,a)=−10, при этом попадая в состояние s′=ss' = ss′=s, где ему снова предлагается купить билет в лотерею (он может выбрать действие «купить» или действие «не купить»). Ну, допустим,  текущая оценка Колабульки будущей награды в случае отказа купить билет равна 0 < +100, поэтому колобок предполагает, что в будущем из состояния s′s's′ он сможет получить max\\u2061a′Q∗(s′,a′)=+100\\\\max\\\\limits_{a'} Q^{*}(s', a') = +100a′max\\u200bQ∗(s′,a′)=+100. Для простоты допустим γ=+1\\\\gamma = +1γ=+1. Тогда получается, что одношаговое приближение будущей награды r(s,a)+γmax\\u2061a′Q∗(s′,a′)=−10+100=90r(s, a) + \\\\gamma \\\\max\\\\limits_{a'} Q^{*}(s', a') = -10 + 100 = 90r(s,a)+γa′max\\u200bQ∗(s′,a′)=−10+100=90. Да, s′s's′ здесь — случайная величина, колобку могло повезти или не повезти (и мы, увидев всего один сэмпл из функции переходов, не можем сказать наверняка, повезло ли нам сейчас или нет), но наша формула говорит сдвинуть аппроксимацию Q∗(s,a)Q^{*}(s,a)Q∗(s,a) в сторону Беллмановского таргета.\\n\\nДопустим, learning rate α=0.5\\\\alpha = 0.5α=0.5: тогда, сдвигая +100 в сторону +90, ожидания от будущей награды после покупки лотерейного билета опускаются до 95. Всё ещё +95>0+95 > 0+95>0, поэтому колобку кажется, что покупать билет выгоднее, чем не покупать, поэтому рассмотрим следующий переходик. Допустим, колобок снова купил билет, снова потерял 10 долларов и снова попал в то же самое s′=ss' = ss′=s. Наше обновление снова скажет уменьшать значение Q∗(s,a)Q^{*}(s,a)Q∗(s,a):\\n\\nВидно, что если колобку продолжит так не везти, таргет будет всё время на 10 меньше, чем текущее приближение, и Q∗(s,a)Q^{*}(s,a)Q∗(s,a) будет всё уменьшаться и уменьшаться, пока не свалится до нуля (а там будет выгоднее уже не покупать билет). Но если на очередной итерации Колабульке повезло, и среда перевела его в s′s's′, соответствующее победе в лотерею (а это, видимо, происходит с какой-то маленькой вероятностью), таргет получится очень большим, и аппроксимацию Q∗(s,a)Q^{*}(s,a)Q∗(s,a) наше обновление скажет сильно увеличить:\\n\\nКуда будет сходиться такой алгоритм? Давайте предположим, что среда на покупку лотерейного билета отвечает с вероятностью ppp возвращением в то же состояние s′=ss' = ss′=s, где колобку предлагается купить ещё один билет, а с вероятностью 1−p1 - p1−p билет оказывается выигрышным, и колобок попадает в такое состояние s′s's′, в котором он может забрать приз и получить +1000 (после этого взаимодействие со средой, скажем, заканчивается). Давайте запишем уравнение оптимальности Беллмана для действия aaa «купить билет» в состоянии sss:\\nQ∗(s,a)=r(s,a)+γ(pmax\\u2061a′Q∗(s′=s,a′)+(1−p)⋅(+1000))Q^{*}(s, a) = r(s, a) + \\\\gamma \\\\left( p \\\\max_{a'} Q^{*}(s' = s, a') + (1 - p) \\\\cdot (+1000) \\\\right)\\nQ∗(s,a)=r(s,a)+γ(pa′max\\u200bQ∗(s′=s,a′)+(1−p)⋅(+1000))Здесь max\\u2061a′Q∗(s′=s,a′)=max\\u2061(Q∗(s,a),0)\\\\max_{a'} Q^{*}(s' = s, a') = \\\\max (Q^{*}(s, a), 0)maxa′\\u200bQ∗(s′=s,a′)=max(Q∗(s,a),0), поскольку колобок может или покупать билет, или не покупать (это, допустим, принесёт ему 0 награды). Понятно, что если покупка билета не принесёт больше 0 награды, то не имеет смысла его покупать. Подставляя все числа из примера, получаем:\\nQ∗(s,a)=−10+pmax\\u2061(Q∗(s,a),0)+1000(1−p)Q^{*}(s, a) = -10 + p \\\\max (Q^{*}(s, a), 0) + 1000(1 - p) \\nQ∗(s,a)=−10+pmax(Q∗(s,a),0)+1000(1−p)Видно, что если вероятность проигрыша в лотерею p=0.99p = 0.99p=0.99, то решением уравнения является Q∗(s,a)=0Q^{*}(s, a) = 0Q∗(s,a)=0: Колабулька платит за билет 10 долларов и получает 1000 награды с вероятностью 0.01. В этом случае действие «купить билет» и «не покупать» равноценны, и оба в будущем принесут в среднем 0 награды. Если же p>0.99p > 0.99p>0.99, то покупать билет становится невыгодно, а если p<0.99p < 0.99p<0.99, то выгодно покупать билет до тех пор, пока не случится победа. Несмотря на то, что в таргете содержится собственная же текущая аппроксимация будущей награды и используется лишь один сэмпл s′s's′ вместо честного усреднения по всевозможным исходам, формула обновления постепенно сойдётся к этому решению. Причём колобку истинное значение ppp неизвестно, и в формуле обновления эта вероятность влияла лишь на появление того или иного s′s's′ в очередном таргете.\\nЭтот алгоритм, к которому мы уже практически пришли, называется Q-learning, «обучение оптимальной Q-функции». Нам, однако, осталось ответить на один вопрос: так как же нужно собирать данные, чтобы удовлетворить требованиям для сходимости? Как взаимодействовать со средой так, чтобы мы каждую ячейку s,as, as,a не прекращали обновлять?\\nДилемма Exploration-exploitation\\nМы уже встречали дилемму exploration-exploitation (букв. «исследования-использования») в параграфе про тюнинг гиперпараметров. Задача многоруких бандитов, которая там встретилась, на самом деле является частным случаем задачи обучения с подкреплением, в котором после первого выбора действия эпизод гарантированно завершается, и этот частный случай задачи часто используется для изучения этой дилеммы. Рассмотрим эту дилемму в нашем контексте.\\nДопустим, на очередном шаге алгоритма у нас есть некоторое приближение Qk(s,a)≈Q∗(s,a)Q_k(s, a) \\\\approx Q^{*}(s, a)Qk\\u200b(s,a)≈Q∗(s,a). Приближение это, конечно, неточное, поскольку алгоритм, если и сходится к истинной оптимальной Q-функции, то на бесконечности. Как нужно взаимодействовать со средой? Если вы хотите набрать максимальную награду, наверное, стоит воспользоваться нашей теорией и заниматься exploitation-ом, выбирая действие жадно:\\nπ(s)=argmax\\u2061aQk(s,a)\\\\pi(s) = \\\\operatorname{argmax}_{a} Q_k(s, a)\\nπ(s)=argmaxa\\u200bQk\\u200b(s,a)Увы, такой выбор не факт что совпадёт с истинной оптимальной стратегией, а главное, он детерминирован. Это значит, что при взаимодействии этой стратегией со средой, многие пары s,as, as,a никогда не будут встречаться просто потому, что мы никогда не выбираем действие aaa в состоянии sss. А тогда мы, получается, рискуем больше никогда не обновить ячейку Qk(s,a)Q_k(s, a)Qk\\u200b(s,a) для таких пар!\\nТакие ситуации запросто могут привести к застреванию алгоритма. Мы хотели научиться кататься на велосипеде и получали +0.1 за каждый пройденный метр и -5 за каждое попадание в дерево. После первых проб и ошибок мы обнаружили, что катание на велосипеде приносит нам -5, поскольку мы очень скоро врезаемся в деревья и обновляли нашу аппроксимацию Q-функции сэмплами с негативной наградой; зато если мы не будем даже забираться на велосипед и просто займёмся ничего не деланьем, то мы сможем избежать деревьев и будем получать 0. Просто из-за того, что в нашей стратегии взаимодействия со средой никогда не встречались те s,as, as,a, которые приводят к положительной награде, и жадная стратегия по отношению к нашей текущей аппроксимации Q-функции никогда не выбирает их. Поэтому нам нужно экспериментировать и пробовать новые варианты.\\nРежим exploration-а предполагает, что мы взаимодействуем со средой при помощи какой-нибудь стохастичной стратегии ∀s,a\\u2009\\u2063:π(a∣s)>0\\\\forall s, a \\\\colon \\\\pi(a \\\\mid s) > 0∀s,a:π(a∣s)>0. Например, такой стратегией является случайная стратегия, выбирающая рандомные действия. Как ни странно, сбор опыта при помощи случайной стратегии позволяет побывать с ненулевой вероятностью во всех областях пространства состояний, и теоретически даже наш алгоритм обучения Q-функции будет сходится. Означает ли это, что exploration-а хватит, и на exploitation можно забить?\\nВ реальности мы понимаем, что добраться до самых интересных областей пространства состояний, где функция награда самая большая, не так-то просто, и случайная стратегия хоть и будет это делать с ненулевой вероятностью, но вероятность эта будет экспоненциально маленькая. А для сходимости нам нужно обновить ячейки Qk(s,a)Q_k(s, a)Qk\\u200b(s,a) для этих интересных состояний бесконечно много раз, то есть нам придётся дожидаться необычайно редкого везения далеко не один раз. Куда разумнее использовать уже имеющиеся знания и при помощи жадной стратегии, которая уже что-то умеет, идти к этим интересным состояниям. Поэтому для решения дилеммы exploration-exploitation обычно берут нашу текущую жадную стратегию и что-нибудь с ней делают такое, чтобы она стала чуть-чуть случайной. Например, с вероятностью ε>0\\\\varepsilon > 0ε>0 выбирают случайное действие, а с вероятностью 1−ε1 - \\\\varepsilon1−ε — жадное. Тогда мы чаще всё-таки и знаниями пользуемся, и любое действие с ненулевой вероятностью выбираем; такая стратегия называется ε\\\\varepsilonε-жадной, и она является самым простым способом как-то порешать эту дилемму.\\nДавайте закрепим, что у нас получилось, в виде табличного алгоритма обучения с подкреплением под названием Q-learning:\\n\\nПроинициализировать Q∗(s,a)Q^{*}(s,a)Q∗(s,a) произвольным образом.\\nПронаблюдать s0s_0s0\\u200b из среды.\\nДля k=0,1,2,…k = 0, 1, 2, \\\\dotsk=0,1,2,…:\\n\\n\\nс вероятностью ε\\\\varepsilonε выбрать действие aka_kak\\u200b случайно, иначе жадно: ak\\u200b=argmax\\u2061akQ∗(sk\\u200b,ak\\u200b){a_k\\u200b}= \\\\operatorname{argmax}_{a_k}Q^{∗}(s_k\\u200b,a_k\\u200b)ak\\u200b\\u200b=argmaxak\\u200b\\u200bQ∗(sk\\u200b\\u200b,ak\\u200b\\u200b)\\nотправить действие aka_kak\\u200b в среду, получить награду за шаг rkr_krk\\u200b и следующее состояниеsk+1s_{k+1}sk+1\\u200b.\\nобновить одну ячейку таблицы:\\n\\nQ∗(sk,ak)←(1−α)Q∗(sk,ak)+α(rk+γmax\\u2061a′Q∗(sk+1,a′))Q^{*}(s_k, a_k) \\\\leftarrow (1 - \\\\alpha)Q^{*}(s_k, a_k) + \\\\alpha (r_k + \\\\gamma \\\\max\\\\limits_{a'} Q^{*}(s_{k+1}, a'))\\nQ∗(sk\\u200b,ak\\u200b)←(1−α)Q∗(sk\\u200b,ak\\u200b)+α(rk\\u200b+γa′max\\u200bQ∗(sk+1\\u200b,a′))Добавим нейросеток\\nНаконец, чтобы перейти к алгоритмам, способным на обучение в сложных MDP со сложным пространством состояний, нужно объединять классическую теорию обучения с подкреплением с парадигмами глубокого обучения.\\nДопустим, мы не можем позволить себе хранить Q∗(s,a)Q^{*}(s,a)Q∗(s,a) как таблицу в памяти, например, если мы играем в видеоигру и на вход нам подаются какие-нибудь изображения. Тогда мы можем обрабатывать любые имеющиеся у агента входные сигналы при помощи нейросетки Q∗(s,a,θ)Q^{*}(s, a, \\\\theta)Q∗(s,a,θ). Для тех же видеоигр мы легко обработаем изображение экрана небольшой свёрточной сеточкой и выдадим для каждого возможного действия aaa вещественный скаляр Q∗(s,a,θ)Q^{*}(s, a, \\\\theta)Q∗(s,a,θ). Допустим также, что пространство действий всё ещё конечное и маленькое, чтобы мы могли для такой модели строить жадную стратегию, выбирать argmax\\u2061aQ∗(s,a,θ)\\\\operatorname{argmax}_{a} Q^{*}(s, a, \\\\theta)argmaxa\\u200bQ∗(s,a,θ). Но как обучать такую нейросетку?\\nДавайте ещё раз посмотрим на формулу обновления в Q-learning для одного переходика (s,a,r,s′)(s, a, r, s')(s,a,r,s′):\\nQk+1∗(s,a)←(1−α)Qk∗(s,a)+α(r+γmax\\u2061a′Qk∗(s′,a′))==Qk∗(s,a)+α(r+γmax\\u2061a′Qk∗(s′,a′)−Qk∗(s,a))\\\\begin{align*}\\nQ^{*}_{k+1}(s, a) \\\\leftarrow (1 - \\\\alpha)Q^{*}_k(s, a) + \\\\alpha (r + \\\\gamma \\\\max_{a'} Q^{*}_k(s', a')) = \\\\\\\\\\n= Q^{*}_k(s, a) + \\\\alpha (r + \\\\gamma \\\\max_{a'} Q^{*}_k(s', a') - Q^{*}_k(s, a))\\n\\\\end{align*}\\nQk+1∗\\u200b(s,a)←(1−α)Qk∗\\u200b(s,a)+α(r+γa′max\\u200bQk∗\\u200b(s′,a′))==Qk∗\\u200b(s,a)+α(r+γa′max\\u200bQk∗\\u200b(s′,a′)−Qk∗\\u200b(s,a))\\u200bТеория Q-learning-а подсказывала, что у процесса такого обучения Q-функции много общего с обычным стохастическим градиентным спуском. В таком виде формула подсказывает, что, видимо,\\nr+γmax\\u2061a′Qk∗(s′,a′)−Qk∗(s,a)r + \\\\gamma \\\\max_{a'} Q^{*}_{k}(s', a') - Q^{*}_{k}(s, a)\\nr+γa′max\\u200bQk∗\\u200b(s′,a′)−Qk∗\\u200b(s,a)— это стохастическая оценка какого-то градиента. Этот градиент сравнивает Беллмановский таргет\\nr+γmax\\u2061a′Qk∗(s′,a′)r + \\\\gamma \\\\max_{a'} Q^{*}_{k}(s', a')\\nr+γa′max\\u200bQk∗\\u200b(s′,a′)с нашим текущим приближением Qk∗(s,a)Q^{*}_{k}(s, a)Qk∗\\u200b(s,a) и чуть-чуть корректирует это значение, сдвигая в сторону таргета. Попробуем «заменить» в этой формуле Q-функцию с табличного представления на нейросетку.\\nРассмотрим такую задачу регрессии. Чтобы построить один прецедент для обучающей выборки, возьмём один имеющийся у нас переходик (s,a,r,s′)(s, a, r, s')(s,a,r,s′). Входом будет пара s,as, as,a. Целевой переменной, таргетом, будет Беллмановский таргет\\ny=r+γmax\\u2061a′Q∗(s′,a′,θ);y = r + \\\\gamma \\\\max_{a'} Q^{*}(s', a', \\\\theta);\\ny=r+γa′max\\u200bQ∗(s′,a′,θ);его зависимость от параметров θ\\\\thetaθ нашей нейронки мы далее будем игнорировать и будем «притворяться», что это и есть наш ground truth. Именно поэтому Монте-Карло оценка правой части уравнения оптимальности Беллмана и называют таргетом. Но важно помнить, что эта целевая переменная на самом деле «зашумлена»: в формуле используется взятый из перехода s′s's′, который есть лишь сэмпл из функции переходов. На самом же деле мы хотели бы выучить среднее значение такой целевой переменной, и поэтому в качестве функции потерь мы возьмём MSE. Как будет выглядеть шаг стохастического градиентного спуска для решения этой задачи регрессии (для простоты — для одного прецедента)?\\nθk+1←θk−α∇θ(y−Q∗(s,a,θ))2==θk+2α(y−Q∗(s,a,θ))∇θQ∗(s,a,θ)==θk+2α(r+γmax\\u2061a′Q∗(s′,a′,θ)−Q∗(s,a,θ))∇θQ∗(s,a,θ)\\\\begin{aligned}\\n\\\\theta_{k+1} \\\\leftarrow &\\\\theta_{k} - \\\\alpha \\\\nabla_{\\\\theta} (y - Q^{*}(s, a, \\\\theta))^2 = \\\\\\\\\\n= &\\\\theta_{k} + 2 \\\\alpha (y - Q^{*}(s, a, \\\\theta)) \\\\nabla_{\\\\theta} Q^{*}(s, a, \\\\theta) = \\\\\\\\\\n= &\\\\theta_{k} + 2 \\\\alpha (r + \\\\gamma \\\\max_{a'} Q^{*}(s', a', \\\\theta) - Q^{*}(s, a, \\\\theta)) \\\\nabla_{\\\\theta} Q^{*}(s, a, \\\\theta)\\n\\\\end{aligned}\\nθk+1\\u200b←==\\u200bθk\\u200b−α∇θ\\u200b(y−Q∗(s,a,θ))2=θk\\u200b+2α(y−Q∗(s,a,θ))∇θ\\u200bQ∗(s,a,θ)=θk\\u200b+2α(r+γa′max\\u200bQ∗(s′,a′,θ)−Q∗(s,a,θ))∇θ\\u200bQ∗(s,a,θ)\\u200bЭто практически в точности повторяет формулу Q-learning, которая гласит, что если таргет r+γmax\\u2061a′Q∗(s′,a′,θ)r + \\\\gamma \\\\max_{a'} Q^{*}(s', a', \\\\theta)r+γmaxa′\\u200bQ∗(s′,a′,θ) больше Q∗(s,a,θ)Q^{*}(s, a, \\\\theta)Q∗(s,a,θ), то нужно подстроить веса нашей модели так, чтобы Q∗(s,a,θ)Q^{*}(s, a, \\\\theta)Q∗(s,a,θ) стало чуть побольше, и наоборот. В среднем при такой оптимизации мы будем двигаться в сторону\\nEs′∼p(s′∣s,a)y=Es′∼p(s′∣s,a)[r+γmax\\u2061a′Q∗(s′,a′,θ)]\\\\mathbb{E}_{s' \\\\sim p(s' \\\\mid s, a)} y = \\\\mathbb{E}_{s' \\\\sim p(s' \\\\mid s, a)} \\\\left[ r + \\\\gamma \\\\max_{a'} Q^{*}(s', a', \\\\theta) \\\\right]\\nEs′∼p(s′∣s,a)\\u200by=Es′∼p(s′∣s,a)\\u200b[r+γa′max\\u200bQ∗(s′,a′,θ)]— в сторону правой части уравнения оптимальности Беллмана, то есть моделировать метод простой итерации для решения системы нелинейных уравнений.\\nЕдинственное отличие такой задачи регрессии от тех, с которыми сталкивается традиционное глубокое обучение — то, что целевая переменная зависит от нашей же собственной модели. Раньше целевые переменные были напрямую источником обучающего сигнала. Теперь же, когда мы хотим выучить будущую награду при условии оптимального поведения, мы не знаем этого истинного значения или даже её стохастичных оценок. Поэтому мы применяем идею бутстрапирования (bootstrapping): берём награду за следующий шаг, и нечестно приближаем всю остальную награду нашей же текущей аппроксимацией max\\u2061a′Q∗(s′,a′,θ)\\\\max_{a'} Q^{*}(s', a', \\\\theta)maxa′\\u200bQ∗(s′,a′,θ). Да, за этим кроется идея метода простой итерации, но важно понимать, что такая целевая переменная лишь указывает направление для обучения, но не является истинным приближением будущих наград или даже их несмещённой оценкой. Поэтому говорят, что в этой задаче регрессии очень смещённые (biased) целевые переменные.\\nНа практике из-за этого возникает беда. Наша задача регрессии в таком виде меняется после каждого же шага. Если вдруг после очередного шага оптимизации и обновления весов нейросети наша модель начала выдавать какие-то немного неадекватные значения, они рискуют попасть в целевую переменную на следующем шаге, мы сделаем шаг обучения под неадекватные целевые переменные, модель станет ещё хуже, и так далее, начнётся цепная реакция. Алгоритмы, в которых целевая переменная вот так напрямую зависит от текущей же модели, из-за этого страшно нестабильны.\\nДля стабилизации применяется трюк, называемый таргет-сетью (target network). Давайте сделаем так, чтобы у нас задача регрессии менялась не после каждого обновления весов нейросетки, а хотя бы раз, скажем, в 1000 шагов оптимизации. Для этого заведём полную копию нашей нейросети («таргет-сеть»), веса которой будем обозначать θ−\\\\theta^{-}θ−. Каждые 1000 шагов будем копировать веса из нашей модели в таргет-сеть θ−←θ\\\\theta^{-} \\\\leftarrow \\\\thetaθ−←θ, больше никак менять θ−\\\\theta^{-}θ− не будем. Когда мы захотим для очередного перехода (s,a,r,s′)(s, a, r, s')(s,a,r,s′) построить таргет, мы воспользуемся не нашей свежей моделью, а таргет-сетью:\\ny=r+γmax\\u2061a′Q∗(s′,a′,θ−)y = r + \\\\gamma \\\\max_{a'} Q^{*}(s', a', \\\\theta^{-})\\ny=r+γa′max\\u200bQ∗(s′,a′,θ−)Тогда правило, по которому строится целевая переменная, будет меняться раз в 1000 шагов, и мы 1000 шагов будем решать одну и ту же задачу регрессии. Такой процесс будет намного стабильнее.\\nExperience Replay\\nЧтобы окончательно собрать алгоритм Deep Q-learning (обычно называемый DQN, Deep Q-network), нам понадобится сделать последний шаг, связанный опять со сбором данных. Коли мы хотим обучать нейросетку, нам нужно для каждого обновления весов откуда-то взять целый мини-батч данных, то есть батч переходов (s,a,r,s′)(s, a, r, s')(s,a,r,s′), чтобы по нему усреднить оценку градиента. Однако, если мы возьмём среду, сделаем в ней NNN шагов, то встреченные нами NNN переходов будут очень похожи друг на друга: они все придут из одной и той же области пространства состояний. Обучение нейросетки на скоррелированных данных — плохая идея, поскольку такая модель быстро забудет, что она учила на прошлых итерациях.\\nБороться с этой проблемой можно двумя способами. Первый способ, доступный всегда, когда среда задана при помощи виртуального симулятора — запуск параллельных агентов. Запускается параллельно NNN процессов взаимодействия агента со средой, и для того, чтобы собрать очередной мини-батч переходов для обучения, во всех экземплярах проводится по одному шагу взаимодействия, собирается по одному переходику. Такой мини-батч уже будет разнообразным.\\nБолее интересный второй способ. Давайте после очередного шага взаимодействия со средой мы не будем тут же использовать переход (s,a,r,s′)(s, a, r, s')(s,a,r,s′) для обновления модели, а запомним этот переход и положим его себе в коллекцию. Память со всеми встретившимися в ходе проб и ошибок переходами (s,a,r,s′)(s, a, r, s')(s,a,r,s′) называется реплей буфером (replay buffer или experience replay). Теперь для того, чтобы обновить веса нашей сети, мы возьмём и случайно засэмплируем из равномерного распределения желаемое количество переходов из всей истории.\\nОднако, использование реплей буфера возможно далеко не во всех алгоритмах обучения с подкреплением. Дело в том, что некоторые алгоритмы обучения с подкреплением требуют, чтобы данные для очередного шага обновления весов были сгенерированы именно текущей, самой свежей версией стратегии. Такие алгоритмы относят к классу on-policy: они могут улучшать стратегию только по данным из неё же самой («on policy»). Примером on-policy алгоритмов выступают, например, эволюционные алгоритмы. Как они устроены: например, можно завести популяцию стратегий, поиграть каждой со средой, отобрать лучшие и как-то породить новую популяцию (подробнее про одну из самых успешных схем в рамках такой идеи можно посмотреть здесь). Как бы ни была устроена эта схема, эволюционный алгоритм никак не может использовать данные из, например, старых, плохих стратегий, которые вели себя, скажем, не сильно лучше случайной стратегии. Поэтому неизбежно в эволюционном подходе нужно свежую популяцию отправлять в среду и собирать новые данные перед каждым следующим шагом.\\nИ вот важный момент: Deep Q-learning, как и обычный Q-learning, относится к off-policy алгоритмам обучения с подкреплением. Совершенно неважно, какая стратегия, умная или не очень, старая или новая, породила переход (s,a,r,s′)(s, a, r, s')(s,a,r,s′), нам всё равно нужно решать уравнение оптимальности Беллмана в том числе и для этой пары s,as, as,a и нам достаточно при построении таргета лишь чтобы s′s's′ был сэмплом из функции переходов (а она-то как раз одна вне зависимости от того, какая стратегия взаимодействует в среде). Поэтому обновлять модель Q∗(s,a)Q^{*}(s,a)Q∗(s,a) мы можем по совершенно произвольному опыту, и, значит, мы в том числе можем использовать experience replay.\\n\\n\\n\\nисточник картинки — курс UC Berkeley AI\\n\\n\\nВ любом случае, даже в сложных средах, при взаимодействии со средой мы всё равно должны как-то разрешить дилемму exploration-exploitation, и пользоваться, например, ε\\\\varepsilonε-жадной стратегией исследования. Итак, алгоритм DQN выглядит так:\\n\\nПроинициализировать нейросеть Q∗(s,a,θ)Q^{*}(s, a, \\\\theta)Q∗(s,a,θ).\\nПроинициализировать таргет-сеть, положив θ−=θ\\\\theta^{-} = \\\\thetaθ−=θ.\\nПронаблюдать s0s_0s0\\u200b из среды.\\nДля k=0,1,2,…k = 0, 1, 2, \\\\dotsk=0,1,2,…:\\n\\n\\nс вероятностью ε\\\\varepsilonε выбрать действие aka_kak\\u200b случайно, иначе жадно:\\n\\nak=argmax\\u2061akQ∗(sk,ak,θ)a_k = \\\\operatorname{argmax}\\\\limits_{a_k} Q^{*}(s_k, a_k, \\\\theta)\\nak\\u200b=argmaxak\\u200b\\u200bQ∗(sk\\u200b,ak\\u200b,θ)\\nотправить действие aka_kak\\u200b в среду, получить награду за шаг rkr_krk\\u200b и следующее состояниеsk+1s_{k+1}sk+1\\u200b.\\nдобавить переход (sk,ak,rk,sk+1)(s_k, a_k, r_k, s_{k+1})(sk\\u200b,ak\\u200b,rk\\u200b,sk+1\\u200b) в реплей буфер.\\nесли в реплей буфере скопилось достаточное число переходиков, провести шаг обучения. Для этого сэмплируем мини-батч переходиков (s,a,r,s′)(s, a, r, s')(s,a,r,s′) из буфера.\\nдля каждого переходика считаем целевую переменную: y=r+γmax\\u2061a′Q∗(s′,a′,θ−)y = r + \\\\gamma \\\\max\\\\limits_{a'} Q^{*}(s', a', \\\\theta^{-})y=r+γa′max\\u200bQ∗(s′,a′,θ−)\\nсделать шаг градиентного спуска для обновления θ\\\\thetaθ, минимизируя\\n\\n∑(y−Q∗(s,a,θ))2\\\\sum (y - Q^{*}(s, a, \\\\theta))^2\\n∑(y−Q∗(s,a,θ))2\\nесли kkk делится на 1000, обновить таргет-сеть: θ−←θ\\\\theta^{-} \\\\leftarrow \\\\thetaθ−←θ.\\n\\nАлгоритм DQN не требует никаких handcrafted признаков или специфических настроек под заданную игру. Один и тот же алгоритм, с одними и теми же гиперпараметрами, можно запустить на любой из 57 игр древней консоли Atari (пример игры в Breakout) и получить какую-то стратегию. Для сравнения алгоритмов RL между собой результаты обычно усредняют по всем 57 играм Atari. Недавно алгоритм под названием Agent57, объединяющий довольно много модификаций и улучшений DQN и развивающий эту идею, смог победить человека сразу во всех этих 57 играх.\\nА если пространство действий непрерывно?\\nВсюду в DQN мы предполагали, что пространство действий дискретно и маленькое, чтобы мы могли считать жадную стратегию π(s)=argmax\\u2061aQ∗(s,a,θ)\\\\pi(s) = \\\\operatorname{argmax}_a Q^{*}(s, a, \\\\theta)π(s)=argmaxa\\u200bQ∗(s,a,θ) и считать максимум в формуле целевой переменной max\\u2061aQ∗(s,a,θ)\\\\max_a Q^{*}(s, a, \\\\theta)maxa\\u200bQ∗(s,a,θ). Если пространство действий непрерывно, и на каждом шаге от агента ожидается выбор нескольких вещественных чисел, то как это делать непонятно. Такая ситуация повсюду возникает в робототехнике. Там каждое сочленение робота можно, например, поворачивать вправо / влево, и такие действия проще описывать набором чисел в диапазоне [-1, 1], где -1 — крайне левое положение, +1 — крайне правое, и доступны любые промежуточные варианты. При этом дискретизация действий не вариант из-за экспоненциального взрыва числа вариантов и потери семантики действий. Нам, в общем-то, нужно в DQN только одну проблему решить: как-то научиться аргмаксимум по действиям брать.\\nА давайте, коли мы не знаем argmax\\u2061aQ∗(s,a)\\\\operatorname{argmax}_a Q^{*}(s, a)argmaxa\\u200bQ∗(s,a), приблизим его другой нейросеткой. А то есть, заведём вторую нейросеть π(s,ϕ)\\\\pi(s, \\\\phi)π(s,ϕ) с параметрами ϕ\\\\phiϕ, и будем учить её так, чтобы\\nπ(s,ϕ)≈argmax\\u2061aQ∗(s,a,θ).\\\\pi(s, \\\\phi) \\\\approx \\\\operatorname{argmax}_a Q^{*}(s, a, \\\\theta).\\nπ(s,ϕ)≈argmaxa\\u200bQ∗(s,a,θ).Как это сделать? Ну, будем на каждой итерации алгоритма брать батч состояний sss из нашего реплей буфера и будем учить π(s,ϕ)\\\\pi(s, \\\\phi)π(s,ϕ) выдавать такие действия, на которых наша Q-функция выдаёт большие скалярные значения:\\n∑sQ∗(s,π(s,ϕ),θ)→max\\u2061ϕ\\\\sum_{s} Q^{*}(s, \\\\pi(s, \\\\phi), \\\\theta) \\\\to \\\\max_{\\\\phi}\\ns∑\\u200bQ∗(s,π(s,ϕ),θ)→ϕmax\\u200bПричём, поскольку действия непрерывные, всё слева дифференцируемо и мы можем напрямую применять самый обычный backpropagation!\\n\\nТеперь когда на руках есть приближение π(s,ϕ)≈argmax\\u2061aQ∗(s,a,θ)\\\\pi(s, \\\\phi) \\\\approx \\\\operatorname{argmax}_a Q^{*}(s, a, \\\\theta)π(s,ϕ)≈argmaxa\\u200bQ∗(s,a,θ), можно просто использовать его всюду, где нам нужны аргмаксимумы и максимумы от нашей Q-функции. Мы получили Actor-Critic схему: у нас есть актёр, π(s,ϕ)\\\\pi(s, \\\\phi)π(s,ϕ) — детерминированная стратегия, и критик Q∗(s,a)Q^{*}(s,a)Q∗(s,a), который оценивает выбор действий актёром и предоставляет градиент для его улучшения. Актёр учится выбирать действия, которые больше всего нравятся критику, а критик учится регрессией с целевой переменной\\ny=r+γmax\\u2061a′Q∗(s′,a′,θ−)≈r+γQ∗(s′,π(s′,ϕ),θ−)y = r + \\\\gamma \\\\max_{a'} Q^{*}(s', a', \\\\theta^{-}) \\\\approx r + \\\\gamma Q^{*}(s', \\\\pi(s', \\\\phi), \\\\theta^{-})\\ny=r+γa′max\\u200bQ∗(s′,a′,θ−)≈r+γQ∗(s′,π(s′,ϕ),θ−)Эта прикольная рабочая эвристика позволяет придумать off-policy алгоритмы для непрерывных пространств действий; к такому подходу относятся такие алгоритмы, как DDPG, TD3 и SAC.\\nPolicy Gradient алгоритмы\\nВ рассмотренных алгоритмах есть несколько приниципиальных ограничений, которые вытекают непосредственно из самой идеи подхода. Мы учимся с таргетов, заглядывающих всего на один шаг вперёд, использующих только s′s's′; это чревато проблемой накапливающейся ошибки, поскольку если между выполнением действия и получением награды +1 проходит 100 шагов, нам нужно на сто шагов «распространять» полученный сигнал. Мы должны учить Q∗(s,a)Q^{*}(s,a)Q∗(s,a) вместо того, чтобы как-то напрямую («end-to-end») запомнить, какие действия в каких состояниях хорошие. Наконец, наша стратегия всегда детерминирована, когда для взаимодействия со средой во время сбора данных, например, нам позарез нужна была стохастичная, чтобы гарантированно обновлять Q-функцию для всех пар s,as, as,a, и эту проблему пришлось закрывать костылями.\\nЕсть второй подход model-free алгоритмов RL, называемый Policy Gradient, который позволяет избежать вышеперечисленных недостатков за счёт on-policy режима работы. Идея выглядит так: давайте будем искать стратегию в классе стохастичных стратегий, то есть заведём нейросеть, моделирующую πθ(a∣s)\\\\pi_{\\\\theta}(a \\\\mid s)πθ\\u200b(a∣s) напрямую. Тогда наш функционал, который мы оптимизируем,\\nJ(θ)=ET∼πθ∑t≥0γtrt→max\\u2061θ,J(\\\\theta) = \\\\mathbb{E}_{\\\\mathcal{T} \\\\sim \\\\pi_{\\\\theta}} \\\\sum_{t \\\\ge 0} \\\\gamma^t r_t \\\\to \\\\max_{\\\\theta},\\nJ(θ)=ET∼πθ\\u200b\\u200bt≥0∑\\u200bγtrt\\u200b→θmax\\u200b,дифференцируем по параметрам θ\\\\thetaθ, и градиент равен:\\n∇θJ(θ)=ET∼πθ∑t≥0∇θlog\\u2061πθ(at∣st)γtRt,\\\\nabla_{\\\\theta} J(\\\\theta) = \\\\mathbb{E}_{\\\\mathcal{T} \\\\sim \\\\pi_{\\\\theta}} \\\\sum_{t \\\\ge 0} \\\\nabla_{\\\\theta} \\\\log \\\\pi_{\\\\theta}(a_t \\\\mid s_t) \\\\gamma^t R_t,\\n∇θ\\u200bJ(θ)=ET∼πθ\\u200b\\u200bt≥0∑\\u200b∇θ\\u200blogπθ\\u200b(at\\u200b∣st\\u200b)γtRt\\u200b,где RtR_tRt\\u200b - reward-to-go с шага ttt, то есть награда, собранная в сыгранном эпизоде после шага ttt:\\nRt=∑t′≥tγt′−trt′R_t = \\\\sum_{t' \\\\ge t} \\\\gamma^{t' - t} r_{t'}\\nRt\\u200b=t′≥t∑\\u200bγt′−trt′\\u200bСкетч доказательстваДавайте распишем мат.ожидание по определению, как следующий интеграл:\\n∇θJ(θ)=∇θET∼πθ∑t≥0γtrt=∇θ∫Tp(T∣πθ)R0dT,\\\\nabla_{\\\\theta} J(\\\\theta) = \\\\nabla_{\\\\theta} \\\\mathbb{E}_{\\\\mathcal{T} \\\\sim \\\\pi_{\\\\theta}} \\\\sum_{t \\\\ge 0} \\\\gamma^t r_t = \\\\nabla_{\\\\theta} \\\\int_{\\\\mathcal{T}} p(\\\\mathcal{T} \\\\mid \\\\pi_{\\\\theta}) R_0 d\\\\mathcal{T},\\n∇θ\\u200bJ(θ)=∇θ\\u200bET∼πθ\\u200b\\u200bt≥0∑\\u200bγtrt\\u200b=∇θ\\u200b∫T\\u200bp(T∣πθ\\u200b)R0\\u200bdT,где интеграл берётся по пространству всевозможных траекторий, p(T∣πθ)p(\\\\mathcal{T}\\\\mid \\\\pi_{\\\\theta})p(T∣πθ\\u200b) — вероятность встретить траекторию T\\\\mathcal{T}T при взаимодействии со средой стратегии πθ\\\\pi_{\\\\theta}πθ\\u200b, а R0R_0R0\\u200b — reward-to-go с шага t=0t = 0t=0, то есть суммарная дисконтированная награда за весь эпизод для рассматриваемой траектории T\\\\mathcal{T}T. Награда от параметров θ\\\\thetaθ не зависит; от выбора стратегии зависят лишь вероятности встретить ту или иную траекторию T\\\\mathcal{T}T. Давайте пронесём градиент внутрь интеграла:\\n∇θJ(θ)=∫T∇θp(T∣πθ)R0dT,\\\\nabla_{\\\\theta} J(\\\\theta) = \\\\int_{\\\\mathcal{T}} \\\\nabla_{\\\\theta} p(\\\\mathcal{T} \\\\mid \\\\pi_{\\\\theta}) R_0 d \\\\mathcal{T},\\n∇θ\\u200bJ(θ)=∫T\\u200b∇θ\\u200bp(T∣πθ\\u200b)R0\\u200bdT,и теперь этот сложный интеграл перестал быть каким-то мат.ожиданием. Давайте исправим это при помощи трюка, который называется log-derivative trick: для этого домножим и поделим внутри на p(T∣πθ)p(\\\\mathcal{T} \\\\mid \\\\pi_{\\\\theta})p(T∣πθ\\u200b), получим:\\n∇θJ(θ)=∫Tp(T∣πθ)∇θp(T∣πθ)p(T∣πθ)R0dT=ET∼πθ∇θp(T∣πθ)p(T∣πθ)R0\\\\nabla_{\\\\theta} J(\\\\theta) = \\\\int_{\\\\mathcal{T}} p(\\\\mathcal{T} \\\\mid \\\\pi_{\\\\theta}) \\\\frac{\\\\nabla_{\\\\theta} p(\\\\mathcal{T} \\\\mid \\\\pi_{\\\\theta})}{p(\\\\mathcal{T} \\\\mid \\\\pi_{\\\\theta})} R_0 d \\\\mathcal{T} = \\\\mathbb{E}_{\\\\mathcal{T} \\\\sim \\\\pi_{\\\\theta}} \\\\frac{\\\\nabla_{\\\\theta} p(\\\\mathcal{T} \\\\mid \\\\pi_{\\\\theta})}{p(\\\\mathcal{T} \\\\mid \\\\pi_{\\\\theta})} R_0\\n∇θ\\u200bJ(θ)=∫T\\u200bp(T∣πθ\\u200b)p(T∣πθ\\u200b)∇θ\\u200bp(T∣πθ\\u200b)\\u200bR0\\u200bdT=ET∼πθ\\u200b\\u200bp(T∣πθ\\u200b)∇θ\\u200bp(T∣πθ\\u200b)\\u200bR0\\u200bИтак, мы увидели, что градиент нашего функционала — тоже мат.ожидание по всевозможным траекториям. Осталось заметить, что отношение градиента правдоподобия к значению правдоподобия — градиент логарифма правдоподобия:\\n∇log\\u2061p(x)=∇p(x)p(x)\\\\nabla \\\\log p(x) = \\\\frac{\\\\nabla p(x)}{p(x)}\\n∇logp(x)=p(x)∇p(x)\\u200b(откуда и название трюка), чем мы и воспользуемся:\\n∇θJ(θ)=ET∼πθ∇θlog\\u2061p(T∣πθ)R0\\\\nabla_{\\\\theta} J(\\\\theta) = \\\\mathbb{E}_{\\\\mathcal{T} \\\\sim \\\\pi_{\\\\theta}} \\\\nabla_{\\\\theta} \\\\log p(\\\\mathcal{T} \\\\mid \\\\pi_{\\\\theta}) R_0\\n∇θ\\u200bJ(θ)=ET∼πθ\\u200b\\u200b∇θ\\u200blogp(T∣πθ\\u200b)R0\\u200bВспоминая, как выглядит trajectory distribution для заданной стратегии πθ\\\\pi_{\\\\theta}πθ\\u200b, замечаем, что функция переходов в среде не вносит вклад в наш градиент:\\n∇θlog\\u2061p(T∣πθ)=∇θlog\\u2061∏t≥0p(st+1∣st,at)πθ(at∣st)=∑t≥0∇θlog\\u2061πθ(at∣st)\\\\nabla_{\\\\theta} \\\\log p(\\\\mathcal{T} \\\\mid \\\\pi_{\\\\theta}) = \\\\nabla_{\\\\theta} \\\\log \\\\prod_{t \\\\ge 0} p(s_{t + 1} \\\\mid s_t, a_t) \\\\pi_{\\\\theta}(a_t \\\\mid s_t) = \\\\sum_{t \\\\ge 0} \\\\nabla_{\\\\theta} \\\\log \\\\pi_{\\\\theta}(a_t \\\\mid s_t)\\n∇θ\\u200blogp(T∣πθ\\u200b)=∇θ\\u200blogt≥0∏\\u200bp(st+1\\u200b∣st\\u200b,at\\u200b)πθ\\u200b(at\\u200b∣st\\u200b)=t≥0∑\\u200b∇θ\\u200blogπθ\\u200b(at\\u200b∣st\\u200b)Итого мы получили следующую формулу:\\n∇θJ(θ)=ET∼πθ∑t≥0∇θlog\\u2061πθ(at∣st)R0\\\\nabla_{\\\\theta} J(\\\\theta) = \\\\mathbb{E}_{\\\\mathcal{T} \\\\sim \\\\pi_{\\\\theta}} \\\\sum_{t \\\\ge 0} \\\\nabla_{\\\\theta} \\\\log \\\\pi_{\\\\theta}(a_t \\\\mid s_t) R_0\\n∇θ\\u200bJ(θ)=ET∼πθ\\u200b\\u200bt≥0∑\\u200b∇θ\\u200blogπθ\\u200b(at\\u200b∣st\\u200b)R0\\u200bОтличие этой формулы от приведённой выше в том, что градиент, отвечающий за выбор действия ata_tat\\u200b в состоянии sts_tst\\u200b, взвешивается не на reward-to-go с момента времени ttt, а на всю награду за эпизод, начиная с нулевого момента времени. Это странно, поскольку получается, что награда, собранная в прошлом, до принятия решения в момент времени ttt, как-то влияет на градиент, соответствующий этому выбору. Мы же понимаем, что решение в момент времени ttt на прошлую награду никак повлиять не могло, и странно, что градиент для решения в момент времени ttt взвешивается на прошлую награду. Это наблюдение называется принципом причинности (causality principle); можно строго показать, что если заменить R0R_0R0\\u200b на RtR_tRt\\u200b, то «удалённые» таким образом слагаемые в среднем по траекториям дают нулевой вклад в градиент, и поэтому такая замена математически корректна.\\nЭта формула говорит нам, что градиент нашего функционала — это тоже мат.ожидание по траекториям. А значит, мы можем попробовать посчитать какую-то оценку этого градиента, заменив мат.ожидание на Монте Карло оценку, и просто начать оптимизировать наш функционал самым обычным стохастическим градиентным спуском! А то есть: берём нашу стратегию πθ\\\\pi_{\\\\theta}πθ\\u200b с текущими значениями параметров θ\\\\thetaθ, играем эпизод (или несколько) в среде, то есть сэмплируем T∼πθ\\\\mathcal{T} \\\\sim \\\\pi_{\\\\theta}T∼πθ\\u200b, и затем делаем шаг градиентного подъёма:\\nθ←θ+α∑t≥0∇θlog\\u2061πθ(at∣st)γtRt\\\\theta \\\\leftarrow \\\\theta + \\\\alpha \\\\sum_{t \\\\ge 0} \\\\nabla_{\\\\theta} \\\\log \\\\pi_{\\\\theta}(a_t \\\\mid s_t) \\\\gamma^t R_t\\nθ←θ+αt≥0∑\\u200b∇θ\\u200blogπθ\\u200b(at\\u200b∣st\\u200b)γtRt\\u200bПочему эта идея приводит к on-policy подходу? Для каждого шага градиентного шага нам обязательно нужно взять T∼πθ\\\\mathcal{T} \\\\sim \\\\pi_{\\\\theta}T∼πθ\\u200b с самыми свежими, с текущими весами θ\\\\thetaθ, и никакая другая траектория, порождённая какой-то другой стратегией, нам не подойдёт. Поэтому для каждой итерации алгоритма нам придётся заново играть очередной эпизод со средой. Это sample-inefficient: неэффективно по числу сэмплов, мы собираем слишком много данных и очень неэффективно с ними работаем.\\nPolicy Gradient алгоритмы пытаются по-разному бороться с этой неэффективностью, опять же обращаясь к теории оценочных функций и бутстрапированным оценкам, позволяющим предсказывать будущие награды, не доигрывая эпизоды целиком до конца. Большинство этих алгоритмов остаются в on-policy режиме и применимы в любых пространствах действий. К этим алгоритмам относятся такие алгоритмы, как Advantage Actor-Critic (A2C), Trust-Region Policy Optimization (TRPO) и Proximal Policy Optimization (PPO).\\nЧто там ещё?\\nМы до сих пор разбирали model-free алгоритмы RL, которые обходились без знаний о p(s′∣s,a)p(s' \\\\mid s, a)p(s′∣s,a) и никак не пытались приближать это распределение. Однако, в каких-нибудь пятнашках функция переходов нам известна: мы знаем, в какое состояние перейдёт среда, если мы выберем некоторое действие в таком-то состоянии. Понятно, что эту информацию было бы здорово как-то использовать. Существует обширный класс model-based, который либо предполагает, что функция переходов дана, либо мы учим её приближение, используя s,a,s′s, a, s's,a,s′ из нашего опыта в качестве обучающей выборки. Алгоритм AlphaZero на основе этого подхода превзошёл человека в игру Го, которая считалась куда более сложной игрой, чем шахматы; причём этот алгоритм возможно запустить обучаться на любой игре: как на Го, так и на шахматах или сёги.\\n\\n\\n\\nисточник картинки — курс UC Berkeley AI\\n\\n\\nОбучение с подкреплением стремится построить алгоритмы, способные обучаться решать любую задачу, представленную в формализме MDP. Как и обычные методы оптимизации, их можно использовать в виде чёрной коробочки из готовых библиотек, например, OpenAI Stable Baselines. Внутри таких коробочек будет, однако, довольно много гиперпараметров, которые пока не совсем понятно как настраивать под ту или иную практическую задачу. И хотя успехи Deep RL демонстрируют, что эти алгоритмы способны обучаться невероятно сложным задачам вроде победы над людьми в Dota 2 и в StarCraft II, они требуют для этого колоссального количества ресурсов. Поиск более эффективных процедур — открытая задача в Deep RL.\\nВ ШАДе есть курс Practical RL, на котором вы погрузитесь глубже в мир глубокого обучения с подкреплением, разберётесь в более продвинутых алгоритмах и попробуете пообучать нейронки решать разные задачки в разных средах.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф10.5. Задача ранжированияСледующий параграф11.2. КраудсорсингЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_50.html', 'title': 'Обобщающая способность – классическая теория'}, page_content=\"Обобщающая способность – классическая теорияЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/613.1.Введение в теорию глубокого обучения13.2.Обобщающая способность – классическая теорияОценка супремумаФундаментальная проблема равномерных оценок13.3.PAC-байесовские оценки риска13.4.Сети бесконечной ширины13.5.Ландшафт функции потерь13.6.Implicit bias14.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Обобщающая способность – классическая теория13.2. Обобщающая способность – классическая теорияАвторыГоликов ЕвгенийКак мы уже видели во введении, мы не можем напрямую оптимизировать истинный риск модели f(x)f(x)f(x)\\nR(f)=Ex,y∼Dr(y,f(x)),R(f) = \\\\mathbb{E}_{x,y \\\\sim \\\\mathcal{D}} r(y,f(x)),\\nR(f)=Ex,y∼D\\u200br(y,f(x)),так как нам недоступно полное распределение данных D\\\\mathcal{D}D. Поэтому вместо задачи минимизации истинного риска, мы будем минимизировать эмпирический риск\\nR^m(f)=Ex,y∈Smr(y,f(x))\\\\hat R_m(f) = \\\\mathbb{E}_{x,y \\\\in S_m} r(y, f(x))\\nR^m\\u200b(f)=Ex,y∈Sm\\u200b\\u200br(y,f(x))на доступном нам наборе данных SmS_mSm\\u200b.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nКлассическая теория предлагает оценивать разность эмпирического и истинного равномерно, что даёт\\nR(f^m)≤R^m(f^m)+sup\\u2061f∈F(R(f)−R^m(f)).R(\\\\hat f_m) \\\\leq \\\\hat R_m(\\\\hat f_m) + \\\\sup_{f \\\\in \\\\mathcal{F}} (R(f) - \\\\hat R_m(f)).\\nR(f^\\u200bm\\u200b)≤R^m\\u200b(f^\\u200bm\\u200b)+f∈Fsup\\u200b(R(f)−R^m\\u200b(f)).Такая оценка не зависит от алгоритма обучения; она зависит лишь от класса моделей F\\\\mathcal{F}F, в котором происходит поиск. Так, в случае нейронных сетей в качестве класса F\\\\mathcal{F}F можно взять класс всех нейронных сетей фиксированной архитектуры, отличающихся только весами.\\nЕсли класс F\\\\mathcal{F}F настолько велик, что для большинства наборов данных размера mmm содержит модель fff, у которой эмпирический риск мал, а истинный велик, то оценка выше теряет смысл. Работа Understanding deep learning requires rethinking generalization показала, что именно это и происходит в нейронных сетях, применяемых на практике, на реальных наборах данных. А именно, пусть A\\\\mathcal{A}A – алгоритм, применяемый для обучения сети, например, градиентный спуск. Предположим, что с высокой вероятностью R^m(A(Sm))=0\\\\hat R_m(\\\\mathcal{A}(S_m)) = 0R^m\\u200b(A(Sm\\u200b))=0, если только размер выборки mmm не слишком велик. Пусть SmS_mSm\\u200b – наша выборка, а Sm′′S'_{m'}Sm′′\\u200b – датасет, в котором примеры берутся из выборки, а разметка случайна. По предположению, модель f^m,m′=A(Sm∪Sm′′)\\\\hat f_{m,m'} = \\\\mathcal{A}(S_m \\\\cup S'_{m'})f^\\u200bm,m′\\u200b=A(Sm\\u200b∪Sm′′\\u200b), обученная на объединённом датасете, имеет нулевой риск на «настоящем» датасете SmS_mSm\\u200b, если только m+m′m+m'm+m′ не слишком велик. С другой стороны, если m′≫mm' \\\\gg mm′≫m, то f^m,m′≈A(Sm′′)\\\\hat f_{m,m'} \\\\approx \\\\mathcal{A}(S'_{m'})f^\\u200bm,m′\\u200b≈A(Sm′′\\u200b) – истинный риск такой модели близок к риску случайного угадывания.\\nТем не менее, если в качестве F\\\\mathcal{F}F взять не все модели, реализуемые данной архитектурой нейронной сети, а лишь реализуемые данным алгоритмом обучения на наборах данных из распределения с высокой вероятностью, то можно надеятся, что равномерная оценка окажется осмысленной.\\nМы говорим «с высокой вероятностью» для того, чтобы исключить «нереалистичные» наборы данных, обучение на которых ведёт к плохим результатам, а также ничтожно-редкие случаи реализации шума в алгоритме обучения, при котором последний сходится в «плохие» решения. Подробнее о том, какие модели реализуются градиентным спуском, мы обсудим в параграфе про implicit bias.\\nОценка супремума\\nПопробуем оценить супремум разницы рисков. Будем считать, что выборка SmS_mSm\\u200b выбирается случайным (и равновероятным) образом из распределения данных D\\\\mathcal{D}D. Некоторые из выборок могут быть катастрофически плохими, поэтому мы будем рассматривать оценки, которые верны не обязательно всегда, а просто с достаточно большой вероятностью.\\nПредположим сначала, что класс моделей F\\\\mathcal{F}F конечен. Тогда\\nP(sup\\u2061f∈F(R(f)−R^m(f))≥ϵ)=\\\\mathbb{P}\\\\left(\\\\sup_{f\\\\in\\\\mathcal{F}} (R(f) - \\\\hat R_m(f)) \\\\geq \\\\epsilon\\\\right) =\\nP(f∈Fsup\\u200b(R(f)−R^m\\u200b(f))≥ϵ)==P(∃f∈F:\\u2005\\u200a(R(f)−R^m(f))≥ϵ)≤= \\\\mathbb{P}(\\\\exists f\\\\in\\\\mathcal{F}: \\\\; (R(f) - \\\\hat R_m(f)) \\\\geq \\\\epsilon) \\\\leq\\n=P(∃f∈F:(R(f)−R^m\\u200b(f))≥ϵ)≤≤∑f∈FP(R(f)−R^m(f)≥ϵ)≤\\\\leq\\\\sum_{f\\\\in\\\\mathcal{F}} \\\\mathbb{P}(R(f) - \\\\hat R_m(f) \\\\geq \\\\epsilon) \\\\leq\\n≤f∈F∑\\u200bP(R(f)−R^m\\u200b(f)≥ϵ)≤≤∣F∣sup\\u2061f∈FP(R(f)−R^m(f)≥ϵ)∀ϵ>0\\\\leq\\\\vert\\\\mathcal{F}\\\\vert \\\\sup_{f\\\\in\\\\mathcal{F}} \\\\mathbb{P}(R(f) - \\\\hat R_m(f) \\\\geq \\\\epsilon)\\n        \\\\quad\\n\\\\forall \\\\epsilon > 0≤∣F∣f∈Fsup\\u200bP(R(f)−R^m\\u200b(f)≥ϵ)∀ϵ>0Заметим, что R(f)=ESm∼DmR^m(f)R(f) = \\\\mathbb{E}_{S_m \\\\sim \\\\mathcal{D}^m} \\\\hat R_m(f)R(f)=ESm\\u200b∼Dm\\u200bR^m\\u200b(f). Поэтому при фиксированном fff разницу рисков R^m(f)−R(f)\\\\hat R_m(f) - R(f)R^m\\u200b(f)−R(f) можно оценить с помощью неравенства Хёффдинга.\\nНеравенство Хёффдинга (Hoeffding's inequality). Пусть X1,…,XmX_1,\\\\ldots,X_mX1\\u200b,…,Xm\\u200b – независимые одинаково распределённые случайные величины со значениями в [0,1][0,1][0,1]. Тогда для всех ϵ>0\\\\epsilon > 0ϵ>0 имеют место неравенства\\nP(∑i=1mXi−E∑i=1mXi≥ϵ)≤e−2ϵ2m,\\\\mathbb{P}\\\\left(\\\\sum_{i=1}^m X_i - \\\\mathbb{E} \\\\sum_{i=1}^m X_i \\\\geq \\\\epsilon \\\\right) \\\\leq\\ne^{-\\\\frac{2\\\\epsilon^2}{m}},P(i=1∑m\\u200bXi\\u200b−Ei=1∑m\\u200bXi\\u200b≥ϵ)≤e−m2ϵ2\\u200b,P(E∑i=1mXi−∑i=1mXi≥ϵ)≤e−2ϵ2m.\\\\mathbb{P}\\\\left(\\\\mathbb{E} \\\\sum_{i=1}^m X_i - \\\\sum_{i=1}^m X_i \\\\geq \\\\epsilon \\\\right) \\\\leq\\ne^{-\\\\frac{2\\\\epsilon^2}{m}}.P(Ei=1∑m\\u200bXi\\u200b−i=1∑m\\u200bXi\\u200b≥ϵ)≤e−m2ϵ2\\u200b.Как следствие неравенства Хёффдинга, получаем, что для любого ϵ>0\\\\epsilon > 0ϵ>0 и для любой f∈Ff \\\\in \\\\mathcal{F}f∈F.\\nP(R(f)−R^m(f)≥ϵ)≤e−2mϵ2        \\\\mathbb{P}(R(f) - \\\\hat R_m(f) \\\\geq \\\\epsilon) \\\\leq\\n        e^{-2 m \\\\epsilon^2}  \\nP(R(f)−R^m\\u200b(f)≥ϵ)≤e−2mϵ2Заметим, что тогда для любой f∈Ff \\\\in \\\\mathcal{F}f∈F,\\nR(f)−R^m(f)≤12mlog\\u20611δс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm.R(f) - \\\\hat R_m(f) \\\\leq\\n        \\\\sqrt{\\\\frac{1}{2m} \\\\log \\\\frac{1}{\\\\delta}}\\n        \\\\quad\\n        \\\\text{с вероятностью $\\\\geq 1 - \\\\delta$ по $S_m$.}\\nR(f)−R^m\\u200b(f)≤2m1\\u200blogδ1\\u200b\\u200bс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm\\u200b.Несмотря на то, что эта оценка является оценкой на обобщающую способность, она не имеет смысла, так как модель fff в ней задана априори и не зависит от SmS_mSm\\u200b. Другими словами, она верна для необученных моделей fff.\\nВозвращаясь к нашей оценке, получаем:\\nP(sup\\u2061f∈F(R(f)−R^m(f))≥ϵ)≤∣F∣e−2mϵ2∀ϵ>0,        \\\\mathbb{P}\\\\left(\\\\sup_{f\\\\in\\\\mathcal{F}} (R(f) - \\\\hat R_m(f)) \\\\geq \\\\epsilon\\\\right)\\n        \\\\leq |\\\\mathcal{F}| e^{-2 m \\\\epsilon^2}\\n        \\\\quad\\n        \\\\forall \\\\epsilon > 0,\\nP(f∈Fsup\\u200b(R(f)−R^m\\u200b(f))≥ϵ)≤∣F∣e−2mϵ2∀ϵ>0,где ∣F∣\\\\vert\\\\mathcal{F}\\\\vert∣F∣ – мощность класса F\\\\mathcal{F}F. Следовательно,\\nsup\\u2061f∈F(R(f)−R^m(f))≤12m(log\\u20611δ+log\\u2061∣F∣)с\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm.\\\\color{#348FEA}{\\\\sup_{f\\\\in\\\\mathcal{F}} (R(f) - \\\\hat R_m(f)) \\\\leq\\n        \\\\sqrt{\\\\frac{1}{2m} \\\\left(\\\\log \\\\frac{1}{\\\\delta} + \\\\log |\\\\mathcal{F}| \\\\right)}\\n        \\\\quad\\n        \\\\text{с вероятностью $\\\\geq 1 - \\\\delta$ по $S_m$.}}\\nf∈Fsup\\u200b(R(f)−R^m\\u200b(f))≤2m1\\u200b(logδ1\\u200b+log∣F∣)\\u200bс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm\\u200b.В случае бесконечного F\\\\mathcal{F}F используем следующее обобщение неравенства Хёффдинга:\\nНеравенство МакДайармида (McDiarmid's inequality). Пусть X1,…,XmX_{1},\\\\ldots,X_mX1\\u200b,…,Xm\\u200b – независимые одинаково распределённые случайные величины, ggg – скалярная функция с mmm аргументами, такая что\\nsup\\u2061x1,…,xm,x~i∣g(x1,…,xi,…,xm)−g(x1,…x~i,…xm)∣≤ci∀i=1,…,m\\\\sup_{x_{1},\\\\ldots,x_m,\\\\tilde x_i} |g(x_1,\\\\ldots,x_i,\\\\ldots,x_m) - g(x_{1},\\\\ldots \\\\tilde x_i,\\\\ldots x_{m})| \\\\leq c_i\\n            \\\\quad\\n            \\\\forall i = 1,\\\\ldots,m\\nx1\\u200b,…,xm\\u200b,x~i\\u200bsup\\u200b∣g(x1\\u200b,…,xi\\u200b,…,xm\\u200b)−g(x1\\u200b,…x~i\\u200b,…xm\\u200b)∣≤ci\\u200b∀i=1,…,mдля некоторых cic_ici\\u200b. Тогда для любого ϵ>0\\\\epsilon > 0ϵ>0 имеет место неравенство\\nP(g(X1,…,Xm)−Eg(X1,…,Xm)≥ϵ)≤e−2ϵ2∑i=1mci2.\\\\mathbb{P}\\\\left( g(X_1,\\\\ldots,X_m) - \\\\mathbb{E} g(X_1,\\\\ldots,X_m) \\\\geq \\\\epsilon \\\\right) \\\\leq\\n            e^{-\\\\frac{2\\\\epsilon^2}{\\\\sum_{i=1}^m c_i^2}}.\\nP(g(X1\\u200b,…,Xm\\u200b)−Eg(X1\\u200b,…,Xm\\u200b)≥ϵ)≤e−∑i=1m\\u200bci2\\u200b2ϵ2\\u200b.Применяя теорему к g({(xi,yi)}i=1m)=sup\\u2061f∈F(R(f)−R^m(f))g(\\\\{(x_i,y_i)\\\\}_{i=1}^m) = \\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R_m(f))g({(xi\\u200b,yi\\u200b)}i=1m\\u200b)=supf∈F\\u200b(R(f)−R^m\\u200b(f)), получаем:\\nPSm(sup\\u2061f∈F(R(f)−R^m(f))−ESm′sup\\u2061f∈F(R(f)−R^m′(f))≥ϵ)≤e−2mϵ2,\\\\mathbb{P}_{S_m} \\\\left(\\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R_m(f)) - \\\\mathbb{E}_{S'_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R'_m(f)) \\\\geq \\\\epsilon\\\\right) \\\\leq\\n        e^{-2 m \\\\epsilon^2},\\nPSm\\u200b\\u200b(f∈Fsup\\u200b(R(f)−R^m\\u200b(f))−ESm′\\u200b\\u200bf∈Fsup\\u200b(R(f)−R^m′\\u200b(f))≥ϵ)≤e−2mϵ2,из чего следует:\\nsup\\u2061f∈F(R(f)−R^m(f))≤ESm′sup\\u2061f∈F(R(f)−R^m′(f))+12mlog\\u20611δс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm,            \\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R_m(f)) \\\\leq\\n        \\\\color{#FFC100}{\\\\mathbb{E}_{S'_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R'_m(f))} + \\\\sqrt{\\\\frac{1}{2m} \\\\log \\\\frac{1}{\\\\delta}}\\n        \\\\quad\\n        \\\\text{с вероятностью $\\\\geq 1 - \\\\delta$ по $S_m$,}\\nf∈Fsup\\u200b(R(f)−R^m\\u200b(f))≤ESm′\\u200b\\u200bf∈Fsup\\u200b(R(f)−R^m′\\u200b(f))+2m1\\u200blogδ1\\u200b\\u200bс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm\\u200b,где R^m′(f)\\\\hat R'_m(f)R^m′\\u200b(f) – эмпирический риск на выборке Sm′S'_mSm′\\u200b. В следующем подразделе мы постараемся оценить жёлтое слагаемое.\\nСимметризация и сложность Радемахера\\nОценим сверху матожидание супремума:\\nESm′sup\\u2061f∈F(R(f)−R^m′(f))=ESm′sup\\u2061f∈F(ESm′′R^m′′(f)−R^m′(f))≤\\\\mathbb{E}_{S'_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R'_m(f))\\n        = \\\\mathbb{E}_{S'_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(\\\\mathbb{E}_{S''_m} \\\\hat R''_m(f) - \\\\hat R'_m(f))\\n        \\\\leq\\nESm′\\u200b\\u200bf∈Fsup\\u200b(R(f)−R^m′\\u200b(f))=ESm′\\u200b\\u200bf∈Fsup\\u200b(ESm′′\\u200b\\u200bR^m′′\\u200b(f)−R^m′\\u200b(f))≤≤ESm′ESm′′sup\\u2061f∈F(R^m′′(f)−R^m′(f))=\\\\leq \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(\\\\hat R''_m(f) - \\\\hat R'_m(f)) =\\n≤ESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(R^m′′\\u200b(f)−R^m′\\u200b(f))==ESm′ESm′′sup\\u2061f∈F(1m∑i=1m(r(yi′′,f(xi′′))−r(yi′,f(xi′)))).= \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m (r(y''_i,f(x''_i)) - r(y'_i,f(x'_i)))\\\\right).\\n=ESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200b(r(yi′′\\u200b,f(xi′′\\u200b))−r(yi′\\u200b,f(xi′\\u200b)))).Этот шаг называется «симметризация»: теперь выражение выше зависит от двух равнозначных обучающих выборок Sm′S'_mSm′\\u200b и Sm′′{S_m''}Sm′′\\u200b. Ниже для краткости будем обозначать ri′(f)=r(yi′,f(xi′))r'_i(f) = r(y'_i,f(x'_i))ri′\\u200b(f)=r(yi′\\u200b,f(xi′\\u200b)) и ri′′(f)=r(yi′′,f(xi′′)){r_i''}(f) = r({y_i''},f({x_i''}))ri′′\\u200b(f)=r(yi′′\\u200b,f(xi′′\\u200b)).\\nКак оценить сверху супремум разности рисков? Наивная оценка, супремум суммы, слишком слаба: в самом деле, при фиксированном наборе данных вполне вероятно может существовать модель, имеющая большой риск на нём (достаточно взять модель, обученную на тех же данных, но с «неправильными» метками), поэтому матожидание супремума эмпирического риска может быть велико.\\nДля обхода этой сложности заметим, что выражение выше симметрично относительно перестановки местами двух выборок:\\nESm′ESm′′sup\\u2061f∈F(1m∑i=1m(ri′′(f)−ri′(f)))=ESm′ESm′′sup\\u2061f∈F(1m∑i=1m(ri′(f)−ri′′(f))).        \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m (r''_i(f) - r'_i(f))\\\\right)\\n        = \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m (r'_i(f) - r''_i(f))\\\\right).\\nESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200b(ri′′\\u200b(f)−ri′\\u200b(f)))=ESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200b(ri′\\u200b(f)−ri′′\\u200b(f))).Более того, так как элементы обеих выборок выбираются независимо, значение выражения не меняется и при перестановке местами отдельно iii-ых элементов двух выборок. А именно, для любого набора σ1,…,σm∈{−1,1}m\\\\sigma_1,\\\\ldots,\\\\sigma_m \\\\in \\\\{-1,1\\\\}^mσ1\\u200b,…,σm\\u200b∈{−1,1}m\\nESm′ESm′′sup\\u2061f∈F(1m∑i=1m(ri′′(f)−ri′(f)))=        \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m (r''_i(f) - r'_i(f))\\\\right) =\\nESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200b(ri′′\\u200b(f)−ri′\\u200b(f)))==ESm′ESm′′sup\\u2061f∈F(1m∑i=1mσi(ri′′(f)−ri′(f))).        = \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i (r''_i(f) - r'_i(f))\\\\right).\\n=ESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200b(ri′′\\u200b(f)−ri′\\u200b(f))).Будем выбирать σi\\\\sigma_iσi\\u200b независимо и равновероятно из {−1,1}\\\\{-1,1\\\\}{−1,1}. Такие случайные величины называются переменными Радемахера.\\nПоскольку оценки выше были верны для любых сигм, они верны и в среднем по переменным Радемахера, выбранным независимо от выборки:\\nESm′ESm′′sup\\u2061f∈F(1m∑i=1m(ri′′(f)−ri′(f)))=        \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m (r''_i(f) - r'_i(f))\\\\right) =\\nESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200b(ri′′\\u200b(f)−ri′\\u200b(f)))==Eσ1:mESm′ESm′′sup\\u2061f∈F(1m∑i=1mσi(ri′′(f)−ri′(f))).        = \\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i (r''_i(f) - r'_i(f))\\\\right).\\n=Eσ1:m\\u200b\\u200bESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200b(ri′′\\u200b(f)−ri′\\u200b(f))).После введения переменных Радемахера оценка супремума разницы рисков через сумму супремумов становится не такой плохой. В самом деле, рассмотрим бинарную классификацию с помощью линейной модели. Если данные хорошо разделяются плоскостью, то ESmsup\\u2061f∈F(1m∑i=1mri(f))\\\\mathbb{E}_{S_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m r_i(f)\\\\right)ESm\\u200b\\u200bsupf∈F\\u200b(m1\\u200b∑i=1m\\u200bri\\u200b(f)) будет большим, так как в качестве fff можно взять линейную модель с противоположно ориентированной разделяющей плоскостью для SmS_mSm\\u200b. В то же время для того, чтобы Eσ1:mESmsup\\u2061f∈F(1m∑i=1mσiri(f))\\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\mathbb{E}_{S_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i r_i(f)\\\\right)Eσ1:m\\u200b\\u200bESm\\u200b\\u200bsupf∈F\\u200b(m1\\u200b∑i=1m\\u200bσi\\u200bri\\u200b(f)) было большим, необходимо, чтобы существовала модель, отвечающая правильно на тех примерах, где σi=−1\\\\sigma_i=-1σi\\u200b=−1, и неправильно, где σi=1\\\\sigma_i=1σi\\u200b=1; для линейной модели это невозможно при большинстве конфигураций сигм.\\nВеличина\\nRadD,m(H)=Ez1:m∼DmEσ1:m∼U({−1,1}m)sup\\u2061h∈H(1m∑i=1mσih(zi)).\\\\mathrm{Rad}_{\\\\mathcal{D},m}(\\\\mathcal{H}) =\\n\\\\mathbb{E}_{z_{1:m} \\\\sim \\\\mathcal{D}^m} \\\\mathbb{E}_{\\\\sigma_{1:m} \\\\sim U(\\\\{-1,1\\\\}^m)} \\\\sup_{h \\\\in \\\\mathcal{H}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i h(z_i)\\\\right).\\nRadD,m\\u200b(H)=Ez1:m\\u200b∼Dm\\u200bEσ1:m\\u200b∼U({−1,1}m)\\u200bh∈Hsup\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200bh(zi\\u200b)).называется сложностью Радемахера класса функций H:\\u2009X→R\\\\mathcal{H}: \\\\, \\\\mathbb{X} \\\\to \\\\mathbb{R}H:X→R (для распределения D\\\\mathcal{D}D на X\\\\mathbb{X}X и длины выборок mmm). Она велика, если в классе H\\\\mathcal{H}H содержатся функции, принимающие большие значения с заданными знаками на любом наборе данных фиксированного размера. Другими словами, сложность Радемахера измеряет, насколько выходы функций из класса H\\\\mathcal{H}H могут коррелировать со случайным шумом.\\nДля нас актуальна сложность Радемахера классов вида H=r∘F\\\\mathcal{H} = r \\\\circ \\\\mathcal{F}H=r∘F, то есть композиций моделей из класса F\\\\mathcal{F}F и функции риска rrr. Если F\\\\mathcal{F}F – класс линейных моделей в пространстве размерности меньшей, чем mmm, то сложность Радемахера невелика. В то же время если F\\\\mathcal{F}F – множество всех возможных решающих деревьев, то, если только наборы данных непротиворечивы, она равна единице. В самом деле, решающее дерево способно запомнить всю обучающую выборку, то есть добиться единичной корреляции с любым случайным шумом.\\nВернёмся к оценке разницы рисков:\\nESm′sup\\u2061f∈F(R(f)−R^m′(f))≤ESm′ESm′′sup\\u2061f∈F(1m∑i=1m(ri′′(f)−ri′(f)))=\\\\mathbb{E}_{S'_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R'_m(f))\\n\\\\leq \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m (r''_i(f) - r'_i(f))\\\\right)\\n=\\nESm′\\u200b\\u200bf∈Fsup\\u200b(R(f)−R^m′\\u200b(f))≤ESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200b(ri′′\\u200b(f)−ri′\\u200b(f)))==Eσ1:mESm′ESm′′sup\\u2061f∈F(1m∑i=1mσi(ri′′(f)−ri′(f)))≤= \\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i (r''_i(f) - r'_i(f))\\\\right)\\n\\\\leq\\n=Eσ1:m\\u200b\\u200bESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200b(ri′′\\u200b(f)−ri′\\u200b(f)))≤≤ESm′ESm′′Eσ1:m(sup\\u2061f∈F(1m∑i=1mσiri′′(f))+sup\\u2061f∈F(1m∑i=1mσiri′(f)))=\\\\leq \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\left(\\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i r''_i(f)\\\\right) + \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i r'_i(f)\\\\right)\\\\right)\\n=\\n≤ESm′\\u200b\\u200bESm′′\\u200b\\u200bEσ1:m\\u200b\\u200b(f∈Fsup\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200bri′′\\u200b(f))+f∈Fsup\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200bri′\\u200b(f)))==2ESm′Eσ1:msup\\u2061f∈F(1m∑i=1mσir(yi′,f(xi′)))== 2 \\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i r(y'_i,f(x'_i))\\\\right) =\\n=2ESm′\\u200b\\u200bEσ1:m\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200br(yi′\\u200b,f(xi′\\u200b)))==2RadD,m(r∘F).        = \\\\color{#FFC100}{2 \\\\mathrm{Rad}_{\\\\mathcal{D},m}(r \\\\circ \\\\mathcal{F})}.\\n=2RadD,m\\u200b(r∘F).Оценка для «0/1-риска»\\nСложность Радемахера зависит от функции риска. Рассмотрим задачу бинарной классификации с классами +1+1+1 и −1-1−1. Возьмём в качестве функции риска индикатор ошибки бинарной классификации, или «0/1-риск»:\\nr(y,z)=r0/1(y,z)=I[yz<0].r(y,z) = r_{0/1}(y,z) = \\\\mathbb{I}[y z < 0].\\nr(y,z)=r0/1\\u200b(y,z)=I[yz<0].Название «0/1-риск» обусловлено тем, что риск принимает значения 000 и 111.\\nЗаметим следующее:\\nEσ1:msup\\u2061f∈F(1m∑i=1mσiri(f))=Eσ1:mmax\\u2061f∈FSm(1m∑i=1mσiri(f)),\\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i r_i(f) \\\\right)\\n= \\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\max_{f \\\\in \\\\mathcal{F}_{S_m}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i r_i(f) \\\\right),\\nEσ1:m\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200bri\\u200b(f))=Eσ1:m\\u200b\\u200bf∈FSm\\u200b\\u200bmax\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200bri\\u200b(f)),где FSm\\\\mathcal{F}_{S_m}FSm\\u200b\\u200b – класс эквивалентности функций из F\\\\mathcal{F}F, в котором две функции считаются эквивалентными тогда и только тогда, когда их образы на выборке SmS_mSm\\u200b имеют одинаковые знаки. Другими словами, среди всех функций, принимающих одни и те же знаки на SmS_mSm\\u200b, мы выберем по одной и сформируем из них множество FSm\\\\mathcal{F}_{S_m}FSm\\u200b\\u200b. Заметим, что это множество конечно: ∣FSm∣≤2m\\\\vert\\\\mathcal{F}_{S_m}\\\\vert \\\\leq 2^m∣FSm\\u200b\\u200b∣≤2m.\\nНам понадобится следующая\\nЛемма. Пусть XXX – случайная величина со значениями в [a,b][a,b][a,b] и нулевым средним. Тогда для любых s>0s > 0s>0 имеет место неравенство\\nEesX≤e(b−a)2s28.        \\\\mathbb{E} e^{sX} \\\\leq\\n        e^{\\\\frac{(b-a)^2 s^2}{8}}.\\nEesX≤e8(b−a)2s2\\u200b.С её помощью получаем:\\nEσ1:msup\\u2061f∈F(1m∑i=1mσiri(f))=Eσ1:mmax\\u2061f∈FSm(1m∑i=1mσiri(f))=\\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\sup_{f \\\\in \\\\mathcal{F}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i r_i(f) \\\\right) =\\n\\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\max_{f \\\\in \\\\mathcal{F}_{S_m}} \\\\left(\\\\frac{1}{m} \\\\sum_{i=1}^m \\\\sigma_i r_i(f) \\\\right)\\n=\\nEσ1:m\\u200b\\u200bf∈Fsup\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200bri\\u200b(f))=Eσ1:m\\u200b\\u200bf∈FSm\\u200b\\u200bmax\\u200b(m1\\u200bi=1∑m\\u200bσi\\u200bri\\u200b(f))==1mslog\\u2061exp\\u2061(sEσ1:mmax\\u2061f∈FSm(∑i=1mσiri(f)))≤= \\\\frac{1}{m s} \\\\log \\\\exp \\\\left(s \\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\max_{f \\\\in \\\\mathcal{F}_{S_m}} \\\\left(\\\\sum_{i=1}^m \\\\sigma_i r_i(f) \\\\right)\\\\right) \\\\leq\\n=ms1\\u200blogexp(sEσ1:m\\u200b\\u200bf∈FSm\\u200b\\u200bmax\\u200b(i=1∑m\\u200bσi\\u200bri\\u200b(f)))≤≤1mslog\\u2061Eσ1:mexp\\u2061(smax\\u2061f∈FSm(∑i=1mσiri(f)))≤\\\\leq\\\\frac{1}{m s} \\\\log \\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\exp \\\\left(s \\\\max_{f \\\\in \\\\mathcal{F}_{S_m}} \\\\left(\\\\sum_{i=1}^m \\\\sigma_i r_i(f) \\\\right)\\\\right)\\n\\\\leq\\n≤ms1\\u200blogEσ1:m\\u200b\\u200bexp(sf∈FSm\\u200b\\u200bmax\\u200b(i=1∑m\\u200bσi\\u200bri\\u200b(f)))≤≤1mslog\\u2061∑f∈FSmEσ1:mexp\\u2061(s∑i=1mσiri(f))=\\\\leq \\\\frac{1}{m s} \\\\log \\\\sum_{f \\\\in \\\\mathcal{F}_{S_m}} \\\\mathbb{E}_{\\\\sigma_{1:m}} \\\\exp \\\\left(s \\\\sum_{i=1}^m \\\\sigma_i r_i(f) \\\\right) =\\n≤ms1\\u200blogf∈FSm\\u200b\\u200b∑\\u200bEσ1:m\\u200b\\u200bexp(si=1∑m\\u200bσi\\u200bri\\u200b(f))==1mslog\\u2061∑f∈FSm∏i=1mEσi(esσiri(f))≤= \\\\frac{1}{m s} \\\\log \\\\sum_{f \\\\in \\\\mathcal{F}_{S_m}} \\\\prod_{i=1}^m \\\\mathbb{E}_{\\\\sigma_i} \\\\left(e^{s \\\\sigma_i r_i(f)}\\\\right)\\n\\\\leq\\n=ms1\\u200blogf∈FSm\\u200b\\u200b∑\\u200bi=1∏m\\u200bEσi\\u200b\\u200b(esσi\\u200bri\\u200b(f))≤≤1mslog\\u2061∑f∈FSmems22=1mslog\\u2061(∣FSm∣ems22)=\\\\leq \\\\frac{1}{m s} \\\\log \\\\sum_{f \\\\in \\\\mathcal{F}_{S_m}} e^{\\\\frac{m s^2}{2}}\\n= \\\\frac{1}{m s} \\\\log \\\\left(|\\\\mathcal{F}_{S_m}| e^{\\\\frac{m s^2}{2}}\\\\right) =\\n≤ms1\\u200blogf∈FSm\\u200b\\u200b∑\\u200be2ms2\\u200b=ms1\\u200blog(∣FSm\\u200b\\u200b∣e2ms2\\u200b)==1mslog\\u2061∣FSm∣+s2.= \\\\frac{1}{m s} \\\\log |\\\\mathcal{F}_{S_m}| + \\\\frac{s}{2}.\\n=ms1\\u200blog∣FSm\\u200b\\u200b∣+2s\\u200b.Эта оценка верна для любого s>0s > 0s>0. Минимизируем её по sss. Легко видеть, что оптимальное sss равняется (2/m)log\\u2061∣FSm∣\\\\sqrt{(2 / m) \\\\log \\\\vert\\\\mathcal{F}_{S_m}\\\\vert}(2/m)log∣FSm\\u200b\\u200b∣\\u200b; подставляя его, получаем:\\nRadD,m(r∘F)≤ESm2mlog\\u2061∣FSm∣.(1)\\\\color{#348FEA}{\\n\\\\mathrm{Rad}_{\\\\mathcal{D},m}(r \\\\circ \\\\mathcal{F}) \\\\leq\\n\\\\mathbb{E}_{S_m} \\\\sqrt{\\\\frac{2}{m} \\\\log |\\\\mathcal{F}_{S_m}|}.\\n\\\\qquad (1)\\n}\\nRadD,m\\u200b(r∘F)≤ESm\\u200b\\u200bm2\\u200blog∣FSm\\u200b\\u200b∣\\u200b.(1)Определим функцию роста класса F\\\\mathcal{F}F как\\nΠF(m)=max\\u2061Sm∣FSm∣.\\\\Pi_\\\\mathcal{F}(m) = \\\\max_{S_m} |\\\\mathcal{F}_{S_m}|.\\nΠF\\u200b(m)=Sm\\u200bmax\\u200b∣FSm\\u200b\\u200b∣.Эта функция показывает, сколько различных разметок класс функций F\\\\mathcal{F}F может породить на наборе данных, в зависимости от размера этого набора. Очевидно, что ΠF(m)≤2m\\\\Pi_\\\\mathcal{F}(m) \\\\leq 2^mΠF\\u200b(m)≤2m и монотонно не убывает.\\nНапример, для линейной модели на ddd-мерном пространстве признаков ΠF(m)=2m\\\\Pi_\\\\mathcal{F}(m) = 2^mΠF\\u200b(m)=2m при m≤d+1m \\\\leq d+1m≤d+1 (любое подмножество d+1d+1d+1 точек в общем положении в ddd-мерном пространстве всегда можно отделить гиперплоскостью), но строго меньше этого числа при m>d+1m > d+1m>d+1 (например, если точки – углы квадрата на плоскости, его диагонали нельзя разделить прямой).\\nКогда ∣FSm∣=2m\\\\vert\\\\mathcal{F}_{S_m}\\\\vert = 2^m∣FSm\\u200b\\u200b∣=2m, будем говорить, что «F\\\\mathcal{F}F разделяет SmS_mSm\\u200b».\\nОпределим размерность Вапника-Червоненкиса (или VC-размерность) как максимальное mmm, при котором семейство F\\\\mathcal{F}F разделяет любой датасет SmS_mSm\\u200b:\\nVC(F)=max\\u2061{m∣ΠF(m)=2m}.\\\\mathrm{VC}(\\\\mathcal{F}) =\\n\\\\max \\\\{m\\\\mid \\\\Pi_\\\\mathcal{F}(m) = 2^m\\\\}.\\nVC(F)=max{m∣ΠF\\u200b(m)=2m}.Таким образом, VC-размерность линейной модели равна d+1d+1d+1.\\nСледующая лемма даёт связь между размерностью Вапника-Червоненкиса и функцией роста:\\nЛемма (Sauer–Shelah, см. подробнее здесь)\\nΠF(m)≤∑k=0VC(F)(mk)\\\\Pi_\\\\mathcal{F}(m) \\\\leq \\\\sum_{k=0}^{\\\\mathrm{VC}(\\\\mathcal{F})} \\\\binom{m}{k}\\nΠF\\u200b(m)≤k=0∑VC(F)\\u200b(km\\u200b)Изучим асимптотическое поведение сложности Радемахера при m→∞m\\\\to\\\\inftym→∞. Обозначим D=VC(F)D = \\\\mathrm{VC}(\\\\mathcal{F})D=VC(F). Для m≤Dm \\\\leq Dm≤D имеем ΠF(m)=2m\\\\Pi_\\\\mathcal{F}(m) = 2^mΠF\\u200b(m)=2m, а для m>Dm > Dm>D:\\nΠF(m)≤∑k=0D(mk)≤(mD)D∑k=0D(mk)(Dm)k≤\\\\Pi_\\\\mathcal{F}(m) \\\\leq\\n\\\\sum_{k=0}^{D} \\\\binom{m}{k} \\\\leq\\n\\\\left(\\\\frac{m}{D}\\\\right)^{D} \\\\sum_{k=0}^{D} \\\\binom{m}{k} \\\\left(\\\\frac{D}{m}\\\\right)^{k} \\\\leq\\nΠF\\u200b(m)≤k=0∑D\\u200b(km\\u200b)≤(Dm\\u200b)Dk=0∑D\\u200b(km\\u200b)(mD\\u200b)k≤≤(mD)D∑k=0m(mk)(Dm)k=(mD)D(1+Dm)m≤(emD)D.\\\\leq\\n\\\\left(\\\\frac{m}{D}\\\\right)^{D} \\\\sum_{k=0}^{m} \\\\binom{m}{k} \\\\left(\\\\frac{D}{m}\\\\right)^{k} =\\n\\\\left(\\\\frac{m}{D}\\\\right)^{D} \\\\left(1 + \\\\frac{D}{m}\\\\right)^m \\\\leq\\n\\\\left(\\\\frac{e m}{D}\\\\right)^D.\\n≤(Dm\\u200b)Dk=0∑m\\u200b(km\\u200b)(mD\\u200b)k=(Dm\\u200b)D(1+mD\\u200b)m≤(Dem\\u200b)D.Подставляя это выражение в (1), получаем окончательную оценку на сложность Радемахера:\\nRadD,m(r∘F)≤2mVC(F)(1+log\\u2061m−log\\u2061(VC(F)))=\\\\mathrm{Rad}_{\\\\mathcal{D},m}(r \\\\circ \\\\mathcal{F}) \\\\leq\\n\\\\sqrt{\\\\frac{2}{m} \\\\mathrm{VC}(\\\\mathcal{F}) \\\\left(1 + \\\\log m - \\\\log(\\\\mathrm{VC}(\\\\mathcal{F}))\\\\right)} = \\nRadD,m\\u200b(r∘F)≤m2\\u200bVC(F)(1+logm−log(VC(F)))\\u200b==Θm→∞(VC(F)log\\u2061mm).=\\\\Theta_{m\\\\to\\\\infty}\\\\left(\\\\sqrt{\\\\mathrm{VC}(\\\\mathcal{F}) \\\\frac{\\\\log m}{m}}\\\\right).\\n=Θm→∞\\u200b(VC(F)mlogm\\u200b\\u200b).Соответствующая оценка на истинный риск тогда примёт вид:\\nR(f^m)≤R^m(f^m)+12mlog\\u20611δ+Θm→∞(VC(F)log\\u2061mm)с\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm.\\\\color{#348FEA}{R(\\\\hat f_m) \\n\\\\leq \\\\hat R_m(\\\\hat f_m) + \\\\sqrt{\\\\frac{1}{2m} \\\\log \\\\frac{1}{\\\\delta}} + \\\\Theta_{m\\\\to\\\\infty}\\\\left(\\\\sqrt{\\\\mathrm{VC}(\\\\mathcal{F}) \\\\frac{\\\\log m}{m}}\\\\right)\\n\\\\quad\\n\\\\text{с вероятностью $\\\\geq 1 - \\\\delta$ по $S_m$.}}\\nR(f^\\u200bm\\u200b)≤R^m\\u200b(f^\\u200bm\\u200b)+2m1\\u200blogδ1\\u200b\\u200b+Θm→∞\\u200b(VC(F)mlogm\\u200b\\u200b)с\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm\\u200b.Для того, чтобы эта оценка была осмыслена, необходимо гарантировать VC(F)<m/(2log\\u2061m)\\\\mathrm{VC}(\\\\mathcal{F}) < m/(2\\\\log m)VC(F)<m/(2logm). Для линейных моделей, при условии m≫dm \\\\gg dm≫d (данных намного больше, чем признаков), оценки действительно получаются осмысленными.\\nК сожалению, для нейронных сетей это подчас неверно. В работе Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks показано, что если F\\\\mathcal{F}F обозначает класс моделей, реализуемых полносвязной сетью ширины nnn с NNN параметрами, то VC(F)=Θ(nN)\\\\mathrm{VC}(\\\\mathcal{F}) = \\\\Theta(n N)VC(F)=Θ(nN). Таким образом, наша оценка на сложность Радемахера становится бесполезной в реалистичных сценариях, когда число весов сети NNN много больше числа примеров в обучающей выборке mmm.\\nЕсли априори известно, что результат обучения лежит в некотором классе FB\\\\mathcal{F}_BFB\\u200b, то в оценке сложности Радемахера можно использовать именно этот класс, а не полный класс моделей F\\\\mathcal{F}F. Очевидно, что сложность FB\\\\mathcal{F}_BFB\\u200b, лежащего в F\\\\mathcal{F}F, не больше сложности F\\\\mathcal{F}F. Так, в работе Spectrally-normalized margin bounds for neural networks получены оценки для сложности полносвязной сети с липшицевыми функциями активации при условии, что нормы весов ограничены; см. также полный конспект лекций. В этом случае под FB\\\\mathcal{F}_BFB\\u200b будем понимать класс сетей с весами нормы не больше BBB. Обозначим соответствующую оценку через B\\\\mathcal{B}B:\\nsup\\u2061f∈FB(R(f)−R^m(f))≤B(B,δ)с\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm.\\\\sup_{f \\\\in \\\\mathcal{F}_B} (R(f) - \\\\hat R_m(f))\\n\\\\leq \\\\mathcal{B}(B, \\\\delta)\\n\\\\quad\\n\\\\text{с вероятностью $\\\\geq 1-\\\\delta$ по $S_m$.}\\nf∈FB\\u200bsup\\u200b(R(f)−R^m\\u200b(f))≤B(B,δ)с\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm\\u200b.К сожалению, нет гарантий, что градиентный спуск всегда сходится в решение с нормой меньше какого-то числа. Чтобы обойти это ограничение, используют следующую технику. Возьмём последовательность ограничений BjB_jBj\\u200b, такую что\\nFBj⊂FBj+1\\xa0и\\xa0⋃j=1∞FBj=F.\\\\mathcal{F}_{B_j} \\\\subset \\\\mathcal{F}_{B_{j+1}}\\\\text{ и }\\\\bigcup_{j=1}^\\\\infty \\\\mathcal{F}_{B_j} = \\\\mathcal{F}.\\nFBj\\u200b\\u200b⊂FBj+1\\u200b\\u200b\\xa0и\\xa0j=1⋃∞\\u200bFBj\\u200b\\u200b=F.Также возьмём последовательность δj\\\\delta_jδj\\u200b, монотонно убывающую к нулю и суммирующуюся в δ\\\\deltaδ. Тогда для любого j≥1j \\\\geq 1j≥1\\nsup\\u2061f∈FBj(R(f)−R^m(f))≤B(Bj,δj)с\\xa0вероятностью\\xa0≥1−δj\\xa0по\\xa0Sm.\\\\sup_{f \\\\in \\\\mathcal{F}_{B_j}} (R(f) - \\\\hat R_m(f))\\n\\\\leq \\\\mathcal{B}(B_j, \\\\delta_j)\\n\\\\quad\\n\\\\text{с вероятностью $\\\\geq 1-\\\\delta_j$ по $S_m$.}\\nf∈FBj\\u200b\\u200bsup\\u200b(R(f)−R^m\\u200b(f))≤B(Bj\\u200b,δj\\u200b)с\\xa0вероятностью\\xa0≥1−δj\\u200b\\xa0по\\xa0Sm\\u200b.А значит,\\nsup\\u2061f∈FBj(R(f)−R^m(f))≤B(Bj,δj)∀j≥1с\\xa0вероятностью\\xa0≥1−∑j=1∞δj=1−δ\\xa0по\\xa0Sm.\\\\sup_{f \\\\in \\\\mathcal{F}_{B_j}} (R(f) - \\\\hat R_m(f))\\n\\\\leq \\\\mathcal{B}(B_j, \\\\delta_j) \\\\quad \\\\forall j \\\\geq 1\\n\\\\quad\\n\\\\text{с вероятностью $\\\\geq 1-\\\\sum_{j=1}^ \\\\infty \\\\delta_j = 1-\\\\delta$ по $S_m$.}\\nf∈FBj\\u200b\\u200bsup\\u200b(R(f)−R^m\\u200b(f))≤B(Bj\\u200b,δj\\u200b)∀j≥1с\\xa0вероятностью\\xa0≥1−∑j=1∞\\u200bδj\\u200b=1−δ\\xa0по\\xa0Sm\\u200b.Из этого следует, что\\nR(f^m)−R^m(f^m)≤B(Bj^m,δj^m)с\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm,R(\\\\hat f_m) - \\\\hat R_m(\\\\hat f_m)\\n\\\\leq \\\\mathcal{B}(B_{\\\\hat j_m}, \\\\delta_{\\\\hat j_m})\\n\\\\quad\\n\\\\text{с вероятностью $\\\\geq 1-\\\\delta$ по $S_m$,}\\nR(f^\\u200bm\\u200b)−R^m\\u200b(f^\\u200bm\\u200b)≤B(Bj^\\u200bm\\u200b\\u200b,δj^\\u200bm\\u200b\\u200b)с\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm\\u200b,где j^m\\\\hat j_mj^\\u200bm\\u200b – минимальное jjj, при котором f^m∈Bj\\\\hat f_m \\\\in B_{j}f^\\u200bm\\u200b∈Bj\\u200b.\\nТакая техника используется, например, в работах Spectrally-normalized margin bounds for neural networks и A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks.\\nФундаментальная проблема равномерных оценок\\n\\n\\n\\nПример модели (розовая кривая), имеющей малый истинный риск, но большой эмпирический на заданном наборе данных (кружочки). Данные одного класса лежат на желтом круге, другого – на голубом; оптимальная разделяющая поверхность обозначена пунктиром. Имея набор из кружочков, мы строим противоположный набор, обозначенный крестиками; заметим, что он мог прийти из того же распределения. Если алгоритм обучения старается отодвинуть границу классов как можно дальше от примеров, то результатом обучения на наборе крестиков может стать розовая кривая. Пример взят из работы\\xa0Uniform convergence may be unable to explain generalization in deep learning.\\n\\n\\nНапомним, что построение равномерных оценок проходило в несколько шагов:\\n\\nОценка супремумом\\n\\nR(f^m)−R^m(f^m)≤sup\\u2061f∈F(R(f)−R^m(f))R(\\\\hat{f}_m)-\\\\hat{R}_m(\\\\hat{f}_m)\\\\leq\\\\sup_{f\\\\in\\\\mathcal{F}}(R(f)-\\\\hat{R}_m(f))\\nR(f^\\u200bm\\u200b)−R^m\\u200b(f^\\u200bm\\u200b)≤f∈Fsup\\u200b(R(f)−R^m\\u200b(f))\\nПрименение неравенства макДайармида:\\n\\nsup\\u2061f∈F(R(f)−R^m(f))≤ESm′sup\\u2061f∈F(R(f)−R^m′(f))+12mlog\\u20611δс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm,\\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R_m(f)) \\\\leq\\n\\\\color{#FFC100}{\\\\mathbb{E}_{S'_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R'_m(f))} + \\\\sqrt{\\\\frac{1}{2m} \\\\log \\\\frac{1}    {\\\\delta}}\\n\\\\quad\\n\\\\text{с вероятностью $\\\\geq 1 - \\\\delta$ по $S_m$,}\\nf∈Fsup\\u200b(R(f)−R^m\\u200b(f))≤ESm′\\u200b\\u200bf∈Fsup\\u200b(R(f)−R^m′\\u200b(f))+2m1\\u200blogδ1\\u200b\\u200bс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm\\u200b,\\nОценка матожидания супремума (жёлтое слагаемое выше) с помощью симметризации с дальнейшим выходом на сложность Радемахера:\\n\\nESm′sup\\u2061f∈F(R(f)−R^m′(f))=ESm′sup\\u2061f∈F(ESm′′R^m′′(f)−R^m′(f))≤ESm′ESm′′sup\\u2061f∈F(R^m′′(f)−R^m′(f)).\\\\mathbb{E}_{S'_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R'_m(f)) =\\n\\\\mathbb{E}_{S'_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(\\\\mathbb{E}_{S''_m} \\\\hat R''_m(f) - \\\\hat R'_m(f)) \\\\leq\\n\\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(\\\\hat R''_m(f) - \\\\hat R'_m(f)).\\nESm′\\u200b\\u200bf∈Fsup\\u200b(R(f)−R^m′\\u200b(f))=ESm′\\u200b\\u200bf∈Fsup\\u200b(ESm′′\\u200b\\u200bR^m′′\\u200b(f)−R^m′\\u200b(f))≤ESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(R^m′′\\u200b(f)−R^m′\\u200b(f)).На каждом шаге предыдущая величина оценивается сверху, и потенциально каждое из этих неравенств может оказаться слишком слабым и привести к бессмысленной оценке. Давайте это проиллюстрируем.\\nВыше мы уже отмечали, что если класс F\\\\mathcal{F}F содержит модель, для которой R^m(f)\\\\hat R_m(f)R^m\\u200b(f) мал, а R(f)R(f)R(f) велик, то равномерная оценка становится бессмысленной. По этой причине, имеет смысл выбирать класс F\\\\mathcal{F}F как можно более маленьким. Самым лучшим из возможных классов мог бы быть класс моделей, к которым сходится наш алгоритм обучения с высокой вероятностью.\\nРассмотрим случай, близкий к идеальному: тот, в котором существует ϵ>0\\\\epsilon > 0ϵ>0, для которого при любых f∈Ff \\\\in \\\\mathcal{F}f∈F имеем R(f)<ϵR(f) < \\\\epsilonR(f)<ϵ. Иными словами, предположим, что все модели класса F\\\\mathcal{F}F хорошо обобщают. В этом случае оценка выше близка к идеальной:\\nR(f^m)−R^m(f^m)≤ϵс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm.R(\\\\hat f_m) - \\\\hat R_m(\\\\hat f_m) \\\\leq\\n\\\\epsilon\\n\\\\quad\\n\\\\text{с вероятностью $\\\\geq 1 - \\\\delta$ по $S_m$.}\\nR(f^\\u200bm\\u200b)−R^m\\u200b(f^\\u200bm\\u200b)≤ϵс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm\\u200b.Но что будет, если мы начнём честно воспроизводить процесс построения равномерных оценок? После второго шага мы получаем оценку вида\\nR(f^m)−R^m(f^m)≤R(\\\\hat f_m) - \\\\hat R_m(\\\\hat f_m) \\\\leq\\nR(f^\\u200bm\\u200b)−R^m\\u200b(f^\\u200bm\\u200b)≤sup\\u2061f∈F(R(f)−R^m(f))≤ϵ+12mlog\\u20611δс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm,\\\\sup_{f \\\\in \\\\mathcal{F}}(R(f) - \\\\hat R_m(f)) \\\\leq\\n\\\\epsilon + \\\\sqrt{\\\\frac{1}{2m} \\\\log \\\\frac{1}{\\\\delta}}\\n\\\\quad\\n\\\\text{с вероятностью $\\\\geq 1 - \\\\delta$ по $S_m$,}\\nf∈Fsup\\u200b(R(f)−R^m\\u200b(f))≤ϵ+2m1\\u200blogδ1\\u200b\\u200bс\\xa0вероятностью\\xa0≥1−δ\\xa0по\\xa0Sm\\u200b,которая не сильно хуже предыдущей, но в которой всё равно появилось лишнее слагаемое.\\nНо допустим, что мы хотим честно проделать третий шаг процедуры получения равномерных оценок. Для этого нам необходимо было оценить матожидание супремума, которое после симметризации получает вот такую верхнюю оценку:\\nESm′ESm′′sup\\u2061f∈F(R^m′′(f)−R^m′(f)).\\\\mathbb{E}_{S'_m} \\\\mathbb{E}_{S''_m} \\\\sup_{f \\\\in \\\\mathcal{F}}(\\\\hat R''_m(f) - \\\\hat R'_m(f)).\\nESm′\\u200b\\u200bESm′′\\u200b\\u200bf∈Fsup\\u200b(R^m′′\\u200b(f)−R^m′\\u200b(f)).Таким образом, малость истинного риска не гарантирует малость эмпирического риска на любом наборе данных. Так, авторы статьи Uniform convergence may be unable to explain generalization in deep learning предъявили пример, в котором для любого Sm′′S_m''Sm′′\\u200b существует модель f~m∈F\\\\tilde f_m \\\\in \\\\mathcal{F}f~\\u200bm\\u200b∈F, такая что R^m′′(f~m)≈1\\\\hat R''_m(\\\\tilde f_m) \\\\approx 1R^m′′\\u200b(f~\\u200bm\\u200b)≈1, но при этом R^m(f~m)\\\\hat R_m(\\\\tilde f_m)R^m\\u200b(f~\\u200bm\\u200b) и R(f~m)R(\\\\tilde f_m)R(f~\\u200bm\\u200b) малы. Иллюстрация такой ситуации приведена в начале параграфа. Тогда sup\\u2061f∈F(R^m′′(f)−R^m′(f))\\\\sup_{f \\\\in \\\\mathcal{F}}(\\\\hat R''_m(f) - \\\\hat R'_m(f))supf∈F\\u200b(R^m′′\\u200b(f)−R^m′\\u200b(f)) велик, и оценки теряют смысл.\\nК счастью, даже эта фундаментальная проблема не ставит крест на равномерных оценках. Так, работы Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign Overfitting и Stability and Deviation Optimal Risk Bounds with Convergence Rate O(1/n)O (1/n)O(1/n) рассматривают равномерную оценку в классе интерполирующих моделей, то есть, имеющих нулевой эмпирический риск:\\nR(f^m)−R^m(f^m)≤sup\\u2061f∈F:\\u2009R^m(f)=0R(f).R(\\\\hat f_m) - \\\\hat R_m(\\\\hat f_m)\\n\\\\leq \\\\sup_{f \\\\in \\\\mathcal{F}: \\\\, \\\\hat R_m(f) = 0} R(f).\\nR(f^\\u200bm\\u200b)−R^m\\u200b(f^\\u200bm\\u200b)≤f∈F:R^m\\u200b(f)=0sup\\u200bR(f).Для таких моделей контрпример выше не работает.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф13.1. Введение в теорию глубокого обученияСледующий параграф13.3. PAC-байесовские оценки рискаЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_27.html', 'title': 'Графовые нейронные сети'}, page_content='Графовые нейронные сетиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/56.1.Свёрточные нейросети6.2.Нейросети для работы с последовательностями6.3.Трансформеры6.4.Графовые нейронные сетиВведениеОписание графовых данныхЗадачи на графахГрафовые нейронные сетиПарадигмы построения графовых сверток6.5.Нейросети для облаков точек7.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Графовые нейронные сети6.4. Графовые нейронные сетиАвторыКузнецов МаксВведение\\nНаряду с обработкой табличных, текстовых, аудио данных и изображений, в глубинном обучении довольно часто приходится решать задачи на данных, имеющих графовую структуру. К таким данным относятся, к примеру, описания дорожных и компьютерных сетей, социальных графов и графов цитирований, молекулярных графов, а также графов знаний, описывающих взаимосвязи между сущностями, событиями и абстрактными категориями.\\n\\nВ этом параграфе мы с вами познакомимся с основными задачами, которые возникают при обработке графов, а также поговорим о графовых свертках и графовых нейронных сетях — специальном классе обучаемых преобразований, способных принимать в качестве входа графы и решать задачи на них.\\nОписание графовых данных\\nГраф G=(V,E)G=(V, E)G=(V,E) принято представлять двумя множествами: множеством VVV, содержащим вершины и их признаковые описания, а также множеством EEE, содержащим связи между вершинами (то есть рёбра) и признаковые описания этих связей. Для простоты математических выкладок и изложения дальнейшего материала давайте считать, что мы всегда работаем с ориентированными графами. Если граф содержит ненаправленное ребро, мы его заменяем на пару направленных ребер. Кроме того, давайте обозначать окрестность вершины как N(v)={v^∣(v^,v)∈E}N(v)=\\\\{\\\\hat{v}\\\\vert(\\\\hat{v}, v) \\\\in E\\\\}N(v)={v^∣(v^,v)∈E}.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\n\\nГрафовые данные довольно разнообразны. Они могут отличаться между собой в следующих моментах:\\n\\nПо размеру, т.е. количеству вершин и/или ребер.\\nПо наличию признаковых описаний вершин и рёбер. В зависимости от решаемой задачи, графы могут содержать информацию только в вершинах, только в ребрах, либо же и там и там.\\nКроме того, графы могут быть гомо- и гетерогенными — в зависимости от того, имеют ли вершины и ребра графа одну природу либо же нет.\\n\\nНапример, социальные графы содержат огромное количество вершин и ребер, часто измеряющееся в тысячах, содержат информацию в вершинах и очень редко в ребрах, а также являются гомогенными, так как все вершины имеют один тип. В то же время, молекулярные графы — это пример графов с, как правило, средним количеством вершин и ребер; вершины и связи в молекулярных графах имеют признаковое описание (типы атомов и ковалентных связей, а также информацию о зарядах и т.п.), но при этом также являются гомогенными графами. К классу гетерогенных графов относятся, например, графы знаний, описывающие некоторую систему, различные сущности в ней и взаимодействия между этими сущностями. Вершины (сущности) и связи (ребра) такого графа могут иметь различную природу: скажем, вершинами могут быть сотрудники и подразделения компании, а рёбра могут отвечать отношениям «Х работает в подразделении Y», «X и Z коллеги» и так далее.\\n\\nЗадачи на графах\\nРазнообразие графовых данных закономерно породило множество разнообразных задач, которые решаются на этих данных.\\nСреди них можно встретить классические постановки классификации, регрессии и кластеризации, но есть и специфичные задачи, не встречающиеся в других областях — например, задача восстановления пропущенных связей внутри графа или генерации графов с нужными свойствами. Однако даже классические задачи могут решаться на различных уровнях: классифицировать можно весь граф (graph-level), а можно отдельные его вершины (node-level) или связи (edge-level).\\nТак, в качестве примера graph-level задач можно привести классификацию и регрессию на молекулярных графах. Имея датасет с размеченными молекулами, можно предсказывать их принадлежность к лекарственной категории и различные химико-биологические свойства.\\nНа node-level, как правило, классифицируют вершины одного огромного графа, например, социального. Имея частичную разметку, хочется восстановить метки неразмеченных вершин. Например, предсказать интересы нового пользователя по интересам его друзей.\\n\\nЧасто бывает такое, что граф приходит полностью неразмеченным и хочется без учителя разделить на компоненты. Например, имея граф цитирований, выделить в нем подгруппы соавторов или выделить области исследования. В таком случае принято говорить о node-level кластеризации графа.\\n\\nНаконец, довольно интересна задача предсказания пропущенных связей в графе. В больших графах часто некоторые связи отсутствуют. Например, в социальном графе пользователь может добавить не всех знакомых в друзья. А в графе знаний могут быть проставлены только простые взаимосвязи, а высокоуровневые могут быть пропущены.\\n\\nВ конце, хотелось бы отметить очень важные особенности всех задач, связанных с графами. Алгоритмы решения этих задач должны обладать двумя свойствами.\\n\\nВо-первых, графы в датасетах, как правило, могут отличаться по размерам: как по количеству вершин, так и по количеству связей. Алгоритмы решения задач на графах должны уметь принимать графы различных размеров.\\nВо-вторых, алгоритмы должны быть инварианты к перестановкам порядка вершин. То есть если взять тот же граф и перенумеровать его вершины, то алгоритмы должны выдавать те же предсказания с учетом этой перестановки.\\n\\nГрафовые нейронные сети\\nРазвитие глубинного обучения повлияло на подходы к решению задач на графовых данных. Был предложен концепт графовых нейронных сетей, которые в последнее время либо полностью заменили классические алгоритмы обработки графов, либо породили мощные синергии с этими алгоритмами.\\n\\nГрафовые нейронные сети по принципу работы и построения идейно очень похожи на сверточные нейронные сети. Более того, забегая немного вперед, графовые нейроные сети являются обобщением сверточных нейронных сетей.\\nНа вход графовой нейронной сети подается граф. В отличие от сверточных нейронных сетей, которые требуют, чтобы все картинки в батче были одинакового размера, графовые нейронные сети допускают разные размеры у объектов батча. Кроме того, в отличие от картинок, у которых информация довольно однородна (это, как правило, несколько цветовых каналов) и хранится в пикселях, у графов информация может также храниться в вершинах и/или ребрах. Причем в одних задачах информация может быть только в вершинах, в других только в ребрах, а в третьих и там, и там. Сама информация может быть довольно разнородной: это могут быть и вещественные значения, и дискретные значения, в зависимости от природы графа и от типа решаемой задачи. Поэтому, довольно часто первым слоем в графовых нейронных сетях идут Embedding слои, которые переводят дискретные токены в вещественные векторы.\\nh0n=Emb(V),h0e=Emb(E)h^{n}_{0} = Emb(V),\\\\quad h^{e}_{0} = Emb(E)\\nh0n\\u200b=Emb(V),h0e\\u200b=Emb(E)Однако, сама суть работы у графовых и сверточных сетей совпадает. В графовой нейронной сети по очереди применяются слои, которые собирают информацию с соседей  и обновляют информацию в вершине. То же самое делают и обычные свертки. Поэтому такие слои и называются графовыми свертками. Графовая свертка принимает на вход граф со скрытыми состояниями у вершин и ребер и выдает тот же граф, но уже с обновленными более информативными скрытыми состояниями.\\nВ отличие от сверточных нейронных сетей, при обработке графа pooling слои вставляют редко, в основном в graph-level задачах, при этом придумать разумную концепцию графового пулинга оказалось нелегко. Если вам станет интересно, вы можете познакомиться с несколькими вариантами графовых пулингов в следующих статьях:\\n\\nLearning Spectral Clustering\\nKernel k-means, Spectral Clustering and Normalized Cuts\\nWeighted Graph Cuts without Eigenvectors\\n\\nВ большинстве же архитектур пулинги не используются, и структура графа на входе и выходе графовой нейронной сети совпадает.\\n\\nПолученная после череды сверток информация с вершин и ребер в конце обрабатывается с помощью полносвязных сетей для получения ответа на задачу. Для node-level классификации и регрессии полносвязная сеть применяется к скрытым состояниям вершин hKnh^{n}_{K}hKn\\u200b, а для edge-level, соответственно, к скрытым состояниям ребер hKeh^{e}_{K}hKe\\u200b. Для получения ответа на graph-level уровне информация с вершин и ребер сначала агрегируется с помощью readout операции. На месте readout операции могут располагаться любые инвариантные к перестановкам операции: подсчет максимума, среднего или даже обучаемый self-attention слой.\\n\\nКак говорилось ранее, графовые нейронные сети являются обобщением сверточных. Если представить пиксели изображения вершинами графа, соединить соседние по свертке пиксели ребрами и предоставить относительную позицию пикселей в информации о ребре, то графовая свертка на таком графе будет работать так же, как и свертка над изображением.\\n\\nК графовым нейронным сетям, как и к сверточным, применим термин receptive field. Это та область графа, которая будет влиять на скрытое состояние вершины после N сверток. Для графов receptive field после N графовых сверток — это все вершины и ребра графа, до которых можно дойти от фиксированной вершины не более чем за N переходов. Знание receptive field полезно при проектировании нейронной сети - имея представление о том, с какой окрестности вершины надо собрать информацию для решения задачи, можно подбирать нужное количество графовых сверток.\\n\\nМногие техники стабилизации обучения и повышения обобщаемости, такие как Dropout, BatchNorm и Residual Connections, применимы и к графовым нейронным сетям. Однако стоит помнить про их особенности. Эти операции могут независимо применяться (или не применяться) к вершинам и ребрам. Так, если вы применяете Dropout, то вы вправе поставить для вершин и для рёбер различные значения dropout rate. Аналогично и для Residual Connections - они могут применяться только для вершин, только для ребер или же и там и там.\\nКроме того, стоит иметь ввиду, что графы различных размеров будут неравноценно влиять на среднее и дисперсию в BatchNorm слое. Более стабильной альтернативой BatchNorm в обработке графов, например, являются LayerNorm и GraphNorm, которые производят нормировку активаций по каждому графу независимо.\\nLayerNorm, по сути, применяет BatchNorm для каждого графа:\\nxi′=x−E[x]Var[x]+ϵ⊙γ+β\\\\mathbf{x}^{\\\\prime}_i = \\\\frac{\\\\mathbf{x} -\\n        \\\\textrm{E}[\\\\mathbf{x}]}{\\\\sqrt{\\\\textrm{Var}[\\\\mathbf{x}] + \\\\epsilon}}\\n        \\\\odot \\\\gamma + \\\\beta\\nxi′\\u200b=Var[x]+ϵ\\u200bx−E[x]\\u200b⊙γ+βA вот GraphNorm содержит несколько обучаемых параметров и является более гибким вариантом нормализации:\\nxi′=x−α⊙E[x]Var[x−α⊙E[x]]+ϵ⊙γ+β\\\\mathbf{x}^{\\\\prime}_i = \\\\frac{\\\\mathbf{x} - \\\\alpha \\\\odot\\n        \\\\textrm{E}[\\\\mathbf{x}]}\\n        {\\\\sqrt{\\\\textrm{Var}[\\\\mathbf{x} - \\\\alpha \\\\odot \\\\textrm{E}[\\\\mathbf{x}]]\\n+ \\\\epsilon}} \\\\odot \\\\gamma + \\\\betaxi′\\u200b=Var[x−α⊙E[x]]+ϵ\\u200bx−α⊙E[x]\\u200b⊙γ+βПарадигмы построения графовых сверток\\nВажно отметить, что в отличие от свертки, применяемой для изображений, являющейся четко определенной операцией, графовая свертка представляет собой именно концепт, абстрактную операцию, обновляющую скрытые представления объектов графа, используя доступную информацию с соседей и ребер. На практике, конкретный механизм графовой свертки разрабатывается для конкретной задачи, и различные реализации графовых сверток могут очень сильно отличаться между собой. И если зайти на сайты популярных фреймворков глубинного обучения на графах (например, PyG), то можно обнаружить десятки различных реализаций графовых сверток.\\nВо-первых, графовые свертки отличаются между собой по тому набору информации, которые они могут использовать. Есть свертки, которые используют только скрытые представления вершин, игнорируя информацию на ребрах. Существуют свертки, которые по разному обрабатывают информацию от ребер различного типа. А есть свертки, которые используют информацию с ребер и вершин, обновляя одновременно и те и другие.\\nВо-вторых, и что более важно, графовые свертки можно разделить на два семейства, которые отличаются математической парадигмой, в которой они работают. Есть spatial (пространственный) и spectral (спектральный) подходы. Пространственные свертки основываются на message-passing парадигме, в то время как спектральные работают с графовым лапласианом и его собственными векторами.\\nНа практике, спектральные свертки чаще применяются и показывают лучшие результаты в задачах связанных с обработкой одного большого графа, где важно понимать относительное месторасположение вершины в этом большом графе. Например, графа соцсетей или графа цитирований. Пространственные свертки показывают хорошие результаты в остальных задачах, где для решения задачи важно находить локальные подструктуры внутри графа.\\nНесмотря на принципиальную противоположность этих двух подходов, активно предпринимаются попытки их совмещения в одну парадигму, например, в этой работе.\\nДавайте разберемся с этими двумя парадигмами.\\n\\nПространственная парадигма\\nПространственная (spatial) парадигма основывает на алгоритме передачи сообщений (message passing) между вершинами графа.\\nКонцепт этого подхода заключается в следующем - каждая вершина графа имеет внутреннее состояние. Каждую итерацию это внутреннее состояние пересчитывается, основываясь на внутренних состояниях соседей по графу. Каждый сосед влияет на состояние вершины, так же как и вершина влияет на состояния соседей.\\n\\nИтерация работы Message passing подхода для одной вершины можно описать следующим абстрактным алгоритмом. Для каждой вершины vvv собираются все тройки (xv,xw,ewv)(x_v, x_w, e_{wv})(xv\\u200b,xw\\u200b,ewv\\u200b) состоящие из скрытых представлений текущей вершин xvx_vxv\\u200b и ее соседа xwx_wxw\\u200b, а также из типа ребра ewve_{wv}ewv\\u200b,соединяющего текущую вершину и её соседа. Ко всем этим тройкам применяется обучаемое преобразование MMM (от слова message), которая считает сообщение — информацию, которая идет от соседа к вершине. Посчитанные сообщения агрегируются в одно, обозначаемое mvm_vmv\\u200b. Сообщения могут быть сагрегированы любой ассоциативной операцией, например взятием поэлементного минимума, максимума или среднего. Далее, агрегированное сообщение и текущее внутреннее состояние вершины подаются на вход обучаемой операции UUU (от слова update), которая обновляет внутреннее состояние вершины.\\nКонкретные имплементации операций M,UM, UM,U непосредственно зависят от алгоритма и той задачи, которую он решает.\\n\\nОдним из самых известных классических алгоритмов, построенных на пространственной парадигме, является PageRank. Алгоритм PageRank проходит по графу веб страниц и выставляет каждой веб-странице значение ее \"важности\" PGPGPG, которое впоследствии можно использовать для ранжирования поисковой выдачи. Формула подсчета PageRank выражается через коэффициент затухания ddd, а также значения PageRank соседей N(A)N(A)N(A) вершины и количество исходящих ссылок из этих соседей LN(A)LN(A)LN(A) следующим образом:\\nPR(A)=(1−d)+d∑B∈N(A)PR(B)LN(B)PR(A) = (1 - d) + d \\\\sum_{B \\\\in N(A)} \\\\frac{PR(B)}{LN(B)} \\nPR(A)=(1−d)+dB∈N(A)∑\\u200bLN(B)PR(B)\\u200bВ такой постановке операции подсчета сообщений MMM и операции обновления UUU имеют следующий вид:\\nM(B)=PR(B)LN(B)ma=∑B∈N(A)M(B)U(A)=(1−d)+dmaPR(A)=U(A) M(B) = \\\\frac{PR(B)}{LN(B)} \\\\\\\\\\nm_{a} = \\\\sum_{B \\\\in N(A)} M(B) \\\\\\\\\\nU(A) = (1 -d) + d  m_{a} \\\\\\\\\\nPR(A) = U(A) M(B)=LN(B)PR(B)\\u200bma\\u200b=B∈N(A)∑\\u200bM(B)U(A)=(1−d)+dma\\u200bPR(A)=U(A)Графовые свертки, работающие на парадигме передачи сообщений, как правило делают MMM и UUU обучаемыми преобразованиями.\\nРассмотрим несколько конкретных примеров архитектур.\\nGraphSAGE\\nСвертка GraphSAGE работает по следующему принципу. Для каждой вершины вычисляется набор скрытых представлений соседних вершин hwth_w^thwt\\u200b, из которых идут связи в текущую. Далее, собранная информация агрегируется с помощью некоторой коммутативной операции AGGRAGGRAGGR в вектор фиксированного размера. В качестве операции агрегации авторы предлагают использовать операции взятия средних или максимальных значений скрытых представлений объектов из набора. Далее агрегированный вектор объединяется со скрытым представлением вершины hvth_v^thvt\\u200b, они домножаются на обучаемую матрицу WWW и к результату умножения поэлементно применяется сигмоида. Обучаемые параметры данного слоя, как и в случае GCN, содержат только одну матрицу.\\nmvt+1=AGGR({hwt,w∈N(v)})hvt+1=σ(Wt+1CONCAT(mvt+1,hvt))m_v^{t+1}=AGGR (\\\\{h_w^t,w \\\\in N(v)\\\\})\\\\\\\\\\nh_v^{t+1}=\\\\sigma(W^{t+1} CONCAT(m_v^{t+1},h_v^t))mvt+1\\u200b=AGGR({hwt\\u200b,w∈N(v)})hvt+1\\u200b=σ(Wt+1CONCAT(mvt+1\\u200b,hvt\\u200b))Данная свертка использует только скрытые представления вершин, однако уделяет больше внимания локальному окружению вершины, нежели её глобальному положению во всем графе. Авторы показали высокое качество данной архитектуры в задачах, связанных с выучиванием представлений вершин, однако использование данной свертки можно встретить и в других задачах, связанных с обработкой графов, не содержащих дополнительной информации о рёбрах.\\nGAT\\nСвертка GAT (Graph ATtention) является развитием идеи GraphSAGE. В качестве механизма агрегации эта архитектура предлагает использовать механизм внимания, у которого матрицы преобразования для ключей, значений и запросов совпадают и обозначены в формуле буквой WWW. Как и в GraphSAGE, агрегированное сообщение проходит через сигмоиду, но не домножается перед этим на обучаемую матрицу.\\nαv∗=softmax(act(aTCONCAT(Whvt,Wh∗t)))hvt+1=σ(∑w∈N(v)[αvwWhwt])\\\\alpha_{v*}=softmax(act(a^T\\nCONCAT(Wh_v^t,Wh_∗^t)))\\\\\\\\\\nh_v^{t+1}=\\\\sigma\\\\left(\\\\sum_{w \\\\in N(v)} [\\\\alpha_{vw} Wh_w^t] \\\\right)αv∗\\u200b=softmax(act(aTCONCAT(Whvt\\u200b,Wh∗t\\u200b)))hvt+1\\u200b=σ\\u200bw∈N(v)∑\\u200b[αvw\\u200bWhwt\\u200b]\\u200bЗдесь act — некоторая функция активации. Как и в случае механизма внимания для последовательностей, в момент обновления представления для вершины vvv attention «смотрит» на все остальные вершины www и генерирует веса αvw\\\\alpha_{vw}αvw\\u200b, которые указывают, информация из каких вершин www «важнее» для нас.\\nБлагодаря мощности и гибкости механизм внимания, эта свертка показала отличные результаты на множестве задач и является одной из самых популярных сверток. По умолчанию, эта свертка, как и GraphSAGE, использует только признаки вершин, однако, в некоторых проектах можно встретить модификации свертки, в которых механизм внимания учитывает ещё и информацию для ребер.\\nRGCN\\nНаконец, есть специально разработанные свертки для обработки графов, ребра которых могут быть нескольких типов. Одна из них называется RGCN (Relational Graph Convolutional Networks). Она суммирует скрытые представления соседей, однако каждое представление соседа домножается на матрицу, зависящую от типа ребра, которое соединяет соседа с текущей вершиной. Если в графе присутствует ребра NNN типов, то данная свертка будет учить NNN матриц - по одной для каждого типа связи.\\nhvt+1=σ(∑r∈R∑w∈Nr(v)1ci,rWrlhwt+W0lhvt)h_v^{t+1}=\\\\sigma\\\\left(\\\\sum_{r \\\\in R}\\\\sum_{w \\\\in N_r(v)} \\\\frac{1}{c_{i,r}}\\nW_r^l  h_w^t + W_0^l h_v^t\\\\right)hvt+1\\u200b=σ\\u200br∈R∑\\u200bw∈Nr\\u200b(v)∑\\u200bci,r\\u200b1\\u200bWrl\\u200bhwt\\u200b+W0l\\u200bhvt\\u200b\\u200bСпектральная парадигма\\nПротивоположностью пространственной парадигме является спектральная (spectral) парадигма. В своей постановке спектральная парадигма опирается на анализ процесса диффузии сигнала внутри графа и анализирует матрицы, описывающих граф — матрицу смежности и матрицу, которая называется Лапласианом графа.\\n\\nЛапласиан графа — это матрица L=D−AL=D-AL=D−A, где DDD — диагональная матрица, хранящая в iii-й диагональной ячейке количество исходящих из iii-й вершины рёбер, а AAA — матрица смежности графа, (i,j)(i,j)(i,j)-й элемент которой равен числу рёбер, соединяющих iii-ю и jjj-ю вершину.\\nЛапласиан графа имеет неотрицательные собственные значения. Количество нулевых собственных значений всегда совпадает с количеством компонент связности. Потрясающим свойством Лапласиана является то, что его собственные векторы, соответствующие положительным собственным значениям, в порядке возрастания собственных значений, описывают разрезы графа — его разделения пополам таким образом, чтобы между разделенным половинами было как можно меньше ребер.\\nТак, собственный вектор, соответствующий наименьшему положительному собственному значению, будет описывать кластеризацию графа на два подграфа. Все индексы, соответствующие положительным элементам вектора задают вершины, которые должны оказаться в первом кластере, а отрицательные элементы будут соответствовать вершинам, которые должны оказаться во втором кластере.\\nЭтим свойством Лапласиана графа пользуются для того, чтобы проводить кластеризацию графа без учителя. Для этого надо:\\n\\nПосчитать Лапласиан LLL матрицы AAA\\nПосчитать kkk собственных векторов, соответствующих наименьшим собственным значениям\\nСформировать из них матрицу размера N×kN \\\\times kN×k, каждая строка которой описывает вершину kkk признаками\\nКластеризовать объекты, описываемые этой матрицей (например, c помощью K-Means)\\n\\nТаким образом, спектральный подход отлично подходит для того, чтобы находить в графе  компоненты,  вершины которых связаны друг с другом и имеют похожие свойства.\\nGCN\\nСвертка GCN, основанная на спектральной парадигме, использует только скрытые состояния вершин hhh и матрицу смежности AAA — она учитывает лишь наличие или отсутствие ребра в графе, но не признаки ребер.\\nС математической точки зрения, GCN очень проста и представляет собой один шаг итеративного процесса поиска собственных значений Лапласиана графа: мы берем скрытые представления вершин и домножаем их на нормированную матрицу смежности — матрицу AAA, домноженную слева и справа на матричный корень матрицы DDD. Этот шаг применяется ко всем каналам скрытого представления вершины. После этого шага, обновленные скрытые представления ещё домножаются на обучаемую матрицу θ\\\\thetaθ:\\nht+1=θD−1/2(A+I)D−1/2hth^{t+1} = \\\\theta D^{-1/2} (A + I) D^{−1/2} h^{t}\\nht+1=θD−1/2(A+I)D−1/2htЗдесь hjh^{j}hj — это матрица размера (число вершин)×\\\\times×(длина вектора представления), то есть к каждому «каналу» представлений свёртка применяется отдельно. Если же мы хотим работать с несколькими каналами, то есть вместо hth^{t}ht у нас матрица HtH^{t}Ht размера (число вершин)×\\\\times×(число каналов), и ещё добавить нелинейность fff, формула переписывается следующим образом:\\nHt+1=f(D−1/2(A+I)D−1/2HtΘ).H^{t+1} = f\\\\left(D^{-1/2} (A + I) D^{−1/2} H^{t}\\\\Theta\\\\right).\\nHt+1=f(D−1/2(A+I)D−1/2HtΘ).Авторы данной свертки показали отличное качество работы в задачах классификации вершин графов цитирования и графа знаний. Однако, различные модификации данной свертки применяются и в других задачах, например, для выучивания векторных представлений вершин и для кластеризации вершин графа.\\nМатематическая интуиция за формуламиПопробуем пояснить подробнее, откуда берётся такая формула для обновления hth^tht.\\nПусть hhh — некоторый скалярный «сигнал» на графе, который мы запишем в виде вектора (длина которого равна числу вершин графа).\\nМы будем работать не с обычным Лапласианом, а с нормализованным, равным L~=D−1/2LD−1/2=I−D−1/2AD−1/2\\\\widetilde{L} = D^{-1/2} L D^{−1/2} = I - D^{-1/2} A D^{−1/2}L=D−1/2LD−1/2=I−D−1/2AD−1/2. Нормализованный лапласиан — симметричная матрица, так что у него есть ортонормированный базис u1,u2,…u_1,u_2,\\\\ldotsu1\\u200b,u2\\u200b,… из собственных векторов. Запишем базис uiu_iui\\u200b в (ортогональную) матрицу UUU. Тогда произведение UThU^ThUTh — это вектор, состоящий из координат сигнала hhh в базисе uiu_iui\\u200b. Казалось бы, мы делаем тривиальные вещи из линейной алгебры, но на самом деле мы занимается анализом Фурье, а именно:\\n\\nuiu_iui\\u200b — гармоники;\\nUThU^ThUTh — коэффициенты Фурье;\\nумножение на UTU^TUT — преобразование Фурье;\\nумножение на UUU — обратное преобразование Фурье.\\n\\nДействительно, преобразование Фурье — это, грубо говоря, всего лишь разложение по какому-то удобному ортонормированному базису. В данном случае — по базису uiu_iui\\u200b.\\nВ мире функций, функциональных свёрток и обычного преобразования Фурье F\\\\mathcal{F}F свёртка удовлетворяет такому соотношению:\\ng∗h=F−1(F(g)⊙F(h)),g\\\\ast h = \\\\mathcal{F}^{-1}\\\\left(\\\\mathcal{F}(g)\\\\odot\\\\mathcal{F}(h)\\\\right),\\ng∗h=F−1(F(g)⊙F(h)),где ⊙\\\\odot⊙ — поэлементное умножение. То есть при переходе в мир коэффициентов Фурье свёртка функций превращается в поэлементное умножение векторов.\\nДля графов кажется логичным свёртку двух сигналов ggg и hhh определить похожим образом:\\ng∗h=U((UTg)⊙(UTh))=UθUTh,g\\\\ast h = U\\\\left((U^Tg)\\\\odot(U^Th)\\\\right) = U\\\\theta U^Th,\\ng∗h=U((UTg)⊙(UTh))=UθUTh,где через gθg_{\\\\theta}gθ\\u200b мы обозначили диагональную матрицу с диагональю UTgU^TgUTg.\\nИтак, мы дали новое определение свёртки на графе — через лапласиан и его собственные векторы. Проблема в том, что вычислять uiu_iui\\u200b очень долго. Поэтому мы линеаризуем задачу. Рассмотрим свёртку h∗gh\\\\ast gh∗g как функцию от нормализованного лапласиана L~\\\\widetilde{L}L и разложим её в ряд Тейлора в точке III (единичная матрица):\\ng∗h≈θ0′h+θ1′(L~−I)hg\\\\ast h\\\\approx \\\\theta\\'_0h + \\\\theta\\'_1(\\\\widetilde{L} - I)h\\ng∗h≈θ0′\\u200bh+θ1′\\u200b(L−I)hТеперь для простоты ещё больше огрубим модель и предположим, что θ0′=−θ1′\\\\theta\\'_0 = -\\\\theta\\'_1θ0′\\u200b=−θ1′\\u200b (обозначим это число буквой θ\\\\thetaθ). Тогда остаётся\\ng∗h≈θ(I+D−1/2AD−1/2)hg\\\\ast h\\\\approx \\\\theta(I + D^{-1/2} A D^{−1/2})h\\ng∗h≈θ(I+D−1/2AD−1/2)hА для улучшения численной устойчивости мы перепишем это так:\\ng∗h≈θD−1/2(I+A)D−1/2hg\\\\ast h\\\\approx \\\\theta D^{-1/2} (I + A) D^{−1/2}h\\ng∗h≈θD−1/2(I+A)D−1/2hПолучилась та самая формула, которую вы видели выше.\\nБолее подробно о том, как устроен анализ Фурье на графах, вы можете прочитать, например, в этой статье. Кроме того, рекомендуем заглянуть в оригинальную статью про GCN за более подробным изложением вывода формул.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф6.3. ТрансформерыСледующий параграф6.5. Нейросети для облаков точекЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_31.html', 'title': 'Введение в генеративное моделирование'}, page_content='Введение в генеративное моделированиеЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/68.1.Введение в генеративное моделированиеИнтерполяции в латентном пространствеПрименения генеративных моделей8.2.Variational Autoencoder (VAE)8.3.Генеративно-состязательные сети (GAN)8.4.Нормализующие потоки8.5.Диффузионные модели8.6.Языковые модели9.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Введение в генеративное моделирование8.1. Введение в генеративное моделированиеАвторыДенис ВолхонскийДо этого вы изучали модели машинного обучения, которые в основном предсказывают какие-то характеристики объектов.Например, метки класса или регрессионные метки. Подобные задачи называют дискриминативным моделированием.\\nВ то же время, существуют обратные задачи, в которых по какой-то характеристике нужно создать объект или оценить плотность распределения объектов. Это называется генеративным моделированием — его нюансы мы и рассмотрим в этом разделе.\\nОбучение генеративных моделей существенно сложнее обучения дискриминативных моделей. Последние работают с намного более простыми распределениями. Например, предсказать вероятность конкретной цифры, нарисованной на картинке, гораздо проще, чем создать картинку с нужной цифрой. При этом генеративные модели в последние годы достигли невероятных успехов и позволяют генерировать изображения, которые трудно отличить от настоящих фотографий.\\nВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nГенеративные модели помогают решать множество задач, которые мы рассмотрим далее. Самая основная задача — это приближение распределения данных и генерация новых данных.\\nДопустим у нас есть набор картинок с нарисованными от руки числами. Будем считать, что мы получили этот набор из генеральной совокупности (то есть из всех возможных изображений). Нам бы хотелось так или иначе смоделировать распределение этой генеральной совокупности.\\nМы можем это сделать двумя подходами:\\n\\nЯвное моделирование. В этом случае мы построим и как-то оценим функцию плотности распределения данных p(x)p(x)p(x). Из этого распределения мы сможем семплировать новые объекты. Примеры таких моделей: авторегрессионные модели (например, PixelCNN++, Video Transformer), диффузионные модели, модели на основе нормализующих потоков и вариационные автокодировщики.\\nНеявное моделирование. При неявном моделировании мы доступ к функции плотности не получим. Но мы сможем из этого распределения сэмплировать новые объекты. В случае нашего примера с нарисованными числами мы сможем генерировать такие изображения. Примерами таких моделей являются генеративно-состязательные сети.\\n\\nРассмотрим дискриминативные и генеративные задачи чуть более формально. При дискриминативном моделировании для объекта xxx и характеристики yyy мы обычно хотим получить плотность распределения p(y∣x)p(y \\\\mid x)p(y∣x).\\nПри генеративном моделировании ставится противоположная задача: восстановить плотность p(x)p(x)p(x) или p(x∣y)p(x \\\\mid y)p(x∣y). В качестве yyy тут может выступать как метка класса, так и другой объект. Например, если мы хотим уметь генерировать изображения на основе текстового описания, то изображения будут являться xxx, а текст — yyy.\\nИнтерполяции в латентном пространстве\\nБольшинство моделей генеративного моделирования позволяют семплировать новые объекты. Как правило, в результате обучения генеративной модели мы получаем генератор — функцию, которая на выходе выдаёт объект.\\nВ таких моделях, как генеративные состязательные нейронные сети, диффузионные модели, вариационные автокодировщики, генератор на вход принимает вектор случайных значений из простого вероятностного распределения (например, нормального или равномерного). Получается, что x=G(z)x = G(z)x=G(z), где xxx — объект, GGG — функция генератора, а zzz — вектор случайных значений. Пространство, в котором располагается zzz, называется латентным.\\nОбычно распределение zzz задаётся ещё до обучения модели и не меняется в процессе. Поскольку мы знаем распределение, мы можем семплировать из него сколько угодно разных zzz.\\nРассмотрим два вектора z1z_1z1\\u200b и z2z_2z2\\u200b из латентного пространства и два соответствующих им сгенерированных объекта x1=G(z1)x_1 = G(z_1)x1\\u200b=G(z1\\u200b) x2=G(z2)x_2 = G(z_2)x2\\u200b=G(z2\\u200b). Так как z1z_1z1\\u200b и z2z_2z2\\u200b — это две точки в латентном пространстве, между ними можно провести линию.\\nТочки, лежащие на этой линии, будут так же принадлежать этому пространству. Если двигаться по этой линии и использовать точки с неё в качестве входа для генератора, то можно получить плавно изменяющийся сгенерированный объект.\\n\\n\\n\\nПример изображений, полученных с помощью интерполяции в латентном пространстве.\\n    Источник\\n\\n\\nВ примере выше мы рассмотрели движение вдоль линии, однако на практике интерполяция может быть по более сложной траектории.\\nМанипуляции с латентным пространством позволяют не только создавать плавные переходы между объектами, но так же редактировать объекты. Обычно в таких случаях требуется найти направления в латентном пространстве, которое отвечает за нужное свойство сгенерированных объектов.\\nНапример, направление, отвечающее за цвет волос или улыбку человека. Подробнее такие методы мы рассмотрим в параграфах про конкретные модели.\\nПрименения генеративных моделей\\nЗачем может понадобиться генерировать новые данные или восстанавливать их плотность? Самый простой пример – это аугментация набора данных, которая мешает переобучению и улучшает обобщаемость модели.\\nПростые аугментации данных (случайные сдвиги, повороты, масштабирование, изменения цвета и контраста) активно используются почти во всех методах машинного обучения. Генеративные же модели представляют собой более сложный вид аугментации данных, который способен существенно расширить датасет, или обогатить его совершенно новыми элементами.\\nНапример, генеративную модель, которая переносит стиль одного изображения на другое (style transfer), можно использовать для обучения более робастных моделей классификации. В статье Sandfort et al. используют аугментацию генеративными нейросетями, чтобы улучшить качество сегментации компьютерной томографии.\\nПомимо этого, у генеративных моделей есть ряд других применений для редактирования изображений. Их используют, чтобы повысить разрешение картинок (задача super-resolution).\\nНа изображении ниже оригинальную картинку (original) сначала сжали в четыре раза, а потом попробовали восстановить до исходных размеров разными методами. Видно, что метод SRGAN, метод на основе генеративных состязательных нейронных сетях работает гораздо лучше бикубической интерполяции (bicubic), которая обычно применяется по умолчанию и смазывает картинку.\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nС помощью генеративных моделей можно закрашивать пропущенные куски изображений. Это полезно, когда мы хотим удалить с фото других людей, и нам нужно закрасить участки, образовавшиеся после их удаления. Эта функция представлена в некоторых современных смартфонах.\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nВ последние несколько лет хорошо стали работать модели, которые генерируют изображения на основе их текстового описания. Среди таких моделей:\\n\\nStable Diffusion (Демо). Модель с открытым исходным кодом\\nDALLE 2. Доступ по платному API\\nMidjourney. Доступ через Discord\\nImagen\\n\\n\\n\\n\\n\\nПримеры генерации изображений из текстового описания. Модель stable diffusion\\n    Источник\\n\\n\\nПоявились даже специальные базы изображений, сгенерированных нейронными сетями: Lexica, Openart.\\nДоступность таких моделей приводит к появлению множества приложений:\\n\\nИллюстрации для книг\\nСоздание логотипов\\nСоздание дизайнов помещений\\nГенерация тату\\n\\nКроме этого, некоторые модели позволяют совместить несколько задач и делать закрашивание изображения на основе текстового описания. Например, удалять какую-то область и говорить модели, что там должно быть нарисовано.\\n\\n\\n\\nПример закрашивания части изображения на основе текстового описания.\\n    Источник\\n\\n\\nНа основе этой технологии появились редакторы изображений с генеративными моделями внутри: Neural love, Photoroom, ZMO.\\nСовременные генеративные модели достигли очень хорошего качества и уже стали использоваться в реальных задачах, о которых мы вам рассказали. В следующих параграфах этой главы мы рассмотрим основные методы генеративного обучения более детально.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф7.2. Дистилляция знанийСледующий параграф8.2. Variational Autoencoder (VAE)Яндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_66.html', 'title': 'Многомерные распределения'}, page_content='Многомерные распределенияЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/716.1.Матричное дифференцирование16.2.Матричная факторизация16.3.Вероятностные распределения16.4.Многомерные распределенияДискретные многомерные распределенияНепрерывные многомерные распределенияМаргинальные распределенияНезависимость случайных величинХарактеристики случайных векторовПреобразования плотностей случайных векторовРаспределение суммы независимых случайных величинПримеры многомерных распределений16.5.Независимость и условные распределения вероятностей16.6.Параметрические оценки16.7.Энтропия и семейство экспоненциальных распределенийГлавная/Хендбуки/Учебник по машинному обучению/Многомерные распределения16.4. Многомерные распределенияАвторыСергей ЛыткинДо этого мы рассматривали только одномерные распределения вероятностей на числовой прямой. Однако ничто не мешает в качестве носителя Ω\\\\OmegaΩ выбрать пространство более высокой размерности. И снова все представляющие практический интерес распределения делятся на два класса: дискретные и непрерывные.\\nДискретные многомерные распределения\\nПусть, например, эксперимент состоит из двух фаз: сначала подбрасывается монетка, а затем кубик. Тогда вероятностная масса сосредоточена в точках (i,j)(i, j)(i,j), i=0,1i=0, 1i=0,1, 1⩽j⩽61\\\\leqslant j \\\\leqslant 61⩽j⩽6. Вероятность каждого исхода можно записать в виде таблицы\\n\\n\\n\\n\\n\\n«Неудача»\\n\\n\\n«Успех»\\n\\n\\n\\n\\n\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n\\n\\n\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n\\n\\n\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n\\n\\n\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n\\n\\n\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n\\n\\n\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n112\\\\frac 1{12}121\\u200b\\n\\n\\n\\n\\nРезультат подбрасывания монеты моделирует бернуллиевская случайная величина ξ\\\\xiξ, а результат броска кубика — равномерно распределённая на множестве {1,2,3,4,5,6}\\\\{1,2,3,4,5,6\\\\}{1,2,3,4,5,6} случайная величина η\\\\etaη. Содержимое таблицы вероятностей каждого исхода можно также представить матрицей\\nP=(112112112112112112112112112112112112)}⏞ηξ,  P = \\n  \\\\overbrace{\\\\left.\\\\begin{pmatrix}\\n    \\\\frac 1{12} & \\\\frac 1{12} & \\\\frac 1{12} & \\\\frac 1{12} & \\\\frac 1{12} & \\\\frac 1{12} \\\\\\\\\\n    \\\\frac 1{12} & \\\\frac 1{12} & \\\\frac 1{12} & \\\\frac 1{12} & \\\\frac 1{12} & \\\\frac 1{12} \\n  \\\\end{pmatrix}\\\\right\\\\}}^\\\\eta  \\\\xi,\\nP=(121\\u200b121\\u200b\\u200b121\\u200b121\\u200b\\u200b121\\u200b121\\u200b\\u200b121\\u200b121\\u200b\\u200b121\\u200b121\\u200b\\u200b121\\u200b121\\u200b\\u200b)}\\u200bη\\u200bξ,которая задаёт совместное распределение случайных величин ξ\\\\xiξ и η\\\\etaη: P(ξ=i,η=j)=Pij\\\\mathbb P(\\\\xi = i, \\\\eta = j) = P_{ij}P(ξ=i,η=j)=Pij\\u200b. Пару случайных величин (ξ,η)(\\\\xi, \\\\eta)(ξ,η) в таком контексте называют также случайным вектором.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nЭлементы матрицы PPP не обязаны совпадать; например, монета может быть несимметричной с вероятностью «успеха» ppp, и тогда таблица вероятностей примет вид\\n\\n\\n\\n\\n\\n«Неудача»\\n\\n\\n«Успех»\\n\\n\\n\\n\\n\\n\\n\\n1−p6\\\\frac {1-p}{6}61−p\\u200b\\n\\n\\np6\\\\frac p{6}6p\\u200b\\n\\n\\n\\n\\n\\n\\n\\n1−p6\\\\frac {1-p}{6}61−p\\u200b\\n\\n\\np6\\\\frac p{6}6p\\u200b\\n\\n\\n\\n\\n\\n\\n\\n1−p6\\\\frac {1-p}{6}61−p\\u200b\\n\\n\\np6\\\\frac p{6}6p\\u200b\\n\\n\\n\\n\\n\\n\\n\\n1−p6\\\\frac {1-p}{6}61−p\\u200b\\n\\n\\np6\\\\frac p{6}6p\\u200b\\n\\n\\n\\n\\n\\n\\n\\n1−p6\\\\frac {1-p}{6}61−p\\u200b\\n\\n\\np6\\\\frac p{6}6p\\u200b\\n\\n\\n\\n\\n\\n\\n\\n1−p6\\\\frac {1-p}{6}61−p\\u200b\\n\\n\\np6\\\\frac p{6}6p\\u200b\\n\\n\\n\\n\\nКонтрольный вопрос. Какая таблица вероятностей соответствует эксперименту, в котором результат подбрасывания монеты «портит» кубик следующим образом: на нём могут равновероятно выпасть только значения 111 или 222 в случае «неудачи» и 444, 555 или 666 в случае «успеха»?\\nОтвет\\n\\n\\n\\n\\n«Неудача»\\n\\n\\n«Успех»\\n\\n\\n\\n\\n\\n\\n\\n14\\\\frac 1{4}41\\u200b\\n\\n\\n000\\n\\n\\n\\n\\n\\n\\n\\n14\\\\frac 1{4}41\\u200b\\n\\n\\n000\\n\\n\\n\\n\\n\\n\\n\\n000\\n\\n\\n000\\n\\n\\n\\n\\n\\n\\n\\n000\\n\\n\\n16\\\\frac 1{6}61\\u200b\\n\\n\\n\\n\\n\\n\\n\\n000\\n\\n\\n16\\\\frac 1{6}61\\u200b\\n\\n\\n\\n\\n\\n\\n\\n000\\n\\n\\n16\\\\frac 1{6}61\\u200b\\n\\n\\n\\n\\nВ общем случае дискретное nnn-мерное распределение задаётся многомерным тензором из неотрицательных чисел pi1…inp_{i_1\\\\ldots i_n}pi1\\u200b…in\\u200b\\u200b, суммирующихся в единицу. Такие тензоры используются для задания совместного распределения вероятностей случайного вектора (ξ1,…,ξn)(\\\\xi_1, \\\\ldots, \\\\xi_n)(ξ1\\u200b,…,ξn\\u200b) из дискретных случайных величин:\\nP(ξ1=i1,ξ2=i2,…,ξn=in)=pi1i2…in. \\\\mathbb P(\\\\xi_1 = i_1, \\\\xi_2 = i_2, \\\\ldots, \\\\xi_n = i_n) = p_{i_1i_2\\\\ldots i_n}.\\nP(ξ1\\u200b=i1\\u200b,ξ2\\u200b=i2\\u200b,…,ξn\\u200b=in\\u200b)=pi1\\u200bi2\\u200b…in\\u200b\\u200b.Непрерывные многомерные распределения\\nНепрерывное распределение на плоскости задаётся плотностью p(x,y)⩾0p(x, y) \\\\geqslant 0p(x,y)⩾0; при этом вероятность события A⊂R2A\\\\subset \\\\mathbb R^2A⊂R2 равна\\nP(A)=∬Ap(x,y)\\u2009dxdy  \\\\mathbb P(A) = \\\\iint\\\\limits_{A} p(x, y)\\\\,dxdy\\nP(A)=A∬\\u200bp(x,y)dxdyпри условии, что этот интеграл имеет смысл. Простейший пример — равномерное распределение на единичном квадрате [0,1]2[0,1]^2[0,1]2: его плотность равна I[0,1]2(x,y)\\\\mathbb I_{[0, 1]^2}(x, y)I[0,1]2\\u200b(x,y), и\\nP(A)=∬Adxdy=∣A∣\\xa0для\\xa0A⊂[0,1]2.\\\\mathbb P(A) = \\\\iint \\\\limits_{A} dxdy = \\\\vert A\\\\vert \\\\text{ для } A\\\\subset [0,1]^2.\\nP(A)=A∬\\u200bdxdy=∣A∣\\xa0для\\xa0A⊂[0,1]2.Именно так на единичном квадрате формально определяется геометрическая вероятность.\\nПлотность непрерывного распределения в Rn\\\\mathbb R^nRn является неотрицательной функцией вида p(x1,…,xn)p(x_1, \\\\ldots, x_n)p(x1\\u200b,…,xn\\u200b) со свойством\\n∫Rnp(x1,…,xn)\\u2009dx1…dxn=1.  \\\\int_{\\\\mathbb R^n} p(x_1, \\\\ldots, x_n)\\\\,dx_1\\\\ldots dx_n = 1.\\n∫Rn\\u200bp(x1\\u200b,…,xn\\u200b)dx1\\u200b…dxn\\u200b=1.Говорят, что случайный вектор ξ=(ξ1,…,ξn)\\\\boldsymbol \\\\xi = (\\\\xi_1, \\\\ldots, \\\\xi_n)ξ=(ξ1\\u200b,…,ξn\\u200b) имеет совместную плотность pξ(x1,…,xn)p_{\\\\boldsymbol \\\\xi}(x_1, \\\\ldots, x_n)pξ\\u200b(x1\\u200b,…,xn\\u200b), если\\nP(ξ∈A)=∫Ap(x1,…,xn)\\u2009dx1…dxn  \\\\mathbb P(\\\\boldsymbol \\\\xi \\\\in A) = \\\\int\\\\limits_A p(x_1, \\\\ldots, x_n)\\\\,dx_1\\\\ldots dx_n\\nP(ξ∈A)=A∫\\u200bp(x1\\u200b,…,xn\\u200b)dx1\\u200b…dxn\\u200bдля всех достаточно «хороших» (измеримых по Лебегу) множеств A⊂RnA \\\\subset \\\\mathbb R^nA⊂Rn.\\nМаргинальные распределения\\nИз совместного распределения можно получить распределение в пространстве меньшей размерности путём суммирования или интегрирования по части переменных. Например, если матрица PijP_{ij}Pij\\u200b задаёт совместное распределение случайных величин ξ\\\\xiξ и η\\\\etaη, Pij=P(ξ=i,η=j)P_{ij} = \\\\mathbb P(\\\\xi = i, \\\\eta = j)Pij\\u200b=P(ξ=i,η=j), то каждый из наборов чисел\\nqi=∑jPij,rj=∑iPij,  q_i = \\\\sum\\\\limits_j P_{ij}, \\\\quad r_j = \\\\sum\\\\limits_i P_{ij},\\nqi\\u200b=j∑\\u200bPij\\u200b,rj\\u200b=i∑\\u200bPij\\u200b,неотрицателен и суммируется в единицу:\\n∑iqi=∑jrj=∑i,jPij=1.  \\\\sum\\\\limits_i q_i = \\\\sum\\\\limits_j r_j = \\\\sum\\\\limits_{i, j} P_{ij} = 1.\\ni∑\\u200bqi\\u200b=j∑\\u200brj\\u200b=i,j∑\\u200bPij\\u200b=1.Таким образом, числа {qi}\\\\{q_i\\\\}{qi\\u200b} и {rj}\\\\{r_j\\\\}{rj\\u200b} задают некоторые распределения вероятностей, называемые маргинальными.\\nУпражнение. Найдите маргинальные распределения, если совместное распределение задано матрицей\\nа)\\xa0(1−p61−p61−p61−p61−p61−p6p6p6p6p6p6p6);\\\\text{а) }\\n\\\\begin{pmatrix}\\n    \\\\frac {1-p}6 & \\\\frac {1-p}6 & \\\\frac {1-p}6 & \\\\frac {1-p}6 & \\\\frac {1-p}6 & \\\\frac {1-p}6 \\\\\\\\\\n    \\\\frac p6 & \\\\frac p6 & \\\\frac p6 & \\\\frac p6 & \\\\frac p6 & \\\\frac p6 \\n  \\\\end{pmatrix};\\\\quad\\nа)\\xa0(61−p\\u200b6p\\u200b\\u200b61−p\\u200b6p\\u200b\\u200b61−p\\u200b6p\\u200b\\u200b61−p\\u200b6p\\u200b\\u200b61−p\\u200b6p\\u200b\\u200b61−p\\u200b6p\\u200b\\u200b);б)\\xa0(14140000161616000).\\\\text{б) }\\n  \\\\begin{pmatrix}\\n    \\\\frac 1{4} & \\\\frac 14 & 0 & 0 & 0 & 0 \\\\\\\\\\n    \\\\frac 16& \\\\frac 16& \\\\frac 16 & 0 & 0 & 0 \\\\\\\\\\n  \\\\end{pmatrix}.\\nб)\\xa0(41\\u200b61\\u200b\\u200b41\\u200b61\\u200b\\u200b061\\u200b\\u200b00\\u200b00\\u200b00\\u200b).ОтветСуммируя столбцы этих матриц, получаем вероятности (1−p,p)(1-p, p)(1−p,p) в случае а) и (12,12)\\\\big(\\\\frac 12, \\\\frac 12\\\\big)(21\\u200b,21\\u200b) в случае б). Если же суммировать строки, то получаются наборы\\nа)\\xa0(16,16,16,16,16,16);б)\\xa0(512,512,16,0,0,0).\\\\text{а) } \\\\Big(\\\\frac 16, \\\\frac 16,\\\\frac 16,\\\\frac 16,\\\\frac 16,\\\\frac 16 \\\\Big);\\\\quad\\n\\\\text{б) } \\\\Big(\\\\frac 5{12}, \\\\frac 5{12},\\\\frac 16,0,0,0 \\\\Big).\\nа)\\xa0(61\\u200b,61\\u200b,61\\u200b,61\\u200b,61\\u200b,61\\u200b);б)\\xa0(125\\u200b,125\\u200b,61\\u200b,0,0,0).Заметим, что в п. а) после маргинализации получились в точности распределения вероятностей компонент случайного вектора (ξ,η)(\\\\xi, \\\\eta)(ξ,η) из приведённого выше примера. Это следствие независимости случайных величин ξ\\\\xiξ и η\\\\etaη.\\nВ непрерывном случае ситуация похожая: если случайный вектор имеет совместную плотность p(x,y)p(x,y)p(x,y), то функции\\nq(x)=∫−∞∞p(x,y)\\u2009dy,r(y)=∫−∞∞p(x,y)\\u2009dx  q(x) = \\\\int\\\\limits_{-\\\\infty}^\\\\infty p(x, y)\\\\,dy, \\\\quad r(y) = \\\\int\\\\limits_{-\\\\infty}^\\\\infty p(x, y)\\\\,dx\\nq(x)=−∞∫∞\\u200bp(x,y)dy,r(y)=−∞∫∞\\u200bp(x,y)dxявляются плотностями маргинальных распределений.\\nДля nnn-мерных распределений можно находить маргинальные распределения, суммируя или интегрируя по любым наборам переменных с индексами 1⩽i1<i2<…<ik⩽n1\\\\leqslant i_1 < i_2 < \\\\ldots < i_k \\\\leqslant n1⩽i1\\u200b<i2\\u200b<…<ik\\u200b⩽n; в результате получится маргинальное распределение по оставшимся n−kn-kn−k переменным.\\nНезависимость случайных величин\\nСлучайные величины ξ\\\\xiξ и η\\\\etaη называются независимыми, если совместное распределение случайного вектора (ξ,η)(\\\\xi, \\\\eta)(ξ,η) распадается на произведение одномерных. Точнее говоря,\\n\\nдискретные случайные величины ξ\\\\xiξ и η\\\\etaη независимы, если P(ξ=xi,η=yj)=P(ξ=xi)P(η=yj)\\\\mathbb P(\\\\xi = x_i, \\\\eta = y_j) = \\\\mathbb P(\\\\xi = x_i)\\\\mathbb P(\\\\eta = y_j)P(ξ=xi\\u200b,η=yj\\u200b)=P(ξ=xi\\u200b)P(η=yj\\u200b) для всех возможных xix_ixi\\u200b и yjy_jyj\\u200b;\\nнепрерывные случайные величины ξ\\\\xiξ и η\\\\etaη независимы, если их совместная плотность\\np(x,y)=pξ(x)pη(y)p(x, y) = p_\\\\xi(x)p_\\\\eta(y)p(x,y)=pξ\\u200b(x)pη\\u200b(y).\\n\\nЕсли случайные величины ξ\\\\xiξ и η\\\\etaη независимы, то распределение каждой из них является маргинальным распределением их совместного распределения, поскольку\\n∑iP(ξ=xi)P(η=yj)=P(η=yj),  \\\\sum\\\\limits_i\\\\mathbb P(\\\\xi = x_i)\\\\mathbb P(\\\\eta = y_j) = \\\\mathbb P(\\\\eta = y_j),\\ni∑\\u200bP(ξ=xi\\u200b)P(η=yj\\u200b)=P(η=yj\\u200b),∑jP(ξ=xi)P(η=yj)=P(ξ=xi),  \\\\sum\\\\limits_j\\\\mathbb P(\\\\xi = x_i)\\\\mathbb P(\\\\eta = y_j) = \\\\mathbb P(\\\\xi = x_i),\\nj∑\\u200bP(ξ=xi\\u200b)P(η=yj\\u200b)=P(ξ=xi\\u200b),и\\n∫−∞+∞pξ(x)pη(y)dx=pη(y),  \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p_\\\\xi(x)p_\\\\eta(y) dx = p_\\\\eta(y),\\n−∞∫+∞\\u200bpξ\\u200b(x)pη\\u200b(y)dx=pη\\u200b(y),∫−∞+∞pξ(x)pη(y)dy=pξ(x).  \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p_\\\\xi(x)p_\\\\eta(y) dy = p_\\\\xi(x).\\n−∞∫+∞\\u200bpξ\\u200b(x)pη\\u200b(y)dy=pξ\\u200b(x).Случайные величины (ξ1,…,ξn)(\\\\xi_1, \\\\ldots, \\\\xi_n)(ξ1\\u200b,…,ξn\\u200b) независимы в совокупности, если их совместное распределение (совместная плотность) распадается в произведение одномерных распределений (плотностей).\\nПример. Рассмотрим nnn гауссовских случайных величин ξk∼N(μk,σk2)\\\\xi_k \\\\sim \\\\mathcal N(\\\\mu_k, \\\\sigma_k^2)ξk\\u200b∼N(μk\\u200b,σk2\\u200b) с плотностями\\npξk(xk)=12πσke−(xk−μk)22σk2.  p_{\\\\xi_k}(x_k) = \\\\frac 1{\\\\sqrt{2\\\\pi}\\\\sigma_k} e^{-\\\\frac{(x_k - \\\\mu_k)^2}{2\\\\sigma_k^2}}.\\npξk\\u200b\\u200b(xk\\u200b)=2π\\u200bσk\\u200b1\\u200be−2σk2\\u200b(xk\\u200b−μk\\u200b)2\\u200b.Совместную плотность случайного вектора ξ=(ξ1,…,ξn)\\\\boldsymbol \\\\xi = (\\\\xi_1, \\\\ldots, \\\\xi_n)ξ=(ξ1\\u200b,…,ξn\\u200b) определим как произведение плотностей его компонент:\\npξ(x1,…,xn)=pξ1(x1)…pξn(xn)=1(2π)n/2σ1…σne−12∑k=1n(xk−μk)2σk2.  p_{\\\\boldsymbol \\\\xi}(x_1, \\\\ldots, x_n) = p_{\\\\xi_1}(x_1)\\\\ldots p_{\\\\xi_n}(x_n) = \\\\frac 1{(2\\\\pi)^{n/2}\\\\sigma_1\\\\ldots\\\\sigma_n} e^{-\\\\frac 12\\\\sum\\\\limits_{k=1}^n \\\\frac{(x_k - \\\\mu_k)^2}{\\\\sigma_k^2}}.\\npξ\\u200b(x1\\u200b,…,xn\\u200b)=pξ1\\u200b\\u200b(x1\\u200b)…pξn\\u200b\\u200b(xn\\u200b)=(2π)n/2σ1\\u200b…σn\\u200b1\\u200be−21\\u200bk=1∑n\\u200bσk2\\u200b(xk\\u200b−μk\\u200b)2\\u200b.Случайный вектор ξ\\\\boldsymbol \\\\xiξ с такой плотностью имеет многомерное нормальное (гауссовское) распределение c независимыми в совокупности компонентами. Любое маргинальное распределение случайного вектора ξ\\\\boldsymbol \\\\xiξ обладает плотностью того же вида, и поэтому также является гауссовским.\\nХарактеристики случайных векторов\\nМатематическое ожидание случайного вектора ξ=(ξ1,…,ξn)\\\\boldsymbol \\\\xi = (\\\\xi_1, \\\\ldots, \\\\xi_n)ξ=(ξ1\\u200b,…,ξn\\u200b) является вектором той же размерности и вычисляется покомпонентно:\\nEξ=(Eξ1,…,Eξn).  \\\\mathbb E \\\\boldsymbol \\\\xi = (\\\\mathbb E \\\\xi_1, \\\\ldots, \\\\mathbb E\\\\xi_n).\\nEξ=(Eξ1\\u200b,…,Eξn\\u200b).Каждая компонента случайного вектора — это обычная случайная величина, и её среднее можно вычислить стандартными методами:\\n\\nEξk=∑i1,…,inikpi1…in\\\\mathbb E\\\\xi_k = \\\\sum\\\\limits_{i_1, \\\\ldots, i_n} i_k p_{i_1\\\\ldots i_n}Eξk\\u200b=i1\\u200b,…,in\\u200b∑\\u200bik\\u200bpi1\\u200b…in\\u200b\\u200b в дискретном случае;\\nEξk=∫Rnxkp(x1,…,xn),dx1…dxn\\\\mathbb{E}\\\\xi_k=\\\\int\\\\limits_{\\\\mathbb{R}^n}x_kp(x_1,\\\\ldots,x_n),dx_1\\\\ldots dx_nEξk\\u200b=Rn∫\\u200bxk\\u200bp(x1\\u200b,…,xn\\u200b),dx1\\u200b…dxn\\u200b в непрерывном случае.\\n\\nМатематическое ожидание перестановочно с линейным преобразованием случайного вектора: E(Cξ)=CEξ\\\\mathbb E(\\\\boldsymbol{C\\\\xi}) = \\\\boldsymbol C \\\\mathbb E \\\\boldsymbol \\\\xiE(Cξ)=CEξ, где C\\\\boldsymbol CC — фиксированная матрица.\\nВместо дисперсии у случайного вектора ξ=(ξ1,…,ξn)\\\\boldsymbol \\\\xi = (\\\\xi_1, \\\\ldots, \\\\xi_n)ξ=(ξ1\\u200b,…,ξn\\u200b) есть матрица ковариаций:\\nVξ=cov(ξ,ξ)=E(ξ−Eξ)(ξ−Eξ)T.  \\\\mathbb V \\\\boldsymbol \\\\xi = \\\\mathrm{cov}(\\\\boldsymbol \\\\xi, \\\\boldsymbol \\\\xi) = \\\\mathbb E\\\\big(\\\\boldsymbol \\\\xi - \\\\mathbb E\\\\boldsymbol \\\\xi\\\\big)\\\\big(\\\\boldsymbol \\\\xi - \\\\mathbb E\\\\boldsymbol \\\\xi\\\\big)^T.\\nVξ=cov(ξ,ξ)=E(ξ−Eξ)(ξ−Eξ)T.Матрица ковариаций симметрична и состоит из попарных ковариаций компонент случайного вектора ξ\\\\boldsymbol \\\\xiξ:\\ncov(ξ,ξ)ij=cov(ξi,ξj).  \\\\mathrm{cov}(\\\\boldsymbol \\\\xi, \\\\boldsymbol \\\\xi)_{ij} = \\\\mathrm{cov}(\\\\xi_i, \\\\xi_j).\\ncov(ξ,ξ)ij\\u200b=cov(ξi\\u200b,ξj\\u200b).Упражнение. Докажите, что ковариационная матрица любого случайного вектора неотрицательно определена.\\nРешение (не открывайте сразу, сначала попробуйте решить самостоятельно)Пользуясь линейностью математического ожидания, получаем\\nxTcov(ξ,ξ)x=ExT(ξ−Eξ)(ξ−Eξ)Tx=    \\\\boldsymbol x^T\\\\mathrm{cov}(\\\\boldsymbol\\\\xi, \\\\boldsymbol\\\\xi)\\\\boldsymbol x = \\\\mathbb{E}\\\\boldsymbol x^T(\\\\boldsymbol\\\\xi - \\\\mathbb{E}\\\\boldsymbol\\\\xi)(\\\\boldsymbol\\\\xi - \\\\mathbb{E}\\\\boldsymbol\\\\xi)^T\\\\boldsymbol x =\\nxTcov(ξ,ξ)x=ExT(ξ−Eξ)(ξ−Eξ)Tx==E(xTξ−E(xTξ))⋅(xTξ−E(xTξ))T=cov(xTξ,xTξ)=V(xTξ)⩾0.    =\\\\mathbb{E}\\\\left(\\\\boldsymbol x^T\\\\boldsymbol\\\\xi - \\\\mathbb{E}(\\\\boldsymbol x^T\\\\boldsymbol\\\\xi)\\\\right)\\\\cdot\\\\left(\\\\boldsymbol x^T\\\\boldsymbol\\\\xi - \\\\mathbb{E}(\\\\boldsymbol x^T\\\\boldsymbol\\\\xi)\\\\right)^T = \\\\mathrm{cov}\\\\left(\\\\boldsymbol x^T\\\\boldsymbol\\\\xi, \\\\boldsymbol x^T\\\\boldsymbol\\\\xi\\\\right)=\\\\mathbb{V}(\\\\boldsymbol x^T\\\\boldsymbol\\\\xi)\\\\geqslant 0.\\n=E(xTξ−E(xTξ))⋅(xTξ−E(xTξ))T=cov(xTξ,xTξ)=V(xTξ)⩾0.Если случайные величины ξ1,…,ξn\\\\xi_1, \\\\ldots, \\\\xi_nξ1\\u200b,…,ξn\\u200b независимы в совокупности, то cov(ξi,ξj)=0\\\\mathrm{cov}(\\\\xi_i, \\\\xi_j) = 0cov(ξi\\u200b,ξj\\u200b)=0, и ковариационая матрица случайного вектора ξ=(ξ1,…,ξn)\\\\boldsymbol \\\\xi = (\\\\xi_1, \\\\ldots, \\\\xi_n)ξ=(ξ1\\u200b,…,ξn\\u200b) диагональна:\\ncov(ξ,ξ)=diag{Vξ1,…,Vξn}.\\\\mathrm{cov}(\\\\boldsymbol \\\\xi , \\\\boldsymbol \\\\xi) = \\\\mathrm{diag}\\\\{\\\\mathbb V \\\\xi_1, \\\\ldots, \\\\mathbb V \\\\xi_n\\\\}.\\ncov(ξ,ξ)=diag{Vξ1\\u200b,…,Vξn\\u200b}.Например, матрица ковариации гауссовского случайного вектора ξ\\\\boldsymbol \\\\xiξ с плотностью\\npξ(x1,…,xn)=1(2π)n/2σ1…σne−12∑k=1n(xk−μk)2σk2=∏k=1n12πσke−(xk−μk)22σk2p_{\\\\boldsymbol \\\\xi}(x_1, \\\\ldots, x_n) = \\\\frac 1{(2\\\\pi)^{n/2}\\\\sigma_1\\\\ldots\\\\sigma_n} e^{-\\\\frac 12\\\\sum\\\\limits_{k=1}^n \\\\frac{(x_k - \\\\mu_k)^2}{\\\\sigma_k^2}} = \\\\prod\\\\limits_{k=1}^n \\\\frac 1{\\\\sqrt{2\\\\pi}\\\\sigma_k} e^{-\\\\frac{(x_k - \\\\mu_k)^2}{2\\\\sigma_k^2}}\\npξ\\u200b(x1\\u200b,…,xn\\u200b)=(2π)n/2σ1\\u200b…σn\\u200b1\\u200be−21\\u200bk=1∑n\\u200bσk2\\u200b(xk\\u200b−μk\\u200b)2\\u200b=k=1∏n\\u200b2π\\u200bσk\\u200b1\\u200be−2σk2\\u200b(xk\\u200b−μk\\u200b)2\\u200bравна diag{σ12,…,σn2}\\\\mathrm{diag}\\\\{\\\\sigma_1^2, \\\\ldots, \\\\sigma_n^2\\\\}diag{σ12\\u200b,…,σn2\\u200b}, поскольку компоненты вектора ξ\\\\boldsymbol \\\\xiξ независимы в совокупности и имеют нормальное распределение N(μk,σk2)\\\\mathcal N(\\\\mu_k, \\\\sigma_k^2)N(μk\\u200b,σk2\\u200b).\\nАналогом ковариации в многомерном случае служит матрица ковариаций между случайными векторами ξ=(ξ1,…,ξn)\\\\boldsymbol \\\\xi = (\\\\xi_1, \\\\ldots, \\\\xi_n)ξ=(ξ1\\u200b,…,ξn\\u200b) и η=(η1,…,ηn)\\\\boldsymbol \\\\eta = (\\\\eta_1, \\\\ldots, \\\\eta_n)η=(η1\\u200b,…,ηn\\u200b):\\ncov(ξ,η)=E(ξ−Eξ)(η−Eη)T.  \\\\mathrm{cov}(\\\\boldsymbol \\\\xi, \\\\boldsymbol \\\\eta) = \\\\mathbb E\\\\big(\\\\boldsymbol \\\\xi - \\\\mathbb E\\\\boldsymbol \\\\xi\\\\big)\\\\big(\\\\boldsymbol \\\\eta - \\\\mathbb E\\\\boldsymbol \\\\eta\\\\big)^T.\\ncov(ξ,η)=E(ξ−Eξ)(η−Eη)T.Матрицу ковариаций можно также вычислить по формуле\\ncov(ξ,η)=EξηT−Eξ(Eη)T.  \\\\mathrm{cov}(\\\\boldsymbol \\\\xi, \\\\boldsymbol \\\\eta) = \\\\mathbb E\\\\boldsymbol \\\\xi\\\\boldsymbol \\\\eta^T - \\\\mathbb E\\\\boldsymbol \\\\xi(\\\\mathbb E\\\\boldsymbol \\\\eta)^T.\\ncov(ξ,η)=EξηT−Eξ(Eη)T.Упражнение. Пусть случайный вектор η\\\\boldsymbol \\\\etaη получен из случайного вектора ξ\\\\boldsymbol \\\\xiξ линейным преобразованием: η=Cξ\\\\boldsymbol \\\\eta = \\\\boldsymbol {C\\\\xi}η=Cξ. Как связаны между собой их ковариационные матрицы?\\nРешение (не открывайте сразу, сначала попробуйте решить самостоятельно)Распишем по определению:\\ncov(Cξ,Cξ)=E(Cξ−E(Cξ))(Cξ−E(Cξ))T=  \\\\mathrm{cov}(\\\\boldsymbol{C\\\\xi}, \\\\boldsymbol{C\\\\xi}) = \\\\mathbb{E}\\\\big(\\\\boldsymbol{C\\\\xi} - \\\\mathbb{E}(\\\\boldsymbol{C\\\\xi})\\\\big)\\\\big(\\\\boldsymbol{C\\\\xi} - \\\\mathbb{E}(\\\\boldsymbol{C\\\\xi})\\\\big)^T =\\ncov(Cξ,Cξ)=E(Cξ−E(Cξ))(Cξ−E(Cξ))T==EC(ξ−Eξ)(ξ−Eξ)TCT=Ccov(ξ,ξ)CT.  =\\\\mathbb{E}\\\\boldsymbol C(\\\\boldsymbol \\\\xi - \\\\mathbb{E}\\\\boldsymbol \\\\xi)(\\\\boldsymbol \\\\xi - \\\\mathbb{E}\\\\boldsymbol \\\\xi)^T\\\\boldsymbol C^T = \\\\boldsymbol C\\\\mathrm{cov}(\\\\boldsymbol \\\\xi, \\\\boldsymbol \\\\xi)\\\\boldsymbol C^T.\\n=EC(ξ−Eξ)(ξ−Eξ)TCT=Ccov(ξ,ξ)CT.Преобразования плотностей случайных векторов\\nНередко приходится иметь дело не с самими случайными векторами, а с функциями от них. Но как найти плотность случайного вектора η=g(ξ)\\\\boldsymbol \\\\eta = g(\\\\boldsymbol \\\\xi)η=g(ξ), зная плотность pξ(x)p_{\\\\boldsymbol \\\\xi}(\\\\boldsymbol x)pξ\\u200b(x)?\\nПредположим, что g\\u2009\\u2063:Rn→Rng \\\\colon \\\\mathbb R^n \\\\to \\\\mathbb R^ng:Rn→Rn — гладкая обратимая функция. Тогда для измеримого A⊂RnA\\\\subset \\\\mathbb R^nA⊂Rn имеем\\nP(η∈A)=P(g(ξ)∈A)=P(ξ∈g−1(A))=∫g−1(A)pξ(x)dx\\\\mathbb{P}(\\\\boldsymbol{\\\\eta}\\\\in A)=\\\\mathbb{P}\\\\big(g(\\\\boldsymbol{\\\\xi})\\\\in A\\\\big)=\\\\mathbb{P}\\\\big(\\\\boldsymbol{\\\\xi}\\\\in g^{-1}(A)\\\\big)=\\\\int\\\\limits_{g^{-1}(A)}p_{\\\\boldsymbol{\\\\xi}}(x)d\\\\boldsymbol{x}\\nP(η∈A)=P(g(ξ)∈A)=P(ξ∈g−1(A))=g−1(A)∫\\u200bpξ\\u200b(x)dxЧтобы перейти к интегралу по AAA, сделаем замену переменной x=g−1(z)\\\\boldsymbol x = g^{-1}(\\\\boldsymbol z)x=g−1(z). По формуле замены координат в кратном интеграле получаем\\n∫g−1(A)pξ(x)dx=∫Apξ(g−1(z))∣det\\u2061J(z)∣dz,\\\\int\\\\limits_{g^{-1}(A)}p_{\\\\boldsymbol \\\\xi}(\\\\boldsymbol x)d\\\\boldsymbol x = \\\\int\\\\limits_{A}p_{\\\\boldsymbol \\\\xi}(g^{-1}(\\\\boldsymbol z))\\\\vert \\\\det J(\\\\boldsymbol z) \\\\vert d\\\\boldsymbol z,\\ng−1(A)∫\\u200bpξ\\u200b(x)dx=A∫\\u200bpξ\\u200b(g−1(z))∣detJ(z)∣dz,где det\\u2061J(z)\\\\det J(\\\\boldsymbol z)detJ(z) – якобиан преобразования g−1(z)g^{-1}(\\\\boldsymbol z)g−1(z), т.е. определитель матрицы Якоби J(z)=∂g−1(z)∂zJ(\\\\boldsymbol z) = \\\\frac{\\\\partial g^{-1}(\\\\boldsymbol z)}{\\\\partial \\\\boldsymbol z}J(z)=∂z∂g−1(z)\\u200b.\\nТаким образом,\\npη(z)=pξ(g−1(z))∣det\\u2061J(z)∣.p_{\\\\boldsymbol \\\\eta}(\\\\boldsymbol z) = p_{\\\\boldsymbol \\\\xi}(g^{-1}(\\\\boldsymbol z))\\\\vert \\\\det J(\\\\boldsymbol z)\\\\vert.\\npη\\u200b(z)=pξ\\u200b(g−1(z))∣detJ(z)∣.Упражнение. Пусть ξ\\\\boldsymbol \\\\xiξ – случайный вектор с плотностью pξ(x)p_{\\\\boldsymbol \\\\xi}(\\\\boldsymbol x)pξ\\u200b(x). Какова плотность случайного вектора η=μ+Cξ\\\\boldsymbol\\\\eta = \\\\boldsymbol\\\\mu + \\\\boldsymbol{C\\\\xi}η=μ+Cξ, где μ\\\\boldsymbol \\\\muμ – постоянный вектор, а C\\\\boldsymbol CC – постоянная обратимая матрица?\\nРешение (не открывайте сразу, сначала попробуйте решить самостоятельно)В данном случае g(x)=μ+Cxg(\\\\boldsymbol x) = \\\\boldsymbol\\\\mu + \\\\boldsymbol{Cx}g(x)=μ+Cx, g−1(z)=C−1(z−μ)g^{-1}(\\\\boldsymbol z) = \\\\boldsymbol C^{-1}(\\\\boldsymbol z - \\\\boldsymbol\\\\mu)g−1(z)=C−1(z−μ). Матрица Якоби преобразования g−1g^{-1}g−1 равна C−1\\\\boldsymbol C^{-1}C−1. Следовательно,\\npη(z)=1∣det\\u2061(C)∣pξ(C−1(z−μ)).  p_{\\\\boldsymbol \\\\eta}(\\\\boldsymbol z) = \\\\frac1{\\\\vert\\\\det(\\\\boldsymbol C)\\\\vert}p_{\\\\boldsymbol \\\\xi}(\\\\boldsymbol C^{-1}(\\\\boldsymbol z - \\\\boldsymbol\\\\mu)).\\npη\\u200b(z)=∣det(C)∣1\\u200bpξ\\u200b(C−1(z−μ)).Распределение суммы независимых случайных величин\\nВ дискретном случае найти распределение суммы двух независимых случайных величин несложно. В самом деле,\\nP(ξ+η=k)=∑iP(ξ+η=k,η=i)=∑iP(ξ=k−i,η=i).\\\\mathbb{P}(\\\\xi + \\\\eta = k) = \\\\sum_{i}\\\\mathbb{P}(\\\\xi + \\\\eta = k, \\\\eta = i)=\\n\\\\sum_{i}\\\\mathbb{P}(\\\\xi = k - i, \\\\eta = i).\\nP(ξ+η=k)=i∑\\u200bP(ξ+η=k,η=i)=i∑\\u200bP(ξ=k−i,η=i).В силу независимости случайных величин ξ\\\\xiξ и η\\\\etaη последняя сумма равна\\n∑iP(ξ=k−i)P(η=i).\\\\sum_{i}\\\\mathbb{P}(\\\\xi = k-i)\\\\mathbb{P}(\\\\eta = i).\\ni∑\\u200bP(ξ=k−i)P(η=i).Полученная формула называется формулой свёртки.\\nПусть теперь ξ1\\\\xi_1ξ1\\u200b и ξ2\\\\xi_2ξ2\\u200b – независимые непрерывные случайные величины с плотностями pξ1(x)p_{\\\\xi_1}(x)pξ1\\u200b\\u200b(x) и pξ2(x)p_{\\\\xi_2}(x)pξ2\\u200b\\u200b(x) соответственно. Сам собой напрашивается аналог формулы свёртки с плотностями вместо вероятностей, но чтобы достаточно строго вывести его и не запутаться, мы немного схитрим. А именно, мы рассмотрим случайный вектор ξ=(ξ1,ξ2)T\\\\boldsymbol\\\\xi = (\\\\xi_1, \\\\xi_2)^Tξ=(ξ1\\u200b,ξ2\\u200b)T и его (обратимое!) преобразование\\ng(ξ)=(ξ1+ξ2ξ2)=(1101)ξ=:η=(η1η2).g(\\\\boldsymbol\\\\xi) = \\\\begin{pmatrix}\\\\xi_1 + \\\\xi_2\\\\\\\\ \\\\xi_2\\\\end{pmatrix} = \\\\begin{pmatrix}1 & 1\\\\\\\\ 0& 1\\\\end{pmatrix}\\\\boldsymbol \\\\xi =: \\\\boldsymbol\\\\eta = \\\\begin{pmatrix}\\\\eta_1 \\\\\\\\ \\\\eta_2\\\\end{pmatrix}.\\ng(ξ)=(ξ1\\u200b+ξ2\\u200bξ2\\u200b\\u200b)=(10\\u200b11\\u200b)ξ=:η=(η1\\u200bη2\\u200b\\u200b).Обратное к нему будет иметь вид\\nh(η)=(1−101)η=(η1−η2η2)h(\\\\boldsymbol \\\\eta) = \\\\begin{pmatrix}1 & -1\\\\\\\\ 0 & 1\\\\end{pmatrix}\\\\boldsymbol\\\\eta = \\\\begin{pmatrix} \\\\eta_1 - \\\\eta_2\\\\\\\\ \\\\eta_2\\\\end{pmatrix}\\nh(η)=(10\\u200b−11\\u200b)η=(η1\\u200b−η2\\u200bη2\\u200b\\u200b)Тогда по правилу преобразования плотности\\npη(z)=∣det(1−101)∣⏟=1pξ(z1−z2,z2)=pξ1(z1−z2)pξ2(z2),p_{\\\\boldsymbol\\\\eta}(\\\\boldsymbol z) = \\\\underbrace{\\\\left|\\\\text{det}\\\\begin{pmatrix}1 & -1 \\\\\\\\ 0 & 1\\\\end{pmatrix}\\\\right|}_{=1}p_{\\\\boldsymbol\\\\xi}\\\\left(z_1 - z_2, z_2\\\\right) =\\np_{\\\\xi_1}(z_1 - z_2)p_{\\\\xi_2}(z_2),\\npη\\u200b(z)==1\\u200bdet(10\\u200b−11\\u200b)\\u200b\\u200b\\u200bpξ\\u200b(z1\\u200b−z2\\u200b,z2\\u200b)=pξ1\\u200b\\u200b(z1\\u200b−z2\\u200b)pξ2\\u200b\\u200b(z2\\u200b),где в последнем равенстве мы воспользовались независимостью ξ1\\\\xi_1ξ1\\u200b и ξ2\\\\xi_2ξ2\\u200b. Распределение случайной величины η1=ξ1+ξ2\\\\eta_1 = \\\\xi_1+\\\\xi_2η1\\u200b=ξ1\\u200b+ξ2\\u200b – это маргинальное распределение, которое вычисляется следующим образом:\\npη1(y)=∫−∞+∞pξ1(y−x)pξ2(x)dx.p_{\\\\eta_1}(y) = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty}p_{\\\\xi_1}(y - x)p_{\\\\xi_2}(x)dx.\\npη1\\u200b\\u200b(y)=−∞∫+∞\\u200bpξ1\\u200b\\u200b(y−x)pξ2\\u200b\\u200b(x)dx.Эта формула также называется формулой свёртки.\\nПримеры многомерных распределений\\nРассмотрим несколько популярных распределений случайных векторов.\\nМультиномиальное распределение\\nБиномиальное распределение Bin(n,p)\\\\mathrm{Bin}(n, p)Bin(n,p) моделирует nnn-кратное подбрасывание монеты с вероятностями «успеха» ppp и «неудачи» q=1−pq = 1-pq=1−p. Мультиномиальное распределение обобщает этот эксперимент: теперь подбрасывается кубик с k⩾2k\\\\geqslant 2k⩾2 гранями, и вероятность выпадения iii-й грани равна pip_ipi\\u200b, ∑i=1kpi=1\\\\sum\\\\limits_{i=1}^k p_i = 1i=1∑k\\u200bpi\\u200b=1. Обозначим через ξi\\\\xi_iξi\\u200b количество выпадений iii-й грани в серии из nnn бросков. Тогда случайный вектор ξ=(ξ1,…,ξk)\\\\boldsymbol \\\\xi = (\\\\xi_1, \\\\ldots, \\\\xi_k)ξ=(ξ1\\u200b,…,ξk\\u200b) имеет мультиномиальное распределение, при котором\\nP(ξ1=m1,…,ξk=mk)=n!m1!⋅…⋅mk!p1m1⋅…⋅pkmk,  \\\\mathbb P(\\\\xi_1 = m_1, \\\\ldots, \\\\xi_k = m_k) = \\\\frac{n!}{m_1!\\\\cdot \\\\ldots \\\\cdot m_k!} p_1^{m_1}\\\\cdot \\\\ldots \\\\cdot p_k^{m_k},\\nP(ξ1\\u200b=m1\\u200b,…,ξk\\u200b=mk\\u200b)=m1\\u200b!⋅…⋅mk\\u200b!n!\\u200bp1m1\\u200b\\u200b⋅…⋅pkmk\\u200b\\u200b,∑i=1kmi=n.  \\\\sum\\\\limits_{i = 1}^k m_i = n.\\ni=1∑k\\u200bmi\\u200b=n.При n=1n=1n=1 мультиномиальное распределение превращается в категориальное, известное также под названием multinoulli. Категориальное распределение моделирует случайный выбор одного из kkk классов с заданными вероятностями (p1,…,pk)(p_1, \\\\ldots, p_k)(p1\\u200b,…,pk\\u200b).\\nМногомерное нормальное распределение\\nМногомерное нормальное (гауссовское) распределение задаётся функцией плотности\\np(x)=1(2π)n/2det\\u2061Σexp\\u2061(−12(x−μ)TΣ−1(x−μ)),p(\\\\boldsymbol x) = \\\\frac1{(2\\\\pi)^{n/2}\\\\sqrt{\\\\det\\\\boldsymbol\\\\Sigma}}\\\\exp\\\\left(-\\\\frac12(\\\\boldsymbol x - \\\\boldsymbol\\\\mu)^T\\\\boldsymbol\\\\Sigma^{-1}(\\\\boldsymbol x - \\\\boldsymbol\\\\mu)\\\\right),\\np(x)=(2π)n/2detΣ\\u200b1\\u200bexp(−21\\u200b(x−μ)TΣ−1(x−μ)),где x,μ∈Rn\\\\boldsymbol x, \\\\boldsymbol \\\\mu\\\\in\\\\mathbb{R}^nx,μ∈Rn, Σ\\\\boldsymbol\\\\SigmaΣ — невырожденная симметричная матрица размера n×nn\\\\times nn×n. Такое распределение обозначается N(μ,Σ)\\\\mathcal{N}(\\\\boldsymbol\\\\mu, \\\\boldsymbol\\\\Sigma)N(μ,Σ).\\nЕсли случайный вектор ξ∼N(μ,Σ)\\\\boldsymbol \\\\xi \\\\sim \\\\mathcal{N}(\\\\boldsymbol\\\\mu, \\\\boldsymbol\\\\Sigma)ξ∼N(μ,Σ), то Eξ=μ\\\\mathbb E\\\\boldsymbol \\\\xi =\\\\boldsymbol \\\\muEξ=μ, cov(ξ,ξ)=Σ\\\\mathrm{cov}(\\\\boldsymbol \\\\xi, \\\\boldsymbol \\\\xi ) = \\\\boldsymbol \\\\Sigmacov(ξ,ξ)=Σ; таким образом, параметры гауссовского распределения — это его среднее и матрица ковариаций.\\nУпражнение. Пусть ξ∼N(μ,Σ)\\\\boldsymbol \\\\xi \\\\sim \\\\mathcal{N}(\\\\boldsymbol\\\\mu, \\\\boldsymbol\\\\Sigma)ξ∼N(μ,Σ) и η=Aξ+b\\\\boldsymbol \\\\eta =  \\\\boldsymbol{A\\\\xi} + \\\\boldsymbol bη=Aξ+b. Докажите, что η∼N(Aμ+b,AΣAT)\\\\boldsymbol \\\\eta \\\\sim \\\\mathcal{N}(\\\\boldsymbol{A\\\\mu} + \\\\boldsymbol b, \\\\boldsymbol{A\\\\Sigma A}^T)η∼N(Aμ+b,AΣAT).\\nРешение (не открывайте сразу, сначала попробуйте решить самостоятельно)Если бы нам стало известно, что вектор η\\\\boldsymbol \\\\etaη гауссовский, то мы нашли бы его параметры по стандартным формулам:\\nEη=E(Aξ+b)=AEξ+b=Aμ+b,  \\\\mathbb E\\\\boldsymbol \\\\eta = \\\\mathbb E (\\\\boldsymbol{A\\\\xi} + \\\\boldsymbol b) =  \\\\boldsymbol A\\\\mathbb E \\\\boldsymbol \\\\xi + \\\\boldsymbol b = \\\\boldsymbol{A\\\\mu} + \\\\boldsymbol b,\\nEη=E(Aξ+b)=AEξ+b=Aμ+b,cov(η,η)=cov(Aξ+b,Aξ+b)=Acov(ξ,ξ)AT=AΣAT.  \\\\mathrm{cov}(\\\\boldsymbol \\\\eta, \\\\boldsymbol \\\\eta) = \\\\mathrm{cov}(\\\\boldsymbol{A\\\\xi} + \\\\boldsymbol b, \\\\boldsymbol{A\\\\xi} + \\\\boldsymbol b) = \\\\boldsymbol A \\\\mathrm{cov}(\\\\boldsymbol \\\\xi, \\\\boldsymbol \\\\xi) \\\\boldsymbol A^T = \\\\boldsymbol{A\\\\Sigma A}^T.\\ncov(η,η)=cov(Aξ+b,Aξ+b)=Acov(ξ,ξ)AT=AΣAT.Решим задачу честно в предположении, что матрица A\\\\boldsymbol AA квадратная и невырожденная. Для этого воспользуемся формулой плотности линейного преобразования случайного вектора:\\npη(z)=1∣\\u2009\\u2063det\\u2061(A)∣pξ(A−1(z−b))=p_{\\\\boldsymbol \\\\eta}(\\\\boldsymbol z) = \\\\frac1{\\\\vert\\\\!\\\\det(\\\\boldsymbol A)\\\\vert}p_{\\\\boldsymbol \\\\xi}(\\\\boldsymbol A^{-1}(\\\\boldsymbol z - \\\\boldsymbol b)) =\\npη\\u200b(z)=∣det(A)∣1\\u200bpξ\\u200b(A−1(z−b))==1(2π)n/2det\\u2061Σ∣det\\u2061(A)∣exp\\u2061(−12(A−1z−A−1b−μ)TΣ−1(A−1z−A−1b−μ))==\\n\\\\frac1{(2\\\\pi)^{n/2}\\\\sqrt{\\\\det\\\\boldsymbol\\\\Sigma}\\\\vert\\\\det(\\\\boldsymbol A)\\\\vert}\\\\exp\\\\left(-\\\\frac12(\\\\boldsymbol A^{-1}\\\\boldsymbol z - \\\\boldsymbol A^{-1}\\\\boldsymbol b - \\\\boldsymbol\\\\mu)^T\\\\boldsymbol\\\\Sigma^{-1}(\\\\boldsymbol A^{-1}\\\\boldsymbol z - \\\\boldsymbol A^{-1}\\\\boldsymbol b  - \\\\boldsymbol\\\\mu)\\\\right) =\\n=(2π)n/2detΣ\\u200b∣det(A)∣1\\u200bexp(−21\\u200b(A−1z−A−1b−μ)TΣ−1(A−1z−A−1b−μ))==1(2π)n/2det\\u2061AΣATexp\\u2061(−12(z−b−Aμ)TA−TΣ−1A−1(z−b−Aμ)).=\\n\\\\frac1{(2\\\\pi)^{n/2}\\\\sqrt{\\\\det\\\\boldsymbol{A\\\\Sigma A}^T}}\\\\exp\\\\left(-\\\\frac12(\\\\boldsymbol z - \\\\boldsymbol b - \\\\boldsymbol{A\\\\mu})^T \\\\boldsymbol A^{-T}\\\\boldsymbol\\\\Sigma^{-1}\\\\boldsymbol A^{-1}(\\\\boldsymbol z - \\\\boldsymbol b  - \\\\boldsymbol{A\\\\mu})\\\\right).\\n=(2π)n/2detAΣAT\\u200b1\\u200bexp(−21\\u200b(z−b−Aμ)TA−TΣ−1A−1(z−b−Aμ)).В полученном выражении нетрудно узнать плотность гауссовского распределения N(Aμ+b,AΣAT)\\\\mathcal{N}(\\\\boldsymbol{A\\\\mu} + \\\\boldsymbol b, \\\\boldsymbol{A\\\\Sigma A}^T)N(Aμ+b,AΣAT).\\nЗаметим, что утверждение сохраняет силу и для случая прямоугольной матрицы A\\\\boldsymbol AA размера m×nm\\\\times nm×n, где nnn — размерность случайного вектора ξ\\\\boldsymbol \\\\xiξ.\\nВажный частный случай случайного гауссовского вектора с независимыми компонентами был рассмотрен в примере из секции про независимость случайных величин. Такое распределение получается, если матрица Σ\\\\boldsymbol\\\\SigmaΣ диагональна, Σ=diag{σ12,…,σn2}\\\\boldsymbol\\\\Sigma = \\\\mathrm{diag}\\\\{\\\\sigma_1^2, \\\\ldots, \\\\sigma_n^2\\\\}Σ=diag{σ12\\u200b,…,σn2\\u200b}. Тогда det\\u2061Σ=σ1…σn\\\\sqrt{\\\\det \\\\boldsymbol\\\\Sigma} = \\\\sigma_1 \\\\ldots \\\\sigma_ndetΣ\\u200b=σ1\\u200b…σn\\u200b, Σ−1=diag{1σ12,…,1σn2}\\\\boldsymbol\\\\Sigma^{-1} = \\\\mathrm{diag}\\\\big\\\\{\\\\frac1{\\\\sigma_1^2}, \\\\ldots, \\\\frac 1{\\\\sigma_n^2}\\\\big\\\\}Σ−1=diag{σ12\\u200b1\\u200b,…,σn2\\u200b1\\u200b}, и поэтому\\n−12(x−μ)TΣ−1(x−μ)=−12∑k=1n(xk−μk)2σk2.-\\\\frac12(\\\\boldsymbol x - \\\\boldsymbol\\\\mu)^T\\\\boldsymbol\\\\Sigma^{-1}(\\\\boldsymbol x - \\\\boldsymbol\\\\mu) = -\\\\frac 12 \\\\sum\\\\limits_{k=1}^n \\\\frac{(x_k-\\\\mu_k)^2}{\\\\sigma_k^2}.\\n−21\\u200b(x−μ)TΣ−1(x−μ)=−21\\u200bk=1∑n\\u200bσk2\\u200b(xk\\u200b−μk\\u200b)2\\u200b.Отсюда снова получаем формулу совместной плотности\\npξ(x)=1(2π)n/2σ1…σne−12∑k=1n(xk−μk)2σk2,  p_{\\\\boldsymbol \\\\xi}(\\\\boldsymbol x) = \\\\frac 1{(2\\\\pi)^{n/2}\\\\sigma_1\\\\ldots\\\\sigma_n} e^{-\\\\frac 12\\\\sum\\\\limits_{k=1}^n \\\\frac{(x_k - \\\\mu_k)^2}{\\\\sigma_k^2}},\\npξ\\u200b(x)=(2π)n/2σ1\\u200b…σn\\u200b1\\u200be−21\\u200bk=1∑n\\u200bσk2\\u200b(xk\\u200b−μk\\u200b)2\\u200b,которую можно переписать в виде\\n∏k=1n12πσke−(xk−μk)22σk2=∏k=1npξk(xk),ξk∼N(ξk,σk2),  \\\\prod\\\\limits_{k=1}^n \\\\frac 1{\\\\sqrt{2\\\\pi}\\\\sigma_k} e^{-\\\\frac{(x_k - \\\\mu_k)^2}{2\\\\sigma_k^2}} = \\\\prod\\\\limits_{k=1}^n p_{\\\\xi_k}(x_k),\\n  \\\\xi_k \\\\sim \\\\mathcal N(\\\\xi_k, \\\\sigma_k^2),\\nk=1∏n\\u200b2π\\u200bσk\\u200b1\\u200be−2σk2\\u200b(xk\\u200b−μk\\u200b)2\\u200b=k=1∏n\\u200bpξk\\u200b\\u200b(xk\\u200b),ξk\\u200b∼N(ξk\\u200b,σk2\\u200b),откуда следует независимость в совокупности компонент вектора ξ\\\\boldsymbol \\\\xiξ.\\nЕсли ковариационная матрица Σ\\\\boldsymbol \\\\SigmaΣ не является диагональной, то отдельные компоненты случайного вектора ξ∼N(μ,Σ)\\\\boldsymbol \\\\xi \\\\sim \\\\mathcal{N}(\\\\boldsymbol\\\\mu, \\\\boldsymbol\\\\Sigma)ξ∼N(μ,Σ) зависимы. Тем не менее, всегда найдётся линейное (и даже ортогональное) преобразование, которое превратит вектор ξ\\\\boldsymbol \\\\xiξ в гауссовский вектор с независимыми компонентами. Для этого достаточно найти ортогональную матрицу Q\\\\boldsymbol QQ со свойством\\nQΣQT=diag{σ12,…,σn2},\\\\boldsymbol Q \\\\boldsymbol \\\\Sigma \\\\boldsymbol Q^T = \\\\mathrm{diag}\\\\big\\\\{\\\\sigma_1^2,\\\\ldots,\\\\sigma_n^2\\\\big\\\\},\\nQΣQT=diag{σ12\\u200b,…,σn2\\u200b},и далее воспользоваться формулой плотности линейного преобразования гауссовского вектора.\\nПо тем же соображениям облако точек, сгенерированных из распределения N(μ,Σ)\\\\mathcal{N}(\\\\boldsymbol\\\\mu, \\\\boldsymbol\\\\Sigma)N(μ,Σ), будет напоминать эллипсоид с полуосями, пропорциональными вектору (σ12,…,σn2)(\\\\sigma_1^2,\\\\ldots,\\\\sigma_n^2)(σ12\\u200b,…,σn2\\u200b). Линии уровня плотности p(x)p(\\\\boldsymbol x)p(x) задаются уравнениями вида p(x)=Cp(\\\\boldsymbol x) = Cp(x)=C, а такое равенство эквивалентно квадратичной форме\\n(x−μ)TΣ−1(x−μ)=C1,(\\\\boldsymbol x - \\\\boldsymbol\\\\mu)^T\\\\boldsymbol\\\\Sigma^{-1}(\\\\boldsymbol x - \\\\boldsymbol\\\\mu) = C_1,\\n(x−μ)TΣ−1(x−μ)=C1\\u200b,где CCC и C1C_1C1\\u200b – некоторые константы. С помощью описанной выше ортогональной замены эта квадратичная форма может быть приведена к главным осям:\\nzTΛ−1z=C2,Λ=diag{σ12,…,σn2};  \\\\boldsymbol z^T \\\\boldsymbol\\\\Lambda^{-1} \\\\boldsymbol z = C_2, \\\\quad \\\\boldsymbol \\\\Lambda = \\\\mathrm{diag}\\\\big\\\\{\\\\sigma_1^2,\\\\ldots,\\\\sigma_n^2\\\\big\\\\};\\nzTΛ−1z=C2\\u200b,Λ=diag{σ12\\u200b,…,σn2\\u200b};в координатах это выглядит как\\n∑l=1nzk2σk2=C2.  \\\\sum\\\\limits_{l=1}^n \\\\frac{z_k^2}{\\\\sigma_k^2} = C_2.\\nl=1∑n\\u200bσk2\\u200bzk2\\u200b\\u200b=C2\\u200b.Мы получили практически каноническое уравнение nnn-мерного эллипсоида. В R2\\\\mathbb R^2R2 это будут эллипсы, сплюснутые тем сильнее, чем дальше от единицы отношение κ=σ1σ2\\\\kappa = \\\\frac{\\\\sigma_1}{\\\\sigma_2}κ=σ2\\u200bσ1\\u200b\\u200b собственных значений матрицы Σ\\\\boldsymbol \\\\SigmaΣ.\\n\\nНормальным будет и всякое маргинальное распределение многомерного гауссовского вектора.\\nУпражнение. Пусть случайный вектор ξ=(ξ1,ξ2)\\\\boldsymbol \\\\xi = (\\\\boldsymbol\\\\xi_1, \\\\boldsymbol \\\\xi_2)ξ=(ξ1\\u200b,ξ2\\u200b) имеет гауссовское распределение с параметрами\\nμ=(μ1μ2),Σ=(Σ11Σ12Σ12TΣ22),  \\\\boldsymbol \\\\mu = \\\\begin{pmatrix}\\\\boldsymbol \\\\mu_1 \\\\\\\\ \\\\boldsymbol \\\\mu_2 \\\\end{pmatrix}, \\\\quad\\n  \\\\boldsymbol \\\\Sigma = \\\\begin{pmatrix}\\n  \\\\boldsymbol \\\\Sigma_{11} & \\\\boldsymbol\\\\Sigma_{12} \\\\\\\\ \\\\boldsymbol\\\\Sigma_{12}^T & \\\\boldsymbol\\\\Sigma_{22}  \\\\end{pmatrix},\\nμ=(μ1\\u200bμ2\\u200b\\u200b),Σ=(Σ11\\u200bΣ12T\\u200b\\u200bΣ12\\u200bΣ22\\u200b\\u200b),где ξ1,μ1∈Rk\\\\boldsymbol \\\\xi_1, \\\\boldsymbol \\\\mu_1 \\\\in \\\\mathbb R^kξ1\\u200b,μ1\\u200b∈Rk, ξ2,μ2∈Rn−k\\\\boldsymbol \\\\xi_2, \\\\boldsymbol \\\\mu_2 \\\\in \\\\mathbb R^{n-k}ξ2\\u200b,μ2\\u200b∈Rn−k, Σ11∈Matk×k\\\\boldsymbol \\\\Sigma_{11} \\\\in \\\\mathrm{Mat}_{k\\\\times k}Σ11\\u200b∈Matk×k\\u200b, Σ12∈Matk×(n−k)\\\\boldsymbol \\\\Sigma_{12} \\\\in \\\\mathrm{Mat}_{k\\\\times (n-k)}Σ12\\u200b∈Matk×(n−k)\\u200b, Σ22∈Mat(n−k)×(n−k)\\\\boldsymbol \\\\Sigma_{22} \\\\in \\\\mathrm{Mat}_{(n-k)\\\\times (n-k)}Σ22\\u200b∈Mat(n−k)×(n−k)\\u200b.\\nДокажите, что случайный вектор ξ1\\\\boldsymbol\\\\xi_1ξ1\\u200b, полученный маргинализацией по компонентам вектора ξ2\\\\boldsymbol\\\\xi_2ξ2\\u200b, является гауссовским с параметрами mu1\\\\boldsymbol \\\\\\\\mu_1mu1\\u200b и Σ11\\\\boldsymbol \\\\Sigma_{11}Σ11\\u200b.\\nРешение (не открывайте сразу, сначала попробуйте решить самостоятельно)Существует прямое и довольно утомительное решение с многочисленными матричными манипуляциями. Мы поступим хитрее: рассмотрим маргинализацию как линейное преобразование\\nξ1=Aξ,\\xa0где\\xa0A=(Ik0k×(n−k))∈Matk×n,  \\\\boldsymbol\\\\xi_1 =  \\\\boldsymbol A\\\\boldsymbol\\\\xi, \\\\text{ где } \\\\boldsymbol A = \\\\begin{pmatrix}\\\\boldsymbol I_{k} & \\\\boldsymbol 0_{k\\\\times(n-k)}\\\\end{pmatrix} \\\\in \\\\mathrm{Mat}_{k\\\\times n},\\nξ1\\u200b=Aξ,\\xa0где\\xa0A=(Ik\\u200b\\u200b0k×(n−k)\\u200b\\u200b)∈Matk×n\\u200b,и воспользуемся результатом предыдущего упражнения. Имеем Aμ=Ikμ1=μ1\\\\boldsymbol A\\\\boldsymbol\\\\mu = \\\\boldsymbol I_{k}\\\\boldsymbol \\\\mu_1 =  \\\\boldsymbol \\\\mu_1Aμ=Ik\\u200bμ1\\u200b=μ1\\u200b, AΣAT=IkΣ11IkT=Σ11\\\\boldsymbol A \\\\boldsymbol \\\\Sigma \\\\boldsymbol A^T  = \\\\boldsymbol I_{k} \\\\boldsymbol \\\\Sigma_{11}\\\\boldsymbol I_{k}^T = \\\\boldsymbol \\\\Sigma_{11}AΣAT=Ik\\u200bΣ11\\u200bIkT\\u200b=Σ11\\u200b, и поэтому\\nξ1∼N(μ1,Σ11)\\\\boldsymbol\\\\xi_1 \\\\sim \\\\mathcal N(\\\\boldsymbol \\\\mu_1, \\\\boldsymbol \\\\Sigma_{11})ξ1\\u200b∼N(μ1\\u200b,Σ11\\u200b).\\nРаспределение Дирихле\\nРаспределение Дирихле сосредоточено на KKK-мерном симплексе\\n{(x1,…,xK)\\u2009\\u2063:x1+…+xK=1,\\u2005\\u200axi⩾0}.\\\\{(x_1,\\\\ldots,x_K)\\\\colon x_1 + \\\\ldots + x_K = 1,\\\\; x_i\\\\geqslant 0\\\\}.\\n{(x1\\u200b,…,xK\\u200b):x1\\u200b+…+xK\\u200b=1,xi\\u200b⩾0}.Плотность распределения Дирихле Dir(α)\\\\mathrm{Dir}(\\\\boldsymbol \\\\alpha)Dir(α) равна\\np(x1,…,xK)=1B(α)∏i=1Kxiαi−1,p(x_1,\\\\ldots,x_K) = \\\\frac1{B(\\\\boldsymbol \\\\alpha)}\\\\prod_{i=1}^Kx_i^{\\\\alpha_i - 1},\\np(x1\\u200b,…,xK\\u200b)=B(α)1\\u200bi=1∏K\\u200bxiαi\\u200b−1\\u200b,где α=(α1,…,αK)\\\\boldsymbol\\\\alpha = (\\\\alpha_1,\\\\ldots,\\\\alpha_K)α=(α1\\u200b,…,αK\\u200b) – вектор положительных параметров, а B(α)=∏iΓ(αi)Γ(∑iαi)B(\\\\boldsymbol\\\\alpha) = \\\\frac{\\\\prod_i\\\\Gamma(\\\\alpha_i)}{\\\\Gamma(\\\\sum_i\\\\alpha_i)}B(α)=Γ(∑i\\u200bαi\\u200b)∏i\\u200bΓ(αi\\u200b)\\u200b – многомерная бета-функция. Если ξ∼Dir(α)\\\\boldsymbol \\\\xi \\\\sim \\\\mathrm{Dir}(\\\\boldsymbol \\\\alpha)ξ∼Dir(α),\\nто\\nEξ=αα0,cov(ξi,ξj)=α0δij−αiαjα02(α0+1),α0=∑k=1Kαk.  \\\\mathbb E \\\\boldsymbol \\\\xi =\\\\frac{\\\\boldsymbol \\\\alpha}{\\\\alpha_0}, \\\\quad \\n  \\\\mathrm{cov} (\\\\xi_i, \\\\xi_j)=\\\\frac{\\\\alpha_0 \\\\delta_{ij} - \\\\alpha_i\\\\alpha_j}{\\\\alpha_0^2(\\\\alpha_0 + 1)}, \\\\quad \\\\alpha_0 = \\\\sum\\\\limits_{k=1}^K \\\\alpha_k.\\nEξ=α0\\u200bα\\u200b,cov(ξi\\u200b,ξj\\u200b)=α02\\u200b(α0\\u200b+1)α0\\u200bδij\\u200b−αi\\u200bαj\\u200b\\u200b,α0\\u200b=k=1∑K\\u200bαk\\u200b.Иллюстрация распределения Дирихле с помощью схемы ПойяПусть у нас есть KKK категорий и на них задано вероятностное распределение\\nq(1)=αα0=(α1α0,…,αKα0),\\\\boldsymbol q^{(1)} = \\\\frac{\\\\boldsymbol\\\\alpha}{\\\\alpha_0} = \\\\left(\\\\frac{\\\\alpha_1}{\\\\alpha_0},\\\\ldots,\\\\frac{\\\\alpha_K}{\\\\alpha_0}\\\\right),\\nq(1)=α0\\u200bα\\u200b=(α0\\u200bα1\\u200b\\u200b,…,α0\\u200bαK\\u200b\\u200b),где α0=∑i=1Kαi\\\\alpha_0 = \\\\sum\\\\limits_{i=1}^K\\\\alpha_iα0\\u200b=i=1∑K\\u200bαi\\u200b. Это корректное распределение вероятностей, так как его компоненты неотрицательны и в сумме дают 111. Будем производить следующий процесс:\\n\\nВ первый момент генерируем одну из категорий с помощью распределения q(1)\\\\boldsymbol q^{(1)}q(1); допустим, выпала i1i_1i1\\u200b-я. Обновляем вероятностное распределение на категориях, прибавив единицу к i1i_1i1\\u200b-й компоненте вектора α\\\\boldsymbol\\\\alphaα; получаем вектор α(2)\\\\boldsymbol\\\\alpha^{(2)}α(2).\\nНа nnn-м шаге генерируем одну из категорий с помощью распределения q(n)=α(n)∑iαi(n)\\\\boldsymbol q^{(n)} = \\\\frac{\\\\boldsymbol \\\\alpha^{(n)}}{\\\\sum\\\\limits_i\\\\alpha^{(n)}_i}q(n)=i∑\\u200bαi(n)\\u200bα(n)\\u200b. Допустим, выпала ini_nin\\u200b-я. Обновляем вероятностное распределение на категориях, прибавив единицу к ini_nin\\u200b-й компоненте вектора α(n)\\\\boldsymbol\\\\alpha^{(n)}α(n); получаем вектор α(n+1)\\\\boldsymbol\\\\alpha^{(n+1)}α(n+1).\\n\\nМожно доказать, что вектор lim\\u2061n→∞q(n)\\\\lim\\\\limits_{n\\\\to\\\\infty} \\\\boldsymbol q^{(n)}n→∞lim\\u200bq(n) подчиняется распределению Дирихле Dir(α)\\\\mathrm{Dir}(\\\\boldsymbol \\\\alpha)Dir(α).\\nЧтобы стало чуть понятнее, проследим, что будет при различных α\\\\boldsymbol\\\\alphaα.\\n\\nЕсли α=(10,10,10)\\\\boldsymbol\\\\alpha = (10,10,10)α=(10,10,10), то прибавление единицы будет не так сильно смещать вероятности, и дальше мы будем продолжать генерировать категорию из распределения, близкого к равномерному. Скорее всего, в пределе мы будем получать что-то, близкое к (13,13,13)(\\\\frac13, \\\\frac13,\\\\frac13)(31\\u200b,31\\u200b,31\\u200b).\\nЕсли α=(1,1,20)\\\\boldsymbol\\\\alpha = (1,1,20)α=(1,1,20), то почти наверняка мы будем генерить третью категорию, причём со всё большей вероятностью (ведь при этом мы будем увеличивать α3(n)\\\\alpha^{(n)}_3α3(n)\\u200b), то есть в пределе будет (почти 000, почти 000, почти 111).\\nЕсли α=(0.1,0.1,0.1)\\\\boldsymbol\\\\alpha = (0.1,0.1,0.1)α=(0.1,0.1,0.1), то та категория, которую мы сгенерировали на первом шаге, сразу вырвется вперёд и скорее всего будет доминировать в дальнейшем. Таким образом, нам следует ожидать в пределе векторов, в которых одна из компонент почти 111, а остальные почти 000. Важным отличием от предыдущего варианта является то, что здесь почти 111 может быть в любой компоненте.\\nЕсли α=(1,1,1)\\\\boldsymbol\\\\alpha = (1,1,1)α=(1,1,1), то соответствующее распределение Дирихле будет равномерным.\\n\\nТакже вам может оказаться полезна визуализация плотности этого распределения при разных α\\\\boldsymbol\\\\alphaα:\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф16.3. Вероятностные распределенияСледующий параграф16.5. Независимость и условные распределения вероятностейЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_49.html', 'title': 'Введение в теорию глубокого обучения'}, page_content=\"Введение в теорию глубокого обученияЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/613.1.Введение в теорию глубокого обучения13.2.Обобщающая способность – классическая теория13.3.PAC-байесовские оценки риска13.4.Сети бесконечной ширины13.5.Ландшафт функции потерь13.6.Implicit bias14.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Введение в теорию глубокого обучения13.1. Введение в теорию глубокого обученияАвторыГоликов ЕвгенийЦентральной теоретической проблемой обучения с учителем является проблема обобщающей способности.\\nВ самом деле, как можно гарантировать, что модель, обученная на некотором наборе данных, будет показывать хорошие результаты на данных, которых в обучении не было? Для классических моделей было доказано много содержательных результатов, которые, может быть, не давали ответы на все вопросы, но позволяли многое понять о работе моделей. Что же касается нейросетей, для них теория ещё только создаётся, и в этом разделе мы познакомим вас с рядом направлений развития этой науки.\\nНейронная сеть фиксированной архитектуры реализует некоторый класс моделей F\\\\mathcal{F}F. Например, разные элементы этого класса могут соответствовать различным наборам весов. Когда такой класс фиксирован, мы обычно решаем задачу минимизации некоторой функции ошибки или, как чаще говорят в теории ML, задачу минимизации эмпирического риска\\nR^m(f)=Ex,y∈Smr(y,f(x))\\\\hat R_m(f) = \\\\mathbb{E}_{x,y \\\\in S_m} r(y,f(x))\\nR^m\\u200b(f)=Ex,y∈Sm\\u200b\\u200br(y,f(x))среди моделей fff из класса F\\\\mathcal{F}F, где SmS_mSm\\u200b – обучающий датасет из mmm примеров, выбранных независимо из распределения данных D\\\\mathcal{D}D, а r(y,y^)r(y,\\\\hat y)r(y,y^\\u200b) – функция риска, например, I[y≠y^]\\\\mathbb{I}[y \\\\ne \\\\hat y]I[y\\ue020=y^\\u200b].Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nНаша цель, однако, минимизировать не эмпирический, а истинный риск, то есть\\nR(f)=Ex,y∼Dr(y,f(x)),R(f) = \\\\mathbb{E}_{x,y \\\\sim \\\\mathcal{D}} r(y,f(x)),\\nR(f)=Ex,y∼D\\u200br(y,f(x)),где математическое ожидание берётся по распределению данных, а не по выборке (математическое ожидание риска на всех мыслимых данных, не только на выборке). К сожалению, в рамках задачи обучения с учителем доступа к истинному распределению данных у нас нет, поэтому минимизировать истинный риск напрямую не удаётся, но мы можем попробовать его оценить. Часто для этого используют риск на валидации, но в этом разделе учебника мы постараемся получить теоретические оценки.\\nПусть f^m\\\\hat f_mf^\\u200bm\\u200b – модель из класса F\\\\mathcal{F}F, которую мы построили исходя из выборки SmS_mSm\\u200b. Интересно оценить, насколько её истинный риск отличается от эмпирического, то есть оценить разность\\nR(f^m)−R^m(f^m).R(\\\\hat f_m) - \\\\hat R_m(\\\\hat f_m).\\nR(f^\\u200bm\\u200b)−R^m\\u200b(f^\\u200bm\\u200b).В случае нейронных сетей очень трудно предсказать, к какой именно модели f^m\\\\hat f_mf^\\u200bm\\u200b сойдётся наш метод обучения (например, градиентный спуск) на данной выборке SmS_mSm\\u200b. Тем не менее, разницу рисков всегда можно оценить сверху супремумом по всем моделям из класса:\\nR(f^m)−R^m(f^m)≤sup\\u2061f∈F(R(f)−R^m(f)).R(\\\\hat f_m) - \\\\hat R_m(\\\\hat f_m) \\\\leq \\\\sup_{f \\\\in \\\\mathcal{F}} (R(f) - \\\\hat R_m(f)).\\nR(f^\\u200bm\\u200b)−R^m\\u200b(f^\\u200bm\\u200b)≤f∈Fsup\\u200b(R(f)−R^m\\u200b(f)).В этом случае риск можно оценить сверху величиной\\nR~:=R^m+sup\\u2061f∈F(R(f)−R^m(f)).\\\\tilde R := \\\\hat R_m + \\\\sup_{f \\\\in \\\\mathcal{F}} (R(f) - \\\\hat R_m(f)).\\nR~:=R^m\\u200b+f∈Fsup\\u200b(R(f)−R^m\\u200b(f)).Такие оценки называются равномерными (uniform bounds); мы рассмотрим их подробно в соответствующем параграфе.\\nПонятно, что подобная оценка становится бесполезной (vacuous), если в классе содержится модель, которая идеально работает на фиксированной выборке SmS_mSm\\u200b (R^m(f)=0\\\\hat R_m(f) = 0R^m\\u200b(f)=0), но на какой-либо другой (потенциально тестовой) выборке из тех же данных работает плохо (R(f)R(f)R(f) велик). Так, известно, что модели класса VGG способны выучить ImageNet даже с перемешанными метками классов (см. статью Understanding deep learning requires rethinking generalization). Понятно, что истинный риск у такой модели будет близок к риску случайного угадывания. «Плохую» модель можно построить следующим образом. Пусть A\\\\mathcal{A}A – наш исходный алгоритм обучения, например, градиентный спуск. Он принимает на вход выборку и выдаёт обученную модель. Возьмём датасет, составленный из двух частей:\\n\\n\\nSmS_mSm\\u200b – это самая обычная выборка, в которой объекты насэмплированы из распределения D\\\\mathcal{D}D,\\n\\n\\nS~M\\\\tilde S_MS~M\\u200b – выборка, объекты которой сгенерированы из того же распределения, но метки перепутаны.\\n\\n\\nРассмотрим модель f~m,M=A(Sm∪S~M)\\\\tilde f_{m,M} = \\\\mathcal{A}(S_m \\\\cup \\\\tilde S_M)f~\\u200bm,M\\u200b=A(Sm\\u200b∪S~M\\u200b). Чем больше MMM будет по сравнению с mmm, тем ближе будет построенная модель к случайному угадыванию. При этом, если суммарный размер двух выборок не слишком велик, то наша модель сможет запомнить их обе, в частности, SmS_mSm\\u200b. Таким образом, эмпирический риск R^m(f~m)\\\\hat R_m(\\\\tilde f_m)R^m\\u200b(f~\\u200bm\\u200b) такой модели окажется мал, а истинный – велик.\\nИнтуитивно понятно, что чем «сложнее» класс F\\\\mathcal{F}F, тем больше шансов найти в нём подобную модель. Одной из классических мер сложности класса моделей является размерность Вапника-Червоненкиса, или VC-размерность, предложенная в 1971 году в статье В. Н. Вапника и А. Я. Червоненкиса «О равномерной сходимости частот появления событий к их вероятностям». Она даёт следующую равномерную оценку:\\nsup\\u2061f∈F(R(f)−R^m(f))≤O(VC(F)log\\u2061VC(F)m).\\\\sup_{f \\\\in \\\\mathcal{F}} (R(f) - \\\\hat R_m(f)) \\\\leq O\\\\left(\\\\sqrt{\\\\frac{\\\\mathrm{VC}(\\\\mathcal{F})\\\\log{\\\\mathrm{VC}(\\\\mathcal{F})}}{m}}\\\\right).\\nf∈Fsup\\u200b(R(f)−R^m\\u200b(f))≤O(mVC(F)logVC(F)\\u200b\\u200b).Как и следовало ожидать, правая часть растёт со сложностью модели и падает с размером выборки.\\nИзвестно, что для полносвязных сетей VC-размерность растёт как Θ(nN)\\\\Theta(n N)Θ(nN), где nnn – ширина сети (число нейронов в слое), а NNN – общее число параметров; см. статью Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks. Рассмотрим полносвязную сеть с одним скрытым слоем. Тогда общее число параметров сети пропорционально ширине, а значит, VC-размерность пропорциональна квадрату ширины. Соответствующая оценка на истинный риск принимает вид:\\nR~(n)≤R^m(n)+Cnm,\\\\tilde R(n) \\\\leq \\\\hat R_m(n) + \\\\frac{C n}{\\\\sqrt{m}},\\nR~(n)≤R^m\\u200b(n)+m\\u200bCn\\u200b,где CCC – константа из равномерной оценки разницы рисков с помощью VC-размерности, а R^m(n)\\\\hat R_m(n)R^m\\u200b(n) – эмпирический риск сети ширины nnn, обученной на данной выборке SmS_mSm\\u200b.\\nКак правило, эмпирический риск монотонно убывает с ростом ширины, пока не достигнет нуля (в самом деле, чем больше ширина, тем сложнее класс моделей и тем больше шансов обнаружить в нём модель, запоминающую фиксированную выборку). В результате R~(n)\\\\tilde R(n)R~(n) может вести себя немонотонно: у этой величины может обнаружиться минимум строго левее точки, где R^m(n)\\\\hat R_m(n)R^m\\u200b(n) впервые достигает нуля:\\n\\n\\n\\nСхематическое изображение изменения эмпирического риска $\\\\hat R_m$ (синяя кривая) и предсказанного риска $\\\\tilde R$ (красная кривая) в зависимости от ширины сети.\\n\\n\\nПравее минимума предсказанный риск монотонно растёт. Но оказывается, что реальный истинный риск, напротив, убывает, выходя на асимптоту:\\n\\n\\n\\nЗависимость эмпирического риска $\\\\hat R_m$ (бирюзовая кривая) и истинного риска $R$ (синяя кривая) для полносвязной сети с одним скрытым слоем, обученной на наборе данных CIFAR10, в зависимости от ширины сети; подробности см. в работе B. Neyshabur, R. Tomioka, N. Srebro, In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning.\\n\\n\\nВозникает вопрос: если у истинного риска есть асимптота, то как ведут себя нейронные сети в пределе бесконечной ширины? Мы остановимся на этом вопросе в соответствующем параграфе.\\nУпомянутый выше эксперимент с перемешиванием меток классов на части обучающей выборки можно рассматривать как модификацию не данных, а алгоритма обучения. А именно, давайте представим алгоритм, который, получая на вход выборку SmS_mSm\\u200b, сделает с ней следующее:\\n\\nКаким-то образом делит её на две части: Sm=Sm1′∪Sm2′′S_m = S_{m_1}'\\\\cup S_{m_2}''Sm\\u200b=Sm1\\u200b′\\u200b∪Sm2\\u200b′′\\u200b,\\nЗаменяет в Sm2′′S_{m_2}''Sm2\\u200b′′\\u200b метки на случайные.\\nОбучает модель на объединении Sm1′S_{m_1}'Sm1\\u200b′\\u200b и «испорченной» Sm2′′S_{m_2}''Sm2\\u200b′′\\u200b.\\n\\nТакой «испорченный» алгоритм обучения приводит к модели, которая запоминает Sm1′S_{m_1}'Sm1\\u200b′\\u200b и поэтому на исходной обучающей выборке SmS_mSm\\u200b работает не так уж и плохо. Но на тестовых данных он показывает качество, сравнимое со случайным угадыванием. Работающие на практике алгоритмы, например, градиентный спуск, тоже могут обучить модель, которая «запомнила» обучающую выборку. Тем не менее, на тестовой выборке обученная модель будет давать нормальное качество (см. опять же вторую картинку в начале параграфа). Возникает вопрос: почему так? Почему среди всех конфигураций весов, для которых риск на обучающей выборке равен нулю, градиентный спуск не выбирает те, для которых истинный риск сравним со случайным угадыванием? Явление, при котором среди всех эквивалентных по эмпирическому риску решений алгоритм выбирает определённые, называется implicit bias, и будет рассмотрен в соответствующем параграфе.\\nЕсли даже известно, какие конфигурации весов «предпочитает» наш алгоритм обучения, это никак не повлияет на равномерную оценку разницы рисков. В параграфе про PAC-байесовские оценки будет рассмотрен класс оценок, которые позволяют учесть предпочтения алгоритма. А именно, пусть обученная модель случайна (это действительно так из-за случайности инициализации весов и, например, шума стохастического градиентного спуска), то есть алгоритм строит распределение на моделях – назовём его апостериорным распределением. Пусть дано другое распределение, не зависящее от выборки, назовём его априорным. Тогда роль сложности в наших оценках будет играть расстояние Кульбака-Лейблера (KL-дивергенция) между апостериорным и априорным распределениями на моделях. Если априорное распределение покрывает «предпочтительные» решения и не покрывает остальные, то KL-дивергенция мала и оценка разницы рисков невелика. Из-за внешней схожести некоторых величин, возникающих в этой теории, с объектами из байесовской статистики, такие оценки называется PAC-байесовскими (PAC-bayesian bounds, от probably approximately correct).\\nВыше мы негласно предполагали, что используемый алгоритм обучения успешно решает задачу минимизации эмпирического риска. Для нейронных сетей наиболее популярный алгоритм – градиентный спуск или его разновидности. Если бы функция потерь была выпуклой как функция от весов сети, то это гарантировало бы сходимость в глобальный минимум. В общем случае теория оптимизации не даёт таких гарантий. Тем не менее, для сетей реалистичного размера градиентный спуск успешно сходится в глобальный минимум, что толкает нас на предположение о том, что все локальные минимумы таких сетей глобальны. Это предположение действительно можно доказать в некоторых частных случаях; см. параграф про ландшафт функции потерь.\\nВ качестве необходимого дополнения следует также гарантировать, что градиентный спуск (или его разновидности) не сходится в возможные седловые точки. При определённых условиях на минимизируемую функцию можно доказать, что для сходимости в седловую точку необходимо инициализировать функцию на множестве меры ноль. Более подробно вы можете почитать в работе Gradient descent only converges to minimizers или в её обобщениях Gradient Descent Only Converges to Minimizers: Non-Isolated Critical Points and Invariant Regions и On the almost sure convergence of stochastic gradient descent in non-convex problems.\\nВпрочем, гарантии сходимости, как правило, формулируются для фиксированной архитектуры сети и ничего не говорят о скорости сходимости. Хотелось бы также иметь гарантии на то, что какой бы широкой или глубокой сеть не была, градиентный спуск сойдётся в минимум за разумное время. А для этого необходимо понимать, что на самом первом шаге градиентного спуска градиент не будет гаснуть или «взрываться» с ростом ширины или глубины. Известен ряд эвристик для инициализации весов, помогающих с этим бороться: например, инициализации Глоро (Xavier Glorot) и Хе (Kaiming He). Подробнее про них вы можете прочитать в параграфе про тонкости обучения нейросетей.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф12.1. Bias-variance decompositionКлассический взгляд на\\xa0то, почему слишком сложные модели переобучаютсяСледующий параграф13.2. Обобщающая способность – классическая теорияЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_2.html', 'title': 'Об этой книге'}, page_content='Об этой книгеЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/31.1.Об этой книгеОб этой книге1.2.Первые шаги1.3.Машинное обучение2.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Об этой книге1.1. Об этой книгеАвторыФедотов СтаниславСиницин ФилиппЭта книга написана коллективом добрых людей, состоящим из\\xa0преподавателей и\\xa0выпускников Школы анализа данныхОб этой книге\\nЭта книга написана коллективом добрых людей, состоящим из преподавателей и выпускников Школы анализа данных. Своим появлением она обязана двум замечательным курсам. Во-первых, это курс Константина Вячеславовича Воронцова, на котором выросло подавляющее большинство авторов книги, да и вообще ML-специалистов в России. Во-вторых, это курс NLP Course | For You Лены Войта, благодаря которому мы поняли, как должен выглядеть современный учебник, и на который мы будем регулярно ссылаться в частях, связанных с анализом текстов.\\nИдея была такая: записать сложившийся в ШАДе курс машинного обучения в виде книги, при этом избежав каких-либо компромиссов: нигде ничего не упрощать чрезмерно, дать необходимую теорию, описать и исторически важные алгоритмы, и применяющиеся сегодня, вместе с теорией рассказывать и практические вопросы о реализации алгоритмов и работе с данными.\\nМатематика — это один из языков, на котором написан учебник. Мы будем стараться давать необходимые пояснения, но всё же уверенное владение линейной алгеброй, математическим анализом и теорией вероятностей будет большим плюсом. Знания статистики и методов выпуклой оптимизации не обязательны, хотя сделают чтение комфортнее.\\nЧитая книгу, вы, возможно, заметите в ней ошибки, неточности и плохо объяснённые детали. В таком случае, пожалуйста, дайте нам знать об этом, написав (сюда) — так вы поможете и другим читателям.\\nИтак, приступим.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанСообщить об ошибкеСледующий параграф1.2. Первые шагиВ этой главе мы поговорим о рабочем окружении ML-специалиста — какие сервисы и библиотеки в него входят, как его развернуть, на что обратить внимание.\\xa0\\nА кроме того, в качестве быстрой практики обучим собственную модель генерировать ответы в стиле Льва Толстого.Следующий параграф1.3. Машинное обучениеЧто такое машинное обучение и каким оно бывает. Основные понятия машинного обучения: признаки, таргеты, метрики, переобучениеВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.ВступитьЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_24.html', 'title': 'Свёрточные нейросети'}, page_content='Свёрточные нейросетиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/56.1.Свёрточные нейросетиФормат данныхMLPСвёрткиСвёрточный слой и обратное распространение ошибкиОстальные важные блоки свёрточных нейронных сетейРегуляризацияБонус №1: знаковые архитектуры в мире свёрточных нейронных сетей для задачи классификации изображенийБонус №2: не классификацией единойИтого6.2.Нейросети для работы с последовательностями6.3.Трансформеры6.4.Графовые нейронные сети6.5.Нейросети для облаков точек7.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Свёрточные нейросети6.1. Свёрточные нейросетиАвторыИванов ГригорийВ этом параграфе мы на примере задачи распознавания изображений познакомимся со свёрточными нейронными сетями, уже ставшими стандартом в области. Для начала мы разберёмся, с какого рода данными придётся работать, затем попробуем решить задачу «в лоб» при помощи знакомых вам полносвязных сетей и поймём, чем это чревато. А после чего рассмотрим свёртки и попробуем выработать нужную интуицию.\\nФормат данных\\nКартинки в большинстве случаев представляют собой упорядоченный набор пикселей, где каждый пиксель — это вектор из трех «каналов»: интенсивность красного, интенсивность зелёного, интенсивность синего.\\n\\nКаждая интенсивность характеризуется числом от 0 до 1, но для привычных нам изображений этот интервал равномерно дискретизирован для экономии памяти, чтобы уместиться в 8 бит (от 0 до 255). При этом нулевая интенсивность (0, 0, 0), соответствует чёрному цвету, а максимальная интенсивность (255, 255, 255) — белому.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nКогда мы наблюдаем изображение на мониторе компьютера, мы видим эти пиксели «уложенными» в строки одинаковой длины (человек не сможет воспринять картинку, вытянутую в один вектор). Длину каждой такой строки называют шириной W картинки, а количество строк — высотой H. Резюмирую, мы можем рассматривать картинку, как тензор HxWx3, состоящий из чисел uint8.\\n\\nСуществует множество разных форматов хранения картинок: вместо трех интенсивностей мы можем использовать триплет «оттенок, насыщенность, интенсивность», а сами картинки хранить, например, как тензор CxHxW.\\nMLP\\nНаверное, самый простой способ построить нейронную сеть для решения задачи классификации на наших данных — это «развернуть» нашу картинку в вектор, а затем использовать обычную многослойную сеть с кросс-энтропией в качестве лосса.\\nОднако, такой подход имеет несколько недостатков.\\nНедостаток №1: количество параметров\\nВ первом слое у нас получается HxWxCxCout параметров, где Cout — это количество нейронов в первом слое. Если поставить Cout слишком маленьким, мы рискуем потерять много важной информации, особенно, если рассматривать картинки размером, например, 1920x1080.\\nЕсли же выставить Cout большим, рискуем получить слишком много параметров (а это только первый слой), а с этим и все вытекающие проблемы — переобучение, сложность оптимизации и так далее.\\nНедостаток №2: структура данных никак не учитывается.\\nЧто здесь имеется в виду под «структурой»? Попробуем объяснить на примере. Для этого рассмотрим картинку щеночка:\\n\\nЕсли мы сдвинем картинку на несколько пикселей, то мы все еще будем уверены в том, что это щенок:\\n\\nТочно также мы останемся неизменны в своем мнении, если картинку отмасштабировать:\\n\\nили повернуть/развернуть:\\n\\n\\n\\nПолучается, что нейронная сеть должна «сама» понять, что ее ответ должен быть инвариантен к описанным преобразованиям. Но, обычно, это достигается за счет увеличения количества нейронов в скрытых слоях (как мы можем помнить из universal approximation theorem), что и так для нас — головная боль из-за первого пункта.\\nС частью этих проблем нам поможет новый «строительный блок» — свёртка. О ней в следующем разделе.\\nСвёртки\\nСтрогое определение свёртки мы дадим ниже, а вначале разберёмся в мотивации.\\nДавайте попробуем решить хотя бы проблему инвариантности к сдвигу. Щенок может быть где угодно на картинке, и мы не можем наверняка сказать, в какой части изображения наша модель «лучше всего» научилась видеть щенков. Поэтому для надёжного предсказания будет логично посдвигать картинку на все возможные смещения (пустоты заполним нулями):\\n\\nЗатем для каждого смещения мы предскажем вероятность наличия щенка на картинке. Получившиеся предсказания можно уже агрегировать как удобно: среднее, максимум и так далее.\\nДавайте взглянем на эту операцию под другим углом. Рассмотрим картинку, размером в 3 раза превышающую оригинальную, в центре которой находится наше изображение щеночка:\\n\\nВозьмём окно размером с исходную картинку, и будем его сдвигать на все возможные смещения внутри нового изображения:\\n\\nЛегко видеть, что получается то же самое, как если бы мы картинку сдвигали относительно окна.\\nПредставим себе самую простую модель, основанную на данном принципе — что-то вроде ансамбля линейных. Каждую из сдвинутых картинок вытянем в вектор и скалярно умножим на вектор весов (для простоты один и тот же для всех сдвигов) — получим линейный оператор, для которого есть специальное имя — свёртка.\\nЭто один из важнейших компонент в свёрточных нейронных сетях. Веса свёртки, упорядоченные в тензор (в нашем случае размерности HxWx3), составляют её ядро. Область картинки, которая обрабатывается в текущий момент, обычно называется окном свёртки.\\nОбратите внимание, что обычно такие свёртки называются двумерными — так как окно свёртки пробегает по двум измерениям картинки (при этом все цветовые каналы участвуют в вычислениях).\\nСледующая картинка поможет разобраться (внимание: на ней нет изображения весов оператора):\\n\\nКаждый «кубик» на картинке — это число. Большой черный тензор слева — это изображение щеночка XXX. Фиолетовым на нем выделено окно, из которого мы достаем все пиксели и разворачиваем в вектор (аналогично операции flatten в numpy) vvv.\\nДалее этот вектор умножается на вектор весов класса «щенок» w1w_1w1\\u200b, и получается число k1k_1k1\\u200b — логит интересующего класса. Добавив остальные классы, получим матрицу весов WWW — прямо как в мультиномиальной логистической регрессии. Эту операцию мы повторяем для каждого возможного сдвига окна свёртки.\\nРезультаты домножения удобно бывает скомпоновать в двумерную табличку, которую при желании можно трактовать, как некоторую новую картинку (в серых тонах, потому что канал уже только один). Воспользуемся этим, чтобы получше осознать, что происходит в ходе свёртки.\\nВопрос на подумать. Какой геометрический смысл имеет свёртка с ядром\\nB1=19(111111111)?B_1 = \\\\frac19\\\\begin{pmatrix}1 & 1 & 1\\\\\\\\ 1 & 1 & 1\\\\\\\\ 1 & 1 & 1\\\\end{pmatrix}?\\nB1\\u200b=91\\u200b\\u200b111\\u200b111\\u200b111\\u200b\\u200b?А с ядром\\nB2=(−1−1−1−18−1−1−1−1)?B_2 = \\\\begin{pmatrix}-1 & -1 & -1\\\\\\\\ -1 & 8 & -1\\\\\\\\ -1 & -1 & -1\\\\end{pmatrix}?\\nB2\\u200b=\\u200b−1−1−1\\u200b−18−1\\u200b−1−1−1\\u200b\\u200b?Ответ (не открывайте сразу; сначала подумайте сами!)Первая свёртка усредняет каждый пиксель с соседними, и изображение размывается. Смысл второй можно грубо описать так: пиксели из однородных участков изображения слабеют, тогда как контрастные точки, напротив, усиливаются. Можно сказать, что такая свёртка выделяет границы. Проиллюстрируем работу этих ядер на примере небольшого изображения в серых тонах:\\n\\nВ донейросетевую эпоху различные свёртки играли существенную роль в обработке изображений, и сейчас мы видим, почему.\\nВопрос на подумать. На краях картинок из ответа к предыдущему вопросу заметны тёмные рамки. Что это такое? Откуда они берутся?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Их появление связано с особенностями обработки свёртками краёв изображения. Вообще, есть несколько стратегий борьбы с краями. Например:\\n\\nДополнить изображение по краям нулями. Когда мы будем рассматривать окна свёртки с центрами в крайних пикселях, они будут захватывать эти нули. Такая свёртка будет превращать изображение размером HxWx3 в изображение размером HxW, без уменьшения размера. Но так как нули соответствуют чёрному цвету, это будет вносить определённые изменения в крайние пиксели результата. Именно благодаря этому у картинок из предыдущего вопроса на подумать по краям появились тёмные рамки.\\nРазрешить только такие окна, которые целиком лежат внутри изображения. Это будет приводить к падению размера. Например, для окна размером 5x5 картинка размером HxWx3 превратится в картинку размером (H-2)x(W-2).\\n\\nРешив проблему обеспечения устойчивости к сдвигу картинки и имея на руках наш огромный свёрточный фильтр, давайте попробуем теперь справиться с первой проблемой — количество параметров. Самое простое, что можно придумать, — это уменьшить размер окна с HxW до, допустим, kxk (обычно нечётное и k∈[3,11]k \\\\in [3,11]k∈[3,11]). В этом случае получается радикальное снижение количества параметров и сложности вычислений.\\n\\nК сожалению, с таким подходом возникает новая проблема: предсказание для какого-то окна никак не учитывает контекст вокруг него. Получается, мы можем получить разумные предсказания только в случае, если распознаваемый объект обладает признаками, которые «помещаются» в окно свёртки (например, лого автомобиля при классификации марок машин), либо объекты заметно отличаются по своей текстуре (шерсть кошки vs кирпич, например).\\nНа картинке ниже сделана попытка изобразить проблему:\\n\\nОбласть картинки, на которую «смотрит» наша нейронная сеть, называется receptive field — и про него приходится часто думать в задачах компьютерного зрения.\\nДавайте и мы подумаем, как его можно было бы увеличить, не увеличивая размер ядра. Вспомним, что в нашей нейронке сейчас есть только один слой, сразу предсказывающий класс. Выглядит так, что мы можем применить уже знакомую технику стекинга слоев: пусть на первой стадии мы делаем C1C_1C1\\u200b разных свёрток с фильтрам размером kxk. Результаты каждой свёртки можно упорядочить в виде новой «картинки», а из этих «картинок» сложить трёхмерный тензор. Получаем так называемую карту признаков размером HxWxC_1.\\nПрименим к ней поэлементно нелинейность и воспользуемся K новыми свёртками для получения предсказаний для каждого пикселя. На таком шаге получается, что наш receptive field для финальных нейронов вырос от kxk до (2k-1)x(2k-1) (пояснение на картинке).\\nПовторяя такую операцию, мы можем добиться, чтобы наши финальные нейроны уже могли «видеть» почти всю нужную информацию для хорошего предикта. Более того, у нас возникает меньшее количество параметров и падает сложность вычислений в сравнении с использованием одной большой полносвязной сети.\\nКак это схематично выглядит:\\n\\nПромежуточный тензор L1L_1L1\\u200b, полученный при помощи C1C_1C1\\u200b свёрток, можно себе представить, как новую картинку, у которой уже C1C_1C1\\u200b каналов.\\nНа следующей картинке можно отследить, как меняется receptive field в зависимости от глубины:\\n\\nНа картинке схематично изображен «плоский» двумерный тензор (количество каналов = 1), к которому последовательно применили три свёртки 3x3. В каждом случае рассматривается пиксель в центре. Каждый соответствующий тензор помечен, как LiL_iLi\\u200b. Если рассматривать первую свёртку (X→L1X\\\\to L_1X→L1\\u200b), то размер receptive field равен размеру е окна = 3.\\nРассмотрим вторую свёртку L1→L2L_1 \\\\to L_2L1\\u200b→L2\\u200b. В ее вычислении участвуют пиксели из квадрата 3х3, причём каждый из них, в свою очередь, был получен при помощи предыдущей свёртки X→L1X \\\\to L_1X→L1\\u200b. Получается, что receptive field композиции свёрток X→L1→L2X\\\\to L_1\\\\to L_2X→L1\\u200b→L2\\u200b — это объединение receptive fields свёртки X→L1X\\\\to L_1X→L1\\u200b по всем пикселям из окна свёртки L1→L2L_1\\\\to L_2L1\\u200b→L2\\u200b, образуя новый, размером 5x5. Аналогичные рассуждения можно повторить и для всех последующих свёрток.\\nЕщё один способ увеличить receptive field — это использовать dilated convolution, в которых окно свёртки (то есть те пиксели картинки, на которые умножается ядро) не обязано быть цельным, а может идти с некоторым шагом (вообще говоря, даже разным по осям H и W).\\nПроиллюстрируем, как будет выглядеть окно для обычной свёртки и для свёртки с шагом dilation=2:\\n\\nЕсли установить параметр dilation=(1,1), получится обычная свёртка.\\nИтак, свёртки помогли нам решить сразу две проблемы: устойчивости к сдвигу и минимизации числа параметров. Теперь давайте попробуем определить оператор более формально.\\nФормальное определение свёртки\\n\\nВопрос на подумать. Пусть у нас есть тензор размером HxWxC_{in}, к которому одновременно применяется CoutC_{out}Cout\\u200b свёрток, размер окна каждой равен kxk. Посчитайте количество обучаемых параметров. Как изменится формула, если к свёртке добавить смещение (bias)? Во сколько раз изменится количество параметров, если увеличить размер окна в 2 раза? А если увеличить количество каналов CinC_{in}Cin\\u200b и CoutC_{out}Cout\\u200b в два раза? А если увеличить размер входного тензора в 2 раза по высоте и ширине?\\nВопрос на подумать. Оцените количество операций сложений-умножений для предыдущего упражнения. Как оно поменяется, если увеличить в два раза размер окна? Количество каналов? Размер входного тензора?\\nВопрос на подумать. Пусть последовательно применяется NNN свёрток k×kk \\\\times kk×k. Посчитайте размер receptive field для последнего оператора.\\nСвёртки не только для изображений\\nНетрудно видеть, что аналоги двумерной свёртки можно определить и для тензоров другой размерности, в любой ситуации, когда для нас актуально поддерживать устойчивость модели к сдвигам данных. Например, это актуально для работы с текстами. Обычно текст разбивается на последовательные токены (например, на слова или какие-то subword units), и каждому из этих токенов ставится в соответствие вектор (более подробно об этом вы можете почитать в параграфе про работу с текстами или в разделе про вложения слов учебника по NLP Лены Войта).\\n\\nПредставим теперь, что мы хотим определить, позитивно или негативно окрашен этот текст. Мы можем предположить, что эмоциональная окраска локальна и может проявляться на любом участке текста, и тогда нам нужна модель, которая может «посмотреть» отдельно на каждый последовательный фрагмент текста некоторой длины. И здесь тоже может сработать свёртка:\\n\\nСуществуют свёртки и для тензоров более высокой размерности, например, для видео (где прибавляется ещё координата «время»).\\nПоворот, отражение, масштабирование\\nА что делать с остальными проблемами: поворотом, отражением, масштабированием? К сожалению, на момент написания параграфа (вторая половина 2021 года), автору не было известно об успешном опыте решения этих проблем в архитектуре сети. При этом оказывается, что приведенного оператора уже достаточно, чтобы нейронная сеть могла хорошо обобщать на невиданные ранее картинки (лишь бы свёрток было больше и сеть глубже).\\nВ качестве потенциально интересного (но пока не проявившего себя на практике) направления исследований можно упомянуть капсульные нейросети. Кроме того, вам может быть интересно познакомиться с геометрическим глубинным обучением. В качестве короткого введения рекомендуем посмотреть вот этот keynote с ICLR 2021, которое ставит своей целью исследование общих принципов, связывающих устойчивость к различным преобразованием и современные нейросетевые архитектуры (авторы сравнивают свои идеи с эрлангенской программой Феликса Кляйна — отсюда название).\\nСвёрточный слой и обратное распространение ошибки\\nПоговорим о том, как через свёрточный слой протекают градиенты. Нам нужно будет научиться градиент по выходу превращать в градиент по входу и в градиент по весам из ядра.\\nНачнём с иллюстрации для одномерной свёртки с одним входным каналом, ядром длины 333 с дополнением по бокам нулями. Заметим, что её можно представить в виде матричного умножения:\\n(x1,…,xd)∗(w−1,w0,w1)=(x_1,\\\\ldots,x_d) \\\\ast (w_{-1},w_0,w_1) = \\n(x1\\u200b,…,xd\\u200b)∗(w−1\\u200b,w0\\u200b,w1\\u200b)==(0,x1,…,xd,0)⋅(w−1w0w−1w1w0w−1w1w0⋱w1⋱w−1⋱w0w1)== (0,x_1,\\\\ldots,x_d,0) \\\\cdot\\\\begin{pmatrix}\\nw_{-1} &  & & & \\\\\\\\\\nw_0 & w_{-1}  & & &  \\\\\\\\\\nw_1 & w_0  & w_{-1} & &  \\\\\\\\\\n& w_1  & w_0  & \\\\ddots &  \\\\\\\\\\n&  & w_1  & \\\\ddots & w_{-1}  \\\\\\\\\\n&  &   & \\\\ddots & w_0  \\\\\\\\\\n&  &   &  & w_1 \\\\\\\\\\n\\\\end{pmatrix} = =(0,x1\\u200b,…,xd\\u200b,0)⋅\\u200bw−1\\u200bw0\\u200bw1\\u200b\\u200bw−1\\u200bw0\\u200bw1\\u200b\\u200bw−1\\u200bw0\\u200bw1\\u200b\\u200b⋱⋱⋱\\u200bw−1\\u200bw0\\u200bw1\\u200b\\u200b\\u200b==(x1,…,xd)⋅(w0w−1w1w0w−1w1w0w−1w1w0⋱w1⋱w−1⋱w0w−1w1w0)== (x_1,\\\\ldots,x_d) \\\\cdot\\\\begin{pmatrix}\\nw_0 & w_{-1} &  & & & & \\\\\\\\\\nw_1 & w_0 & w_{-1}  & & & &  \\\\\\\\\\n& w_1 & w_0  & w_{-1} & &&  \\\\\\\\\\n& & w_1  & w_0  & \\\\ddots & &  \\\\\\\\\\n& &  & w_1  & \\\\ddots & w_{-1} &  \\\\\\\\\\n& &  &   & \\\\ddots & w_0 & w_{-1}  \\\\\\\\\\n& &  &   &  & w_1 & w_0 \\\\\\\\\\n\\\\end{pmatrix} = =(x1\\u200b,…,xd\\u200b)⋅\\u200bw0\\u200bw1\\u200b\\u200bw−1\\u200bw0\\u200bw1\\u200b\\u200bw−1\\u200bw0\\u200bw1\\u200b\\u200bw−1\\u200bw0\\u200bw1\\u200b\\u200b⋱⋱⋱\\u200bw−1\\u200bw0\\u200bw1\\u200b\\u200bw−1\\u200bw0\\u200b\\u200b\\u200b=Обозначим последнюю матрицу через W^\\\\widehat{W}W, а ядро свёртки через WWW. Что происходит с градиентом при переходе через матричное умножение, мы уже отлично знаем. Градиент по весам равен\\n∇X0L=∇X0∗WL⋅W^T\\\\nabla_{X_0}\\\\mathcal{L} = \\\\nabla_{X_0\\\\ast W}\\\\mathcal{L}\\\\cdot\\\\widehat{W}^T\\n∇X0\\u200b\\u200bL=∇X0\\u200b∗W\\u200bL⋅WTРазберёмся, что из себя представляет умножение на W^T\\\\widehat{W}^TWT справа. Эта матрица имеет вид\\n(w0w1w−1w0w1w−1w0w1w−1w0⋱w−1⋱w1⋱w0w1w−1w0)\\\\begin{pmatrix}\\nw_0 & w_1 &  & & & & \\\\\\\\\\nw_{-1} & w_0 & w_1  & & & &  \\\\\\\\\\n& w_{-1} & w_0  & w_1 & &&  \\\\\\\\\\n& & w_{-1}  & w_0  & \\\\ddots & &  \\\\\\\\\\n& &  & w_{-1}  & \\\\ddots & w_1 &  \\\\\\\\\\n& &  &   & \\\\ddots & w_0 & w_1  \\\\\\\\\\n& &  &   &  & w_{-1} & w_0 \\\\\\\\\\n\\\\end{pmatrix}\\u200bw0\\u200bw−1\\u200b\\u200bw1\\u200bw0\\u200bw−1\\u200b\\u200bw1\\u200bw0\\u200bw−1\\u200b\\u200bw1\\u200bw0\\u200bw−1\\u200b\\u200b⋱⋱⋱\\u200bw1\\u200bw0\\u200bw−1\\u200b\\u200bw1\\u200bw0\\u200b\\u200b\\u200bОна тоже соответствует свёртке, только:\\n\\nс симметричным исходному ядром (w1,w0,w−1)(w_1, w_0, w_{-1})(w1\\u200b,w0\\u200b,w−1\\u200b);\\nс дополнением вектора ∇X0∗W\\\\nabla_{X_0\\\\ast W}∇X0\\u200b∗W\\u200b нулями (это как раз соответствует неполным столбцам: можно считать, что «выходящие» за границы матрицы и отсутствующие в ней элементы умножаются на нули).\\n\\nВопрос на подумать. Поменяется ли что-нибудь, если исходный вектор не дополнять нулями?\\nОбщий случай\\nРассмотрим теперь двумерную свёртку, для простоты нечётного размера и без свободного члена\\n(X∗W)ijc=∑p=1cin∑k1=−kk∑k2=−kkWk+1+k1,k+1+k2,pcXi+k1,j+k1,p(X\\\\ast W)_{ijc} = \\\\sum_{p=1}^{c_{\\\\text{in}}}\\\\sum_{k_1 = -k}^k\\\\sum_{k_2=-k}^kW^{c}_{k+1+k_1, k+1+k_2, p}X_{i + k_1, j + k_1, p}\\n(X∗W)ijc\\u200b=p=1∑cin\\u200b\\u200bk1\\u200b=−k∑k\\u200bk2\\u200b=−k∑k\\u200bWk+1+k1\\u200b,k+1+k2\\u200b,pc\\u200bXi+k1\\u200b,j+k1\\u200b,p\\u200b\\nПродифференцируем по XstlX_{stl}Xstl\\u200b:\\n\\n∂L∂Xstl=∑i,j,c∂(X∗W)ijc∂Xstl⋅∂L∂(X∗W)ijc\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial X_{stl}} = \\\\sum_{i, j, c}\\\\frac{\\\\partial (X\\\\ast W)_{ijc}}{\\\\partial X_{stl}}\\\\cdot\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial(X\\\\ast W)_{ijc}}\\n∂Xstl\\u200b∂L\\u200b=i,j,c∑\\u200b∂Xstl\\u200b∂(X∗W)ijc\\u200b\\u200b⋅∂(X∗W)ijc\\u200b∂L\\u200bРазберёмся с производной ∂(X∗W)ijc∂Xstl\\\\frac{\\\\partial (X\\\\ast W)_{ijc}}{\\\\partial X_{stl}}∂Xstl\\u200b∂(X∗W)ijc\\u200b\\u200b. Во всей большой сумме из определения свёртки для (X∗W)ijc(X\\\\ast W)_{ijc}(X∗W)ijc\\u200b элемент XstlX_{stl}Xstl\\u200b может встретиться в позициях Xi+k1,j+k2,lX_{i+k_1, j+k_2, l}Xi+k1\\u200b,j+k2\\u200b,l\\u200b при i+k1=si + k_1 = si+k1\\u200b=s, j+k2=tj + k_2 = tj+k2\\u200b=t и всевозможных ccc, причём это возможно лишь если k1=s−i∈{−k,…,k}k_1 = s - i\\\\in\\\\{-k,\\\\ldots,k\\\\}k1\\u200b=s−i∈{−k,…,k}, k2=t−j∈{−k,…,k}k_2 = t - j\\\\in\\\\{-k,\\\\ldots,k\\\\}k2\\u200b=t−j∈{−k,…,k} (для всех остальных (X∗W)ijc(X\\\\ast W)_{ijc}(X∗W)ijc\\u200b производная по XstlX_{stl}Xstl\\u200b нулевая). Соответствующий коэффициент при XstlX_{stl}Xstl\\u200b будет равен Wk+1+k1,k+1+k2,cW_{k + 1 + k_1, k + 1 + k_2, c}Wk+1+k1\\u200b,k+1+k2\\u200b,c\\u200b. Таким образом, производная будет иметь вид:\\n∂L∂Xstl=∑c=1cout∑k1=−kk∑k2=−kkWk+1+k1,k+1+k2,c⋅∂L∂(X∗W)s−k1,t−k2,c\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial X_{stl}} = \\\\sum_{c=1}^{c_{\\\\text{out}}}\\\\sum_{k_1=-k}^k\\\\sum_{k_2=-k}^kW_{k + 1 + k_1, k + 1 + k_2, c}\\\\cdot\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial(X\\\\ast W)_{s - k_1, t - k_2, c}}\\n∂Xstl\\u200b∂L\\u200b=c=1∑cout\\u200b\\u200bk1\\u200b=−k∑k\\u200bk2\\u200b=−k∑k\\u200bWk+1+k1\\u200b,k+1+k2\\u200b,c\\u200b⋅∂(X∗W)s−k1\\u200b,t−k2\\u200b,c\\u200b∂L\\u200bЛегко заметить, что это тоже свёртка, но поскольку индексы k1,k2k_1, k_2k1\\u200b,k2\\u200b в WWW и в ∂L∂(X∗W)\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial(X\\\\ast W)}∂(X∗W)∂L\\u200b стоят с разными знаками, получаем, что\\n∇XL=W[::-1,::-1,:]∗∇X∗WL\\\\color{blue}{\\\\nabla_{X}\\\\mathcal{L} = W\\\\text{[::-1,::-1,:]}\\\\ast\\\\nabla_{X\\\\ast W}\\\\mathcal{L}}\\n∇X\\u200bL=W[::-1,::-1,:]∗∇X∗W\\u200bL\\nПродифференцируем по WabqW^q_{ab}Wabq\\u200b:\\n\\n∂L∂Wabq=∑i,j,c∂(X∗W)ijc∂Wabq⋅∂L∂(X∗W)ijc\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial W^q_{ab}} = \\\\sum_{i, j, c}\\\\frac{\\\\partial (X\\\\ast W)_{ijc}}{\\\\partial W^q_{ab}}\\\\cdot\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial(X\\\\ast W)_{ijc}}\\n∂Wabq\\u200b∂L\\u200b=i,j,c∑\\u200b∂Wabq\\u200b∂(X∗W)ijc\\u200b\\u200b⋅∂(X∗W)ijc\\u200b∂L\\u200bВ формуле для (X∗W)ijc(X\\\\ast W)_{ijc}(X∗W)ijc\\u200b элемент WabqW^q_{ab}Wabq\\u200b может встретиться в позициях Wk+1+k1,k+1+k2qW^q_{k + 1 + k_1, k + 1 + k_2}Wk+1+k1\\u200b,k+1+k2\\u200bq\\u200b, для k+1+k1=ak + 1 + k_1 = ak+1+k1\\u200b=a, k+1+k2=bk + 1 + k_2 = bk+1+k2\\u200b=b, с коэффициентами Xi+k1,j+k2,pX_{i + k_1, j + k_2, p}Xi+k1\\u200b,j+k2\\u200b,p\\u200b (для любых ppp). Значит, производная будет иметь вид:\\n∂L∂Wabq=∑p=1cin∑i=1H∑j=1WXa−k−1,b−k−1,p⋅∂L∂(X∗W)a−k−1,b−k−1,q\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial W^q_{ab}} = \\\\sum_{p=1}^{c_{\\\\text{in}}}\\\\sum_{i=1}^H\\\\sum_{j=1}^WX_{a - k - 1, b - k - 1, p}\\\\cdot\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial(X\\\\ast W)_{a - k - 1, b - k - 1, q}}\\n∂Wabq\\u200b∂L\\u200b=p=1∑cin\\u200b\\u200bi=1∑H\\u200bj=1∑W\\u200bXa−k−1,b−k−1,p\\u200b⋅∂(X∗W)a−k−1,b−k−1,q\\u200b∂L\\u200bВ этой формуле тоже нетрудно узнать свёртку:\\n∇WL=X∗∇X∗WL\\\\color{blue}{\\\\nabla_{W}\\\\mathcal{L} = X\\\\ast\\\\nabla_{X\\\\ast W}\\\\mathcal{L}}\\n∇W\\u200bL=X∗∇X∗W\\u200bLВопрос на подумать. Если всё-таки есть свободные члены, как будет выглядить градиент по bcb_cbc\\u200b?\\nОстальные важные блоки свёрточных нейронных сетей\\nНаигравшись с нашими мысленными экспериментами, давайте обратимся к опыту инженеров и исследователей, который копился с 2012 года – alexnet. Он поможет нам разобраться  с тем, как эффективней всего строить картиночные нейронки. Здесь будут перечислены самые важные (на момент написания и по мнению автора) блоки.\\nMax pool\\nКаждая из CCC свёрток очередного свёрточного слоя — это новая карта признаков для нашего изображения, и нам, конечно, хотелось бы, чтобы таких карт было побольше: ведь это позволит нам выучивать больше новых закономерностей.\\nНо для картинок в высоком разрешении это может быть затруднительно: слишком уж много будет параметров. Выходом оказалось использование следующей эвристики: сначала сделаем несколько свёрток с C1C_1C1\\u200b каналами, а затем как-нибудь уменьшим нашу карту признаков в 2 раза и одновременно увеличим количество свёрток во столько же.\\nПосчитаем, как в таком случае изменится число параметров: было H×W×K×K×C1×C1H \\\\times W \\\\times K \\\\times K \\\\times C_1 \\\\times C_1H×W×K×K×C1\\u200b×C1\\u200b, стало (H/2)×(W/2)×K×K×(C1×2)×(C1×2)=H×W×K×K×C1×C1(H/2) \\\\times (W/2) \\\\times K \\\\times K \\\\times (C_1 \\\\times 2) \\\\times (C_1 \\\\times 2) = H \\\\times W \\\\times K \\\\times K \\\\times C_1 \\\\times C_1(H/2)×(W/2)×K×K×(C1\\u200b×2)×(C1\\u200b×2)=H×W×K×K×C1\\u200b×C1\\u200b, то есть, ничего не изменилось, а количество фильтров удвоилось, что приводит к выучиванию более сложных зависимостей.\\nОсталось разобраться, как именно можно понижать разрешение картинки. Тривиальный способ — взять все пиксели с нечетными индексами. Такой подход будет работать, но, как может подсказать здравый смысл, выкидывать пиксели — значит терять информацию, а этого не хотелось бы делать.\\nЗдесь есть много вариантов: например, брать среднее/максимум по обучаемым весам в окне 2x2, которое идет по карте признаков с шагом 2. Экспериментально выяснилось, что максимум — хороший выбор, и, в большинстве архитектур, используют именно его. Обратите внимание, что максимум берется для каждого канала независимо.\\nЕще одно преимущество — увеличение receptive field. Получается, что он увеличивается в 2 раза:\\n\\nОперация понижения разрешения со взятием максимума в окне называется max pooling, а со взятием среднего — average pooling.\\nВопрос на подумать. Как будет преобразовываться градиент во время error backpropagation для maxpool с окном и шагом 2x2? А для average pool?\\nКстати, ещё один способ уменьшать размер карт признаков по ходу применения свёрточной сети — использование strided convolution, в которых ядро свёртки сдвигается на каждом шаге на некоторое большее единицы число пикселей (возможно, разное для осей H и W; обычная свёртка получается, если установить параметр stride=(1,1)).\\n\\nGlobal average pool\\nКак свёрточные слои, так и пулинг превращают картинку в «стопку» карт признаков. Но если мы решаем задачу классификации или регрессии, то в итоге нам надо получить число (или вектор логитов, если речь про многоклассовую классификацию).\\nОдин из способов добиться этого — воспользоваться тем, что свёртка без дополнения нулями и пулинг уменьшают размер карты признаков, и в итоге при должном терпении и верном расчёте мы можем получить тензор 1x1xC (финальные, общие признаки изображения), к которому уже можно применить один или несколько полносвязных слоёв. Или же можно, не дождавшись, пока пространственные измерения схлопнутся, «растянуть» всё в один вектор и после этого применить полносвязные слои (именно так, как мы не хотели делать, не правда ли?). Примерно так и происходило в старых архитектурах (alexnet, vgg).\\nВопрос на подумать. Попробуйте соорудить конструкцию из свёточных слоёв и слоёв пулинга, превращающую изображение размера 128x128x3 в тензор размера 1x1xC.\\nНо у такого подхода есть как минимум один существенный недостаток: для каждого размера входящего изображения нам придётся делать новую сетку.\\nПозднее было предложено следующее: после скольких-то свёрточных слоёв мы будем брать среднее вдоль пространственных осей нашего последнего тензора и усреднять их активации, а уже после этого строить MLP. Это и есть Global Average Pooling. У такого подхода есть несколько преимуществ:\\n\\nрадикально меньше параметров;\\nтеперь мы можем применять нейронку к картинку любого размера;\\nмы сохраняем «магию» инвариантности предсказаний к сдвигам.\\n\\n\\nResidual connection\\nОказывается, что, если мы будем бесконтрольно стекать наши свёртки, то, несмотря на использование relu и batch normalization, градиенты все равно будут затухать, и на первых слоях будут почти нулевыми. Интересное решение предлагают авторы архитектуры resnet: давайте будем «прокидывать» признаки на предыдущем слое мимо свёрток на следующем:\\n\\nТаким образом получается, что градиент доплывет даже до самых первых слоев, что существенно ускоряет сходимость и качество полученной модели. Вопрос: почему именно сумма? Может, лучше конкатенировать? Авторы densenet именно такой подход и предлагают (с оговорками), получая результаты лучше, чем у resnet. Однако, такой подход получается вычислительно сложным и редко используется на практике.\\nРегуляризация\\nНесмотря на наши ухищрения со свёртками, в современных нейронных сетях параметров все равно оказывается больше, чем количество картинок. Поэтому часто оказывается важным использовать различные комбинации регуляризаторов, которых уже стало слишком много, чтобы все описывать в этом параграфе, так что мы рассмотрим лишь несколько наиболее важных.\\nКлассические\\nПочти все регуляризаторы, которые использовались в классической машинке и полносвязных сетях, применимы и здесь: l1/l2, dropout и так далее.\\nВопрос на подумать. Насколько разумно использовать dropout в свёрточных слоях? Как можно модифицировать метод, чтобы он стал «более подходящим»?\\nАугментации\\nЭто один из самых мощных инструментов при работе с картинками. Помогает, даже если картинок несколько тысяч, а нейронная сеть с миллионами параметров. Мы уже выяснили, что смещение\\\\поворот\\\\прочее не меняют (при разумных параметрах) факта наличия на картинке того или иного объекта.\\nНа самом деле, есть огромное множество операций, сохраняющих это свойство:\\n\\nсдвиги, повороты и отражения;\\nдобавление случайного гауссового шума;\\nвырезание случайной части картинки (cutout);\\nперспективные преобразования;\\nслучайное изменение оттенка\\\\насыщенности\\\\яркости для всей картинки;\\nи многое другое.\\n\\nПример хорошой библиотеки с аугментациями: Albumentations.\\nLabel smoothing\\nЧасто оказывается, что нейронная сеть делает «слишком уверенные предсказания»: 0.9999 или 0.00001. Это становится головной болью, если в нашей разметке есть шум — тогда градиенты на таких объектах могут сильно портить сходимость.\\nИсследователи пришли к интересной идее: давайте предсказывать не one-hot метку, а ее сглаженный вариант. Итак, пусть у нас есть KKK классов:\\nyohot=(0,0,…,1,…,0)y_{ohot}=(0, 0, \\\\dots, 1, \\\\dots, 0)\\nyohot\\u200b=(0,0,…,1,…,0)yls=(εk−1,εk−1,…,1−ε,εk−1,…,εk−1)y_{ls}=\\\\left(\\\\frac{\\\\varepsilon}{k-1},\\\\frac{\\\\varepsilon}{k-1}, \\\\dots, 1-\\\\varepsilon,\\\\frac{\\\\varepsilon}{k-1}, \\\\dots, \\\\frac{\\\\varepsilon}{k-1}\\\\right)\\nyls\\u200b=(k−1ε\\u200b,k−1ε\\u200b,…,1−ε,k−1ε\\u200b,…,k−1ε\\u200b)∑iyohoti=∑iylsi=1\\\\sum_i y^i_{ohot}=\\\\sum_i y^i_{ls}=1\\ni∑\\u200byohoti\\u200b=i∑\\u200bylsi\\u200b=1Обычно берут ε=0.1\\\\varepsilon=0.1ε=0.1. Тем самым модель штрафуется за слишком уверенные предсказания, а шумные лейблы уже не вносят такого большого вклада в градиент.\\nMixup\\nСамый интересный вариант. А что будет, если мы сделаем выпуклую комбинацию двух картинок и их лейблов:\\n\\nгде α\\\\alphaα обычно семплируется из какого-нибудь Бета распределения. Оказывается, что такой подход заставляет модель выучивать в каком-то смысле более устойчивые предсказания, так как мы форсируем некую линейность в отображении из пространства картинок в пространство лейблов. На практике часто оказывается, что это дает значимое улучшение в качестве модели.\\nБонус №1: знаковые архитектуры в мире свёрточных нейронных сетей для задачи классификации изображений\\nДисклеймер: это мнение одного автора. Приведённые в этом разделе вехи связаны преимущественно с архитектурами моделей, а не способом их оптимизации.\\nЗдесь перечислены знаковые архитектуры, заметно повлиявшие на мир свёрточных нейронных сетей в задаче классификации картинок (и не только). К каждой архитектуре указана ссылка на оригинальную статью, а также комментарий автора параграфа с указанием ключевых нововведений. Значение метрики error rate на одном из влиятельных датасетов imagenet указано для финального ансамбля из нейросетей, если не указано иное.\\nЗачем это полезно изучить (вместе с чтением статей)? Основных причин две:\\n\\nОбщее развитие. Полезно понимать, откуда взялись и чем мотивированы те или иные компоненты.\\nЭтот вопрос задают на собеседовании, когда не знают, что еще спросить 😃\\n\\nlenet (1998)\\nСсылка на статью\\n7 слоев\\nПервая свёрточная нейронная сеть, показавшая SOTA (State Of The Art) результаты на задаче классификации изображений цифр MNIST. В архитектуре впервые успешно использовались свёрточные слои с ядром 5x5. В качестве активации использовался tanh, а вместо max pool в тот момент использовался average.\\nalexnet (2012)\\nСсылка на статью\\n11 слоев\\nПервая CNN (Convolutional Neural Network), взявшая победу на конкурсе imagenet. Автор предложил использовать ReLU вместо сигмоид (чтобы градиенты не затухали) и популяризовал max-pool вместо average. Что самое важное, обучение модели было перенесено на несколько GPU, что позволило обучать достаточно большую модель за относительное небольшое время (6 дней на двух видеокартах того времени). Также автор обратил внимание, что глубина нейросети важна, так как выключение хотя бы одного слоя стабильно ухудшало качество на несколько процентов.\\nnetwork in network (2013)\\nСсылка на статью\\nВ статье не привели интересных SOTA результатов, но зато ввели два очень популярных впоследствии модуля. Первый — это GAP (Global Average Pooling), который стоит после последнего свёрточного слоя и усредняет все активации вдоль пространственных осей. Второй — стекинг 1x1 свёрток поверх 3x3, что эквивалентно тому, что вместо линейной свёртки используется полносвязный слой.\\nvgg (2014)\\nСсылка на статью\\n19 слоев\\nАвторы предложили декомпозировать большие свёртки (5x5, 7x7 и выше) на последовательное выполнение свёрток 3x3 с нелинейностями между ними. Впоследствии, за нечастым исключением, свёртки 3x3 стали стандартом в индустрии (вместе со свёртками 1x1).\\ngoogleLeNet aka Inception (2014)\\nСсылка на статью\\n22 слоя\\nВвели inception слой, просуществовавший довольно продолжительное время. Сейчас сам слой уже не используется, но идея лежащая в его основе, эксплуатируется. Идея следующая: будем параллельно применять свёртки с разным пространственными размерами ядер, чтобы можно было одновременно обрабатывать как low-, так и high-level признаки. Еще полезной для сообщества оказалась идея с dimensionality reduction: перед тяжелой операцией поставим свёртку 1x1, чтобы уменьшить количество каналов и кратно ускорить вычисление.\\nbatch normalization (2015)\\nСсылка на статью\\nАвторы внедрили вездесущую batch normalization, которая стабилизирует сходимость, позволяя увеличить шаг оптимизатора и скорость сходимости. Применив идею к архитектуре inception, они превзошли человека на imagenet.\\nkaiming weight initialization (2015)\\nСсылка на статью\\nВ статье предложили использовать инициализацию весов, берущую во внимание особенность активации ReLU (в предыдущих работах предполагалось, что Var[x]=E[x2]Var[x] = \\\\mathbb{E}[x^2]Var[x]=E[x2], что, очевидно, нарушается для x^=max(0,x)\\\\hat{x} = max(0, x)x^=max(0,x)). Применение этой и других «свистелок» на VGG19 позволило существенно уменьшить ошибку на imagenet.\\nResNet (2015)\\nСсылка на статью\\n152 слоя\\nАрхитектура, которая на момент написания этого параграфа до сих пор бейзлайн и отправная точка во многих задачах. Основная идея — использование skip connections, что позволило градиенту протекать вплоть до первых слоев. Благодаря этому эффекту получилось успешно обучать очень глубокие нейронные сети, например, с 1202 слоями (впрочем, результаты на таких моделях менее впечатляющие, чем на 152-слойной). После этой статьи также стали повсеместно использоваться GAP и уменьшение размерности свёртками 1x1.\\nMobileNet (2017)\\nСсылка на статью\\nОчень популярная модель для быстрого инференса (на мобильных устройствах или gpu). По качеству хоть и немного проигрывает «монстрам», но в индустрии, оказывается, зачастую этого достаточно (особенно если брать последние варианты модели).\\nОсновная деталь — это использование depthwise convolutions: параллельный стекинг свёрток 3x3x1x1 — то есть таких, в которых вычисление для каждого сoutс_{\\\\text{out}}сout\\u200b канала просходит только на основе признаков одного cinc_{\\\\text{in}}cin\\u200b канала. Чтобы скомбинировать фичи между каналами, используется классическая 1x1 свёртка.\\nEfficientNet (2019)\\nСсылка на статью\\nОдна из первых моделей, полученных при помощи NAS (Neural Architecture Search), которая взяла SOTA на imagenet. После этого, модели, где компоненты подбирались вручную, уже почти не показывали лучших результатов на классических задачах.\\nБонус №2: не классификацией единой\\nСвёрточными нейронными сетями можно решать большой спектр задач, например:\\n\\nСегментация. Если убрать в конце слои GlobalAveragePool или flatten, то можно делать предсказания для каждого пикселя в отдельности (подумайте, что делать, если в сети есть maxpool) — получаем сегментацию картинки. Проблема — долгая и дорогая разметка.\\nДетекция. Часто намного дешевле получить разметку объектов обрамляющими прямоугольниками. Здесь уже можно для каждого пикселя предсказывать размеры прямоугольника, который обрамляет объект, к которому принадлежит пиксель. Проблемы — нужен этап агрегации прямоугольников + много неоднозначностей во время разметки + много эверистик на всех этапах + данных нужно больше.\\nПонимание видео. Добавляем в тензор новый канал — временной, считаем четырехмерные свёртки — и получаем распознавание сцен на видео.\\nMetric learning. Часто мы не можем собрать все интересующие нас классы, например, в задаче идентификации человека по лицу (или товара на полке). В этом случае используют такой трюк: научим модель в некотором смысле (обычно по косиносному расстоянию) разделять эмбеддинги существующих классов (уникальных людей). Если на руках была репрезентативная выборка, то модель, скорее всего (а обычно — всегда), выучит генерировать дискриминативные эмбеддинги, которые уже позволят различать между собой ранее невиданные лица.\\nи многое другое\\n\\nИтого\\nМы разобрались, что для картинок эффективно использовать свёрточные фильтры в качестве основных операторов. Выяснили, какие основные блоки есть почти в каждой картиночной нейронной сети и зачем они там нужны. Разобрались, какие методы регуляризаторы сейчас самые популярные и какая за ними идея.\\nИ наконец — рассмотрели знаковые архитектуры в мире свёрточных нейронных сетей.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф5.4. Тонкости обученияИнициализация весов. Регуляризация нейросетей. Dropout и\\xa0BatchnormСледующий параграф6.2. Нейросети для работы с последовательностямиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_32.html', 'title': 'Variational Autoencoder (VAE)'}, page_content='Variational Autoencoder (VAE)Яндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/68.1.Введение в генеративное моделирование8.2.Variational Autoencoder (VAE)Постановка задачиОбучение VAEОбзор статейЗаключение8.3.Генеративно-состязательные сети (GAN)8.4.Нормализующие потоки8.5.Диффузионные модели8.6.Языковые модели9.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Variational Autoencoder (VAE)8.2. Variational Autoencoder (VAE)Авторы Елистратова ЕвгенияВ машинном обучении есть довольно широкая область, посвящённая обучению генеративных моделей. Их задача — выучить распределение, из которого могли бы быть насемплированы объекты обучающей выборки.\\nОбученная генеративная модель способна семплировать из выученного распределения новые объекты, не принадлежащие исходным данным. Чаще всего это связано с задачей генерации новых изображений: от изображений рукописных чисел до замены лиц на видео с помощью deepfake.\\nМодель, о которой пойдёт речь в данном параграфе, называется «Вариационный автоэнкодер» или VAE (variational autoencoder). Она относится к семейству генеративных моделей. Коротко расскажем, что вас ждёт дальше.\\n\\nВ разделах «Постановка задачи» и «Обучение VAE» мы опишем построение и обучение VAE в классическом описании. Этих двух разделов достаточно для общего представления о VAE.\\nРаздел «Обзор статей» для первоначального понимания не обязателен, но может быть интересен тем, кто захочет узнать о недавних интересных работах, связанных с VAE.\\n\\nПрежде чем двинуться дальше — небольшое напоминание: большинство картинок в тексте кликабельны, и при клике вы сможете перейти к источнику, из которого была заимствована картинка.\\nПостановка задачи\\nДавайте представим себе, что нам нужно нарисовать лошадь. Как бы мы это сделали?Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nНаверное, сначала наметили бы общий силуэт лошади, её размер и позу, а затем стали бы добавлять детали: гриву, хвост, копыта, выбирать окраску шерсти и так далее. Кажется, что в процессе обучения рисованию мы учимся выделять для себя основной набор каких-то факторов, наиболее важных для генерации нового изображения: общий силуэт, размер, цвет и тому подобное, а во время рисования уже просто подставляем какие-то значения факторов.\\nПри этом одинаковые сочетания одних и тех же факторов могут привести к разным картинкам — ведь нарисовать что-то два раза абсолютно одинаково вы, скорее всего, не сможете.\\nПопробуем формализовать описанный выше процесс. Пусть у нас есть датасет DDD в многомерном пространстве исходных данных XNX^NXN, — объектов, которые мы желаем генерировать, — и пространство ZMZ^MZM скрытых (латентных) переменных меньшей размерности, которыми кодируются скрытые факторы в данных. Тогда генеративный процесс состоит из двух последовательных стадий (см. картинку ниже):\\n\\nСемплирование z∈ZMz \\\\in Z^Mz∈ZM из распределения p(z)p(z)p(z) (красное)\\nСемплирование x∈XNx \\\\in X^Nx∈XN из распределения p(x∥z)p(x \\\\| z)p(x∥z) (синее)\\n\\n\\nТо есть, рассуждая в терминах рисования картинок с лошадками, мы сначала мысленно семплируем некоторое zzz (размер, форму, цвет, ...), затем дорисовываем все необходимые детали, то есть семплируем из распределения p(x∥z)p(x \\\\| z)p(x∥z), и в итоге надеемся, что получившееся будет напоминать лошадку.\\nТаким образом, построить генеративную модель в нашем случае — значит уметь семплировать с помощью описанного двустадийного процесса объекты, близкие к объектам из обучающей выборки DDD.\\nГоворя более формально, нам бы хотелось, чтобы наша модель максимизировала правдоподобие p(x)p(x)p(x) элементов обучающего множества DDD при описанной процедуре генерации:\\np(x)=∫ZMp(x∣z)p(z)dz→max\\u2061p(x) = \\\\int \\\\limits_{Z^M} p(x | z) p(z) dz \\\\to \\\\max\\np(x)=ZM∫\\u200bp(x∣z)p(z)dz→maxПредположим, что совместное распределение p(x,z)p(x, z)p(x,z) параметризовано некоторым параметром θ∈Θ\\\\theta \\\\in \\\\Thetaθ∈Θ и выражается непрерывной по θ\\\\thetaθ функцией при каждых фиксированных xxx и zzz:\\npθ(x,z)=p(x,z∣θ)∈C(Θ)p_\\\\theta(x, z) = p(x, z | \\\\theta) \\\\in C(\\\\Theta)\\npθ\\u200b(x,z)=p(x,z∣θ)∈C(Θ)Тогда\\npθ(x,z)=p(x∣z,θ)p(z∣θ)=pθ(x∣z)pθ(z),p_\\\\theta(x, z) = p(x| z, \\\\theta) p(z | \\\\theta) = p_\\\\theta(x | z) p_\\\\theta(z),\\npθ\\u200b(x,z)=p(x∣z,θ)p(z∣θ)=pθ\\u200b(x∣z)pθ\\u200b(z),и мы можем записать следующую задачу оптимизации:\\npθ(x)=∫ZMpθ(x∣z)pθ(z)dz→max\\u2061θ∈Θ\\\\begin{equation}\\n    p_\\\\theta(x) = \\\\int \\\\limits_{Z^M} p_\\\\theta(x | z) p_\\\\theta(z) dz \\\\to \\\\max_{\\\\theta \\\\in \\\\Theta} \\\\tag{1}\\n\\\\end{equation}\\npθ\\u200b(x)=ZM∫\\u200bpθ\\u200b(x∣z)pθ\\u200b(z)dz→θ∈Θmax\\u200b\\u200b(1)\\u200bРешив её, мы построим нашу генеративную модель.\\nЗамечание 1. После приведённой выше аналогии с обучением рисованию может ошибочно показаться, что в скрытые переменные всегда заложен некоторый хорошо интерпретируемый смысл. Но на практике это всё же не обязано быть так: те скрытые переменные, которые мы найдём, могут как иметь простую интерпретацию, так и не иметь. С помощью объяснений выше мы прежде всего хотели проиллюстрировать понятие «скрытые переменные».\\nЗамечание 2. Может показаться, что p(x)p(x)p(x) нам откуда-то уже известно, и тогда не ясно, зачем все эти сложности с введением латентных переменных и интегралами. На самом деле, мы действительно можем построить статистическую оценку p^(x)\\\\hat p(x)p^\\u200b(x) по данным DDD и даже пытаться генерировать новые данные с помощью таких моделей (как, например, делается тут). Но у статистических методов есть разные ограничения, наиболее серьёзным из которых представляется проклятие размерности: чем больше измерений у ваших данных, тем больше разнообразных примеров вам нужно для построения адекватной оценки p^(x)\\\\hat p(x)p^\\u200b(x). О проклятии размерности мы поговорим чуть подробнее далее.\\nЗамечание 3. Также может возникать вопрос — а зачем вообще нужно вводить латентные переменные, моделировать совместное распределение p(x,z)p(x, z)p(x,z), а целевое распределение p(x)p(x)p(x) определять как маргинализацию p(x,z)p(x, z)p(x,z) по zzz? Почему такой подход в принципе должен работать? Ответ состоит в том, что, даже имея относительно простые выражения для p(z)p(z)p(z) и p(x∥z)p(x \\\\| z)p(x∥z), можно описать достаточно сложное распределение p(x)p(x)p(x), что достаточно наглядно проиллюстрировано в примере ниже.\\nПример: смесь гауссианПредставьте себе, что у вас есть таблица с конечным числом строк, в kkk-й строке которой записано два числа — среднее μk\\\\mu_kμk\\u200b и дисперсия σk2\\\\sigma^2_kσk2\\u200b нормального распределения. Пусть на индексах строк этой таблицы определено дискретное распределение p(z)p(z)p(z), такое что:\\np(z=k)=λk    p(z = k) = \\\\lambda_k\\np(z=k)=λk\\u200bПусть мы насемплировали индекс kkk, взяли параметры распределения из соответствующей ему строки и насемплировали с этими параметрами объект xxx. Распределение, из которого был получен xxx, равно:\\np(x∣z=k)=N(μk,σk2)    p(x | z = k) = \\\\mathcal{N}(\\\\mu_k, \\\\sigma_k^2)\\np(x∣z=k)=N(μk\\u200b,σk2\\u200b)Распределение p(x)p(x)p(x) получается маргинализацией совместного распределения p(x,z)p(x, z)p(x,z) по zzz:\\np(x)=∑k=1Kp(x,z=k)=∑k=1Kp(x∣z=k)p(z=k)=∑k=1KλkN(μk,σk2)    p(x) =  \\\\sum_{k = 1}^K p(x, z = k) = \\\\sum_{k = 1}^K p(x | z = k) p(z = k) = \\\\sum_{k = 1}^K \\\\lambda_k \\\\mathcal{N}(\\\\mu_k, \\\\sigma_k^2)\\np(x)=k=1∑K\\u200bp(x,z=k)=k=1∑K\\u200bp(x∣z=k)p(z=k)=k=1∑K\\u200bλk\\u200bN(μk\\u200b,σk2\\u200b)Получилось, что p(x)p(x)p(x) описывается смесью гауссиан и имеет более сложный вид, чем p(z)p(z)p(z) и p(x∥z)p(x \\\\| z)p(x∥z):\\n\\nЯсно, что чем больше гауссиан в нашей сумме, тем более сложную форму может иметь p(x)p(x)p(x). Так, имея простые p(z)p(z)p(z) и p(x∥z)p(x \\\\| z)p(x∥z), мы можем моделировать сложные мультимодальные распределения. А теперь представим себе, что априорное распределение p(z)p(z)p(z) имеет уже не дискретные, а непрерывные значения. Рассмотрим, например, такой случай:\\np(z)=N(0,1),    p(z) = \\\\mathcal{N}(0, 1),\\np(z)=N(0,1),p(x∣z)=N(μ(z),σ2(z))    p(x | z) = \\\\mathcal{N}(\\\\mu(z), \\\\sigma^2(z))\\np(x∣z)=N(μ(z),σ2(z))Распределение p(x)p(x)p(x), аналогично случаю с дискретным априорным распределением, будет получено интегрированием p(x,z)p(x, z)p(x,z) по zzz и будет как бы «бесконечной» смесью гауссиан:\\n\\nС этим разобрались. В следующей главе продолжим говорить о задаче оптимизации, о которой мы начали разговор чуть выше — не теряйтесь!\\nОбучение VAE\\nПрежде чем пытаться решать задачу оптимизации (1)(1)(1) давайте подумаем, а как мы вообще могли бы посчитать такой интеграл? Первое, что приходит на ум, — попробовать получить его приближённое значение методом Монте-Карло:\\npθ(x)=∫ZMpθ(x∣z)pθ(z)dz=Ez∼pθ(z)[pθ(x∣z)]≈1K∑kpθ(x∣zk),p_\\\\theta(x) = \\\\int \\\\limits_{Z^M} p_\\\\theta(x | z) p_\\\\theta(z) dz = \\\\mathbb{E}_{z \\\\sim p_\\\\theta(z)} [p_\\\\theta(x | z)] \\\\approx \\\\frac{1}{K} \\\\sum_k p_\\\\theta(x | z_k),\\npθ\\u200b(x)=ZM∫\\u200bpθ\\u200b(x∣z)pθ\\u200b(z)dz=Ez∼pθ\\u200b(z)\\u200b[pθ\\u200b(x∣z)]≈K1\\u200bk∑\\u200bpθ\\u200b(x∣zk\\u200b),где в последнем переходе мы используем сэмплы zk∼pθ(z)z_k \\\\sim p_\\\\theta(z)zk\\u200b∼pθ\\u200b(z). Однако, если z∈ZMz \\\\in Z^Mz∈ZM и MMM — достаточно большое, мы столкнёмся с проклятием размерности — количество семплов, необходимых для того, чтобы хорошо покрыть ZMZ^MZM, растёт экспоненциально с ростом MMM:\\n\\nЕсть ли способ как-то сократить число необходимых семплов для подсчёта (1)(1)(1)? На самом деле, часто оказывается, что далеко не все возможные zzz отображаются в элементы DDD, и вклад большинства zzz в оценку pθ(x∥z)p_\\\\theta(x \\\\| z)pθ\\u200b(x∥z) практически нулевой. Это наводит на мысль, что для каждого xxx нам может пригодиться знание распределения q(z∥x)q(z \\\\| x)q(z∥x) таких zzz, которые являются прообразами xxx. Мы можем предположить, что распределение qqq параметризовано некоторым семейством параметров Φ\\\\PhiΦ:\\nq(z∣x)=qϕ(z∣x),ϕ∈Φq(z | x) = q_{\\\\phi}(z | x), \\\\phi \\\\in \\\\Phi\\nq(z∣x)=qϕ\\u200b(z∣x),ϕ∈ΦЗная распределение qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x), мы могли бы семлировать уже только из него, а не из всего pθ(z)p_\\\\theta(z)pθ\\u200b(z), и, если распределение qqq окажется достаточно хорошим, число необходимых семплов значительно сократится.\\nО том, как построить qϕq_\\\\phiqϕ\\u200b, мы поговорим позже. Сейчас стоит обратить внимание на то, что процессы семплирования из распределений qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x) и pθ(x∥z)p_\\\\theta(x \\\\| z)pθ\\u200b(x∥z) взаимно обратны друг к другу: первое отображает элементы датасета в подмножество латентного пространства ZMZ^MZM, то есть действует как энкодер, а второе отображает латентные переменные в подмножество XNX^NXN, то есть действует как декодер:\\n\\nТак как оба эти распределения будут участвовать в обучении VAE, возникает аналогия между VAE и моделями-автоэнкодерами, имеющими похожую структуру.\\nВывод функции потерь\\nСейчас у нас всё готово для того, чтобы записать общий вид функции потерь для обучения вариационного автоэнкодера. Напомним, что мы обучаем модель путём максимизации правдоподобия pθ(x)p_{\\\\theta}(x)pθ\\u200b(x) по θ\\\\thetaθ. Для удобства мы перейдём к логарифму правдоподобия:\\nlog\\u2061pθ(x)=log\\u2061∫ZMpθ(x∣z)pθ(z)dz→max\\u2061θ∈Θ\\\\log p_{\\\\theta}(x) = \\\\log \\\\int_{Z^M} p_{\\\\theta}(x | z) p_{\\\\theta}(z) d z \\\\to \\\\max_{\\\\theta \\\\in \\\\Theta}\\nlogpθ\\u200b(x)=log∫ZM\\u200bpθ\\u200b(x∣z)pθ\\u200b(z)dz→θ∈Θmax\\u200bОптимизировать напрямую это выражение тяжело из-за проклятия размерности, обсуждавшегося в прошлом разделе. Чтобы победить проклятие размерности, мы хотели бы заменить семплирование из априорного распределения pθ(z)p_{\\\\theta}(z)pθ\\u200b(z) на семплирование из qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x), для чего придётся осуществить некоторый трюк. Для любого qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x), отличного от нуля для всех z∈ZMz \\\\in Z^Mz∈ZM, мы можем выписать следующую цепочку равенств:\\nlog\\u2061pθ(x)=Eqϕ(z∣x)[log\\u2061pθ(x)]=\\\\log p_\\\\theta(x) = \\\\mathbb E_{q_\\\\phi(z | x)} [\\\\log p_\\\\theta (x)] = \\\\\\\\\\nlogpθ\\u200b(x)=Eqϕ\\u200b(z∣x)\\u200b[logpθ\\u200b(x)]==Eqϕ(z∣x)[log\\u2061(pθ(x,z)pθ(z∣x))]== \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n    \\\\log \\\\left( \\n        \\\\frac{p_\\\\theta (x, z)}{p_\\\\theta(z | x)} \\n    \\\\right) \\n\\\\right] = \\\\\\\\\\n=Eqϕ\\u200b(z∣x)\\u200b[log(pθ\\u200b(z∣x)pθ\\u200b(x,z)\\u200b)]==Eqϕ(z∣x)[log\\u2061(pθ(x,z)qϕ(z∣x)qϕ(z∣x)pθ(z∣x))]== \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n    \\\\log \\\\left( \\n        \\\\frac{p_\\\\theta (x, z)}{q_\\\\phi(z | x)} \\\\frac{q_\\\\phi(z | x)}{p_\\\\theta(z | x)}\\n    \\\\right) \\n\\\\right] = \\\\\\\\\\n=Eqϕ\\u200b(z∣x)\\u200b[log(qϕ\\u200b(z∣x)pθ\\u200b(x,z)\\u200bpθ\\u200b(z∣x)qϕ\\u200b(z∣x)\\u200b)]==Eqϕ(z∣x)[log\\u2061(pθ(x,z)qϕ(z∣x))]⏟Lθ,ϕ(x)(ELBO)+Eqϕ(z∣x)[log\\u2061(qϕ(z∣x)pθ(z∣x))]⏟DKL(qϕ(z∣x)∥pθ(z∣x))= \\\\underbrace{\\n    \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\log \\\\left( \\n            \\\\frac{p_\\\\theta (x, z)}{q_\\\\phi(z | x)}\\n        \\\\right) \\n    \\\\right]\\n    }_{\\\\mathcal L_{\\\\theta, \\\\phi}(x) \\\\\\\\ (ELBO)} + \\n    \\\\underbrace{\\n    \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\log \\\\left( \\n            \\\\frac{q_\\\\phi(z | x)}{p_\\\\theta(z | x)}\\n        \\\\right) \\n    \\\\right]\\n    }_{D_{KL}(q_{\\\\phi}(z | x) \\\\parallel p_\\\\theta (z | x))}\\n=Lθ,ϕ\\u200b(x)(ELBO)Eqϕ\\u200b(z∣x)\\u200b[log(qϕ\\u200b(z∣x)pθ\\u200b(x,z)\\u200b)]\\u200b\\u200b+DKL\\u200b(qϕ\\u200b(z∣x)∥pθ\\u200b(z∣x))Eqϕ\\u200b(z∣x)\\u200b[log(pθ\\u200b(z∣x)qϕ\\u200b(z∣x)\\u200b)]\\u200b\\u200bВторое слагаемое в последнем равенстве — KLKLKL-дивергенция между qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x) и pθ(z∥x)p_\\\\theta(z \\\\| x)pθ\\u200b(z∥x), которая, как известно, неотрицательна:\\nDKL(qϕ(z∣x)∥pθ(z∣x))≥0    D_{KL}(q_{\\\\phi}(z | x) \\\\parallel p_\\\\theta (z | x)) \\\\ge 0\\nDKL\\u200b(qϕ\\u200b(z∣x)∥pθ\\u200b(z∣x))≥0А первое слагаемое — это величина, именуемая в английской литературе evidence lower bound (ELBO):\\nLθ,ϕ(x)=Eqϕ(z∣x)[log\\u2061pθ(x,z)−log\\u2061qϕ(z∣x)]=\\\\mathcal L_{\\\\theta, \\\\phi} (x) =\\n\\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n    \\\\log p_\\\\theta (x, z) - \\\\log q_\\\\phi(z | x)\\n\\\\right] = \\\\\\\\\\nLθ,ϕ\\u200b(x)=Eqϕ\\u200b(z∣x)\\u200b[logpθ\\u200b(x,z)−logqϕ\\u200b(z∣x)]==Eqϕ(z∣x)[log\\u2061pθ(x∣z)]⏟reconstruction\\xa0loss−DKL(qϕ(z∣x)∥pθ(z))⏟regularization\\xa0term= \\\\underbrace{ \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n    \\\\log p_\\\\theta (x | z)\\n\\\\right]}_{\\\\text{reconstruction loss}} - \\\\underbrace{D_{KL} (q_\\\\phi(z | x) \\\\parallel p_\\\\theta (z))}_{\\\\text{regularization term}}\\n=reconstruction\\xa0lossEqϕ\\u200b(z∣x)\\u200b[logpθ\\u200b(x∣z)]\\u200b\\u200b−regularization\\xa0termDKL\\u200b(qϕ\\u200b(z∣x)∥pθ\\u200b(z))\\u200b\\u200bПервое слагаемое в последнем переходе обычно называют reconstruction loss, так как оно оценивает качество восстановления декодером объекта xxx из его латентного представления zzz. А второе играет роль регуляризационного члена и подталкивает распределение, генерируемое энкодером, быть ближе к априорному распределению.\\nТак как KLKLKL-дивергенция неотрицательна, ELBO является нижней границей для логарифма правдоподобия данных:\\nLθ,ϕ(x)=log\\u2061pθ(x)−DKL(qϕ(z∣x)∥pθ(z∣x))≤log\\u2061pθ(x)\\\\mathcal L_{\\\\theta, \\\\phi} (x) =  \\\\log p_\\\\theta (x) - D_{KL}(q_\\\\phi (z | x) \\\\parallel p_\\\\theta(z | x)) \\\\le \\\\log p_\\\\theta (x)\\nLθ,ϕ\\u200b(x)=logpθ\\u200b(x)−DKL\\u200b(qϕ\\u200b(z∣x)∥pθ\\u200b(z∣x))≤logpθ\\u200b(x)Посмотрим повнимательнее на равенства, которые мы выписали.\\n\\nФункцию Lθ,ϕ\\\\mathcal L_{\\\\theta, \\\\phi}Lθ,ϕ\\u200b можно оптимизировать градиентным спуском (SGD), предварительно выбрав удобный вид для pθ(x∥z)p_\\\\theta (x \\\\| z)pθ\\u200b(x∥z), qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x) и pθ(z)p_\\\\theta (z)pθ\\u200b(z). Максимизируя Lθ,ϕ\\\\mathcal L_{\\\\theta, \\\\phi}Lθ,ϕ\\u200b, мы растим log\\u2061pθ(x)\\\\log p_\\\\theta (x)logpθ\\u200b(x), тем самым улучшая нашу генеративную модель. Оптимизацию ELBO с помощью SGD мы будем подробно обсуждать в следующем разделе.\\nМаксимизируя Lθ,ϕ\\\\mathcal L_{\\\\theta, \\\\phi}Lθ,ϕ\\u200b, мы одновременно минимизируем DKL(qϕ(z∥x)∥pθ(z∥x))D_{KL}(q_\\\\phi(z \\\\| x) \\\\parallel p_\\\\theta (z \\\\| x))DKL\\u200b(qϕ\\u200b(z∥x)∥pθ\\u200b(z∥x)). Распределение pθ(z∥x)p_\\\\theta (z \\\\| x)pθ\\u200b(z∥x) оценивает, из каких zzz мог бы быть сгенерирован объект xxx, и заранее оно нам не известно. Но если мы выберем достаточно большую модель для qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x), то qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x) в процессе оптимизации может очень сильно приблизиться к pθ(z∥x)p_\\\\theta (z \\\\| x)pθ\\u200b(z∥x), и тогда мы будем напрямую оптимизировать log\\u2061pθ(x)\\\\log p_\\\\theta(x)logpθ\\u200b(x). Заодно мы получаем приятный бонус: для оценки распределения прообразов xxx мы сможем использовать qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x) вместо невычислимого pθ(z∥x)p_\\\\theta (z \\\\| x)pθ\\u200b(z∥x). То есть qϕq_\\\\phiqϕ\\u200b, которое мы при выводе формулы ввели в рассмотрение как произвольное распределение, действительно будет играть роль энкодера для модели.\\n\\nАльтернативный вывод выражения для ELBOВ рассуждениях выше введение qϕ(z∥x)q_{\\\\phi}(z \\\\| x)qϕ\\u200b(z∥x) в рассмотрение могло показаться довольно формальным. Поэтому мы приведём здесь ещё один подход к выводу выражения для ELBO, который может показаться более естественным. Он состоит в последовательном применении приёма, называемого importance sampling, и неравенства Йенсена.\\n\\nВо многих практических задачах возникает ситуация, в которой мы хотим вычислить μ=E[f(X)]\\\\mu = \\\\mathbb E[ f(X) ]μ=E[f(X)], но при этом f(x)f(x)f(x) близка к нулю вне некоторой области AAA, а вероятность попасть в эту важную область очень мала: P(X∈A)≈0P(X \\\\in A) \\\\approx 0P(X∈A)≈0.\\nМножество AAA может либо иметь слишком маленькую мощность, либо быть в хвосте распределения случайной величины XXX. Обычное семплирование по методу Монте-Карло может почти не сгенерировать примеров, которые бы попадали в множество AAA. Проблемы такого типа довольно часто встречаются в физике высоких энергий, байесовском выводе, прогнозировании опасных природных явлений и во многих других областях.\\nДостаточно интуитивным выглядит решение, состоящие в том, чтобы попробовать как-то искусственно увеличить долю важных примеров среди всех остальных. Это можно сделать, используя распределение, дающее больше веса примерам из важной области. Отсюда и название метода — importance sampling (выборка по значимости).\\nИтак, пусть наша задача — вычислить математическое ожидание μ=Ep[f(x)]=∫Df(x)p(x)dx\\\\mu = \\\\mathbb E_p\\\\left[ f(x) \\\\right] = \\\\int_{\\\\mathcal D} f(x) p(x) dxμ=Ep\\u200b[f(x)]=∫D\\u200bf(x)p(x)dx, где\\n\\nppp — плотность распределения на множестве D∈Rd\\\\mathcal D \\\\in \\\\mathbb R^dD∈Rd,\\nfff — некоторая интегрируемая функция.\\n\\nПусть qqq — функция плотности вероятности, определённая и положительная на D\\\\mathcal DD, позволяющая осуществлять семплирование примеров из некоторого интересующего нас узкого подмножества. Наша задача — перейти от семплирования из ppp к семплированию из qqq для оценки μ\\\\muμ. Поскольку среднее Eq[f(x)]\\\\mathbb E_q\\\\left[ f(x) \\\\right]Eq\\u200b[f(x)], вообще говоря, не равно μ\\\\muμ, запишем следующее:\\nμ=Ep[f(x)]=∫Df(x)p(x)dx=∫Df(x)p(x)q(x)q(x)dx=Eq[f(x)p(x)q(x)]\\\\mu = \\\\mathbb E_p\\\\left[ f(x) \\\\right] = \\\\int_{\\\\mathcal D} f(x) p(x) dx = \\\\int_{\\\\mathcal D} \\\\frac{f(x) p(x)}{q(x)} q(x) dx = \\\\mathbb E_q\\\\left[ \\\\frac{f(x) p(x)}{q(x)} \\\\right]\\nμ=Ep\\u200b[f(x)]=∫D\\u200bf(x)p(x)dx=∫D\\u200bq(x)f(x)p(x)\\u200bq(x)dx=Eq\\u200b[q(x)f(x)p(x)\\u200b]Исходная плотность ppp называется номинальной (nominal distribution), а плотность qqq — смещённой (importance distribution). Отношение правдоподобия p(x)q(x)\\\\frac{p(x)}{q(x)}q(x)p(x)\\u200b компенсирует смещение, возникающее при переходе от ppp к qqq.\\nНапомним также формулировку неравенства Йенсена для случайных величин.\\nНапомним также формулировку неравенства Йенсена для случайных величин: если ξ\\\\xiξ — случайная величина с конечным математическим ожиданием и g(x)g(x)g(x) — выпуклая функция, то:\\nE[g(ξ)]≥g(E[ξ])\\\\mathbb E[g (\\\\xi)] \\\\ge g(\\\\mathbb E [\\\\xi])\\nE[g(ξ)]≥g(E[ξ])Теперь вернёмся к исходной задаче. Снова, для любого qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x), отличного от нуля для всех z∈ZMz \\\\in Z^Mz∈ZM, мы можем записать:\\nlog\\u2061pθ(x)=log\\u2061∫ZMpθ(x∣z)pθ(z)dz=    \\\\log p_\\\\theta (x) = \\\\log \\\\int_{Z^M} p_\\\\theta(x | z) p_{\\\\theta}(z) dz = \\\\\\\\\\nlogpθ\\u200b(x)=log∫ZM\\u200bpθ\\u200b(x∣z)pθ\\u200b(z)dz==log\\u2061Epθ(z)[pθ(x∣z)]=    = \\\\log \\\\mathbb{E}_{p_\\\\theta(z)} [p_\\\\theta(x | z)] = \\\\\\\\\\n=logEpθ\\u200b(z)\\u200b[pθ\\u200b(x∣z)]==log\\u2061Eqϕ(z∣x)[pθ(x∣z)pθ(z)qϕ(z∣x)]≥    = \\\\log \\\\mathbb{E}_{q_\\\\phi(z | x)} \\\\left[ \\\\frac{p_\\\\theta(x | z) p_\\\\theta(z)}{q_\\\\phi(z | x)} \\\\right] \\\\ge \\\\\\\\\\n=logEqϕ\\u200b(z∣x)\\u200b[qϕ\\u200b(z∣x)pθ\\u200b(x∣z)pθ\\u200b(z)\\u200b]≥≥Eqϕ(z∣x)log\\u2061[pθ(x∣z)pθ(z)qϕ(z∣x)]=    \\\\ge \\\\mathbb E_{q_\\\\phi(z | x)} \\\\log \\\\left[ \\\\frac{p_\\\\theta(x | z) p_\\\\theta(z)}{q_\\\\phi(z | x)} \\\\right] = \\\\\\\\\\n≥Eqϕ\\u200b(z∣x)\\u200blog[qϕ\\u200b(z∣x)pθ\\u200b(x∣z)pθ\\u200b(z)\\u200b]==Eqϕ(z∣x)log\\u2061[pθ(x∣z)]−Eqϕ(z∣x)log\\u2061[qϕ(z∣x)pθ(z)]=   = \\\\mathbb E_{q_\\\\phi(z | x)} \\\\log \\\\left[p_\\\\theta(x | z) \\\\right] - \\\\mathbb E_{q_\\\\phi(z | x)} \\\\log \\\\left[\\\\frac{q_\\\\phi(z | x)}{p_\\\\theta(z)} \\\\right] = \\\\\\\\\\n=Eqϕ\\u200b(z∣x)\\u200blog[pθ\\u200b(x∣z)]−Eqϕ\\u200b(z∣x)\\u200blog[pθ\\u200b(z)qϕ\\u200b(z∣x)\\u200b]==Eqϕ(z∣x)log\\u2061[pθ(x∣z)]−DKL(qϕ(z∣x)∥pθ(z))   = \\\\mathbb E_{q_\\\\phi(z | x)} \\\\log \\\\left[p_\\\\theta(x | z) \\\\right] - D_{KL} (q_\\\\phi(z | x) \\\\parallel p_\\\\theta (z))\\n=Eqϕ\\u200b(z∣x)\\u200blog[pθ\\u200b(x∣z)]−DKL\\u200b(qϕ\\u200b(z∣x)∥pθ\\u200b(z))В результате проведённых выкладок мы, как можно заметить, снова получили выражение для ELBO. На третьем переходе мы применили importance sampling, а на четвёртом — неравенство Йенсена для g(x)=log\\u2061(x)g(x) = \\\\log (x)g(x)=log(x).\\nМинус данного подхода состоит в том, что он, в отличие от предыдущего способа, не позволяет выписать в явном виде формулу для разности между log\\u2061pθ(x)\\\\log p_\\\\theta (x)logpθ\\u200b(x) и ELBO:\\nlog\\u2061pθ(x)−ELBO=DKL(qϕ(z∣x)∥pθ(z∣x))    \\\\log p_\\\\theta (x) - ELBO = D_{KL}(q_{\\\\phi}(z | x) \\\\parallel p_\\\\theta (z | x))\\nlogpθ\\u200b(x)−ELBO=DKL\\u200b(qϕ\\u200b(z∣x)∥pθ\\u200b(z∣x))Но зато данный вывод естественным образом следует из более общих методов, не требуя применения искусственных трюков.\\nОбучение VAE с помощью градиентного спуска\\nВажное свойство ELBO в том, что его можно оптимизировать градиентным спуском относительно параметров ϕ\\\\phiϕ и θ\\\\thetaθ. Если объекты датасета DDD независимы и одинаково распределены, то Lθ,ϕ(D)\\\\mathcal L_{\\\\theta, \\\\phi} (D)Lθ,ϕ\\u200b(D) запишется как сумма (или среднее) значений Lθ,ϕ(x)\\\\mathcal L_{\\\\theta, \\\\phi} (x)Lθ,ϕ\\u200b(x) на объектах x∈Dx \\\\in Dx∈D:\\nLθ,ϕ(D)=∑x∈DLθ,ϕ(x)    \\\\mathcal L_{\\\\theta, \\\\phi} (D) = \\\\sum_{x \\\\in D} \\\\mathcal L_{\\\\theta, \\\\phi} (x)\\nLθ,ϕ\\u200b(D)=x∈D∑\\u200bLθ,ϕ\\u200b(x)Значения Lθ,ϕ(x)\\\\mathcal L_{\\\\theta, \\\\phi} (x)Lθ,ϕ\\u200b(x) и их градиенты ∇Lθ,ϕ(x)\\\\nabla \\\\mathcal L_{\\\\theta, \\\\phi} (x)∇Lθ,ϕ\\u200b(x) в общем случае вычислить невозможно, однако можно получить их несмещённые оценки, что позволит нам использовать стохастический градиентный спуск.\\nОценку для градиента по параметрам θ\\\\thetaθ получить несложно:\\n∇θLθ,ϕ(x)=∇θEqϕ(z∣x)[log\\u2061pθ(x,z)−log\\u2061qϕ(z∣x)]=    \\\\nabla_\\\\theta \\\\mathcal L_{\\\\theta, \\\\phi}(x)\\n    = \\\\nabla_\\\\theta \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\log p_\\\\theta(x, z) - \\\\log q_\\\\phi(z | x)\\n    \\\\right] = \\\\\\\\\\n∇θ\\u200bLθ,ϕ\\u200b(x)=∇θ\\u200bEqϕ\\u200b(z∣x)\\u200b[logpθ\\u200b(x,z)−logqϕ\\u200b(z∣x)]==Eqϕ(z∣x)[∇θ(log\\u2061pθ(x,z)−log\\u2061qϕ(z∣x))]=    = \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\nabla_\\\\theta(\\\\log p_\\\\theta(x, z) - \\\\log q_\\\\phi(z | x))\\n    \\\\right] = \\\\\\\\\\n=Eqϕ\\u200b(z∣x)\\u200b[∇θ\\u200b(logpθ\\u200b(x,z)−logqϕ\\u200b(z∣x))]==Eqϕ(z∣x)[∇θlog\\u2061pθ(x,z)]≈    = \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\nabla_\\\\theta \\\\log p_\\\\theta(x, z)\\n    \\\\right] \\\\approx \\\\\\\\\\n=Eqϕ\\u200b(z∣x)\\u200b[∇θ\\u200blogpθ\\u200b(x,z)]≈≈1K∑k∇θlog\\u2061pθ(x,zk),    \\\\approx \\\\frac{1}{K} \\\\sum_k \\\\nabla_\\\\theta \\\\log p_\\\\theta(x, z_k),\\n≈K1\\u200bk∑\\u200b∇θ\\u200blogpθ\\u200b(x,zk\\u200b),где в последней строчке zk∼qϕ(z∥x)z_k \\\\sim q_\\\\phi(z \\\\| x)zk\\u200b∼qϕ\\u200b(z∥x). Однако оценку на градиент по параметрам ϕ\\\\phiϕ получить сложнее, ведь они также участвуют и в семплировании:\\n∇ϕLθ,ϕ(x)=∇ϕEqϕ(z∣x)[log\\u2061pθ(x,z)−log\\u2061qϕ(z∣x)]≠     \\\\nabla_\\\\phi \\\\mathcal L_{\\\\theta, \\\\phi}(x)\\n    = \\\\nabla_\\\\phi \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\log p_\\\\theta(x, z) - \\\\log q_\\\\phi(z | x)\\n    \\\\right] \\\\ne \\\\\\\\\\n∇ϕ\\u200bLθ,ϕ\\u200b(x)=∇ϕ\\u200bEqϕ\\u200b(z∣x)\\u200b[logpθ\\u200b(x,z)−logqϕ\\u200b(z∣x)]\\ue020=≠Eqϕ(z∣x)[∇ϕ(log\\u2061pθ(x,z)−log\\u2061qϕ(z∣x))]    \\\\ne \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\nabla_\\\\phi(\\\\log p_\\\\theta(x, z) - \\\\log q_\\\\phi(z | x))\\n    \\\\right]\\n\\ue020=Eqϕ\\u200b(z∣x)\\u200b[∇ϕ\\u200b(logpθ\\u200b(x,z)−logqϕ\\u200b(z∣x))]В общем случае эта проблема не разрешима. Однако некоторые распределения позволяют применить репараметризацию (reparameterization trick): представить переменную zzz как обратимую дифференцируемую функцию от случайного шума, параметров ϕ\\\\phiϕ и переменнной x∈Dx \\\\in Dx∈D:\\nz=g(ε,ϕ,x)    z = g(\\\\varepsilon, \\\\phi, x)\\nz=g(ε,ϕ,x)Здесь распределение ε∼pε\\\\varepsilon \\\\sim p_\\\\varepsilonε∼pε\\u200b не зависит от ϕ\\\\phiϕ и xxx. Например, пусть ε∼N(0,I)\\\\varepsilon \\\\sim \\\\mathcal N (0, I)ε∼N(0,I). Тогда ggg может иметь следующий вид:\\nz=g(ε,ϕ,x)=μϕ(x)+ε⋅σϕ(x)∼N(μϕ(x),σϕ2(x))    z = g(\\\\varepsilon, \\\\phi, x) = \\\\mu_\\\\phi(x) + \\\\varepsilon \\\\cdot \\\\sigma_\\\\phi(x) \\\\sim \\\\mathcal N (\\\\mu_\\\\phi(x), \\\\sigma^2_\\\\phi(x))\\nz=g(ε,ϕ,x)=μϕ\\u200b(x)+ε⋅σϕ\\u200b(x)∼N(μϕ\\u200b(x),σϕ2\\u200b(x))После такой замены мы сможем получить оценку на градиент по ϕ\\\\phiϕ:\\n∇ϕLθ,ϕ(x)=∇ϕEqϕ(z∣x)[log\\u2061pθ(x,z)−log\\u2061qϕ(z∣x)]=     \\\\nabla_\\\\phi \\\\mathcal L_{\\\\theta, \\\\phi}(x)\\n    = \\\\nabla_\\\\phi \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\log p_\\\\theta(x, z) - \\\\log q_\\\\phi(z | x)\\n    \\\\right] = \\\\\\\\\\n∇ϕ\\u200bLθ,ϕ\\u200b(x)=∇ϕ\\u200bEqϕ\\u200b(z∣x)\\u200b[logpθ\\u200b(x,z)−logqϕ\\u200b(z∣x)]==∇ϕEpε[log\\u2061pθ(x,g(ε,ϕ,x))−log\\u2061qϕ(g(ε,ϕ,x)∣x)]=    = \\\\nabla_\\\\phi \\\\mathbb E_{p_\\\\varepsilon} \\\\left[ \\n        \\\\log p_\\\\theta(x, g(\\\\varepsilon, \\\\phi, x)) - \\\\log q_\\\\phi(g(\\\\varepsilon, \\\\phi, x) | x)\\n    \\\\right] = \\\\\\\\\\n=∇ϕ\\u200bEpε\\u200b\\u200b[logpθ\\u200b(x,g(ε,ϕ,x))−logqϕ\\u200b(g(ε,ϕ,x)∣x)]==Epε[∇ϕ(log\\u2061pθ(x,g(ε,ϕ,x))−log\\u2061qϕ(g(ε,ϕ,x)∣x))]≈    = \\\\mathbb E_{p_\\\\varepsilon} \\\\left[ \\n        \\\\nabla_\\\\phi \\\\big( \\n            \\\\log p_\\\\theta(x, g(\\\\varepsilon, \\\\phi, x)) - \\\\log q_\\\\phi(g(\\\\varepsilon, \\\\phi, x) | x) \\n        \\\\big)\\n    \\\\right] \\\\approx \\\\\\\\\\n=Epε\\u200b\\u200b[∇ϕ\\u200b(logpθ\\u200b(x,g(ε,ϕ,x))−logqϕ\\u200b(g(ε,ϕ,x)∣x))]≈≈1K∑k∇ϕ(log\\u2061pθ(x,g(εk,ϕ,x))−log\\u2061qϕ(g(εk,ϕ,x)∣x)),    \\\\approx \\\\frac{1}{K} \\\\sum_{k} \\\\nabla_\\\\phi \\\\big( \\n        \\\\log p_\\\\theta(x, g(\\\\varepsilon_k, \\\\phi, x)) - \\\\log q_\\\\phi(g(\\\\varepsilon_k, \\\\phi, x) | x)\\n    \\\\big),\\n≈K1\\u200bk∑\\u200b∇ϕ\\u200b(logpθ\\u200b(x,g(εk\\u200b,ϕ,x))−logqϕ\\u200b(g(εk\\u200b,ϕ,x)∣x)),где в последней строчке εk∼pε\\\\varepsilon_k \\\\sim p_\\\\varepsilonεk\\u200b∼pε\\u200b. Репараметризация хорошо иллюстрируется следующей картинкой:\\n\\nЗдесь fff — функция потерь. Значения fff на обеих схемах одинаковы, но на левой картинке градиенты по ϕ\\\\phiϕ рассчитать не получится, так как мы не можем дифференцировать по случайной переменной zzz.\\nОднако на правой картинке источник случайности перемещается во входные данные благодаря репараметризации, а градиенты вычисляются по детерминированным переменным. Таким образом, мы получили сетап, типичный для оптимизации с помощью SGD: там мы приближаем градиент функции потерь по случайным батчам входных данных, а здесь роль случайных батчей играют одновременно батчи из переменных xxx и случайных переменных ε\\\\varepsilonε.\\nКроме нормального распределения, есть довольного много примеров распределений, допускающих репараметризацию. Их можно найти по ссылке в разделе \"The reparameterization trick\". Однако большая часть реализаций VAE используют именно нормальное распределение.\\nВ итоге примерный алгоритм обучения VAE такой:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1dataset = np.array(...)\\n2epsilon = RandomDistribution(...)\\n3\\n4# Энкодер q_phi(z|x) — нейронная сеть с параметрами phi\\n5encoder = Encoder()\\n6\\n7# Декодер p_theta(x|z) — нейронная сеть с параметрами theta\\n8decoder = Decoder()\\n9\\n10for step in range(max_steps):\\n11     # Семплируем батч исходных данных и случайного шума\\n12     batch_x = sample_batch(dataset)\\n13     batch_noise = sample_batch(epsilon)\\n14    \\n15     # Считаем параметры распределения q(z | x) с помощью энкодера\\n16     latent_distribution_parameters = encoder(batch_x)\\n17     \\n18     # Делаем репараметризацию (семплируем из q(z | x))\\n19     z = reparameterize(latent_distribution_parameters, batch_noise)\\n20\\n21     # Декодер отдаёт параметры выходного распределения\\n22     output_distribution_parameters = decoder(z)\\n23\\n24     # Вычисляем ELBO и обновляем параметры моделей\\n25     L = -ELBO(\\n26        latent_distribution_parameters, \\n27        output_distribution_parameters, \\n28        batch_x\\n29     )\\n30     L.backward()\\n\\n\\nСтоит подчеркнуть, что декодер выдаёт именно параметры выходного распределения, а не конкретный семпл из этого распределения. Например, если вы моделируете выходные изображения с помощью нормального распределения N(μ(z),σ2(z))\\\\mathcal{N}(\\\\mu(z), \\\\sigma^2(z))N(μ(z),σ2(z)), то декодер на выходе предскажет некоторые μ^(z)\\\\hat \\\\mu(z)μ^\\u200b(z) и σ^(z)\\\\hat \\\\sigma(z)σ^(z), которые вместе с параметрами латентного распределения (выход энкодера) будут поданы в ELBO.\\nДля генерации конкретной картинки на этапе инференса нужно будет либо честно провести семплирование из N(μ^(z),σ^2(z))\\\\mathcal{N}(\\\\hat \\\\mu(z), \\\\hat \\\\sigma^2(z))N(μ^\\u200b(z),σ^2(z)), либо, как часто делают, просто взять среднее μ^(z)\\\\hat \\\\mu(z)μ^\\u200b(z) в качестве выходного изображения. В общем случае конкретный способ проведения инференса зависит от вида используемого выходного распределения.\\nВыбор вида используемых распределений\\nПришло время привести примеры конкретных pθ(x∥z)p_\\\\theta(x \\\\| z)pθ\\u200b(x∥z), qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x) и pθ(z)p_\\\\theta(z)pθ\\u200b(z), с которыми можно построить VAE. Для начала предположим, что pθ(z)p_\\\\theta(z)pθ\\u200b(z) можно положить равным стандартному нормальному распределению:\\npθ(z)=N(0,I)    p_\\\\theta(z) = \\\\mathcal N(0, I)\\npθ\\u200b(z)=N(0,I)Заметим, что в этом случае у априорного распределения zzz отсутствует зависимость от параметров θ\\\\thetaθ.\\nРаспределение pθ(x∥z)p_\\\\theta(x \\\\| z)pθ\\u200b(x∥z) зависит от того, к какому распределению принадлежат ваши данные. Если ваши данные имеют непрерывное распределение, то pθ(x∥z)p_\\\\theta(x \\\\| z)pθ\\u200b(x∥z) можно задать, например, как гауссовское распределение:\\npθ(x∣z)=N(fθ(z),σ2)    p_\\\\theta(x | z) = \\\\mathcal N(f_\\\\theta(z), \\\\sigma^2)\\npθ\\u200b(x∣z)=N(fθ\\u200b(z),σ2)Вектор средних в этом примере определяется функцией fff с переменными θ\\\\thetaθ и zzz, а матрица ковариаций определяется постоянной диагональной матрицей. Функцию fff можно задать с помощью нейронной сети с параметрами θ\\\\thetaθ. При желании, матрицу ковариаций тоже можно задавать некоторой функцией и не ограничивать её вид только постоянными матрицами. Если же ваши данные дискретны, то может подойти категориальное распределение:\\npθ(x∣z)=Categorical\\u2061(fθ(z)),    p_\\\\theta(x | z) = \\\\operatorname{Categorical}(f_\\\\theta(z)),\\npθ\\u200b(x∣z)=Categorical(fθ\\u200b(z)),в котором вектор вероятностей fθ(z)=(p1,…,pn)f_\\\\theta(z) = (p_1, \\\\ldots, p_n)fθ\\u200b(z)=(p1\\u200b,…,pn\\u200b) — выход нейросети после применения softmax\\\\text{softmax}softmax. Если у вас бинарные данные, вы можете использовать бернуллиевское распределение:\\npθ(x∣z)=Bernoulli\\u2061(fθ(z)),    p_\\\\theta(x | z) = \\\\operatorname{Bernoulli}(f_\\\\theta(z)),\\npθ\\u200b(x∣z)=Bernoulli(fθ\\u200b(z)),где fθ(z)=pf_\\\\theta(z) = pfθ\\u200b(z)=p — выход нейронной сети после применения сигмоиды.\\nРаспределение qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x) может, в принципе, быть любым, но в самом простом случае оно имеет вид гауссовского распределения c диагональной матрицей ковариаций:\\nqϕ(z∣x)=N(μϕ(x),σϕ2(x))    q_\\\\phi(z | x) = \\\\mathcal N(\\\\mu_\\\\phi(x), \\\\sigma_\\\\phi^2(x))\\nqϕ\\u200b(z∣x)=N(μϕ\\u200b(x),σϕ2\\u200b(x))Такое распределение позволяет, в частности, применить репараметризацию, обсуждавшуюся выше. Если выбрать zzz двумерным, то распределения, определямые qqq, хорошо визуализируются:\\n\\nА теперь вспомним, как определяется ELBO:\\nLθ,ϕ(x)=Eqϕ(z∣x)[log\\u2061pθ(x∣z)]−DKL(qϕ(z∣x)∥pθ(z))    \\\\mathcal L_{\\\\theta, \\\\phi} (x) \\n    = \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\log p_\\\\theta (x | z)\\n    \\\\right] - D_{KL} (q_\\\\phi(z | x) \\\\parallel p_\\\\theta (z))\\nLθ,ϕ\\u200b(x)=Eqϕ\\u200b(z∣x)\\u200b[logpθ\\u200b(x∣z)]−DKL\\u200b(qϕ\\u200b(z∣x)∥pθ\\u200b(z))Вычислим его для приведённых выше распределений.\\nНачнём с DKL(qϕ(z∥x)∥pθ(z))D_{KL} (q_\\\\phi(z \\\\| x) \\\\parallel p_\\\\theta (z))DKL\\u200b(qϕ\\u200b(z∥x)∥pθ\\u200b(z)). KLKLKL-дивергенция между распределениями N(μ,Σ)\\\\mathcal N(\\\\mu, \\\\Sigma)N(μ,Σ) и N(0,I)\\\\mathcal N(0, I)N(0,I) равна:\\nDKL(N(μ,Σ)∥N(0,I))=12(μTμ+trΣ−M−log\\u2061(det\\u2061Σ)),     D_{KL} ( N(\\\\mu, \\\\Sigma) \\\\parallel \\\\mathcal N(0, I)) = \\\\frac{1}{2} \\\\left(\\n        \\\\mu^T \\\\mu + tr \\\\Sigma - M - \\\\log (\\\\det \\\\Sigma)\\n     \\\\right),\\nDKL\\u200b(N(μ,Σ)∥N(0,I))=21\\u200b(μTμ+trΣ−M−log(detΣ)),где MMM — размерность этих распределений. Вывод этого соотношения можно найти здесь. В нашем случае μϕ(x)=(μ1,…,μM)\\\\mu_\\\\phi(x) = (\\\\mu_1, \\\\ldots, \\\\mu_M)μϕ\\u200b(x)=(μ1\\u200b,…,μM\\u200b), σϕ2(x)=diag\\u2061(σ12,…,σM2)\\\\sigma^2_\\\\phi(x) = \\\\operatorname{diag}(\\\\sigma^2_1, \\\\ldots, \\\\sigma^2_M)σϕ2\\u200b(x)=diag(σ12\\u200b,…,σM2\\u200b) и\\nDKL(qϕ(z∥x)∥pθ(z))=DKL(N(μϕ(x),σϕ2(x))∥N(0,I))=    D_{KL} (q_\\\\phi(z \\\\| x) \\\\parallel p_\\\\theta (z)) = D_{KL} (\\\\mathcal N(\\\\mu_\\\\phi(x), \\\\sigma_\\\\phi^2(x)) \\\\parallel \\\\mathcal{N}(0, I)) = \\\\\\\\\\nDKL\\u200b(qϕ\\u200b(z∥x)∥pθ\\u200b(z))=DKL\\u200b(N(μϕ\\u200b(x),σϕ2\\u200b(x))∥N(0,I))==12∑j=1M(σj2+μj2−1−ln\\u2061σj2)    = \\\\frac{1}{2} \\\\sum_{j = 1}^M (\\\\sigma_j^2 + \\\\mu_j^2 - 1 - \\\\ln \\\\sigma_j^2)\\n=21\\u200bj=1∑M\\u200b(σj2\\u200b+μj2\\u200b−1−lnσj2\\u200b)Тогда ELBO будет вычисляться как:\\nLθ,ϕ(x)=Eqϕ(z∣x)[log\\u2061pθ(x∣z)]−DKL(qϕ(z∣x)∥pθ(z))=    \\\\mathcal L_{\\\\theta, \\\\phi} (x) = \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\log p_\\\\theta (x | z)\\n    \\\\right] - D_{KL} (q_\\\\phi(z | x) \\\\parallel p_\\\\theta (z)) = \\\\\\\\\\nLθ,ϕ\\u200b(x)=Eqϕ\\u200b(z∣x)\\u200b[logpθ\\u200b(x∣z)]−DKL\\u200b(qϕ\\u200b(z∣x)∥pθ\\u200b(z))==EN(μϕ(x),σϕ2(x))[log\\u2061pθ(x∣z)]−12∑j=1M(σj2+μj2−1−ln\\u2061σj2)≈    = \\\\mathbb E_{\\\\mathcal N(\\\\mu_\\\\phi(x), \\\\sigma_\\\\phi^2(x))} \\\\left[ \\n        \\\\log p_\\\\theta (x | z)\\n    \\\\right] - \\\\frac{1}{2} \\\\sum_{j = 1}^M (\\\\sigma_j^2 + \\\\mu_j^2 - 1 - \\\\ln \\\\sigma_j^2) \\\\approx \\\\\\\\\\n=EN(μϕ\\u200b(x),σϕ2\\u200b(x))\\u200b[logpθ\\u200b(x∣z)]−21\\u200bj=1∑M\\u200b(σj2\\u200b+μj2\\u200b−1−lnσj2\\u200b)≈≈1K∑k=1Klog\\u2061pθ(x∣zk)+12∑j=1M(1+ln\\u2061σj2−μj2−σj2),    \\\\approx \\\\frac{1}{K} \\\\sum_{k = 1}^K \\\\log p_\\\\theta (x | z_k) + \\\\frac{1}{2} \\\\sum_{j = 1}^M (1 + \\\\ln \\\\sigma_j^2 - \\\\mu_j^2 -\\\\sigma_j^2),\\n≈K1\\u200bk=1∑K\\u200blogpθ\\u200b(x∣zk\\u200b)+21\\u200bj=1∑M\\u200b(1+lnσj2\\u200b−μj2\\u200b−σj2\\u200b),где zk∼N(μϕ(x),σϕ2(x))z_k \\\\sim \\\\mathcal N(\\\\mu_\\\\phi(x), \\\\sigma_\\\\phi^2(x))zk\\u200b∼N(μϕ\\u200b(x),σϕ2\\u200b(x)). Как было упомянуто в этой статье от авторов VAE в разделе 2.3, число семплирований KKK можно положить равным единице при достаточно большом размере батча (например, 100).\\nЕсли вы выберете биномиальное pθ(x∥z)p_\\\\theta (x \\\\| z)pθ\\u200b(x∥z), то\\nlog\\u2061pθ(x∣z)=∑j=1Dlog\\u2061pθ(xj∣z)=∑j=1Dlog\\u2061Bernoulli\\u2061(xj,pj)=    \\\\log p_\\\\theta (x | z) = \\\\sum_{j = 1}^D \\\\log p_\\\\theta(x_j | z) = \\\\sum_{j = 1}^D \\\\log \\\\operatorname{Bernoulli}(x_j, p_j) = \\\\\\\\\\nlogpθ\\u200b(x∣z)=j=1∑D\\u200blogpθ\\u200b(xj\\u200b∣z)=j=1∑D\\u200blogBernoulli(xj\\u200b,pj\\u200b)==∑j=1Dxjlog\\u2061pj+(1−xj)log\\u2061(1−pj)    = \\\\sum_{j = 1}^D x_j \\\\log p_j + (1 - x_j) \\\\log(1 - p_j)\\n=j=1∑D\\u200bxj\\u200blogpj\\u200b+(1−xj\\u200b)log(1−pj\\u200b)Если гауссовское N(fθ(z),σ2)\\\\mathcal N(f_\\\\theta(z), \\\\sigma^2)N(fθ\\u200b(z),σ2), то\\nlog\\u2061pθ(x∣z)=∑j=1Dlog\\u2061pθ(xj∣z)=∑j=1Dlog\\u2061(12πσ2exp\\u2061(−(xj−fθ,j(z))22σ2))=    \\\\log p_\\\\theta (x | z) = \\\\sum_{j = 1}^D \\\\log p_\\\\theta(x_j | z) = \\\\sum_{j = 1}^D \\\\log \\\\left( \\n        \\\\frac{1}{\\\\sqrt{2 \\\\pi \\\\sigma^2}} \\\\exp \\\\left( \\n            -\\\\frac{(x_j - f_{\\\\theta, j}(z))^2}{2 \\\\sigma^2}\\n        \\\\right)\\n    \\\\right) = \\\\\\\\\\nlogpθ\\u200b(x∣z)=j=1∑D\\u200blogpθ\\u200b(xj\\u200b∣z)=j=1∑D\\u200blog(2πσ2\\u200b1\\u200bexp(−2σ2(xj\\u200b−fθ,j\\u200b(z))2\\u200b))==−D2log\\u20612π−Dlog\\u2061σ−12σ2∑j=1D(xj−fθ,j(z))2    = - \\\\frac{D}{2} \\\\log 2 \\\\pi - D \\\\log \\\\sigma - \\\\frac{1}{2 \\\\sigma^2} \\\\sum_{j = 1}^D (x_j - f_{\\\\theta, j}(z))^2\\n=−2D\\u200blog2π−Dlogσ−2σ21\\u200bj=1∑D\\u200b(xj\\u200b−fθ,j\\u200b(z))2Пример реализации обучения и применения VAE на датасете MNIST на Keras можно найти здесь, а на PyTorch — здесь.\\nИнференс обученной модели\\nКогда мы обучили VAE, мы сможем генерировать новые семплы, просто подавая z∼N(0,I)z \\\\sim \\\\mathcal{N}(0, I)z∼N(0,I) на вход декодеру:\\n![2](https://yastatic.net/s3/education-portal/media/vae_decoder_diagram_385be2e566_6c396a28e8.svg\">\\nЭнкодер для генерации новых семплов не нужен. Однако нам может понадобиться оценить p(x)=∫p(x∥z)p(z)dzp(x) = \\\\int p(x \\\\| z) p(z) dzp(x)=∫p(x∥z)p(z)dz для xxx из тестового множества, чтобы понять, с какой вероятностью модель сможет сгенерировать xxx. Для оценки интеграла нам нужно насемплировать некоторое количество zzz, и если брать семплы из z∼N(0,I)z \\\\sim \\\\mathcal{N}(0, I)z∼N(0,I), то оценка может плохо сойтись. Но можно снова использовать ELBO как нижнюю границу для log\\u2061p(x)\\\\log p(x)logp(x) и оценивать уже её, семплируя из распределения qϕ(z∥x)q_\\\\phi(z \\\\| x)qϕ\\u200b(z∥x). Такая оценка сойдётся быстрее и даст примерное представление о том, насколько хорошо модель справляется с конкретным примером xxx.\\nТакже интересно бывает взглянуть на то, как распределены коды обучающих примеров в латентном пространстве. Так, например, может выглядеть распределение латентных кодов цифр MNIST для обученного VAE в двумерном латентном пространстве:\\n\\nРазные типы цифр обозначены разными цветами (соответствие цифр и цветов показано на шкале сбоку). Здесь видно, что лучше всего модель различает нули и единицы, а восьмёрки и тройки — хуже всего. Стоит, конечно, отметить, что латентное пространство выбрано двумерным в целях визуализации, и при большей его размерности модель могла бы научиться различать цифры более качественно.\\nДля двумерного латентного пространства есть ещё один интересный способ визуализировать структуру многообразия, выученного VAE. Можно взять равномерную сетку на единичном квадрате и отобразить её в латентное пространство, применив к ней функцию, обратную к CDF нормального распределения.\\nПочему это сработаетУзлы равномерной сетки uiju_{ij}uij\\u200b можно в некотором приближении считать семплами из равномерного распределения: uij∼Uniform([0,1])u_{ij} \\\\sim \\\\text{Uniform}([0, 1])uij\\u200b∼Uniform([0,1]). Поэтому семплы Φ−1(uij)\\\\Phi^{-1}(u_{ij})Φ−1(uij\\u200b) приближённо подчиняются нормальному распределению:\\nP(Φ−1(uij)≤t)=P(uij≤Φ(t))=Φ(t)    \\\\mathbb P(\\\\Phi^{-1}(u_{ij}) \\\\le t) = \\\\mathbb P(u_{ij} \\\\le \\\\Phi(t)) = \\\\Phi(t)\\nP(Φ−1(uij\\u200b)≤t)=P(uij\\u200b≤Φ(t))=Φ(t)Полученные семплы можно подать в декодер и посмотреть, какие картинки будут соответствовать узлам сетки:\\n\\nЗдесь изображены примеры, сгенерированные для датасетов Frey Face и MNIST (оба доступны по ссылке). Такая визуализация позволяет увидеть плавный переход латентных кодов одних объектов в коды других, а также взаимное расположение латентных кодов.\\nДля MNIST снова видно, в частности, что коды нулей и единиц модель разнесла далеко друг от друга, а коды троек и восьмёрок очень близки. А ещё интересно наблюдать плавный переход от шестёрок к нулям и от семёрок к единицам. Для Frey Face видно, что весёлые лица расположены далеко от грустных, а по главной диагонали квадрата можно проследить плавный переход от серьёзного лица к улыбающемуся.\\nЕщё интересно посмотреть на то, как меняется качество генерируемых цифр в зависимости от размерности латентного пространства (на картинках просто случайные семплы из модели):\\n\\nЗаметный переход виден между размерностями 2 и 5, дальнейший рост размерности почти не оказывает значимого эффекта.\\nConditional VAE (CVAE)\\nИногда мы можем захотеть сгенерировать не просто какой-то произвольный объект из датасета, а относящийся к конкретной группе или классу. Ранее мы выписывали уравнение для log\\u2061pθ(x)\\\\log p_\\\\theta(x)logpθ\\u200b(x):\\nlog\\u2061pθ(x)=Eqϕ(z∣x)[log\\u2061pθ(x∣z)]−DKL(qϕ(z∣x)∥pθ(z))+DKL(qϕ(z∣x)∥pθ(z∣x))    \\\\log p_\\\\theta(x) = \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n        \\\\log p_\\\\theta (x | z)\\n    \\\\right] - D_{KL} (q_\\\\phi(z | x) \\\\parallel p_\\\\theta (z)) + D_{KL}(q_{\\\\phi}(z | x) \\\\parallel p_\\\\theta (z | x))\\nlogpθ\\u200b(x)=Eqϕ\\u200b(z∣x)\\u200b[logpθ\\u200b(x∣z)]−DKL\\u200b(qϕ\\u200b(z∣x)∥pθ\\u200b(z))+DKL\\u200b(qϕ\\u200b(z∣x)∥pθ\\u200b(z∣x))Все распределения, участвующие в этом уравнении, мы можем сделать обусловленными по переменной yyy:\\nlog\\u2061pθ(x∣y)=Eqϕ(z∣x,y)[log\\u2061pθ(x∣z,y)]−DKL(qϕ(z∣x,y)∥pθ(z∣y))+DKL(qϕ(z∣x,y)∥pθ(z∣x,y))    \\\\log p_\\\\theta(x | y) = \\\\mathbb E_{q_\\\\phi(z | x, y)} \\\\left[ \\n        \\\\log p_\\\\theta (x | z, y)\\n    \\\\right] - D_{KL} (q_\\\\phi(z | x, y) \\\\parallel p_\\\\theta (z | y)) + D_{KL}(q_{\\\\phi}(z | x, y) \\\\parallel p_\\\\theta (z | x, y))\\nlogpθ\\u200b(x∣y)=Eqϕ\\u200b(z∣x,y)\\u200b[logpθ\\u200b(x∣z,y)]−DKL\\u200b(qϕ\\u200b(z∣x,y)∥pθ\\u200b(z∣y))+DKL\\u200b(qϕ\\u200b(z∣x,y)∥pθ\\u200b(z∣x,y))Переменная yyy может быть лейблом объекта xxx или вообще произвольным тензором, как-то характеризующим xxx. Вместо pθ(z)p_\\\\theta(z)pθ\\u200b(z), единого для всех xxx из обучающей выборки, для каждого значения yyy теперь будет отдельное априорное распределение pθ(z∥y)p_\\\\theta(z \\\\| y)pθ\\u200b(z∥y).\\nПеременная yyy может принимать и дискретные, и непрерывные значения. Она может даже, например, быть половиной изображения, которую модели предлагается дополнить. На всякий случай подчеркнём, что обучение CVAE — это не то же самое, что обучение нескольких независимых VAE, так как веса CVAE общие для всех классов.\\nНа уровне имплементации это реализуется довольно просто: нужно всего лишь сконкатенировать входы энкодера и декодера с тензором, соответствующим yyy. Если yyy имеет категориальные значения, то бывает полезно предварительно закодировать их one-hot векторами. Алгоритм будет примерно таким:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1dataset, labels = np.array(...), np.array(...)\\n2epsilon = RandomDistribution(...)\\n3\\n4# Энкодер q_phi(z|x) — нейронная сеть с параметрами phi\\n5encoder = Encoder()\\n6\\n7# Декодер p_theta(x|z) — нейронная сеть с параметрами theta\\n8decoder = Decoder()\\n9\\n10for step in range(max_steps):\\n11     # Семплируем батч исходных данных, лейблов и случайного шума\\n12     batch_x = sample_batch(dataset)\\n13     batch_y = sample_batch(labels)\\n14     batch_noise = sample_batch(epsilon)\\n15\\n16     # Подаём в энкодер конкатенацию входных данных и лейблов\\n17     encoder_input = concatenate([batch_x, batch_y])\\n18    \\n19     # Считаем параметры распределения z с помощью энкодера\\n20     latent_distribution_parameters = encoder(encoder_input)\\n21     # Делаем репараметризацию\\n22     z = reparameterize(latent_distribution_parameters, batch_noise)\\n23\\n24     # Конкатенируем полученный случайный вектор и лейблы\\n25     decoder_input = concatenate([z, batch_y])\\n26\\n27     # Декодер отдаёт нам выходное изображение\\n28     output_distribution_parameters = decoder(decoder_input)\\n29\\n30     # Вычисляем ELBO и обновляем параметры\\n31     L = -ELBO(\\n32        latent_distribution_parameters, \\n33        output_distribution_parameters, \\n34        batch_x\\n35     )\\n36     L.backward()\\n\\n\\nРеализацию CVAE на PyTorch и Tensorflow можно найти, например, здесь.\\nЕсли визуализировать распределение латентных кодов для цифр MNIST, полученных после обуславливания модели на класс цифры, то можно увидеть что-то такое:\\n\\nМы видим непонятную смесь из точек вместо явных кластеров, которые выделяла обычная модель VAE. Однако дело тут в том, что, вместо того, чтобы пытаться размещать все цифры в одном пространстве p(z)∼N(0,I)p(z) \\\\sim \\\\mathcal N (0, I)p(z)∼N(0,I), модель использует отдельное латентное пространство p(z∥y)∼N(0,I)p(z \\\\| y) \\\\sim \\\\mathcal N (0, I)p(z∥y)∼N(0,I) для каждой цифры:\\n\\n\\nНа картинке справа — априорные распределения для цифр 6 и 7, а слева — визуализация структуры выученных многообразий для этих цифр, построенная так же, как аналогичная визуализация для VAE. Качество изображений каждой отдельной цифры заметно повышается:\\n\\nВидно, что вариабельность генерации цифр теперь тоже заметно выросла, и модель может имитировать написание цифр разными почерками.\\nОбзор статей\\nКроме стандратного описания работы VAE, приведём результаты нескольких недавних интересных работ, базирующихся на идее VAE.\\nVQ-VAE и VQ-VAE-2\\nМодели VQ-VAE и VQ-VAE-2 интересны тем, что в них в качестве априорных распределений были задействованы дискретные распределения. В каких ситуациях дискретные распределения могут быть более применимы, чем непрерывные? Например, если мы имеем дело с токенам в задачах NLP или фонемами в обработке речи. Картинки также можно было бы кодировать некоторым набором из целых чисел: например, одно число могло бы кодировать тип объекта, другое — его цвет, третье — цвет фона и так далее:\\n\\nКроме того, существуют довольно мощные алгоритмы (например, Трансформер), предназначенные для работы с дискретными данными. Выучивание хороших дискретных представлений даёт возможность эффективно использовать такие алгоритмы для, например, задачи генерации картинок.\\nVQ-VAE\\nАвторы VQ-VAE вводят дискретное латентное пространство в виде KKK вещественных векторов e1,…,eKe_1, \\\\ldots, e_Ke1\\u200b,…,eK\\u200b размерности DDD. Векторы из этого пространства называются кодовыми векторами или кодами. На рисунке ниже приведена примерная схема обучения предлагаемой модели.\\n\\nЭнкодер принимает на вход картинку xxx и выдаёт на выходе тензор ze(x)z_e(x)ze\\u200b(x). На рисунке этот тензор имеет размерность M×M×DM \\\\times M \\\\times DM×M×D: последняя размерность совпадает с длиной кодовых векторов, а M×MM \\\\times MM×M — это пространственная размерность выхода CNN (для простоты мы здесь не пишем явно размерность батчей).\\nКаждый из M×MM \\\\times MM×M векторов из ze(x)z_e(x)ze\\u200b(x) отображается в ближайший к нему по L2L_2L2\\u200b-расстоянию кодовый вектор. После такой процедуры тензор ze(x)z_e(x)ze\\u200b(x) переходит в тензор zq(x)z_q(x)zq\\u200b(x), состоящий из M×MM \\\\times MM×M кодовых векторов. Декодер получает на вход тензор zq(x)z_q(x)zq\\u200b(x) и отображает его в исходную картинку. Для работы с речью и текстами авторы использовали двумерный тензор ze(x)z_e(x)ze\\u200b(x) вместо трёхмерного.\\nВыходное распределение энкодера q(z∥x)q(z \\\\| x)q(z∥x) определено здесь следующим образом:\\nq(z=k∣x)={1,k=arg\\u2061min\\u2061j∥ze(x)−ej∥2,0,иначеq(z=k|x)=\\\\begin{cases}1,&k=\\\\arg\\\\min_j\\\\|z_e(x)-e_j\\\\|_2,\\\\\\\\0,&\\\\text{иначе}\\\\end{cases}\\nq(z=k∣x)={1,0,\\u200bk=argminj\\u200b∥ze\\u200b(x)−ej\\u200b∥2\\u200b,иначе\\u200bВо время обучения в качестве априорного распределения в латентном пространстве используется равномерное распределение p(z)=1Kp(z) = \\\\frac{1}{K}p(z)=K1\\u200b, поэтому слагаемое DKL(q(z∥x)∥p(z))D_{KL} (q(z \\\\| x) \\\\parallel p (z))DKL\\u200b(q(z∥x)∥p(z)) оказывается постоянным и равным log\\u2061K\\\\log KlogK:\\nDKL(q(z∣x)∥p(z))=−∑k=1Kq(z=k∣x)log\\u2061(p(z)q(z=k∣x))=log\\u2061K    D_{KL} (q(z | x) \\\\parallel p (z)) = - \\\\sum_{k = 1}^K q(z = k | x) \\\\log \\\\left( \\\\frac{p(z)}{q(z = k | x)} \\\\right) = \\\\log K\\nDKL\\u200b(q(z∣x)∥p(z))=−k=1∑K\\u200bq(z=k∣x)log(q(z=k∣x)p(z)\\u200b)=logKВ точках, где q(z=k∥x)=0q(z = k \\\\| x) = 0q(z=k∥x)=0, предпоследнее выражение продолжается нулём по непрерывности. Таким образом, ELBO для таких распределений примет вид\\nELBO(x)=Eq(z∣x)[log\\u2061pθ(x∣ze(x))]−DKL(q(z∣x)∥p(z))=log\\u2061pθ(x∣zq(x))−log\\u2061K,    ELBO(x) = \\\\mathbb{E}_{q(z | x)} [\\\\log p_\\\\theta (x | z_e(x))] - D_{KL} (q(z | x) \\\\parallel p (z)) = \\\\log p_\\\\theta (x | z_q(x)) - \\\\log K,\\nELBO(x)=Eq(z∣x)\\u200b[logpθ\\u200b(x∣ze\\u200b(x))]−DKL\\u200b(q(z∣x)∥p(z))=logpθ\\u200b(x∣zq\\u200b(x))−logK,где θ\\\\thetaθ — параметры декодера. При оптимизации log\\u2061K\\\\log KlogK можно не учитывать. Отображение выхода энкодера в кодовые векторы не дифференцируемо, поэтому при обучении применяется следующий трюк: при обратном проходе градиент копируется напрямую из декодера в энкодер, пропуская при этом слой, отображающий выходы энкодера в кодовые векторы.\\nЭтот трюк очень близок к приёму, известному как straight-through estimator, впервые предложенному в этой статье (а его простое описание можно найти тут). Использование straight-through estimator, однако, не позволяет обучать сами кодовые векторы, так как по ним не будут вычисляться градиенты. Поэтому лосс для обучения модели складывается из трёх компонент:\\nL=log\\u2061p(x∣zq(x))+∥sg[ze(x)]−zq(x)∥22+β∥ze(x)−sg[zq(x)]∥22    \\\\mathcal{L} = \\\\log p(x | z_q(x)) + \\\\| sg[z_e(x)] - z_q(x) \\\\|_2^2 + \\\\beta \\\\| z_e(x) - sg[z_q(x)] \\\\|_2^2\\nL=logp(x∣zq\\u200b(x))+∥sg[ze\\u200b(x)]−zq\\u200b(x)∥22\\u200b+β∥ze\\u200b(x)−sg[zq\\u200b(x)]∥22\\u200bЗдесь sg[⋅]sg[\\\\cdot]sg[⋅] обозначает оператор остановки дифференцирования: через его аргумент не текут градиенты.\\nВ статье лосс записан несколько иначе:\\nL=log\\u2061p(x∣zq(x))+∥sg[ze(x)]−e∥22+β∥ze(x)−sg[e]∥22\\\\mathcal{L} = \\\\log p(x | z_q(x)) + \\\\| sg[z_e(x)] - e \\\\|_2^2 + \\\\beta \\\\| z_e(x) - sg[e] \\\\|_2^2\\nL=logp(x∣zq\\u200b(x))+∥sg[ze\\u200b(x)]−e∥22\\u200b+β∥ze\\u200b(x)−sg[e]∥22\\u200bЭти обозначения кажутся несколько путающими по двум причинам:\\n\\nБуква eee в нижнем индексе ze(x)z_e(x)ze\\u200b(x) призвана обозначить только то, что это выход энкодера, а не наличие связи между кодовыми векторами eee и параметрами энкодера. Но второе довольно легко для себя предположить.\\nВычитание eee обозначает вычитание не всех элементов словаря из соответствующей позиции тензора ze(x)z_e(x)ze\\u200b(x), а только лишь ближайшего соседа к элементу ze(x)z_e(x)ze\\u200b(x) на этой позиции. То есть по факту вычитание eee в этой записи равносильно вычитанию zq(x)z_q(x)zq\\u200b(x). Это не уточняется в статье, но можно увидеть в официальной реализации.\\n\\nПервое слагаемое — это ELBO с точностью до константы. Второе слагаемое отвечает за сдвиг кодовых векторов в сторону выходов энкодера. Чтобы не получилось так, что выходы энкодера всё время меняют кодовые векторы за счёт второй компоненты лосса, а сами на каждой итерации выдают векторы, далёкие от текущих кодовых векторов, добавляется третье слагаемое. Оно отвечает за то, чтобы энкодер стремился выдавать векторы, близкие к кодовым векторам, а его значимость регулируется с помощью коэффициента β\\\\betaβ.\\nОднако при обучении мы потеряли регуляризационное слагаемое DKL(q(z∥x)∥p(z))D_{KL} (q(z \\\\| x) \\\\parallel p (z))DKL\\u200b(q(z∥x)∥p(z)), из-за чего распределение энкодера не было обязано приближать собой априорное распределение и осталось его узким подмножеством. Из-за этого с наибольшей вероятностью при семплировании из равномерного категориального распределения мы будем получать просто шумы вместо хороших картинок:\\n\\nЧуть подробнееПри обучении обычного VAE мы минимизируем расстояние между априорным распределением и распределением, которое выдаёт энкодер, с помощью регуляризационного слагаемого DKL(q(z∥x)∥p(z))D_{KL} (q(z \\\\| x) \\\\parallel p (z))DKL\\u200b(q(z∥x)∥p(z)).\\nБлагодаря нему, например, двумерные латентные коды цифр MNIST приближённо распределяются по шарику — априорному нормальному распределению. А если каждой цифре выделить собственное латентное пространство (провести обуславливание на класс цифры), то априорное условное распределение для каждой цифры очень близко к нормальному.\\nА в случае VQ-VAE мы не можем заставить распределение, предсказываемое энкодером, быть равномерным категориальным, и получаем просто какое-то категориальное распределение с неизвестной параметризацией. Это напоминает ситуацию с обычным автоэнкодером: он тоже переводит входные картинки в латентное пространство, но семплировать из такого пространства мы не можем.\\nЧтобы исправить эту проблему, авторы предлагают с помощью дополнительной модели выучить априорное распределение p(z)p(z)p(z) тех латентных переменных, которые модель научилась генерировать в процессе обучения. Поскольку любое кодовое представление можно вытянуть в последовательность, а самих кодов — конечное наперёд заданное число, то эта задача близка к задаче обучения языковой модели.\\nДействительно, ведь там мы должны по последовательности предыдущих слов предложения предсказать следующее слово из доступного словаря, а в нашем случае — по входной последовательности дискретных латентных кодов предсказать следующий латентный код.\\nДля картинок авторы предложили моделировать априорное распределение латентных кодов с помощью PixelCNN. Детали архитектуры и обучения этой модели можно найти в оригинальной статье, здесь мы опишем только общую идею.\\nPixelCNN последовательно генерирует пиксели картинки, двигаясь из верхнего левого угла в правый нижний. Она проходит все ряды последовательно от верхнего до нижнего, а внутри каждого ряда движется слева направо:\\n\\nДля цветных картинок каналы (R, G, B) также моделируются последовательно: канал B при генерации зависит от R и G, а G — только от R. При предсказании значения каждого следующего пикселя модель использует значения уже сгенерированных соседей из некоторого окружающего квадрата. Чтобы модель не могла читать пиксели, идущие после текущего предсказываемого пикселя, используется специальная маска, пример которой изображён на правой части рисунка.\\nВ случае VQ-VAE обучение PixelCNN происходит не на пикселях, а на латентных кодах. Семплирование из выученного априорного распределения выглядит гораздо лучше, чем попытки семплировать из равномерного:\\n\\nДля аудио вместо PixelCNN авторами используется WaveNet. При обучении моделей априорных распределений есть возможность подавать метки классов, чтобы потом можно было семплировать из этих классов (принцип тот же, что и для CVAE).\\nРезультаты реконструкции картинок из ImageNet с помощью VQ-VAE выглядят довольно неплохо (под реконструкцией понимается выход полной модели, состоящей из энкодера и декодера):\\n\\nА так выглядят результаты семплирования из VQ-VAE с априорным распределением, выученным PixelCNN:\\n\\nVQ-VAE-2\\nМодель VQ-VAE-2 — это расширение VQ-VAE. Она показывает значительный скачок по качеству генерируемых изображений:\\n\\nВпечатляет то, что на картинке именно результат семплирования из выученного моделью распределения, а не результат реконструкции. Первое основное отличие модели VQ-VAE от VQ-VAE-2 — использование иерархических латентных переменных:\\n\\nПрежде чем перейти к описанию архитектуры, хочется сделать небольшой дисклеймер: когда в тексте далее будет говориться «тензор размера M×MM \\\\times MM×M», то будет иметься в виду, что тензор имеет шейп (B,M,M,C)(B, M, M, C)(B,M,M,C), где первая размерность соответствует батчам, а последняя — каналам.\\nНа картинке показан пример двухуровневой архитектуры (хотя уровней может быть и больше). Каждому уровню соответствуют свои энкодер, декодер и набор кодовых векторов (общей размерности DDD для всех уровней). Обозначим нижний и верхний энкодеры как EncbottomEnc_{\\\\text{bottom}}Encbottom\\u200b и EnctopEnc_{\\\\text{top}}Enctop\\u200b, а декодеры — как DecbottomDec_{\\\\text{bottom}}Decbottom\\u200b и DectopDec_{\\\\text{top}}Dectop\\u200b.\\n\\nEncbottomEnc_{\\\\text{bottom}}Encbottom\\u200b принимает на вход трёхканальную картинку размера 256×256256 \\\\times 256256×256 пикселей, отображает её в тензор размера 64×6464 \\\\times 6464×64 и передаёт на вход EnctopEnc_{\\\\text{top}}Enctop\\u200b. EnctopEnc_{\\\\text{top}}Enctop\\u200b выдаёт тензор размера 32×3232 \\\\times 3232×32, который затем отображается в тензор из кодовых векторов ztopz_{\\\\text{top}}ztop\\u200b (квантизуется)\\nztopz_{\\\\text{top}}ztop\\u200b передаётся на вход DectopDec_{\\\\text{top}}Dectop\\u200b, затем выходы EncbottomEnc_{\\\\text{bottom}}Encbottom\\u200b и DectopDec_{\\\\text{top}}Dectop\\u200b конкатенируются и квантизуются в zbottomz_{\\\\text{bottom}}zbottom\\u200b\\nztopz_{\\\\text{top}}ztop\\u200b и zbottomz_{\\\\text{bottom}}zbottom\\u200b конкатенируются и передаются на вход DecbottomDec_{\\\\text{bottom}}Decbottom\\u200b, который отображает их в исходную картинку\\n\\nДля обучения модели используется почти такой же лосс, как для VQ-VAE. Для VQ-VAE он имел вид:\\nL=log\\u2061p(x∣zq(x))+∥sg[ze(x)]−zq(x)∥22+β∥ze(x)−sg[zq(x)]∥22    \\\\mathcal{L} = \\\\log p(x | z_q(x)) + \\\\| sg[z_e(x)] - z_q(x) \\\\|_2^2 + \\\\beta \\\\| z_e(x) - sg[z_q(x)] \\\\|_2^2\\nL=logp(x∣zq\\u200b(x))+∥sg[ze\\u200b(x)]−zq\\u200b(x)∥22\\u200b+β∥ze\\u200b(x)−sg[zq\\u200b(x)]∥22\\u200bДля VQ-VAE-2 первое и третье слагаемые сохраняют свой вид, а второе слагаемое заменяется на обновление кодовых векторов eie_iei\\u200b с помощью экспоненциального скользящего среднего. Пусть E(x)(t)E(x)^{(t)}E(x)(t) — выход энкодера на шаге ttt, выпрямленный в двумерный тензор, последняя размерность которого равна размерности DDD кодовых векторов.\\nПусть {Ei,1(t),…,Ei,ni(t)(t)}\\\\{ E^{(t)}_{i, 1}, \\\\ldots, E^{(t)}_{i, n_i^{(t)}} \\\\}{Ei,1(t)\\u200b,…,Ei,ni(t)\\u200b(t)\\u200b} — множество из ni(t)n_i^{(t)}ni(t)\\u200b векторов, для которых на шаге ttt ближайшим оказался кодовый вектор ei(t−1)e_i^{(t - 1)}ei(t−1)\\u200b. Тогда обновление eie_iei\\u200b на шаге ttt происходит по следующим формулам:\\nei(t)=mi(t)Ni(t)    e_i^{(t)} = \\\\frac{m_i^{(t)}}{N_i^{(t)}}\\nei(t)\\u200b=Ni(t)\\u200bmi(t)\\u200b\\u200bmi(t)=mi(t−1)⋅γ+∑jni(t)E(x)i,j(t)(1−γ)    m_i^{(t)} = m_i^{(t - 1)} \\\\cdot \\\\gamma + \\\\sum_j^{n_i^{(t)}} E(x)_{i, j}^{(t)} (1 - \\\\gamma)\\nmi(t)\\u200b=mi(t−1)\\u200b⋅γ+j∑ni(t)\\u200b\\u200bE(x)i,j(t)\\u200b(1−γ)Ni(t)=Ni(t−1)⋅γ+ni(t)(1−γ)    N_i^{(t)} =  N_i^{(t - 1)} \\\\cdot \\\\gamma + n_i^{(t)} (1 - \\\\gamma)\\nNi(t)\\u200b=Ni(t−1)\\u200b⋅γ+ni(t)\\u200b(1−γ)Здесь γ\\\\gammaγ — некоторый вещественный параметр.\\nТак же, как и для VQ-VAE, априорное распределение для VQ-VAE-2 выучивается отдельно уже после обучения основной модели, но в случае VQ-VAE-2 оно имеет иерархическую структуру. На картинке изображён пример такого распределения для двухуровневой архитектуры:\\n\\nДля каждого уровня обучается отдельная модель PixelCNN: одна — на кодовых векторах первого уровня, вторая — на кодовых векторах первого и второго уровней. Обе модели также принимают на вход метку класса, изображение из которого нужно насемплировать.\\nСемплирование из финальной модели происходит так:\\n\\nсемплируются векторы etope_{\\\\text{top}}etop\\u200b из верхнего распределения\\nиз нижнего распределения семплируются векторы ebottome_{\\\\text{bottom}}ebottom\\u200b при условии векторов etope_{\\\\text{top}}etop\\u200b\\nдекодер принимает на вход векторы etope_{\\\\text{top}}etop\\u200b и ebottome_{\\\\text{bottom}}ebottom\\u200b и выдаёт финальную картинку\\n\\nРезультаты семплирования из двухуровневой модели VQ-VAE-2, обученной на ImageNet:\\n\\nА это — результаты семплирования из трёхуровневой модели VQ-VAE-2, обучавшейся на FFHQ:\\n\\nDALL-E\\nОдна из недавних работ, связанных с VAE, — это DALL-E от OpenAI. Они обучили модель с 12 миллиардами параметров, генерирующую картинки по их текстовому описанию. Для обучения авторами был собран датасет, состоящий из 250 миллионов пар картинок и их описаний. Вот примеры работы этой модели:\\n\\n\\nВ блог-посте OpenAI, посвящённом DALL-E, есть возможность самостоятельно составлять текстовые описания из некоторого ограниченного словаря и смотреть на результаты. Осторожно, это затягивает 😃\\n\\nDALL-E идейно основывается на результатах VQ-VAE: сначала выучиваются кодовые векторы для картинок, а затем обучается Трансформер, моделирующий совместное априорное распределение текстов и кодовых векторов. Подробнее о трансформерах мы рассказывали в главе 6.3 этого хендбука.\\nВ DALL-E задействована архитектура, основанная на декодер-части исходной архитектуры Трансформера, поэтому стоит также почитать про модель GPT-2, работающую аналогичным образом.\\nОбучение проходит в две стадии:\\n\\nСначала обучается дискретизованный VAE (dVAE) c энкодером для сжатия RGB-картинок размера 256×256256 \\\\times 256256×256 в тензор из 32×32=102432 \\\\times 32 = 102432×32=1024 кодовых векторов. Эта стадия обучения очень напоминает VQ-VAE, но вместо добавления в лосс дополнительных слагаемых для кодовых векторов авторы DALL-E используют релаксацию Гумбеля — трюк, позволяющий проводить честное дифференцирование по параметрам энкодера. Об обучении dVAE мы будем говорить подробнее далее.\\nЗатем обучается Трансформер (точнее, только декодер-часть исходной архитектуры Трансформера), задача которого — выучить совместное распределение картинок и их текстовых описаний. Он принимает на вход конкатенацию из эмбеддингов текстовых токенов и кодовых векторов картинок и учится для каждой входной последовательности предсказывать её продолжение. О некоторых деталях обучения Трансформера также будет рассказано далее.\\n\\nИнференс обученной модели происходит так: эмбеддинги текстового описания картинки подаются на вход Трансформеру, и он авторегрессионно предсказывает кодовые векторы картинки, соответствующей этому описанию, а затем полученные кодовые векторы пропускаются через декодер dVAE.\\ndVAE\\nОбучение dVAE происходит путём максимизации ELBO для картинок xxx и их дискретных латентных представлений zzz:\\nln\\u2061pθ(x)≥Eqϕ(z∣x)[log\\u2061pθ(x∣z)]−β\\u2009DKL(qϕ(z∣x)∥p(z)),\\\\ln p_{\\\\theta}(x) \\\\ge \\\\mathbb E_{q_\\\\phi(z | x)} \\\\left[ \\n    \\\\log p_\\\\theta (x | z) \\n\\\\right] - \\\\beta \\\\, D_{KL} (q_\\\\phi(z | x) \\\\parallel p(z)),\\nlnpθ\\u200b(x)≥Eqϕ\\u200b(z∣x)\\u200b[logpθ\\u200b(x∣z)]−βDKL\\u200b(qϕ\\u200b(z∣x)∥p(z)),где ϕ\\\\phiϕ и θ\\\\thetaθ — параметры энкодера и декодера дискретизованного VAE, a p(z)p(z)p(z) — равномерное категориальное распределение над кодовыми векторами. Здесь можно заметить дополнительный коэффициент β\\\\betaβ, который в стандартном VAE всегда равен 1. Однако авторы DALL-E ввели дополнительный параметр β\\\\betaβ, опираясь на результаты статьи о β\\\\betaβ-VAE. Но, в отличие от исходной статьи, в их экспериментах значение betabetabeta постепенно понижается в ходе обучения.\\nЭнкодер dVAE отображает картинки размера 256×256256 \\\\times 256256×256 в тензор ze(x)z_e(x)ze\\u200b(x) с шейпом 32×32×819232 \\\\times 32 \\\\times 819232×32×8192, где 819281928192 — число кодовых векторов. То есть каждой из 32×3232 \\\\times 3232×32 позиций энкодер сопоставляет категориальное распределение над 819281928192 кодовыми векторами, параметризованное выходными логитами.\\nДля получения тензора zq(x)z_q(x)zq\\u200b(x) из кодовых векторов можно было бы сначала применить softmax\\\\text{softmax}softmax к распределениям на каждой из 32×3232 \\\\times 3232×32 позиций, а затем сопоставить каждой позиции кодовый вектор, номеру которого соответствует максимальная вероятность (взять argmax\\\\text{argmax}argmax для этой позиции).\\nОднако операция argmax\\\\text{argmax}argmax не дифференцируема, и, к тому же, в концепции VAE на вход декодеру должен пойти семпл из распределения, предсказываемого энкодером, а взятие argmax\\\\text{argmax}argmax на каждой позиции не является семплированием из предсказанного распределения.\\nПоэтому нам потребуется применение некоторых трюков, которые позволят нам одновременно:\\n\\nаппроксимировать семплирование из softmax\\\\text{softmax}softmax\\nсделать семплирование дифференцируемым\\n\\nGumbel-Max Trick и Gumbel-Softmax\\nПервый трюк известен в англоязычной литературе как Gumbel-Max Trick. Представим, что у нас есть логиты-выходы сетки x1,…,xkx_1, \\\\ldots, x_kx1\\u200b,…,xk\\u200b, и мы хотим с их помощью получить семпл из категориального распределения, то есть стохастически предсказать класс. Для этого мы обычно применяем к логитам softmax\\\\text{softmax}softmax, чтобы получить вероятности πi\\\\pi_iπi\\u200b:\\nπi=exp\\u2061xi∑jexp\\u2061xj,    \\\\pi_i = \\\\frac{\\\\exp{x_i}}{\\\\sum_j \\\\exp{x_j}},\\nπi\\u200b=∑j\\u200bexpxj\\u200bexpxi\\u200b\\u200b,а затем из получившегося категориального распределения {π1,…,πk}\\\\left\\\\{ \\\\pi_1, \\\\ldots, \\\\pi_k \\\\right\\\\}{π1\\u200b,…,πk\\u200b} семплируем класс. Оказывается, этим двум шагам будет эквивалентна следующая процедура:\\n\\nнасемплировать числа g1,...,gkg_1, ..., g_kg1\\u200b,...,gk\\u200b из стандартного распределения Гумбеля,\\nприбавить к каждому из логитов xix_ixi\\u200b семпл gig_igi\\u200b,\\nвыбрать класс jjj, такой что j=argmaxi(xi+gi)j = \\\\text{argmax}_i(x_i + g_i)j=argmaxi\\u200b(xi\\u200b+gi\\u200b).\\n\\nО том, почему это действительно так, можно почитать здесь. Однако сам по себе Gumbel-Max Trick нам не поможет — ведь операция так и не стала дифференцируемой. Поэтому придётся использовать ещё один трюк, предложенный практически одновременно в двух статьях (первая и вторая) и названный Gumbel-Softmax в одной из них.\\nЧтобы описать этот трюк, отметим, что результат операции argmax\\\\text{argmax}argmax — это индекс некоторого класса jjj. Такой индекс можно описать one-hot кодированием, то есть вектором длиной kkk, в котором все элементы равны нулю, кроме jjj-го, который равен единице.\\nGumbel-Softmax состоит в том, чтобы вместо взятия argmax\\\\text{argmax}argmax на последнем этапе Gumbel-Max Trick делать следующее:\\n\\nвычислить yi=exp\\u2061((xi+gi)/τ)∑j=1kexp\\u2061((xj+gj)/τ)y_i = \\\\frac{\\\\exp\\\\left( (x_i + g_i) / \\\\tau \\\\right)}{\\\\sum_{j = 1}^k \\\\exp \\\\left( (x_j + g_j) / \\\\tau \\\\right)}yi\\u200b=∑j=1k\\u200bexp((xj\\u200b+gj\\u200b)/τ)exp((xi\\u200b+gi\\u200b)/τ)\\u200b, i=1,…,ki = 1, \\\\ldots, ki=1,…,k, — аппроксимацию one-hot при помощи softmax\\\\text{softmax}softmax с температурой\\nсложить кодовые векторы eie_iei\\u200b с весами yiy_iyi\\u200b: z=∑iyieiz = \\\\sum_i y_i e_iz=∑i\\u200byi\\u200bei\\u200b\\nвыдать вектор zzz в качестве латентного вектора для данной позиции\\n\\nНа самом деле авторы DALL-E не уточняли, как выходной вектор zzz агрегируется из кодовых векторов и yiy_iyi\\u200b, но такой подход применён в реализации DALL-E на PyTorch.\\nПри τ→0\\\\tau \\\\to 0τ→0 семплирование из распределения exp\\u2061((xi+gi)/τ)∑j=1kexp\\u2061((xj+gj)/τ)\\\\frac{\\\\exp\\\\left( (x_i + g_i) / \\\\tau \\\\right)}{\\\\sum_{j = 1}^k \\\\exp \\\\left( (x_j + g_j) / \\\\tau \\\\right)}∑j=1k\\u200bexp((xj\\u200b+gj\\u200b)/τ)exp((xi\\u200b+gi\\u200b)/τ)\\u200b стремится к argmax\\\\text{argmax}argmax, и в процессе обучения dVAE авторы постепенно уменьшали значение τ\\\\tauτ. На следующей картинке слева — просто Gumbel-Max Trick, а справа — дифференцируемый вариант Gumbel-Max Trick:\\n\\nТаким образом, для обучения кодовых векторов для dVAE не требуется дополнительных слагаемых в лоссе относительно ELBO, а также копирования градиентов из декодера в энкодер (как было в VQ-VAE).\\nКроме того, стоит отметить, что DKL(qϕ(z∣x)∥p(z))D_{KL} (q_\\\\phi(z | x) \\\\parallel p(z))DKL\\u200b(qϕ\\u200b(z∣x)∥p(z)) в данном случае не вырождается в константу, а действительно действует как регурялизатор, заставляя категориальное распределение, параметризованное логитами энкодера, быть ближе к равномерному распределению над кодовыми векторами.\\nРаспределение Logit-Laplace\\nЕщё один трюк в обучении dVAE касается выходного распределения pθ(x∥z)p_\\\\theta (x \\\\| z)pθ\\u200b(x∥z). Авторы DALL-E подметили проблему, возникающую при часто встречающемся выборе лапласовского и гауссовского распределений в качестве pθ(x∥z)p_\\\\theta (x \\\\| z)pθ\\u200b(x∥z): оба они определены на всей вещественной прямой, в то время как пиксели принимают значения из ограниченного интервала. Таким образом, часть плотности при моделировании «теряется», оказываясь вне возможных границ значений пикселей.\\nЧтобы исправить эту проблему, авторы предлагают использовать распределение, которое они назвали “Logit-Laplace”. Его плотность определена на интервале (0,1)(0, 1)(0,1) и выражается следующей формулой:\\nf(x∣μ,b)=12bx(1−x)exp\\u2061(−∣logit(x)−μ∣b),    f(x | \\\\mu, b) = \\\\frac{1}{2 b x (1 - x)} \\\\exp \\\\left( -\\\\frac{|\\\\text{logit}(x) - \\\\mu|}{b} \\\\right),\\nf(x∣μ,b)=2bx(1−x)1\\u200bexp(−b∣logit(x)−μ∣\\u200b),logit(x)=x1−x    \\\\text{logit}(x) = \\\\frac{x}{1 - x}\\nlogit(x)=1−xx\\u200bЭта плотность соответствует случайной переменной, полученной применением сигмоиды к распределённой по Лапласу случайной переменной. Выражение для распределения Logit-Laplace можно получить по стандартной формуле для плотности случайной величины, полученной применением монотонной дифференцируемой функции к другой случайной величине (см. формулу, например, тут). Логарифм этой плотности подставляется в ELBO вместо ln\\u2061pθ(x∥z)\\\\ln p_\\\\theta(x \\\\| z)lnpθ\\u200b(x∥z).\\nДекодер на выходе выдаёт 6 тензоров: первые три соответствуют μ\\\\muμ для RGB-каналов, оставшиеся три соответствуют ln\\u2061b\\\\ln blnb, и эти 6 тензоров используются для подсчёта лосса. При подаче в энкодер значения картинок нормируются функцией ϕ:[0,255]→(ε,1−ε)\\\\phi: [0, 255] \\\\to (\\\\varepsilon, 1 - \\\\varepsilon)ϕ:[0,255]→(ε,1−ε):\\nϕ:x↦1−2ε255x+ε    \\\\phi: x \\\\mapsto \\\\frac{1 - 2 \\\\varepsilon}{255} x + \\\\varepsilon\\nϕ:x↦2551−2ε\\u200bx+εЭтим авторы добиваются того, чтобы декодер моделировал значения из (ε,1−ε)(\\\\varepsilon, 1 - \\\\varepsilon)(ε,1−ε), что позволяет нивелировать вычислительные проблемы, связанные с делением на x(1−x)x(1 - x)x(1−x) в формуле плотности. Во время инференса реконструкция x^\\\\hat xx^ картинки xxx вычисляется по формуле:\\nx^=ϕ−1(sigmoid(μ)),    \\\\hat x = \\\\phi^{-1}(\\\\text{sigmoid}(\\\\mu)),\\nx^=ϕ−1(sigmoid(μ)),где μ\\\\muμ — первые три тензора из выхода декодера. Выходы, соответствующие ln\\u2061b\\\\ln blnb, при этом не используются.\\nАприорное распределение на текстах и картинках\\nНа втором этапе авторы фиксируют параметры ϕ\\\\phiϕ и θ\\\\thetaθ и моделируют совместное распределение картинок и их текстовых описаний с помощью Sparse Transformer с 12 миллиардами параметров. На вход он получает конкатенацию из текстового описания картинки и её кодовых векторов. Картинка представляется 1024 кодовыми векторами, получаемыми из энкодера qϕq_\\\\phiqϕ\\u200b, причём при семплировании кодовых последовательностей используется обычный argmax\\\\text{argmax}argmax без добавления шума из распределения Гумбеля.\\nТекстовое описание токенизируется с помощью процедуры BPE (см. раздел про BPE здесь), и каждому токену ставится в соответствие представляющий его вектор из вещественных чисел (эмбеддинг). Для представления текста используется не более 256 токенов, а размер используемого словаря — 16 384 токена.\\nЗадача Трансформера во время обучения — для каждого начального отрезка входной последовательности предсказать следующий за ним токен. Это может быть как текстовый токен, так и кодовый вектор картинки. Поскольку кодовые векторы картинок всегда идут за текстовыми токенами, при генерации кодовых векторов attention-механизм учитывает также и все предыдущие текстовые токены.\\nКроме того, маска attention для кодовых векторов учитывает, что исходно они расположены не линейно друг за другом, а на прямоугольной сетке. В статье приводится несколько вариантов геометрических паттернов, которые использовались для attention-маски на кодовых векторах.\\nВ качестве лосса используется взвешенная сумма кросс-энтропии для текстовых токенов и кросс-энтропии для кодовых векторов картинок c весами 18\\\\frac{1}{8}81\\u200b и 78\\\\frac{7}{8}87\\u200b соответственно (больший приоритет отдаётся генерации картнок, отсюда и больший вес для лосса).\\nКонечно, огромный Трансформер обучить крайне непросто, и очень существенная часть статьи посвящена трюкам, которые авторы применили для обучения такой большой модели.\\n\\nИнференс\\nНа этапе инференса в модель подаются токены текстового описания картинки, и на их основании модель авторегрессионно предсказывает кодовые векторы:\\n\\nКодовые векторы картинки подаются в декодер dVAE, который отображает их в финальную картинку:\\n\\nДля повышения качества предсказания авторы сначала генерируют 512 картинок для каждого текстового описания, а затем выбирают лучшую картинку из предсказанных. Разные наборы кодовых векторов для одного и того же текста можно получить, например, случайно выбирая на каждом шаге генерации какой-то кодовый вектор согласно предсказанному Трансформером распределению. Ранжирование полученных 512 картинок осуществляется с помощью CLIP — большой нейросети, обучавшейся в режиме без учителя на большом количестве данных моделировать совместное распределение картинок и текстов.\\nЗаключение\\nИтак, в этом параграфе мы поговорили о том, как устроен VAE в классическом смысле, — с непрерывным распределением латентных переменных, а также поговорили о работах, основанных на идеях использования дискретных распределений для VAE.\\nКонечно, различные модификации VAE не исчерпываются только лишь отказом от непрерывных латентных переменных в пользу дискретных. Есть множество других возможных направлений для улучшения модели: использование иерархических латентных распределений (которые мы, кстати, видели в контексте VQ-VAE-2), использование функций потерь, отличающихся от ELBO, выбор различных форм латентных пространств, применение adversarial-обучения и многое другое.\\nХороший список различных статей, посвящённых модификациям VAE, можно найти здесь. Из недавних работ, связанных с применением иерархических распределений, интересной кажется NVAE — семплы из модели выглядят весьма впечатляюще. Про неё есть хороший видеообзор от Yannic Kilcher.\\nНа этом мы завершаем рассказ о VAE. Будем надеяться, что он дал вам общее представление и об исходных идеях, из которых выросла модель VAE, и о наиболее интересных последних результатах, связанных с ней.\\nА в следующем параграфе мы поговорим о генеративно-состязательных сетях.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф8.1. Введение в генеративное моделированиеСледующий параграф8.3. Генеративно-состязательные сети (GAN)Яндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_65.html', 'title': 'Вероятностные распределения'}, page_content=\"Вероятностные распределенияЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/716.1.Матричное дифференцирование16.2.Матричная факторизация16.3.Вероятностные распределенияВероятностное пространствоДискретные распределенияНепрерывные распределенияХарактеристики случайных величин16.4.Многомерные распределения16.5.Независимость и условные распределения вероятностей16.6.Параметрические оценки16.7.Энтропия и семейство экспоненциальных распределенийГлавная/Хендбуки/Учебник по машинному обучению/Вероятностные распределения16.3. Вероятностные распределенияАвторыСергей ЛыткинПринимая то или иное решение в условиях недостаточной информации, нам часто приходится взвешивать шансы, просчитывать риски, а то и вовсе уповать на удачу. Теория вероятностей предоставляет математические инструменты для проведения корректных рассуждений в условиях неопределённости, количественного измерения характеристик случайных событий и оценки правдоподобия их реализации.\\nЭтот и последующий параграфы следует рассматривать как расширенный справочник, позволяющий освежить знания по вероятности и статистике, сделав при этом упор на приложении к машинному обучению. За более систематическим курсом по теории вероятностей читателю следует обратиться к серьёзным учебникам вроде Ширяева или Феллера.\\nДля погружения в статистику смотри, например, книгу Лагутина. А особо нетерпеливым рекомендуем взглянуть на короткий и ёмкий Probability and Statistics Cookbook.\\nВероятностное пространство\\nВ учебниках вероятность традиционно поставляется в комплекте с вероятностным пространством. Не увлекаясь чрезмерным формализмом, можно сказать, что для задания вероятностного пространства нужны:\\n\\n\\nнепустое множество Ω\\\\OmegaΩ, называемое пространством элементарных событий (исходов);\\n\\n\\nалгебра множеств F⊂2Ω\\\\mathcal F \\\\subset 2^{\\\\Omega}F⊂2Ω — набор подмножеств Ω\\\\OmegaΩ, замкнутый относительно дополнений, объединений и пересечений; каждый элемент A∈FA\\\\in\\\\mathcal FA∈F называется событием;\\n\\n\\nвероятностная мера P\\u2009\\u2063:F→[0,1]\\\\mathbb P \\\\colon \\\\mathcal F \\\\to [0, 1]P:F→[0,1], приписывающая каждому событию A∈FA \\\\in \\\\mathcal FA∈F некоторую вероятность P(A)∈[0,1]\\\\mathbb P(A) \\\\in [0,1]P(A)∈[0,1].\\n\\n\\nК вероятностному пространству (Ω,F,P)(\\\\Omega, \\\\mathcal F, \\\\mathbb P)(Ω,F,P) предъявляются следующие требования:\\n\\n∅∈F\\\\varnothing \\\\in \\\\mathcal F∅∈F (невозможное событие), Ω∈F\\\\Omega \\\\in \\\\mathcal FΩ∈F (достоверное событие);\\nP(Ω)=1\\\\mathbb P(\\\\Omega) = 1P(Ω)=1;\\nP(A∪B)=P(A)+P(B)\\\\mathbb P(A\\\\cup B) = \\\\mathbb P(A) + \\\\mathbb P(B)P(A∪B)=P(A)+P(B), если A,B∈FA, B \\\\in \\\\mathcal FA,B∈F и A∩B=∅A\\\\cap B= \\\\varnothingA∩B=∅ (аддитивность).\\n\\nУпражнение. Докажите, что P(∅)=0\\\\mathbb P(\\\\varnothing) = 0P(∅)=0 и P(Aˉ)=1−P(A)\\\\mathbb P(\\\\bar A) = 1 - \\\\mathbb P(A)P(Aˉ)=1−P(A), если A∈FA\\\\in\\\\mathcal FA∈F.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nРешение (не открывайте сразу, попробуйте сначала решить самостоятельно)События AAA и Aˉ=Ω∖A\\\\bar A = \\\\Omega \\\\setminus AAˉ=Ω∖A не пересекаются (или, как говорят, несовместны), поэтому из аддитивности вероятностной меры следует, что\\nP(A)+P(Aˉ)=P(Ω)=1\\\\mathbb P(A) + \\\\mathbb P(\\\\bar A) = P(\\\\Omega) = 1P(A)+P(Aˉ)=P(Ω)=1. Полагая A=∅A = \\\\varnothingA=∅, получаем P(∅)+1=1\\\\mathbb P(\\\\varnothing) + 1 = 1P(∅)+1=1, т.е. P(∅)=0\\\\mathbb P(\\\\varnothing) = 0P(∅)=0.\\nАддитивность вероятности легко обобщается по индукции до свойства конечной аддитивности: если события A1,…,AnA_1, \\\\ldots, A_nA1\\u200b,…,An\\u200b попарно несовместны, то\\nP(⋃k=1nAk)=∑k=1nP(Ak).    \\\\mathbb P\\\\Big(\\\\bigcup\\\\limits_{k=1}^n A_k\\\\Big) = \\\\sum\\\\limits_{k=1}^n \\\\mathbb P(A_k).\\nP(k=1⋃n\\u200bAk\\u200b)=k=1∑n\\u200bP(Ak\\u200b).ПримечаниеЕсли подходить совсем строго, то система множеств F\\\\mathcal FF должна быть сигма-алгеброй, а вероятностная мера P\\\\mathbb PP — сигма-аддитивной, т.е.\\nP(⋃n=1∞An)=∑n=1∞P(An),Ai∩Aj=∅\\xa0при\\xa0i≠j.  \\\\mathbb P\\\\Big(\\\\bigcup\\\\limits_{n=1}^\\\\infty A_n\\\\Big) = \\\\sum\\\\limits_{n=1}^\\\\infty \\\\mathbb P(A_n), \\\\quad A_i \\\\cap A_j = \\\\varnothing \\\\text{ при } i\\\\ne j.\\nP(n=1⋃∞\\u200bAn\\u200b)=n=1∑∞\\u200bP(An\\u200b),Ai\\u200b∩Aj\\u200b=∅\\xa0при\\xa0i\\ue020=j.Впрочем, эти свойства носят преимущественно теоретичеческий интерес, поскольку в\\nприкладных задачах редко встречаются бесконечные наборы событий.\\nМножество Ω\\\\OmegaΩ часто называют носителем; говорят также, что вероятностная мера (масса) P\\\\mathbb PP сосредоточена, или распределена, на носителе Ω\\\\OmegaΩ. В зависимости от типа носителя Ω\\\\OmegaΩ распределения делятся на два типа: дискретные и непрерывные.\\nДискретные распределения\\nВероятность на не более чем счётном пространстве элементарных событий Ω={ω1,ω2,…}\\\\Omega = \\\\{\\\\omega_1, \\\\omega_2, \\\\ldots\\\\}Ω={ω1\\u200b,ω2\\u200b,…} задаётся просто приписыванием неотрицательного числа pkp_kpk\\u200b каждому элементарному исходу ωk\\\\omega_kωk\\u200b с условием ∑kpk=1\\\\sum\\\\limits_k p_k = 1k∑\\u200bpk\\u200b=1. Для произвольного события A⊂ΩA \\\\subset \\\\OmegaA⊂Ω полагают P(A)=∑ωk∈Apk\\\\mathbb P(A) =  \\\\sum\\\\limits_{\\\\omega_k \\\\in A} p_kP(A)=ωk\\u200b∈A∑\\u200bpk\\u200b. Набор чисел {pk}\\\\{p_k\\\\}{pk\\u200b} называют также распределением вероятностей на множестве Ω\\\\OmegaΩ. В англоязычной литературе распространён термин probability mass function (сокращённо pmf).\\nЗачастую в результате эксперимента нас интересуют не вероятности событий сами по себе, а значения некоторой связанной с ними случайной величины, принимающей числовые значения. Например:\\n\\nсумма очков, выпавших при броске двух кубиков;\\nчисло метеоритов диаметром более одного метра, падающих на Землю в течение года;\\nежедневный доход от показа рекламных объявлений в интернете.\\n\\nНа каждом элементарном исходе ωk\\\\omega_kωk\\u200b случайная величина ξ\\\\xiξ принимает некоторое числовое значение ξk=ξ(ωk)\\\\xi_k = \\\\xi(\\\\omega_k)ξk\\u200b=ξ(ωk\\u200b). Иными словами, случайная величина — это функция ξ\\u2009\\u2063:Ω→R\\\\xi \\\\colon \\\\Omega \\\\to \\\\mathbb Rξ:Ω→R, принимающая значение ξk\\\\xi_kξk\\u200b с вероятностью pkp_kpk\\u200b; её математическое ожидание (среднее) и дисперсия (среднеквадратичное отклонение) вычисляются по формулам\\nEξ=∑kξkpk\\xa0и\\xa0Vξ=E(ξ−Eξ)2=Eξ2−(Eξ)2\\\\mathbb E\\\\xi = \\\\sum\\\\limits_{k} \\\\xi_k p_k \\\\text{ и }\\\\mathbb V\\\\xi = \\\\mathbb E\\\\big(\\\\xi - \\\\mathbb E \\\\xi\\\\big)^2 = \\\\mathbb E \\\\xi^2 - \\\\big(\\\\mathbb E \\\\xi\\\\big)^2\\nEξ=k∑\\u200bξk\\u200bpk\\u200b\\xa0и\\xa0Vξ=E(ξ−Eξ)2=Eξ2−(Eξ)2соответственно. Корень из дисперсии Vξ\\\\sqrt{\\\\mathbb V \\\\xi}Vξ\\u200b назвают стандартным отклонением случайной величины ξ\\\\xiξ. Стандартное отклонение и дисперсия показывают, насколько далеко значения случайной величины могут отклоняться от среднего значения. Стандартное отклонение хорошо тем, что, в отличие от дисперсии, измеряется в тех же единицах, что и сама случайная величина.\\nРавномерное распределение\\nТак называется распределение вероятностней на множестве Ω={ω1,…,ωn}\\\\Omega = \\\\{\\\\omega_1, \\\\ldots, \\\\omega_n\\\\}Ω={ω1\\u200b,…,ωn\\u200b}, у которого pk=P(ωk)=1np_k =\\\\mathbb P(\\\\omega_k) = \\\\frac 1npk\\u200b=P(ωk\\u200b)=n1\\u200b, 1⩽k⩽n1\\\\leqslant k\\\\leqslant n1⩽k⩽n. Тогда P(A)=∑ωk∈A1n=∣A∣∣Ω∣\\\\mathbb P(A) = \\\\sum\\\\limits_{\\\\omega_k \\\\in A} \\\\frac 1n  = \\\\frac{\\\\vert A\\\\vert}{\\\\vert \\\\Omega\\\\vert}P(A)=ωk\\u200b∈A∑\\u200bn1\\u200b=∣Ω∣∣A∣\\u200b, и мы получили формулу из классического подхода к вероятности, при котором вероятность события равна отношению числа благоприятных исходов к общему их количеству.\\nРавномерным распределением моделируются различные игровые ситуации:\\n\\nподбрасывание симметричной монеты (ω1=«орёл»\\\\omega_1 = «орёл»ω1\\u200b=«орёл», ω2=«решка»\\\\omega_2 = «решка»ω2\\u200b=«решка»);\\nподбрасывание кубика (ωk=k\\\\omega_k = kωk\\u200b=k, 1⩽k⩽61\\\\leqslant k \\\\leqslant 61⩽k⩽6);\\nвращение рулетки в казино (n=37n=37n=37 для европейской, n=38n=38n=38 для американской).\\n\\nУпражнение. У европейской рулетки по 181818 чёрных и красных секторов и один сектор «зеро». Игрок ставит €10 на чёрное. В случае успеха казино выплачивает ему ещё €10, в противном случае забирает ставку. Чему равно математическое ожидание, дисперсия и стандартное отклонение выигрыша?\\nРешение (не открывайте сразу, попробуйте сначала решить самостоятельно)Здесь выигрыш — это случайная величина ξ\\\\xiξ, принимающая значение −10-10−10 евро в 191919-ти исходах и +10+10+10 евро в 181818-ти. Распределение равномерное (если верить в честность казино), поэтому Eξ=10⋅1837−10⋅1937=−1037\\\\mathbb E\\\\xi = 10\\\\cdot\\\\frac{18}{37} - 10\\\\cdot\\\\frac{19}{37} = -\\\\frac {10}{37}Eξ=10⋅3718\\u200b−10⋅3719\\u200b=−3710\\u200b евро.\\nСлучайная величина ξ2\\\\xi^2ξ2 постоянна (а вовсе не «случайна») и при любом исходе равна 100 («квадратных евро»?), поэтому\\nVξ=Eξ2−(Eξ)2=100⋅(1−1372)≈99.927.  \\\\mathbb V\\\\xi = \\\\mathbb E \\\\xi^2 - \\\\big(\\\\mathbb E \\\\xi\\\\big)^2 = 100 \\\\cdot\\\\Big(1 - \\\\frac{1}{37^2}\\\\Big)\\\\approx 99.927.\\nVξ=Eξ2−(Eξ)2=100⋅(1−3721\\u200b)≈99.927.Стандартное отклонение выигрыша равно 10136837≈9.996\\\\frac{10 \\\\sqrt{1368}}{37} \\\\approx 9.99637101368\\u200b\\u200b≈9.996 евро, что почти совпадает со ставкой игрока. Это отражает сущность игры в рулетку: либо пан, либо пропал. Причём последнее чуть вероятнее из-за отрицательности математического ожидания.\\nВопрос на подумать. Бывают ли равномерные распределения в пространствах со счётным носителем?\\nОтвет (не открывайте сразу, сначала подумайте самостоятельно)Нет, поскольку условие нормировки требует сходимости ряда ∑kpk=1\\\\sum\\\\limits_{k} p_k = 1k∑\\u200bpk\\u200b=1, а это равенство никак не может быть выполнено при одинаковых числах pkp_kpk\\u200b. Либо все эти вероятности равны нулю, и тогда их сумма тоже равна нулю; либо же получается бесконечная сумма одинаковых положительных слагаемых, которая равна +∞+\\\\infty+∞.\\nРавномерные распределения преимущественно встречаются в разного сорта играх. В более жизненных ситациях случайность обычно распределена отнюдь не равномерно.\\nРаспределение Бернулли\\nТак называется очень простое распределение всего лишь с двумя исходами:\\nP(«успех»)=p,P(«неудача»)=1−p,0⩽p⩽1.    \\\\mathbb P(\\\\text{«успех»}) = p, \\\\quad \\\\mathbb P(\\\\text{«неудача»}) = 1-p, \\\\quad 0\\\\leqslant p \\\\leqslant 1.\\nP(«успех»)=p,P(«неудача»)=1−p,0⩽p⩽1.Бернуллиевская случайная величина ξ∼Bern(p)\\\\xi\\\\sim\\\\mathrm{Bern}(p)ξ∼Bern(p) — это просто индикаторная функция успешного события: ξ=1\\\\xi = 1ξ=1, если случился «успех», ξ=0\\\\xi = 0ξ=0, если нас постигла «неудача». Несложные вычисления показывают, что\\nEξ=1⋅p+0⋅(1−p)=p,Vξ=p−p2=p(1−p).\\\\mathbb E \\\\xi = 1\\\\cdot p + 0 \\\\cdot (1-p) = p, \\\\quad\\n\\\\mathbb V \\\\xi = p-p^2 = p(1-p).\\nEξ=1⋅p+0⋅(1−p)=p,Vξ=p−p2=p(1−p).Если p=12p=\\\\frac 12p=21\\u200b, то снова получается равномерное распределение с двумя исходами. При p≠12p\\\\ne \\\\frac 12p\\ue020=21\\u200b бернуллиевская случайная величина моделирует подбрасывание несимметричной монеты. В машинном обучении часто встречается задача бинарной классификации, и разбиение на классы обычно кодируется с помощью Bern(p)\\\\mathrm{Bern}(p)Bern(p), например:\\n\\nдиагностика болезни (болен — 111, здоров — 000);\\nоценка кредитоспособности клиента (одобрить кредит — 000, отказать 111);\\nпредсказание поведения пользователя (кликнет на рекламу — 111, пропустит — 000).\\n\\nВ этих примерах вероятности классов явно не равны, поэтому несимметричное распределение Бернулли — типичная ситуация в реальных задачах.\\nБиномиальное распределение\\nБиномиальное распределение Bin(n,p)\\\\mathrm{Bin}(n, p)Bin(n,p) имеет сумма независимых бернуллиевских случайных величин ξk∼Bern(p)\\\\xi_k\\\\sim \\\\mathrm{Bern}(p)ξk\\u200b∼Bern(p): η∼Bin(n,p)\\\\eta \\\\sim \\\\mathrm{Bin}(n, p)η∼Bin(n,p), если η=ξ1+…+ξn\\\\eta = \\\\xi_1 + \\\\ldots + \\\\xi_nη=ξ1\\u200b+…+ξn\\u200b. Другими словами, случайная величина η\\\\etaη равна количеству успехов в nnn независимых испытаниях Бернулли с вероятностью успеха ppp. Случайная величина η\\\\etaη принимает значения от 000 до nnn, и\\npk=P(η=k)=(nk)pk(1−p)n−k,0⩽k⩽n.  p_k=P(\\\\eta = k) = \\\\binom nk p^k (1-p)^{n-k},\\\\quad 0\\\\leqslant k \\\\leqslant n.\\npk\\u200b=P(η=k)=(kn\\u200b)pk(1−p)n−k,0⩽k⩽n.\\nОтметим, что согласно биному Ньютона\\n∑k=0npk=(p+(1−p))n=1,  \\\\sum\\\\limits_{k=0}^n p_k = \\\\big(p + (1-p)\\\\big)^n = 1,\\nk=0∑n\\u200bpk\\u200b=(p+(1−p))n=1,поэтому числа {pk}\\\\{p_k\\\\}{pk\\u200b} действительно представляют собой распределение вероятностей, называемое также биномиальным. Если ξ∼Bin(n,p)\\\\xi \\\\sim\\\\mathrm{Bin}(n, p)ξ∼Bin(n,p), то\\nEξ=np,Vξ=np(1−p).\\\\mathbb E \\\\xi = np, \\\\quad \\\\mathbb V \\\\xi = np(1-p).\\nEξ=np,Vξ=np(1−p).Пример. Каждый день рекламу компании А поисковой выдаче Яндекса видят ровно 100010001000 человек. Вчера 505050 из них кликнули на рекламу. Для прогнозирования объемов продаж компании А хочется знать, с какой вероятностью не менее 50 людей кликнут на ее рекламу сегодня.\\nЕсли моделировать наличие или отсутствие клика бернуллиевской случайной величиной, то общее количество кликов за день моделируется случайной величиной ξ∼Bin(n,p)\\\\xi \\\\sim \\\\mathrm{Bin}(n, p)ξ∼Bin(n,p) с параметрами n=1000n=1000n=1000 и p=501000=0.05p = \\\\frac{50}{1000} = 0.05p=100050\\u200b=0.05. Тогда с помощью вычислительной техники получаем, что\\nP(ξ⩾50)=∑k=50n(nk)pk(1−p)n−k=1−∑k=049(1000k)0.05k0.951000−k≈0.52.  \\\\mathbb P(\\\\xi \\\\geqslant 50) = \\\\sum\\\\limits_{k = 50}^{n} \\\\binom{n}{k} p^k (1 - p)^{n-k} = 1 - \\\\sum\\\\limits_{k = 0}^{49} \\\\binom {1000}{k} 0.05^k 0.95^{1000 - k} \\\\approx 0.52.\\nP(ξ⩾50)=k=50∑n\\u200b(kn\\u200b)pk(1−p)n−k=1−k=0∑49\\u200b(k1000\\u200b)0.05k0.951000−k≈0.52.Отметим, что параметр ppp в предыдущем примере нам, строго говоря, не был известен, и вместо него мы использовали частотную оценку.\\nРаспределение Пуассона\\nЭто распределение имеет счётный носитель Ω=N∪{0}\\\\Omega = \\\\mathbb N \\\\cup \\\\{0\\\\}Ω=N∪{0}. Случайная величина ξ\\\\xiξ имеет пуассоновское распределение с параметром λ\\\\lambdaλ, ξ∼Pois(λ)\\\\xi \\\\sim \\\\mathrm{Pois}(\\\\lambda)ξ∼Pois(λ), если\\nP(ξ=k)=e−λλkk!,k∈N∪{0}.  \\\\mathbb P(\\\\xi = k) = e^{-\\\\lambda} \\\\frac{\\\\lambda^k}{k!}, \\\\quad k \\\\in \\\\mathbb N \\\\cup \\\\{0\\\\}.\\nP(ξ=k)=e−λk!λk\\u200b,k∈N∪{0}.\\nИзвестное разложение экспоненты в ряд Тейлора eλ=∑k=0∞λkk!e^\\\\lambda = \\\\sum\\\\limits_{k=0}^\\\\infty  \\\\frac{\\\\lambda^k}{k!}eλ=k=0∑∞\\u200bk!λk\\u200b позволяет заключить, что вероятности распределения Пуассона действительно суммируются в единицу. Этот же ряд позволяет вычислить, что\\nEξ=Vξ=λ.\\\\mathbb E \\\\xi = \\\\mathbb V \\\\xi = \\\\lambda.\\nEξ=Vξ=λ.Пуассоновская случайная величина моделирует число редких событий, происходящих в течение фиксированного промежутка времени: если события наступают со средней скоростью rrr, то\\nP(k\\xa0событий\\xa0на\\xa0промежутке\\xa0t)=e−rt(rt)kk!,k∈N∪{0}.  \\\\mathbb P(k \\\\text{ событий на промежутке } t) = e^{-rt} \\\\frac{(rt)^k}{k!}, \\\\quad k \\\\in \\\\mathbb N \\\\cup \\\\{0\\\\}.\\nP(k\\xa0событий\\xa0на\\xa0промежутке\\xa0t)=e−rtk!(rt)k\\u200b,k∈N∪{0}.Иногда приходится рассматривать биномиальное распределение Bin(n,p)\\\\mathrm{Bin}(n, p)Bin(n,p) с большим числом попыток nnn и вероятностью успеха ppp с условием np≈λ>0np \\\\approx \\\\lambda > 0np≈λ>0. Оказывается, что вне зависимости от nnn такое распределение быстро стабилизируется, сходясь к пуассоновскому распределению с параметром λ\\\\lambdaλ. Точнее говоря, справедлива следующая теорема.\\nТеорема (Пуассон). Пусть ξ∼Bin(n,pn)\\\\xi \\\\sim \\\\mathrm{Bin}(n, p_n)ξ∼Bin(n,pn\\u200b) и lim\\u2061n→∞npn=λ>0\\\\lim\\\\limits_{n \\\\to \\\\infty} np_n = \\\\lambda > 0n→∞lim\\u200bnpn\\u200b=λ>0. Тогда\\nlim\\u2061n→∞P(ξ=k)=e−λλkk!,k∈N∪{0}.  \\\\lim\\\\limits_{n \\\\to \\\\infty} \\\\mathbb{P}(\\\\xi = k) =  e^{-\\\\lambda} \\\\frac{\\\\lambda^k}{k!}, \\\\quad k \\\\in \\\\mathbb N \\\\cup \\\\{0\\\\}.\\nn→∞lim\\u200bP(ξ=k)=e−λk!λk\\u200b,k∈N∪{0}.Пример. Известно, что на поисковой выдаче яндекса на рекламу компании А кликает в среднем примерно 50 пользоваталей в день. Количество показов достаточно большое и может меняться изо дня в день. Требуется оценить вероятность того, что сегодня будет совершено не менее 50 кликов по рекламным объявлениям.\\nРаспределение количества кликов снова будем моделировать биномиальным распределением Bin(n,p)\\\\mathrm{Bin}(n, p)Bin(n,p). На этот раз число nnn нам неизвестно, но сказано, что оно велико и np≈50np \\\\approx 50np≈50 (вспомним, что Eξ=np\\\\mathbb E\\\\xi = npEξ=np, если ξ∼Bin(n,p)\\\\xi \\\\sim \\\\mathrm{Bin}(n, p)ξ∼Bin(n,p)). Поэтому можно воспользоваться теоремой Пуассона и заменить биномиальное распределение пуассоновским с параметром λ=50\\\\lambda = 50λ=50. Тогда искомая вероятность равна\\n1−∑k=049e−5050kk!≈0.518,  1 - \\\\sum\\\\limits_{k = 0}^{49} e^{-50} \\\\frac{50^k}{k!} \\\\approx 0.518,\\n1−k=0∑49\\u200be−50k!50k\\u200b≈0.518,что практически совпадает ответом, полученным с помощью биномиального распределения при n=1000n = 1000n=1000.\\nГеометрическое распределение\\nПусть монетка с вероятностью «успеха» ppp подбрасывается до тех пор, пока впервые не случится «успех». Случайная величина ξ\\\\xiξ, равная общему количеству попыток на этом пути, имеет геометрическое распределение, т.е.\\nP(ξ=k)=qk−1p,q=1−p,k∈N.  \\\\mathbb P(\\\\xi = k) = q^{k-1}p, \\\\quad q = 1-p, \\\\quad k \\\\in \\\\mathbb N.\\nP(ξ=k)=qk−1p,q=1−p,k∈N.\\nПо формуле геометрической прогрессии находим, что\\n∑k=1∞P(ξ=k)=∑k=0∞qkp=p1−q=1,  \\\\sum\\\\limits_{k=1}^\\\\infty \\\\mathbb P(\\\\xi = k) = \\\\sum\\\\limits_{k=0}^\\\\infty q^kp = \\\\frac p{1-q} = 1,\\nk=1∑∞\\u200bP(ξ=k)=k=0∑∞\\u200bqkp=1−qp\\u200b=1,поэтому с нормировкой тут всё в порядке. Чем меньше ppp, тем больше геометрическое распределение похоже на равномерное, что подтверждают и формулы для среднего и дисперсии:\\nEξ=1p,Vξ=1−pp2.\\\\mathbb E \\\\xi = \\\\frac 1p, \\\\quad \\\\mathbb V \\\\xi = \\\\frac{1-p}{p^2}.\\nEξ=p1\\u200b,Vξ=p21−p\\u200b.Пример. По оценкам за предыдущие дни пользователь нажимает на рекламу с вероятностью p=0.05p=0.05p=0.05. Сегодня компания B планирует показать очень важное рекламное объявление и требует от Яндекса, чтобы с вероятностью не менее 99%99\\\\%99% на него кликнули хотя бы раз. Скольким различным людям следует показать это объявление?\\nЗдесь мы имеем дело с геометрическим распределением с вероятностью «успеха» (клика) ppp: именно так распределена случайная величина ξ\\\\xiξ, равная количеству показов объявления до первого клика по нему. Следовательно,\\nP(ξ⩽n)=∑k=1nP(ξ=k)=∑k=1nqk−1p=p⋅1−qn1−q=1−qn.  \\\\mathbb P(\\\\xi \\\\leqslant n) = \\\\sum\\\\limits_{k=1}^n \\\\mathbb P(\\\\xi = k) = \\\\sum\\\\limits_{k=1}^n q^{k-1}p = p\\\\cdot \\\\frac{1 - q^{n}}{1-q} = 1-q^n.\\nP(ξ⩽n)=k=1∑n\\u200bP(ξ=k)=k=1∑n\\u200bqk−1p=p⋅1−q1−qn\\u200b=1−qn.Эта вероятность должна быть не меньше 99%99\\\\%99%, т. е. 0.95n⩾0.010.95^n \\\\geqslant 0.010.95n⩾0.01. Отсюда находим, что\\nn⩾log\\u20610.01log\\u20610.95≈89.78n \\\\geqslant \\\\frac{\\\\log 0.01}{\\\\log 0.95} \\\\approx 89.78n⩾log0.95log0.01\\u200b≈89.78. Таким образом, рекламу надо показать как минимум 909090 раз.\\nГипергеометрическое распределение\\nПример. Известно, что партия из NNN деталей содержит KKK бракованных. Какова вероятность того, что среди выбранных наугад nnn деталей окажется ровно kkk бракованных?\\nВсего есть (Nn)\\\\binom N n(nN\\u200b) способов выбора nnn деталей из партии. Число вариантов выбрать kkk деталей из KKK бракованных и n−kn-kn−k из N−KN-KN−K деталей без дефектов равно (Kk)(N−Kn−k)\\\\binom K k \\\\binom{N-K}{n-k}(kK\\u200b)(n−kN−K\\u200b). По классическому определению вероятности получаем, что искомая вероятность равна\\npk=(Kk)(N−Kn−k)(Nn),0⩽k⩽min\\u2061{K,n}.  p_k = \\\\frac{\\\\binom K k \\\\binom{N-K}{n-k}}{\\\\binom N n}, \\\\quad 0 \\\\leqslant k \\\\leqslant \\\\min\\\\{K, n\\\\}.\\npk\\u200b=(nN\\u200b)(kK\\u200b)(n−kN−K\\u200b)\\u200b,0⩽k⩽min{K,n}.Такое распределение называется гипергеометрическим. Равенство\\n∑k=0min\\u2061{K,n}pk=1\\\\sum\\\\limits_{k=0}^{\\\\min\\\\{K, n\\\\}} p_k = 1\\nk=0∑min{K,n}\\u200bpk\\u200b=1следует из тождества Вандермонда. Если случайная величина ξ\\\\xiξ имеет гипергеометрическое распределение с параметрами NNN, KKK, nnn, то\\nEξ=nKN,Vξ=nK(N−K)(N−n)N2(N−1).  \\\\mathbb E \\\\xi = \\\\frac{nK}{N}, \\\\quad \\\\mathbb V\\\\xi = n\\\\frac{K(N-K)(N-n)}{N^2(N-1)}.\\nEξ=NnK\\u200b,Vξ=nN2(N−1)K(N−K)(N−n)\\u200b.Гипергеометрическое распределение является аналогом биномиального, при котором моделируется выбор без возвращения с вероятностью успеха p≈KNp\\\\approx \\\\frac KNp≈NK\\u200b.\\nНепрерывные распределения\\nВероятностная модель с конечным или счётным носителем не подходит в тех случаях, когда результатом эксперимента удобно считать произвольное действительное число, например, распределение людей по росту или по весу. Для этого требуется пересмотреть подход к построению пространства элементарных событий Ω\\\\OmegaΩ: ведь множество действительных чисел R\\\\mathbb RR континуально, и поэтому вероятность события не получится определить как сумму вероятностей всех составляющих исходов, коих тоже может оказаться континуум. Приходится искать другие способы задания вероятности.\\nНаиболее часто встречающийся на практике класс непрерывных распределений на числовой прямой задаётся с помощью неотрицательной интегрируемой функции плотности (probability density function, pdf) p(x)p(x)p(x) со свойством\\n∫−∞+∞p(x)dx=1.  \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p(x) dx = 1.\\n−∞∫+∞\\u200bp(x)dx=1.Вероятность события AAA определяется как\\nP(A)=∫Ap(x)dx  \\\\mathbb{P}(A) = \\\\int\\\\limits_{A} p(x) dx\\nP(A)=A∫\\u200bp(x)dxпри условии, что этот интеграл имеет смысл. В частности,\\nP([a,b))=∫abp(x)dx.  \\\\mathbb{P}([a, b)) = \\\\int\\\\limits_a^b p(x) dx.\\nP([a,b))=a∫b\\u200bp(x)dx.Замечание. Связь между вероятностью и плотностью распределения весьма напоминает связь между массой и физической плотностью. Когда плотность объекта всюду одинакова, то масса равна плотности, умноженной на объём. Если же объект неоднороден, то плотность становится функцией, сопоставляющей каждой точке некое число (что-то вроде предела отношения массы малого шарика вокруг этой точки к объёму шарика). Тогда масса любого куска объекта может быть вычислена, как интеграл функции плотности по объёму этого куска.\\nС плотностью вероятности p(x)p(x)p(x) автоматически связана случайная величина ξ\\u2009\\u2063:R→R\\\\xi\\\\colon  \\\\mathbb R  \\\\to  \\\\mathbb Rξ:R→R, для которой P(a⩽ξ<b)=∫abp(x)\\u2009dx\\\\mathbb P(a\\\\leqslant \\\\xi < b) = \\\\int\\\\limits_a^b p(x)\\\\,dxP(a⩽ξ<b)=a∫b\\u200bp(x)dx. Функция p(x)p(x)p(x) называется плотностью случайной величины ξ\\\\xiξ, и обозначается также как pξ(x)p_\\\\xi(x)pξ\\u200b(x). Иногда используется запись ξ∼p(x)\\\\xi \\\\sim p(x)ξ∼p(x). Среднее и дисперсия случайной величины ξ∼p(x)\\\\xi \\\\sim p(x)ξ∼p(x) вычисляются по формулам\\nEξ=∫−∞∞xp(x)\\u2009dx,Vξ=∫−∞∞x2p(x)\\u2009dx−(Eξ)2.  \\\\mathbb E \\\\xi = \\\\int\\\\limits_{-\\\\infty}^{\\\\infty} xp(x)\\\\,dx,\\\\quad\\n  \\\\mathbb V \\\\xi = \\\\int\\\\limits_{-\\\\infty}^{\\\\infty} x^2p(x)\\\\,dx - (\\\\mathbb E \\\\xi)^2.\\nEξ=−∞∫∞\\u200bxp(x)dx,Vξ=−∞∫∞\\u200bx2p(x)dx−(Eξ)2.Равномерное распределение\\nРавномерное распределение на отрезке [a;b][a;b][a;b], которое часто обозначают U[a,b]U[a,b]U[a,b], имеет постоянную плотность на этом отрезке:\\np(x)=I(a⩽x⩽b)b−a={1b−a,x∈[a,b],0,x∉[a,b].  p(x) = \\\\frac {\\\\mathbb I(a\\\\leqslant x \\\\leqslant b)}{b-a} = \\\\begin{cases}\\n  \\\\frac 1{b-a},& x\\\\in[a, b],\\\\\\\\\\n  0, & x \\\\notin [a, b].\\n  \\\\end{cases}\\np(x)=b−aI(a⩽x⩽b)\\u200b={b−a1\\u200b,0,\\u200bx∈[a,b],x∈/[a,b].\\u200bЕсли ξ∼U[a,b]\\\\xi \\\\sim U[a,b]ξ∼U[a,b], то\\nEξ=a+b2,Vξ=(b−a)212.\\\\mathbb E \\\\xi = \\\\frac {a+b}2,\\\\quad \\\\mathbb V \\\\xi = \\\\frac{(b-a)^2}{12}.\\nEξ=2a+b\\u200b,Vξ=12(b−a)2\\u200b.Вопрос на подумать. Можно ли задать равномерное распределения на неограниченном промежутке, например, на R\\\\mathbb RR или на [0,+∞)\\\\mathbb [0, +\\\\infty)[0,+∞)?\\nОтвет (не открывайте сразу, сначала подумайте самостоятельно)Строго говоря, нет, покольку интеграл от константы по неограниченному промежутку расходится, и потому не может равняться единице. Однако при байесовском выводе в качестве априорного распределения иногда приходится выбирать ненормируемое распределение, в том числе, например, равномерное на [0,+∞)[0, +\\\\infty)[0,+∞). Такое «распределение» (которое на самом деле никакое не распределение) также называют несобственным.\\nАналогичным образом вводится равномерное распределение в многомерном пространстве: если множество V⊂RnV \\\\subset \\\\mathbb R^nV⊂Rn имеет объём ∣V∣\\\\vert V\\\\vert∣V∣, то плотность равномерно распределённой на VVV случайной величины ξ\\\\xiξ задаётся как pξ(x)=I(x∈V)∣V∣p_\\\\xi(\\\\boldsymbol x) = \\\\frac{\\\\mathbb I(\\\\boldsymbol x \\\\in V)}{\\\\vert V\\\\vert}pξ\\u200b(x)=∣V∣I(x∈V)\\u200b. Если A⊂VA \\\\subset VA⊂V, то\\nP(A)=1∣V∣∫Adx=∣A∣∣V∣,\\\\mathbb P(A) = \\\\frac 1{\\\\vert V\\\\vert}\\\\int\\\\limits_A d\\\\boldsymbol x = \\\\frac {\\\\vert A\\\\vert}{\\\\vert V\\\\vert},\\nP(A)=∣V∣1\\u200bA∫\\u200bdx=∣V∣∣A∣\\u200b,и мы получили формулу геометрической вероятности.\\nНормальное распределение\\nСлучайная величина ξ\\\\xiξ имеет нормальное (гауссовское) распределение N(μ,σ2)\\\\mathcal N(\\\\mu, \\\\sigma^2)N(μ,σ2), если её плотность равна\\npξ(x)=12πσe−(x−μ)22σ2.p_\\\\xi(x) = \\\\frac 1{\\\\sqrt{2\\\\pi}\\\\sigma} e^{-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}}.\\npξ\\u200b(x)=2π\\u200bσ1\\u200be−2σ2(x−μ)2\\u200b.Параметры нормального распределения N(μ,σ2)\\\\mathcal N(\\\\mu, \\\\sigma^2)N(μ,σ2) представляют собой его среднее и дисперсию:\\nEξ=μ,Vξ=σ2.\\\\mathbb E \\\\xi = \\\\mu,\\\\quad \\\\mathbb V \\\\xi = \\\\sigma^2.\\nEξ=μ,Vξ=σ2.Параметр σ\\\\sigmaσ отвечает за выраженность «колокола» плотности нормального распределения:\\n\\nпри σ→0\\\\sigma \\\\to 0σ→0 «колокол» приобретает очертания резко выраженного пика, то есть практически вся вероятностная масса сосретдоточена в малой окрестности точки x=μx = \\\\mux=μ;\\nпри σ→+∞\\\\sigma \\\\to +\\\\inftyσ→+∞ «колокол», наоборот, размывается, и распределение становится больше похоже на равномерное.\\n\\nГауссиана, у которой μ=0\\\\mu=0μ=0 и σ=1\\\\sigma = 1σ=1, называется стандартным нормальным распределением.\\n\\nИногда бывает полезно тесно связанное с гауссовским логнормальное распределение.\\nСлучайная величина ξ\\u2009\\u2063:(0,+∞)→R\\\\xi \\\\colon (0, +\\\\infty) \\\\to \\\\mathbb Rξ:(0,+∞)→R имеет логнормальное распределение, ξ∼LogN(μ,σ2)\\\\xi \\\\sim \\\\mathcal{LogN}(\\\\mu, \\\\sigma^2)ξ∼LogN(μ,σ2), если log\\u2061ξ∼N(μ,σ2)\\\\log \\\\xi \\\\sim \\\\mathcal N(\\\\mu, \\\\sigma^2)logξ∼N(μ,σ2). Плотность логнормальной случайной величины равна\\npξ(x)=12πσxe−(log\\u2061x−μ)22σ2,x>0,p_\\\\xi(x) = \\\\frac 1{\\\\sqrt{2\\\\pi}\\\\sigma x} e^{-\\\\frac{(\\\\log x-\\\\mu)^2}{2\\\\sigma^2}}, \\\\quad x > 0,\\npξ\\u200b(x)=2π\\u200bσx1\\u200be−2σ2(logx−μ)2\\u200b,x>0,а её среднее и дисперсию можно вычислить по формулам\\nEξ=eμ+σ22,Vξ=(eσ2−1)e2μ+σ2.  \\\\mathbb E\\\\xi = e^{\\\\mu + \\\\frac{\\\\sigma^2}2}, \\\\quad  \\\\mathbb V\\\\xi=\\\\big(e^{\\\\sigma^2} - 1\\\\big) e^{2\\\\mu + \\\\sigma^2}.\\nEξ=eμ+2σ2\\u200b,Vξ=(eσ2−1)e2μ+σ2.Показательное распределение\\nПлотность показательного (экспоненциального) распределения Exp(λ)\\\\mathrm{Exp}(\\\\lambda)Exp(λ) сосредоточена на луче [0,+∞)[0, +\\\\infty)[0,+∞) и имеет параметр λ>0\\\\lambda > 0λ>0: p(x)=λe−λxp(x) = \\\\lambda e^{-\\\\lambda x}p(x)=λe−λx, x⩾0x \\\\geqslant 0x⩾0. Если ξ∼Exp(λ)\\\\xi \\\\sim \\\\mathrm{Exp}(\\\\lambda)ξ∼Exp(λ), то\\nEξ=1λ,Vξ=1λ2.\\\\mathbb E \\\\xi = \\\\frac 1\\\\lambda,\\\\quad \\\\mathbb V \\\\xi = \\\\frac 1{\\\\lambda^2}.\\nEξ=λ1\\u200b,Vξ=λ21\\u200b.Плотность показательного распределения является убывающей функцией на [0,+∞)[0, +\\\\infty)[0,+∞), а параметр λ\\\\lambdaλ отвечает за скорость этого убывания:\\n\\nпри λ→0\\\\lambda \\\\to 0λ→0 убывание очень медленное, и распределение больше похоже на равномерное;\\nпри λ→+∞\\\\lambda \\\\to +\\\\inftyλ→+∞, наоборот, вся вероятностная масса сосредоточена около точки 000.\\n\\n\\nПоказательное распределение моделирует временные интервалы между случайными событиями, наступающими с постоянной скоростью, например:\\n\\nвремя ожидания автобуса на остановке;\\nвремя между телефонными звонками в колл-центре;\\nвремя до выхода из строя вычислительного узла в дата-центре.\\n\\nГамма-распределение с положительными параметрами α\\\\alphaα и β\\\\betaβ имеет плотность\\np(x)=1Γ(α)βαxα−1e−xβ,x⩾0,  p(x) = \\\\frac 1{\\\\Gamma(\\\\alpha) \\\\beta^\\\\alpha} x^{\\\\alpha - 1} e^{-\\\\frac x \\\\beta},\\\\quad x \\\\geqslant 0,\\np(x)=Γ(α)βα1\\u200bxα−1e−βx\\u200b,x⩾0,где Γ(α)\\\\Gamma(\\\\alpha)Γ(α) — гамма-функция Эйлера. При α=1\\\\alpha =1α=1 гамма-распределение превращается в показательное с параметром λ=1β\\\\lambda = \\\\frac 1\\\\betaλ=β1\\u200b. Среднее и дисперсия случайной величины ξ\\\\xiξ, имеющей гамма-распределение с параметрами α\\\\alphaα и β\\\\betaβ, равны\\nEξ=αβ,Vξ=αβ2.\\\\mathbb E\\\\xi = \\\\alpha\\\\beta, \\\\quad \\\\mathbb V\\\\xi = \\\\alpha\\\\beta^2.\\nEξ=αβ,Vξ=αβ2.Бета-распределение\\nПлотность бета-распределения с параметрами α,β>0\\\\alpha, \\\\beta > 0α,β>0 равна\\np(x)=1B(α,β)xα−1(1−x)β−1,0<x<1,  p(x) = \\\\frac 1{B(\\\\alpha, \\\\beta)} x^{\\\\alpha - 1} (1-x)^{\\\\beta -1}, \\\\quad 0 < x < 1,\\np(x)=B(α,β)1\\u200bxα−1(1−x)β−1,0<x<1,где B(α,β)B(\\\\alpha, \\\\beta)B(α,β) — бета-функция Эйлера.\\nБета-распределение имеет следующее статистическое приложение. Выберем случайным образом точки x1,…,xn∈[0,1]x_1, \\\\ldots, x_n \\\\in [0,1]x1\\u200b,…,xn\\u200b∈[0,1], и упорядочим их по возрастанию. Получим набор значений\\n0⩽x(1)⩽x(2)⩽…⩽x(k)⩽…⩽x(n)⩽1.  0\\\\leqslant x_{(1)} \\\\leqslant x_{(2)} \\\\leqslant \\\\ldots \\\\leqslant x_{(k)} \\\\leqslant \\\\ldots \\\\leqslant x_{(n)} \\\\leqslant 1.\\n0⩽x(1)\\u200b⩽x(2)\\u200b⩽…⩽x(k)\\u200b⩽…⩽x(n)\\u200b⩽1.Оказывается, что случайная величина ξ=x(k)\\\\xi = x_{(k)}ξ=x(k)\\u200b, называемая kkk-й порядковой статистикой, имеет бета распределение с параметрами kkk и n+1−kn+1 - kn+1−k:\\npξ(x)=n!(k−1)!(n−k)!xk−1(1−x)n−k=k(nk)xk−1(1−x)n−k.  p_\\\\xi(x) = \\\\frac{n!}{(k-1)!(n-k)!} x^{k-1}(1-x)^{n-k} = k \\\\binom nk x^{k-1}(1-x)^{n-k}.\\npξ\\u200b(x)=(k−1)!(n−k)!n!\\u200bxk−1(1−x)n−k=k(kn\\u200b)xk−1(1−x)n−k.Доказательство Пусть y∈[0,1]y\\\\in [0, 1]y∈[0,1]. Неравенство x(k)⩽yx_{(k)}\\\\leqslant yx(k)\\u200b⩽y означает, что из nnn элементов выборки хотя бы kkk не превосходят yyy. Заметим, что для каждого iii индикатор I(xi⩽y)\\\\mathbb{I}(x_i\\\\leqslant y)I(xi\\u200b⩽y) является бернуллиевской случайной величиной с вероятностью успеха yyy. У нас есть nnn таких бернуллиевских величин, и неравенство x(k)⩽yx_{(k)}\\\\leqslant yx(k)\\u200b⩽y равносильно тому, что среди nnn испытаний Бернулли случилось не менее kkk успехов. Вероятность того, что случилось ровно jjj успехов, равна, как мы уже знаем,\\n(nj)yj(1−y)n−j.\\\\binom {n}{j}y^j(1 - y)^{n-j}.\\n(jn\\u200b)yj(1−y)n−j.Чтобы получить вероятность того, что успехов хотя бы kkk, надо просуммировать эти числа по jjj от kkk до nnn:\\nFξ(y)=P(x(k)⩽y)=∑j=kn(nj)yj(1−y)n−j.F_\\\\xi(y) = \\\\mathbb P (x_{(k)} \\\\leqslant y) = \\\\sum_{j=k}^n\\\\binom {n}{j}y^j(1 - y)^{n-j}.\\nFξ\\u200b(y)=P(x(k)\\u200b⩽y)=j=k∑n\\u200b(jn\\u200b)yj(1−y)n−j.Чтобы получить плотность, продифференцируем функцию распределения:\\npξ(y)=∑j=knj(nj)yj−1(1−y)n−j−∑j=kn−1(n−j)(nj)yj(1−y)n−1−j.p_{\\\\xi}(y) = \\\\sum_{j=k}^n j\\\\binom nj y^{j-1}(1 - y)^{n-j} - \\\\sum_{j=k}^{n-1} (n-j)\\\\binom nj y^{j}(1 - y)^{n-1-j}.\\npξ\\u200b(y)=j=k∑n\\u200bj(jn\\u200b)yj−1(1−y)n−j−j=k∑n−1\\u200b(n−j)(jn\\u200b)yj(1−y)n−1−j.Легко проверяется, что j(nj)=n(n−1j−1)j\\\\binom nj = n\\\\binom {n-1}{j-1}j(jn\\u200b)=n(j−1n−1\\u200b), (n−j)(nj)=n(n−1j)(n-j)\\\\binom nj = n\\\\binom{n-1}j(n−j)(jn\\u200b)=n(jn−1\\u200b), и поэтому\\n∑j=knj(nj)yj−1(1−y)n−j=∑j=k−1n−1n(n−1j)yj(1−y)n−1−j  \\\\sum_{j=k}^n j\\\\binom nj y^{j-1}(1 - y)^{n-j}  = \\\\sum_{j=k-1}^{n-1} n\\\\binom{n-1}j y^{j}(1 - y)^{n-1-j}\\nj=k∑n\\u200bj(jn\\u200b)yj−1(1−y)n−j=j=k−1∑n−1\\u200bn(jn−1\\u200b)yj(1−y)n−1−jИ мы видим, что большинство слагаемых сокращается и выживает лишь одно:\\npξ(y)=n!(k−1)!(n−k)!yk−1(1−y)n−k.p_{\\\\xi}(y) = \\\\frac{n!}{(k-1)!(n-k)!}y^{k-1}(1 - y)^{n-k}.\\npξ\\u200b(y)=(k−1)!(n−k)!n!\\u200byk−1(1−y)n−k.Распределение Стьюдента\\nПри проверке статистических гипотез бывает полезно распределение Стьюдента (t-distribution) с ν\\\\nuν степенями свободы, плотность которого равна\\np(x)=Γ(ν+12)νπ\\u2009Γ(ν2)(1+x2ν)−(ν+1)/2,ν>0,  p(x) = \\\\frac{\\\\Gamma(\\\\frac{\\\\nu+1}{2})} {\\\\sqrt{\\\\nu\\\\pi}\\\\,\\\\Gamma(\\\\frac{\\\\nu}{2})} \\\\left(1+\\\\frac{x^2}\\\\nu \\\\right)^{-(\\\\nu+1)/2}, \\\\quad \\\\nu > 0,\\np(x)=νπ\\u200bΓ(2ν\\u200b)Γ(2ν+1\\u200b)\\u200b(1+νx2\\u200b)−(ν+1)/2,ν>0,где Γ(α)\\\\Gamma(\\\\alpha)Γ(α) — гамма-функция Эйлера. Распределение Стьюдента похоже на стандартное нормальное распределение; более того, при ν→+∞\\\\nu\\\\to +\\\\inftyν→+∞ оно превращается в N(0,1)\\\\mathcal N(0, 1)N(0,1).\\n\\nОднако при малых значениях ν\\\\nuν распределение Стьюдента имеет гораздо более тяжёлые «хвосты»: например, при ν⩽2\\\\nu \\\\leqslant 2ν⩽2 его дисперсия бесконечна, а при ν⩽1\\\\nu \\\\leqslant 1ν⩽1 та же участь постигает и математическое ожидание (всё из-за расходимости соответствующих интегралов). В остальных случаях\\nEξ=0,Vξ=νν−2,  \\\\mathbb E \\\\xi = 0, \\\\quad \\\\mathbb V \\\\xi = \\\\frac{\\\\nu}{\\\\nu-2},\\nEξ=0,Vξ=ν−2ν\\u200b,если ξ\\\\xiξ имеет распределение Стьюдента с ν\\\\nuν степенями свободы.\\nРаспределение Лапласа\\nПлотность распределения Лапласа с параметрами μ,b\\\\mu, bμ,b равна\\np(x)=12be−∣x−μ∣b.   p(x) = \\\\frac 1{2b} e^{-\\\\frac{\\\\vert x - \\\\mu\\\\vert}b}.\\np(x)=2b1\\u200be−b∣x−μ∣\\u200b.Такое распределение иногда обозначают Laplace(μ,b)\\\\mathrm{Laplace}(\\\\mu, b)Laplace(μ,b). Если ξ∼Laplace(μ,b)\\\\xi \\\\sim \\\\mathrm{Laplace}(\\\\mu, b)ξ∼Laplace(μ,b), то\\nEξ=μ,Vξ=2b2.  \\\\mathbb E \\\\xi = \\\\mu, \\\\quad \\\\mathbb V \\\\xi = 2b^2.\\nEξ=μ,Vξ=2b2.При μ=0\\\\mu=0μ=0 распределение Лапласа представляет собой экспоненциальное распределение, плотность которого симметрично отражена на отрицательную полуось: если ξ∼Laplace(0,b)\\\\xi \\\\sim \\\\mathrm{Laplace}(0, b)ξ∼Laplace(0,b), то ∣ξ∣∼Exp(1b)\\\\vert \\\\xi \\\\vert \\\\sim \\\\mathrm{Exp}\\\\big(\\\\frac 1b\\\\big)∣ξ∣∼Exp(b1\\u200b). Распределение Лапласа похоже на нормальное и отличается от него немного более тяжёлыми «хвостами» и тем, что его плотность теряет гладкость в нуле.\\nХарактеристики случайных величин\\nМоменты\\nЕсли n∈Nn\\\\in \\\\mathbb Nn∈N, то nnn-й момент μn\\\\mu_nμn\\u200b случайной величины ξ\\\\xiξ равен Eξn\\\\mathbb E \\\\xi^nEξn. В зависимости от типа случайной величины моменты вычисляются по-разному:\\n\\nμn=∑kxknP(ξ=xk)\\\\mu_n = \\\\sum\\\\limits_k x_k^n \\\\mathbb P(\\\\xi = x_k)μn\\u200b=k∑\\u200bxkn\\u200bP(ξ=xk\\u200b), если ξ\\\\xiξ принимает дискретные значения x1,x2,…,xk,…x_1, x_2, \\\\ldots, x_k, \\\\ldotsx1\\u200b,x2\\u200b,…,xk\\u200b,…;\\nμn=∫−∞+∞xnpξ(x)\\u2009dx\\\\mu_n = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} x^n p_\\\\xi(x)\\\\, dxμn\\u200b=−∞∫+∞\\u200bxnpξ\\u200b(x)dx, если ξ\\\\xiξ имеет плотность pξ(x)p_\\\\xi(x)pξ\\u200b(x).\\n\\nПервый момент μ1\\\\mu_1μ1\\u200b — это в точности математическое ожидание (среднее) случайной величины ξ\\\\xiξ. Дисперсию тоже можно выразить через моменты:\\nVξ=Eξ2−(Eξ)2=μ2−μ12.\\\\mathbb V\\\\xi = \\\\mathbb E \\\\xi^2 - \\\\big(\\\\mathbb E\\\\xi\\\\big)^2 = \\\\mu_2 - \\\\mu_1^2.\\nVξ=Eξ2−(Eξ)2=μ2\\u200b−μ12\\u200b.Не у всех случайных величин есть конечные среднее и дисперсия. Например, распределение Коши (оно же распределение Стьюдента с одной степенью свободы) имеет плотность p(x)=1π11+x2p(x) = \\\\frac 1\\\\pi \\\\frac 1{1+x^2}p(x)=π1\\u200b1+x21\\u200b, и если мы попытаемся вычислить первые два момента, то получим расходящиеся интегралы\\n1π∫−∞+∞x1+x2dx\\xa0и\\xa01π∫−∞+∞x21+x2dx.  \\\\frac 1\\\\pi\\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} \\\\frac{x}{1+x^2} dx \\\\text{ и }\\n  \\\\frac 1\\\\pi\\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} \\\\frac{x^2}{1+x^2} dx.\\nπ1\\u200b−∞∫+∞\\u200b1+x2x\\u200bdx\\xa0и\\xa0π1\\u200b−∞∫+∞\\u200b1+x2x2\\u200bdx.Упражнение. Приведите пример дискретной случайной величины с бесконечным средним.\\nОтветТакое может быть только если случайная величина ξ\\\\xiξ принимает счётное число значений. Пусть, например, P(ξ=k)=1k(k+1)\\\\mathbb P(\\\\xi = k) = \\\\frac 1{k(k+1)}P(ξ=k)=k(k+1)1\\u200b, k∈Nk\\\\in \\\\mathbb Nk∈N. Поскольку ∑k=1∞1k(k+1)=1\\\\sum\\\\limits_{k=1}^\\\\infty \\\\frac 1{k(k+1)} = 1k=1∑∞\\u200bk(k+1)1\\u200b=1, это корректное распределение вероятностей. С другой стороны, Eξ=∑k=1∞1k+1\\\\mathbb E\\\\xi = \\\\sum\\\\limits_{k=1}^\\\\infty \\\\frac 1{k+1}Eξ=k=1∑∞\\u200bk+11\\u200b, а это почти что гармонический ряд, который расходится.\\nСвойства математического ожидания\\n\\n\\nЕсли ξ=C\\\\xi = Cξ=C, то Eξ=C\\\\mathbb E \\\\xi = CEξ=C.\\n\\n\\nE(aξ+bη)=aEξ+bEη\\\\mathbb E(a\\\\xi + b \\\\eta) = a\\\\mathbb E \\\\xi + b \\\\mathbb E \\\\etaE(aξ+bη)=aEξ+bEη (линейность).\\n\\n\\nЕсли ξ⩽η\\\\xi \\\\leqslant \\\\etaξ⩽η, то Eξ⩽Eη\\\\mathbb E \\\\xi \\\\leqslant \\\\mathbb E \\\\etaEξ⩽Eη (монотонность).\\n\\n\\nEI(A)=P(A)\\\\mathbb E \\\\mathbb I(A) = \\\\mathbb P(A)EI(A)=P(A).\\n\\n\\nЕсли случайные величины ξ\\\\xiξ и η\\\\etaη независимы, то Eξη=EξEη\\\\mathbb E\\\\xi\\\\eta = \\\\mathbb E\\\\xi \\\\mathbb E\\\\etaEξη=EξEη.\\n\\n\\nЕсли ξ⩾0\\\\xi \\\\geqslant 0ξ⩾0, то P(ξ⩾a)⩽Eξa\\\\mathbb P(\\\\xi \\\\geqslant a) \\\\leqslant \\\\frac{\\\\mathbb E \\\\xi}{a}P(ξ⩾a)⩽aEξ\\u200b (неравенство Маркова).\\n\\n\\nЕсли функция fff выпукла вниз, то f(Eξ)⩽E(f(ξ))f(\\\\mathbb E \\\\xi) \\\\leqslant \\\\mathbb E(f(\\\\xi))f(Eξ)⩽E(f(ξ)) (неравенство Йенсена).\\n\\n\\nLaw of the unconscious statistician (LOTUS)\\nЕсли случайная величина η\\\\etaη получена применением некоторой детерминированной функцией из случайной величины ξ\\\\xiξ, η=g(ξ)\\\\eta = g(\\\\xi)η=g(ξ), то\\n\\nEη=∑kg(xk)P(ξ=xk)\\\\mathbb E\\\\eta = \\\\sum\\\\limits_k g(x_k) \\\\mathbb P(\\\\xi = x_k)Eη=k∑\\u200bg(xk\\u200b)P(ξ=xk\\u200b), если ξ\\\\xiξ дискретна;\\nEη=∫−∞+∞g(x)pξ(x)\\u2009dx\\\\mathbb E\\\\eta = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} g(x) p_{\\\\xi}(x)\\\\,dxEη=−∞∫+∞\\u200bg(x)pξ\\u200b(x)dx, если ξ\\\\xiξ непрерывна.\\n\\nДисперсия и ковариация\\nКовариация случайных величин ξ\\\\xiξ и η\\\\etaη определяется по формуле\\ncov(ξ,η)=E((ξ−Eξ)⋅(η−Eη))=E(ξ⋅η)−Eξ⋅Eη\\\\mathrm{cov}(\\\\xi,\\\\eta) = \\\\mathbb E((\\\\xi - \\\\mathbb E\\\\xi) \\\\cdot (\\\\eta - \\\\mathbb E \\\\eta)) = \\\\mathbb E(\\\\xi \\\\cdot\\\\eta) - \\\\mathbb E\\\\xi \\\\cdot \\\\mathbb E\\\\eta\\ncov(ξ,η)=E((ξ−Eξ)⋅(η−Eη))=E(ξ⋅η)−Eξ⋅EηВ частности, cov(ξ,ξ)=Vξ\\\\mathrm{cov}(\\\\xi,\\\\xi) = \\\\mathbb V \\\\xicov(ξ,ξ)=Vξ. На практике часто применяют коэффициент корреляции, который получается нормированием ковариации:\\ncorr(ξ,η)=cov(ξ,η)VξVη.  \\\\mathrm{corr}(\\\\xi, \\\\eta) = \\\\frac{\\\\mathrm{cov}(\\\\xi,\\\\eta)}{\\\\sqrt{\\\\mathbb V \\\\xi}\\\\sqrt{\\\\mathbb V \\\\eta}}.\\ncorr(ξ,η)=Vξ\\u200bVη\\u200bcov(ξ,η)\\u200b.Коэффициент корреляции всегда принимает значения из отрезка [−1;1][-1;1][−1;1]. Если corr(ξ,η)=0\\\\mathrm{corr}(\\\\xi, \\\\eta) = 0corr(ξ,η)=0, то случайные величины ξ\\\\xiξ и η\\\\etaη называют некоррелированными.\\nСвойства дисперсии и ковариации\\n\\n\\nVξ⩾0\\\\mathbb V \\\\xi \\\\geqslant 0Vξ⩾0, причём Vξ=0\\u2005\\u200a⟺\\u2005\\u200a∃a∈R\\u2009\\u2063:P(ξ=a)=1\\\\mathbb V \\\\xi = 0 \\\\iff \\\\exists a\\\\in\\\\mathbb R \\\\colon \\\\mathbb P(\\\\xi = a) = 1Vξ=0⟺∃a∈R:P(ξ=a)=1.\\n\\n\\nV(aξ)=a2Vξ\\\\mathbb V (a\\\\xi) = a^2 \\\\mathbb V \\\\xiV(aξ)=a2Vξ, V(ξ+a)=Vξ\\\\mathbb V(\\\\xi + a) = \\\\mathbb V\\\\xiV(ξ+a)=Vξ.\\n\\n\\ncov(ξ,η)=cov(η,ξ)\\\\mathrm{cov}(\\\\xi, \\\\eta) = \\\\mathrm{cov}(\\\\eta, \\\\xi)cov(ξ,η)=cov(η,ξ), cov(aξ,bη)=abcov(ξ,η)\\\\mathrm{cov}(a\\\\xi, b\\\\eta) = ab\\\\mathrm{cov}(\\\\xi, \\\\eta)cov(aξ,bη)=abcov(ξ,η).\\n\\n\\nV(ξ+η)=Vξ+Vη+2cov(ξ,η)\\\\mathbb V(\\\\xi + \\\\eta) = \\\\mathbb V \\\\xi + \\\\mathbb V \\\\eta + 2\\\\mathrm{cov}(\\\\xi, \\\\eta)V(ξ+η)=Vξ+Vη+2cov(ξ,η).\\n\\n\\nЕсли случайные величины ξ\\\\xiξ и η\\\\etaη независимы, то cov(ξ,η)=0\\\\mathrm{cov}(\\\\xi, \\\\eta) = 0cov(ξ,η)=0 и V(ξ+η)=Vξ+Vη\\\\mathbb V(\\\\xi + \\\\eta) =  \\\\mathbb V\\\\xi + \\\\mathbb V\\\\etaV(ξ+η)=Vξ+Vη.\\n\\n\\nP(∣ξ−Eξ∣⩾a)⩽Vξa2\\\\mathbb P(\\\\vert\\\\xi - \\\\mathbb E\\\\xi\\\\vert \\\\geqslant a) \\\\leqslant \\\\frac{\\\\mathbb V \\\\xi}{a^2}P(∣ξ−Eξ∣⩾a)⩽a2Vξ\\u200b (неравенство Чебышева).\\n\\n\\nФункции распределения и плотности\\nСлучайная величина ξ\\u2009\\u2063:Ω→R\\\\xi \\\\colon \\\\Omega \\\\to \\\\mathbb{R}ξ:Ω→R является числовой функцией, заданной на пространстве элементарных событий; однако, больший интерес обычно представляет порождаемое ею распределение вероятностей. В дискретном случае достаточно задать вероятности отдельных значений P(ξ=xi)\\\\mathbb{P}(\\\\xi = x_i)P(ξ=xi\\u200b); для непрерывных же случайных величин на помощь приходят функция распределения и функция плотности.\\nФункцией распределения (cumulative distribution function, cdf) случайной величины ξ\\\\xiξ называется функция\\nFξ(x)=P(ξ⩽x).F_\\\\xi(x) = \\\\mathbb{P}(\\\\xi \\\\leqslant x).\\nFξ\\u200b(x)=P(ξ⩽x).Свойства функции распределения FξF_\\\\xiFξ\\u200b:\\n\\nFξ(−∞)=0F_\\\\xi(-\\\\infty) = 0Fξ\\u200b(−∞)=0, Fξ(+∞)=1F_\\\\xi(+\\\\infty) = 1Fξ\\u200b(+∞)=1;\\nфункция FξF_\\\\xiFξ\\u200b неубывающая;\\nфункция FξF_\\\\xiFξ\\u200b непрерывна справа: lim\\u2061h→0+Fξ(x+h)=Fξ(x)\\\\lim\\\\limits_{h \\\\to 0+ } F_\\\\xi(x + h) = F_\\\\xi(x)h→0+lim\\u200bFξ\\u200b(x+h)=Fξ\\u200b(x);\\nP(a<ξ⩽b)=Fξ(b)−Fξ(a)\\\\mathbb{P}(a < \\\\xi \\\\leqslant b) = F_\\\\xi(b) - F_\\\\xi(a)P(a<ξ⩽b)=Fξ\\u200b(b)−Fξ\\u200b(a).\\n\\nЛюбая дискретная случайная величина имеет ступенчатую функцию распределения. К примеру, вот как выглядит график функции FξF_\\\\xiFξ\\u200b для ξ∼Bin(10,0.5)\\\\xi \\\\sim \\\\mathrm{Bin}(10, 0.5)ξ∼Bin(10,0.5):\\n\\nЕсли непрерывная случайная величина ξ\\\\xiξ имеет непрерывную плотность pξ(x)p_\\\\xi(x)pξ\\u200b(x), то\\nFξ(x)−Fξ(a)=∫axpξ(t)\\u2009dt,  F_\\\\xi(x) - F_\\\\xi(a) = \\\\int\\\\limits_a^x p_\\\\xi(t)\\\\, dt,\\nFξ\\u200b(x)−Fξ\\u200b(a)=a∫x\\u200bpξ\\u200b(t)dt,откуда следует, что Fξ′(x)=pξ(x)F'_\\\\xi(x) = p_\\\\xi(x)Fξ′\\u200b(x)=pξ\\u200b(x). В типичных случаях непрерывная случайная величина имеет гладкую возрастающую функцию распределения с двумя горизонтальными асимптотами. Вот примеры графиков функций распределения гауссовских случайных величин:\\n\\nЗамечание о плотностях дискретных случайных величинДискретные случайные величины не имеют плотности в описанном выше смысле. Например, возьмем ξ\\\\xiξ – выпавшее число на идеальной кости. Тогда pξp_\\\\xipξ\\u200b равна 0 везде кроме 1, 2, 3, 4, 5, 6. При этом вероятность выпасть 1 равна 16\\\\tfrac1661\\u200b, то есть\\n∫1−ε1+εpξ(x)\\u2009dx=16\\\\int\\\\limits_{1 - \\\\varepsilon}^{1 + \\\\varepsilon} p_\\\\xi(x)\\\\, dx = \\\\frac16\\n1−ε∫1+ε\\u200bpξ\\u200b(x)dx=61\\u200bдля любого малого ε>0\\\\varepsilon > 0ε>0. Среди обычных функций мы такой плотности не найдём, однако, её можно выразить в терминах обобщенных функций:\\npξ(x)=16∑k=16δ(x−k),p_\\\\xi(x) = \\\\frac 16 \\\\sum\\\\limits_{k=1}^6 \\\\delta(x-k),\\npξ\\u200b(x)=61\\u200bk=1∑6\\u200bδ(x−k),где δ(x)\\\\delta(x)δ(x) – дельта-функция Дирака, обладающая свойством\\n∫−∞+∞δ(x)f(x)\\u2009dx=f(0)\\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} \\\\delta(x)f(x)\\\\,dx = f(0)\\n−∞∫+∞\\u200bδ(x)f(x)dx=f(0)для любой непрерывной функции fff. Отсюда, в частности, следует, что\\n∫x0−εx0+εδ(x−x0)\\u2009dx=1.\\\\int\\\\limits_{x_0-\\\\varepsilon}^{x_0 + \\\\varepsilon} \\\\delta(x - x_0)\\\\, dx = 1.\\nx0\\u200b−ε∫x0\\u200b+ε\\u200bδ(x−x0\\u200b)dx=1.До какой-то степени δ(x)\\\\delta(x)δ(x) можно представлять себе как «функцию», равную 000 везде, кроме x=0x=0x=0, а в нуле принимающую некоторое экзотическое «бесконечное» значение.\\nМедиана и мода\\nМатематическое ожидание — не единственная числовая метрика, с помощью которой можно пытаться охарактеризовать, чему равно в среднем значение случайной величины. Медиана разбивает вероятностную массу распределения на две равные части. Если случайная величина ξ\\\\xiξ имеет плотность pξ(x)p_\\\\xi(x)pξ\\u200b(x), то её медиана m=medξm = \\\\mathrm{med}\\\\xim=medξ определяется из условия\\nP(ξ⩽m)=∫−∞mpξ(x)\\u2009dx=∫m+∞pξ(x)\\u2009dx=P(ξ⩾m)=12.  \\\\mathbb P(\\\\xi \\\\leqslant m) = \\\\int\\\\limits_{-\\\\infty}^m p_\\\\xi(x)\\\\,dx =\\n  \\\\int\\\\limits_m^{+\\\\infty} p_\\\\xi(x)\\\\,dx = \\\\mathbb P(\\\\xi \\\\geqslant m) = \\\\frac 12.\\nP(ξ⩽m)=−∞∫m\\u200bpξ\\u200b(x)dx=m∫+∞\\u200bpξ\\u200b(x)dx=P(ξ⩾m)=21\\u200b.В терминах функции распределения это означает, что Fξ(m)=1−Fξ(m)F_\\\\xi(m) = 1 - F_\\\\xi(m)Fξ\\u200b(m)=1−Fξ\\u200b(m), или Fξ(m)=12F_\\\\xi(m) = \\\\frac 12Fξ\\u200b(m)=21\\u200b. В непрерывном случае функция распределения Fξ(x)F_\\\\xi(x)Fξ\\u200b(x) строго возрастает, поэтому уравнение Fξ(m)=12F_\\\\xi(m) = \\\\frac 12Fξ\\u200b(m)=21\\u200b имеет единственное решение. Для дискретных случайных величин это может быть не так, и поэтому в общем случае медиану определяют как число mmm, удовлетворяющее условиям\\nP(ξ⩽m)⩾12,P(ξ⩾m)⩾12.  \\\\mathbb P(\\\\xi \\\\leqslant m) \\\\geqslant \\\\frac 12, \\\\quad \\\\mathbb P(\\\\xi \\\\geqslant m) \\\\geqslant \\\\frac 12.\\nP(ξ⩽m)⩾21\\u200b,P(ξ⩾m)⩾21\\u200b.Например, если ξ∼Bern(12)\\\\xi \\\\sim \\\\mathrm{Bern}\\\\big(\\\\frac 12\\\\big)ξ∼Bern(21\\u200b), то P(ξ=0)=P(ξ=1)=12\\\\mathbb P(\\\\xi = 0) = \\\\mathbb P(\\\\xi = 1) = \\\\frac 12P(ξ=0)=P(ξ=1)=21\\u200b, и поэтому любое число m∈(0,1)m \\\\in (0, 1)m∈(0,1) является медианой симметричного бернуллиевского распределения. Бесконечное количество медиан будет у всякой дискретной случайной величины ξ\\\\xiξ, для которой Fξ(x)=12F_\\\\xi(x) = \\\\frac 12Fξ\\u200b(x)=21\\u200b на целом промежутке.\\nМода распределения максимизирует его pmf или pdf:\\nmode(ξ)=argmax\\u2061kP(ξ=k)\\xa0или\\xa0mode(ξ)=argmax\\u2061xpξ(x).  \\\\mathrm{mode}(\\\\xi) = \\\\mathrm{arg}\\\\max\\\\limits_k \\\\mathbb P(\\\\xi = k) \\\\text{ или }\\n  \\\\mathrm{mode}(\\\\xi) = \\\\mathrm{arg}\\\\max\\\\limits_x p_\\\\xi(x).\\nmode(ξ)=argkmax\\u200bP(ξ=k)\\xa0или\\xa0mode(ξ)=argxmax\\u200bpξ\\u200b(x).Мод у распределения может быть больше одной; самое вырожденное в этом смысле распределение — равномерное, каждая точка носителя является его модой. Если плотность случайной величины имеет единственную точку максимума, то она и является модой. Например:\\n\\nmode(ξ)=μ\\\\mathrm{mode}(\\\\xi) = \\\\mumode(ξ)=μ, если ξ∼N(μ,σ2)\\\\xi \\\\sim \\\\mathcal N(\\\\mu, \\\\sigma^2)ξ∼N(μ,σ2);\\nmode(ξ)=0\\\\mathrm{mode}(\\\\xi) = 0mode(ξ)=0, если ξ∼Exp(λ)\\\\xi \\\\sim \\\\mathrm{Exp}(\\\\lambda)ξ∼Exp(λ);\\nмода t-распределения Стьюдента также равна нулю.\\n\\nВсе такие распределения унимодальны. Если плотность pξ(x)p_\\\\xi(x)pξ\\u200b(x) имеет два или более максимума, то случайная величина ξ\\\\xiξ называется бимодальной или мультимодальной.\\n\\nДля симметричных распределений вроде нормального математическое ожидание, медиана и мода совпадают, однако, в общем случае это три различные меры типичного среднего значения случайной величины. Смысл каждой из этой мер наглядно демострирует следующая иллюстрация:\\n\\nУпражнение. Найдите среднее, медиану и моду экспоненциального распределения с параметром λ\\\\lambdaλ и сравните их между собой.\\nОтветЕсли ξ∼Exp(λ)\\\\xi \\\\sim \\\\mathrm{Exp}(\\\\lambda)ξ∼Exp(λ), то\\nEξ=1λ>med(ξ)=ln\\u20612λ>mode(ξ)=0.\\\\mathbb E\\\\xi = \\\\frac 1\\\\lambda > \\\\mathrm{med}(\\\\xi) = \\\\frac{\\\\ln 2}\\\\lambda >\\\\mathrm{mode}(\\\\xi) = 0.\\nEξ=λ1\\u200b>med(ξ)=λln2\\u200b>mode(ξ)=0.Классификация случайных величин\\nУ внимательного читателя (отягощённого математическим образованием впридачу) может возникнуть вопрос: а все ли случайные величины относятся к дискретным или непрерывным? В буквально такой постановке ответ, конечно, отрицательный, поскольку можно получить гибридную случайную величину, сложив дискретную и непрерывную. Но, может быть, всякая случайная величина равна сумме непрерывной и дискретной компонент?\\nВ терминах функций распределения этот вопрос можно переформулировать так: верно ли, что всякая монотонная функция F\\u2009\\u2063:R→[0,1]F \\\\colon \\\\mathbb R \\\\to \\\\mathbb [0, 1]F:R→[0,1] может быть представлена в виде F=Fjump+FsmoothF = F_{\\\\mathrm{jump}} + F_{\\\\mathrm{smooth}}F=Fjump\\u200b+Fsmooth\\u200b, где FjumpF_{\\\\mathrm{jump}}Fjump\\u200b — неубывающая ступенчатая функция (функция скачков), а\\nFsmooth(x)=∫−∞xp(t)\\u2009dt  F_{\\\\mathrm{smooth}}(x) = \\\\int\\\\limits_{-\\\\infty}^x p(t)\\\\,dt\\nFsmooth\\u200b(x)=−∞∫x\\u200bp(t)dt— гладкая возрастающая функция, полученная интегрированием плотности?\\nОтвет (не открывайте, если не хотите его знать)И здесь ответ отрицательный: существуют непрерывные монотонные функции вроде лестницы Кантора с производной, равной нулю почти всюду. Поскольку ∫−∞x0\\u2009dt=0\\\\int\\\\limits_{-\\\\infty}^x 0\\\\,dt = 0−∞∫x\\u200b0dt=0, лестница Кантора не может быть получена интегрированием никакой плотности. Случайные величины с такими функциями распределения, как лестница Кантора, называются сингулярными. Из теоремы Лебега о декомпозиции вытекает, что любая вероятностная мера на числовой прямой может быть представлена в виде суммы дискретной, абсолютно непрерывной (имеющей плотность) и сингулярной компонент.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф16.2. Матричная факторизацияСледующий параграф16.4. Многомерные распределенияЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_12.html', 'title': 'Подбор гиперпараметров'}, page_content=\"Подбор гиперпараметровЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/33.1.Метрики классификации и регрессии3.2.Кросс-валидация3.3.Подбор гиперпараметровGrid SearchRandom SearchExploration vs exploitationБайесовская оптимизацияTree-structured Parzen Estimator (TPE)Population Based Training (PBT)Open-source-библиотекиSummaryПочитать по теме4.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Подбор гиперпараметров3.3. Подбор гиперпараметровАвторы Елистратова ЕвгенияКак эффективно подбирать значения гиперпараметров модели и\\xa0не\\xa0переобучиться при этомДля начала поймём, в чём отличие параметров модели от гиперпараметров:\\n\\nпараметры настраиваются в процессе обучения модели на данных. Например, веса в линейной регрессии, нейросетях, структура решающего дерева;\\nгиперпараметры — это характеристики модели, которые фиксируются до начала обучения: глубина решающего дерева, значение силы регуляризации в линейной модели, learning rate для градиентного спуска.\\n\\nРассмотрим, например, модель линейной регрессии:\\nf(X)=Xw,    f(X) = X w, \\nf(X)=Xw,где\\n\\nw=(w0,w1,…,wn)w = (w_0, w_1, \\\\ldots, w_n)w=(w0\\u200b,w1\\u200b,…,wn\\u200b) — веса модели;\\nX=(xij)X = (x_{ij})X=(xij\\u200b) — матрица, в которой каждая строка содержит признаки одного объекта выборки (для удобства записи считаем, что первый столбец в этой матрице константный).\\n\\nЭта модель может обучаться посредством минимизации следующего функционала:Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nL=∥y−Xw∥2+C∥w∥2,    \\\\mathcal{L} = \\\\| y - X w\\\\|^2 + C \\\\| w \\\\|^2, \\nL=∥y−Xw∥2+C∥w∥2,где yyy — целевая переменная, CCC — коэффициент регуляризации. В процессе минимизации L\\\\mathcal{L}L веса www настраиваются по обучающей выборке, то есть являются параметрами. В то же время величина коэффициента регуляризации задаётся до начала обучения, то есть она — гиперпараметр.\\n\\nЕщё хороший пример — решающее дерево. Его гиперпараметры: максимальная глубина дерева, критерий ветвления, минимальное число семплов в листе дерева и ещё много других. А параметр — сама структура решающего дерева: обучение состоит в том, чтобы на каждом уровне дерева выбрать, по какому признаку должно произойти ветвление и с каким пороговым значением этого признака.\\nКачество модели может очень сильно варьироваться в зависимости от гиперпараметров, поэтому существуют разнообразные методы и инструменты для их подбора. При этом, вне зависимости от выбранного вами метода подбора гиперпараметров, оценку и сравнение моделей нужно проводить грамотно. Пусть у нас есть несколько моделей разной природы (метод ближайших соседей, случайный лес, логистическая регрессия) или несколько нейросеток с разными архитектурами. Нужно для каждой из моделей подобрать гиперпараметры, а затем модели с наилучшими гиперпараметрами сравнить между собой.\\nЕсть два наиболее часто используемых варианта.\\nПервый вариант\\nРазделить выборку на тренировочную, валидационную и тестовую части, для каждой модели выбирать гиперпараметры, максимизирующие её метрики на валидации, а окончательное сравнение моделей проводить по тестовым метрикам.\\nРазделения только на тренировочную и тестовую выборки недостаточно, так как в модель через подобранные гиперпараметры просачивается информация о тестовой выборке. Это означает, что на новых данных модели могут не сохранить свои качества и что их сравнение не будет честным.\\n\\n\\n\\nИсточник\\n\\n\\nВторой вариант\\nПровести кросс-валидацию.\\nКросс-валидация может быть нужна в случаях, если данных мало или мы не хотим зависеть от конкретного выбора валидационного множества. Примерный алгоритм:\\n\\nзафиксировать некоторое тестовое множество и отложить его;\\nразделить оставшееся множество данных на kkk фолдов (подмножеств), пройтись по ним циклом, на каждой итерации фиксируя один фолд в качестве валидационного и обучаясь на остальных;\\nв качестве оценки качества модели взять среднее значение валидационной метрики по фолдам;\\nфинальное сравнение моделей с уже подобранными гиперпараметрами проводить на отложенном тестовом множестве.\\n\\n\\n\\n\\n  Визуализация алгоритма.\\n    Источник\\n\\n\\nПодробное описание процесса сравнения моделей между собой можно найти в параграфах, посвящённых кросс-валидации и сравнению и оценке качества моделей.\\nДалее мы рассмотрим несколько методов подбора гиперпараметров для моделей, а в конце будет приведён список питоновских библиотек, в которых эти методы реализованы, и дано верхнеуровневое сравнение всех описанных методов между собой.\\nGrid Search\\nСамый естественный способ организовать перебор наборов гиперпараметров — сделать перебор по сетке (Grid Search):\\n\\nдля каждого гиперпараметра фиксируется несколько значений;\\nперебираются все комбинации значений различных гиперпараметров, на каждой из этих комбинаций модель обучается и тестируется;\\nвыбирается комбинация, на которой модель показывает лучшее качество.\\n\\nПримеры:\\n\\nдля метода ближайших соседей можно, например, перебирать по сетке число соседей (например, от 1 до 20) и метрику, по которой будет измеряться расстояние между объектами выборки (евклидова, манхэттенская и так далее);\\nдля решающих деревьев можно перебирать по сетке сочетания значений максимальной глубины дерева и различные критерии ветвления (критерий Джини, энтропийный критерий).\\n\\nПеребор некоторых значений гиперпараметров можно вести по логарифмической шкале, так как это позволяет быстрее определить правильный порядок параметра и в то же время значительно уменьшить время поиска. Так можно подбирать, например, значение learning rate для градиентного спуска, значение константы регуляризации для линейной регрессии или метода SVM.\\nСразу же видно естественное ограничение данного метода: если комбинаций параметров слишком много либо каждое обучение / тест длится долго, алгоритм не завершится за разумное время.\\nRandom Search\\nЕсли у вас возникает очень большое количество комбинаций параметров, вы можете какими-то способами пытаться справляться с этой проблемой:\\n\\nможно взять меньше значений каждого гиперпараметра, но тогда есть шансы пропустить наилучшую комбинацию;\\nможно уменьшить число фолдов в кросс-валидации, но оценка параметров станет более шумной;\\nможно оптимизировать параметры последовательно, а не перебирать их комбинации, но снова есть шанс получить неоптимальное решение;\\nможно перебирать не все комбинации гиперпараметров, а только случайное подмножество.\\n\\nПоследний способ называется Random Search. Для каждого гиперпараметра задаётся распределение, из которого выбирается его значение, и комбинация гиперпараметров составляется семплированием из этих распределений (хорошие советы по поводу выбора распределений можно найти в документации sklearn). Таким образом, благодаря случайному выбору очередной комбинации гиперпараметров вы можете найти оптимальную комбинацию за меньшее число итераций.\\nВот это изображение хорошо иллюстрирует отличия поиска по сетке от случайного поиска:\\n\\n\\n\\nИсточник\\n\\n\\nТо есть: качество нашей модели в зависимости от гиперпараметров — это функция многих переменных с некоторой нетривиальной поверхностью. Но эта поверхность может зависеть от одной из своих переменных сильно меньше, чем от другой. Если бы мы знали, какой гиперпараметр важнее для перформанса модели, мы бы рассмотрели больше его возможных значений, но часто у нас нет такой информации, и мы рассматриваем некоторое наперёд заданное число значений для каждого гиперпараметра.\\nRandom Search может за то же число итераций, что и Grid Search, рассмотреть более разнообразные значения гиперпараметров. Тем самым он с большей вероятностью найдёт те значения, которые больше всего влияют на качество модели, а значит, с большей вероятностью найдёт наилучшую комбинацию значений гиперпараметров.\\nЕсть ещё одно довольно интересное объяснение, почему Random Search работает хорошо. Рассмотрим случай, когда у нас конечная сетка гиперпараметров (каждому гиперпараметру сопоставлено конечное число значений).\\nВ этой сетке выделим группу размера 5%5\\\\%5% от общего числа наборов гиперпараметров, на которой модель достигает лучшего качества (можно мысленно отранжировать все наборы по качеству в некоторый список и взять топ 5%5\\\\%5% этого списка). Тогда некоторый набор гиперпараметров не попадает в эту группу с вероятностью 1−0.051 - 0.051−0.05. Если мы насемплировали nnn наборов, то каждый из них не попал в эту группу с вероятностью (1−0.05)n(1 - 0.05)^n(1−0.05)n, и, соответственно, вероятность того, что хотя бы один насемплированный набор попал в лучшую группу, равна 1−(1−0.05)n1 - (1 - 0.05)^n1−(1−0.05)n. Мы можем решить неравенство\\n1−(1−0.05)n≥0.95    1 - (1 - 0.05)^n \\\\ge 0.95 \\n1−(1−0.05)n≥0.95и выяснить, что при n≥60n \\\\ge 60n≥60 мы попадём в топ 5% с вероятностью, не меньшей 0.950.950.95. Это в большинстве случаев значительно быстрее, чем перебор всех комбинаций гиперпараметров с помощью Grid Search.\\nЕсли в рассуждении выше у нас некоторым гиперпараметрам соответствует непрерывное распределение, то всегда можно предположить, что мы уже насемплировали из этих распределений некоторое конечное число значений (равное числу итераций Random Search), а дальше считать, что мы работаем с конечной сеткой.\\nКонечно, остаётся наша зависимость от самой сетки гиперпараметров, и не всякая сетка обязана содержать в себе глобальный максимум перформанса модели или даже гиперпараметры из интервала вокруг него.\\nExploration vs exploitation\\nВ машинном обучении достаточно часто встречаются такие термины, как exploration и exploitation. Суть этих терминов хорошо поясняет следующий пример из реальной жизни. Допустим, перед вами стоит выбор, в какой ресторан пойти сегодня. Пусть ваш любимый ресторан находится прямо за углом.\\nВы ходите туда каждый день и поэтому достаточно уверены в том, насколько вкусным будет ваш обед. Но при этом не рассматриваете никакие другие опции и, возможно, упускаете возможность поесть гораздо вкуснее в другом месте. Если же вы будете обедать каждый раз в новом месте, то очень часто будете не удовлетворены результатом.\\n\\nВ описанных далее методах подбора гиперпараметров будет так или иначе происходить поиск баланса между exploration и exploitation. Одно из основных отличий всех методов, которые будут описаны далее, от Grid Search и Random Search — возможность учитывать результаты предыдущих вычислений.\\nОдна из возможных стратегий выбора точки для следующей итерации — exploration: исследование тех областей, в которых у нас мало семплов на текущей итерации, что даёт нам возможность с меньшей вероятностью пропустить оптимальное значение.\\nДругая стратегия — exploitation: выбирать больше семплов в областях, которые мы достаточно неплохо изучили и где, как мы считаем, с большой вероятностью находится оптимум.\\nБайесовская оптимизация\\nБайесовская оптимизация — это итерационный метод, позволяющий оценить оптимум функции, не дифференцируя её. Кроме того, на каждой итерации метод указывает, в какой следующей точке мы с наибольшей вероятностью улучшим нашу текущую оценку оптимума. Это позволяет значительно сократить количество вычислений функции, каждое из которых может быть довольно затратным по времени.\\nПодбор гиперпараметров тоже можно сформулировать в виде задачи, которая может решаться с помощью байесовской оптимизации. Пусть, например, наша функция — значение валидационных метрик в зависимости от текущего сочетания гиперпараметров. Её вычисление затратно по времени (нужно натренировать и провалидировать модель), и мы не можем вычислить градиенты этой функции по её переменным (нашим гиперпараметрам).\\nБайесовская оптимизация имеет две основные компоненты:\\n\\nвероятностную модель, которая приближает распределение значений целевой функции в зависимости от имеющихся исторических данных (часто в качестве такой модели выбирают гауссовские процессы);\\nфункцию, которая позволяет по некоторым статистикам текущей вероятностной модели функции fff указать, в какой следующей точке нужно вычислить значение fff. Эта функция называется acquisition function. Она должна балансировать между exploration и exploitation в следующем смысле:\\n\\nexploration — исследовать те точки, в которых дисперсия нашей вероятностной модели велика;\\nexploitation — исследовать те точки, где среднее нашей модели велико (и может служить оценкой максимума fff).\\n\\n\\n\\nПростой пример acquisition function — сумма среднего вероятностной модели и стандартного отклонения с некоторым весом:\\nα(x)=μ(x)+βσ(x),    \\\\alpha(x) = \\\\mu(x) + \\\\beta \\\\sigma(x), \\nα(x)=μ(x)+βσ(x),где xxx — точка из пространства, в котором мы оптимизируем целевую функцию (в нашем контексте это вектор значений гиперпараметров). На картинке ниже изображены обе компоненты, из которых складывается данная acquisition function, — среднее вероятностной модели μ\\\\muμ (синий график) и доверительный интервал, ширина которого в каждой точке пропорциональна стандартному отклонению вероятностной модели (жёлтая область).\\nСреднее модели μ\\\\muμ стремится приблизить искомую функцию fff и в точности равно fff в тех точках, где значения fff известны. Доверительный интервал имеет переменную ширину, так как чем дальше находится некоторая точка от тех, значения в которых известны, тем более модель не уверена в том, какое значение функции в этой точке, и тем шире доверительный интервал. Наоборот, в точках, где значения известны, доверительный интервал имеет нулевой радиус.\\n\\n\\n\\nИсточник\\n\\n\\nБайесовская оптимизация в общем случае представляет из себя следующий алгоритм. Пусть StS_tSt\\u200b — множество предыдущих наблюдений целевой функции fff: (f(x1),…,f(xt))(f(x_1), \\\\ldots, f(x_t))(f(x1\\u200b),…,f(xt\\u200b)), а α(⋅)\\\\alpha(\\\\cdot)α(⋅) — некоторая acquisition function.\\n\\nНа итерации t+1t + 1t+1 вычисляется точка xt+1x_{t + 1}xt+1\\u200b, в которой нужно провести следующее вычисление целевой функции:\\n\\nxt+1=arg\\u2061max\\u2061x∈Xα(x∣St).    x_{t + 1} = \\\\arg \\\\max_{x \\\\in X} \\\\alpha(x|S_t). \\nxt+1\\u200b=argx∈Xmax\\u200bα(x∣St\\u200b).\\nВычисляется значение f(xt+1)f(x_{t + 1})f(xt+1\\u200b), и обновляется множество наблюдений St+1=(St,f(xt+1))S_{t + 1} = (S_t, f(x_{t + 1}))St+1\\u200b=(St\\u200b,f(xt+1\\u200b)).\\nОбновляется статистическая модель.\\n\\nЧтобы такой алгоритм работал эффективно, α\\\\alphaα должна быть легко вычислимой и дифференцируемой.\\nНа рисунке ниже изображены три итерации этого алгоритма. Здесь пунктирная линия — это целевая функция, сплошная линия — график среднего вероятностной модели, жёлтым цветом обозначен доверительный интервал модели.\\nСерый график снизу — это график acquisition function. Её значения велики там, где вероятностная модель предсказывает большие значения целевой функции (exploitation), и там, где велика неуверенность вероятностной модели (exploration).\\nНа каждой итерации находится точка максимума acquisition function (чёрный крестик), и следующая итерация произойдёт в этой точке (серый кружок на графике функции). На нижнем графике побеждает exploitation, так как acquisition function верно предсказала, что наблюдения из неизвестных областей слабо повлияют на нашу текущую оценку максимума fff.\\n\\n\\n\\nИсточник\\n\\n\\nБайесовская оптимизация хорошо работает, когда нужно оптимизировать небольшое число гиперпараметров, так как в наивной реализации алгоритм не поддаётся распараллеливанию. При большой размерности пространства гиперпараметров скорость сходимости не лучше, чем у обычного Random Search (как утверждается в этой статье).\\nБайесовская оптимизация в изначальной постановке предполагалась для работы с непрерывными гиперпараметрами, а для работы с категориальными гиперпараметрами ей нужны некоторые трюки:\\n\\n\\nЕсли нужно найти оптимальное значение только одного гиперпараметра и этот параметр категориальный, то можно, например, использовать Thompson sampling (как тут в разделе «Bernoulli bandit»). Вообще, проблему выбора наилучшего значения категориального гиперпараметра можно переформулировать как multi-armed bandit problem и использовать любой известный способ решения этой задачи.\\n\\n\\nЕсли категориальных гиперпараметров больше одного и кроме них есть некатегориальные, то:\\n\\n\\n\\nможно попробовать использовать специальные виды ядер в гауссовских процессах, как, например, сделано здесь;\\nможно заменить гауссовские процессы на Random Forest (подробнее можно посмотреть здесь в разделе «Random Forests»).\\n\\nTree-structured Parzen Estimator (TPE)\\nАлгоритм TPE, как и алгоритм байесовской оптимизации, итерационный: на каждой итерации принимается решение о том, какие следующие значения гиперпараметров нужно выбрать, исходя из результатов предыдущих итераций. Но идейно имеет довольно сильные отличия.\\nПредположим сначала, что мы хотим сделать поиск оптимального значения для одного гиперпараметра.\\nНа нескольких первых итерациях алгоритму требуется «разогрев»: нужно иметь некоторую группу значений данного гиперпараметра, на которой известно качество модели. Самый простой способ собрать такие наблюдения — провести несколько итераций Random Search (количество итераций определяется пользователем).\\nСледующим шагом будет разделение собранных во время разогрева данных на две группы. В первой группе будут те наблюдения, для которых модель продемонстрировала лучшее качество, а во второй — все остальные. Размер доли лучших наблюдений задаётся пользователем: чаще всего это 10-25% от всех наблюдений. Картинка ниже иллюстрирует такое разбиение:\\n\\n\\n\\nИсточник\\n\\n\\nДалее некоторым образом строятся оценки распределения ℓ(x)\\\\ell(x)ℓ(x) лучших наблюдений и распределения g(x)g(x)g(x) всех остальных в пространстве значений рассматриваемого гиперпараметра.\\nО том, как оцениваютсяℓ(x)\\\\ell(x)ℓ(x)и g(x)g(x)g(x)Если гиперпараметр принимает непрерывные значения, то распределения ℓ(x)\\\\ell(x)ℓ(x) и g(x)g(x)g(x) можно оценить на основе Parzen window density estimation. Идея данного метода в следующем. Пусть у нас имеются точки x1,…,xnx_1, \\\\ldots, x_nx1\\u200b,…,xn\\u200b, которые были насемплированы из некоторого неизвестного распределения fff. Нам нужно каким-то образом оценить fff по известным данным. Для этого каждое наблюдение xix_ixi\\u200b помещается в центр некоторого симметричного распределения KKK с дисперсией hhh, а оценкой для fff становится смесь этих распределений:\\nf^h(x)=1nh∑i=1nK(x−xih)\\\\hat f_h(x) = \\\\frac{1}{nh} \\\\sum_{i = 1}^n K \\\\left(\\\\frac{x - x_i}{h}\\\\right) \\nf^\\u200bh\\u200b(x)=nh1\\u200bi=1∑n\\u200bK(hx−xi\\u200b\\u200b)Распределения KKK обычно называют ядрами, примеры ядер можно найти тут. На картинке ниже показана зависимость вида итогового распределения от параметра hhh (который часто называют bandwidth):\\n\\n\\n\\nИсточник\\n\\n\\nЧем больше у нас наблюдений, тем точнее можем оценить целевое распределение:\\n\\n\\n\\nИсточник\\n\\n\\nЕсли гиперпараметр категориальный и принимает значения c1,…,cnc_1, \\\\ldots, c_nc1\\u200b,…,cn\\u200b, то в качестве ℓ(x)\\\\ell(x)ℓ(x) и g(x)g(x)g(x) можно задать категориальные распределения в виде наборов из nnn вероятностей (p1,…,pn)(p_1, \\\\ldots, p_n)(p1\\u200b,…,pn\\u200b), где pip_ipi\\u200b соответствует вероятности насемплировать значение cic_ici\\u200b. Значения pip_ipi\\u200b для ℓ(x)\\\\ell(x)ℓ(x) будут пропорциональны числу раз, которое каждое из значений cic_ici\\u200b встретилось в группе лучших наблюдений (и, соответственно, худших наблюдений в случае g(x)g(x)g(x)). Например, пусть у гиперпараметра всего 3 значения и уже прошло 60 итераций алгоритма. Пусть среди лучших 15 испытаний 2 раза встретилось значение c1c_1c1\\u200b, 5 раз встретилось значение с2с_2с2\\u200b и 8 раз встретилось значение c3c_3c3\\u200b. Тогда ℓ(x)∼(p1=215,p2=515,p3=815)\\\\ell(x) \\\\sim \\\\left( p_1 = \\\\frac{2}{15}, p_2 = \\\\frac{5}{15}, p_3 = \\\\frac{8}{15} \\\\right)ℓ(x)∼(p1\\u200b=152\\u200b,p2\\u200b=155\\u200b,p3\\u200b=158\\u200b). Аналогично будет строиться g(x)g(x)g(x).\\nНа следующем шаге алгоритма мы семплируем несколько значений-кандидатов из распределения ℓ(x)\\\\ell(x)ℓ(x) (количество таких семплирований тоже задаётся пользователем, можно задать их число равным, например, 1000). Из насемплированных кандидатов мы хотим найти тех, кто с большей вероятностью окажется в первой группе (состоящей из лучших наблюдений), чем во второй. Для этого для каждого кандидата xxx вычисляется Expected Improvement:\\nEI(x)=ℓ(x)g(x)EI(x) = \\\\frac{\\\\ell(x)}{g(x)} \\nEI(x)=g(x)ℓ(x)\\u200bЗамечание: На самом деле стоит отметить, что в оригинальной статье величина EIEIEI имеет более общее определение. Но там же доказывается, что максимизация EIEIEI в исходном определении эквивалентна максимизации отношения выше.\\nКандидат с наибольшим значением EI(x)EI(x)EI(x) будет включён в множество рассматриваемых гиперпараметров на следующей итерации:\\n\\n\\n\\nИсточник\\n\\n\\nПосле того как было выбрано значение-кандидат, максимизирующее EIEIEI, обучается модель с этим значением гиперпараметра. После обучения мы замеряем её качество на валидационной выборке и в соответствии с этим результатом обновляем распределения ℓ(x)\\\\ell(x)ℓ(x) и g(x)g(x)g(x): снова ранжируем всех имеющихся кандидатов по качеству модели с учётом последнего, из топ 10-25% формируется обновлённое ℓ(x)\\\\ell(x)ℓ(x), из остальных — g(x)g(x)g(x). Так происходит столько раз, сколько итераций алгоритма мы задали.\\nТеперь опишем, как алгоритм работает в общем случае, когда гиперпараметров более одного. Алгоритм работает с гиперпараметрами, представляя их в форме дерева (отсюда «tree» в названии). Например, в документации Hyperopt можно увидеть такой пример:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1from hyperopt import hp\\n2 \\n3space = hp.choice('classifier_type', [\\n4    {\\n5        'type': 'naive_bayes',\\n6    },\\n7    {\\n8        'type': 'svm',\\n9        'C': hp.lognormal('svm_C', 0, 1),\\n10        'kernel': hp.choice('svm_kernel', [\\n11            {'ktype': 'linear'},\\n12            {'ktype': 'RBF', 'width': hp.lognormal('svm_rbf_width', 0, 1)},\\n13            ]),\\n14    },\\n15    {\\n16        'type': 'dtree',\\n17        'criterion': hp.choice('dtree_criterion', ['gini', 'entropy']),\\n18        'max_depth': hp.choice('dtree_max_depth',\\n19            [None, hp.qlognormal('dtree_max_depth_int', 3, 1, 1)]),\\n20        'min_samples_split': hp.qlognormal('dtree_min_samples_split', 2, 1, 1),\\n21    },\\n22])\\n\\n\\nНа рисунке ниже изображено дерево, соответствующее данному примеру:\\n\\nКорень дерева ε\\\\varepsilonε — фиктивная вершина, введённая для удобства. Здесь первый уровень дерева — выбор классификатора (наивный байес, SVM, решающее дерево). Дальнейшие уровни — гиперпараметры самих классификаторов и зависящие уже от них гиперпараметры (например, SVM →\\\\to→ kernel →\\\\to→ RBF →\\\\to→ width). Движение по дереву во время итераций алгоритма происходит по некоторому пути от корня к листу и обратно вдоль пройденного пути (этот процесс подробнее описан ниже).\\nПод некоторыми вершинами записан набор гиперпараметров в скобках (например, kernel и C под SVM). Это означает, что при приходе в эту вершину значения всех гиперпараметров, перечисленных в скобках, должны так или иначе быть выбраны.\\nКаждой вершине дерева, в которой будет происходить семплирование значений, сопоставляется своя пара  ℓ(x)\\\\ell(x)ℓ(x) и g(x)g(x)g(x) с учётом значений, насемплированных на этапе «разогрева». Каждому гиперпараметру, перечисленному в скобках, соответствует своя собственная пара. Если из названия гиперпараметра не идут стрелки (например, C у SVM и min_samples_split у Decision Tree), то это означает, что от его значения не зависят значения никаких других гиперпараметров.\\nПоэтому либо будет выбрано его значение, максимизирующее EIEIEI для соответствующих ему ℓ\\\\ellℓ и ggg, либо уже ничего не нужно семплировать (как, например, в вершинах linear или gini). Если же из гиперпараметра идут стрелки на следующий уровень, то с помощью максимизации EIEIEI будет выбрано, в каком направлении сделать переход. Например, из корня ε\\\\varepsilonε выбирается, какой классификатор рассмотреть на следующем этапе, а из параметра kernel можно перейти либо к RBF, либо к linear.\\nТеперь опишем сам алгоритм. Сначала так же, как и в одномерном случае, происходит «разогрев»: проводится некоторое количество итераций Random Search с теми изначальными распределениями, которые были заданы для гиперпараметров (в примере из Hyperopt эти распределения задаются как hp.qlognormal, hp.lognormal и так далее). Затем начинается итерационное обновление дерева гиперпараметров. Обновление дерева на каждой итерации происходит в два этапа:\\n\\nСначала алгоритм идёт из корня дерева до некоторого листа. В каждой вершине для каждого соответствующего ей гиперпараметра он находит значение, максимизирующее EIEIEI. Если выбор значения для некоторого гиперпараметра означает переход на следующий уровень дерева, он идёт в ту вершину, которая соответствует максимизации EIEIEI. Так он идёт до тех пор, пока не упрётся в какой-то лист. Пройденный путь от корня до листа задаёт полный набор значений гиперпараметров для модели, и её с этими значениями можно провалидировать.\\n\\nПримерПусть вы находитесь в корне ε\\\\varepsilonε и выбираете классификатор. Допустим, классификатор SVM оказался оптимальным по критерию EIEIEI. Вы переходите в соответствующую ему вершину, и здесь вам нужно провести семплирование значений для двух гиперпараметров: kernel и C. Для C вы выбираете некоторое значение, которое максимизирует EIEIEI. Пусть оно оказалось равно 0.10.10.1. А для kernel вы с помощью максимизации EIEIEI выбираете, в какую вершину на следующем уровне вы отправитесь. Пусть эта вершина — RBF. Для него вы семплируете конкретное значение width — пусть оно оказалось равным 0.90.90.9. Получилось, что вы прошли полный путь и получили модель с заданным набором гиперпараметров: SVM(C=0.1,kernel=RBF(width=0.9))SVM(C = 0.1, kernel = RBF(width = 0.9))SVM(C=0.1,kernel=RBF(width=0.9)), которую теперь можно провалидировать.\\n\\nПосле того как модель, полученная на предыдущем этапе, была провалидирована, распределения в вершинах дерева нужно обновить в соответствии с информацией о полученном качестве. Для этого алгоритм поднимается из листа наверх, обновляя распределения во всех вершинах дерева вдоль своего пути. В каждой вершине для каждого гиперпараметра процедура обновления та же, что была описана для одного гиперпараметра: имеющиеся значения гиперпараметров переранжируются по качеству с учётом результата последнего кандидата (этот результат общий для всех вершин вдоль пути), по топ 10-25% оценивается ℓ(x)\\\\ell(x)ℓ(x), по остальным — g(x)g(x)g(x).\\n\\nВ качестве окончательного ответа алгоритм выдаёт набор гиперпараметров (или, как в примере выше, не только гиперпараметры, но даже саму модель), на котором было получено лучшее качество за все итерации. Число итераций алгоритма задаётся пользователем.\\nЗа дальнейшими деталями о процедуре обновления дерева для алгоритма TPE можно обратиться к этой статье и к исходному коду алгоритма TPE из библиотеки Hyperopt.\\nСтоит заметить, что если гиперпараметры не лежат вместе ни в одном пути в дереве, то TPE считает их независимыми. Это — недостаток данного алгоритма, так как некоторые гиперпараметры, находящиеся по смыслу в разных путях в дереве, зависят от друг от друга.\\nНапример, с регуляризацией мы можем тренировать нейросеть большее число эпох, чем без регуляризации, потому что без регуляризации сеть на большом числе эпох может начать переобучаться. В этом конкретном примере можно использовать такой трюк:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1hp.choice('training_parameters', [\\n2    {\\n3        'regularization': True,\\n4        'n_epochs': hp.quniform('n_epochs', 500, 1000, q=1),\\n5    }, {\\n6        'regularization': False,\\n7        'n_epochs': hp.quniform('n_epochs', 20, 300, q=1),\\n8    },\\n9])\\n\\n\\nНо если внутренние зависимости между гиперпараметрами вам неизвестны, то алгоритм не сможет найти их сам.\\nКритерий EIEIEI позволяет методу TPE балансировать между exploration и exploitation. Семплирование из распределения ℓ(x)\\\\ell(x)ℓ(x) — это, с одной стороны, exploitation, так как гиперпараметры, семплируемые из него, близки к оптимуму, но это же привносит элемент exploration, так как семплируемые гиперпараметры не равны оптимуму в точности.\\nPopulation Based Training (PBT)\\nЭтот метод использует идеи из теории эволюционных стратегий и с самого начала включает в себя параллельные вычисления.\\nМетоды, описанные выше, имеют свои сильные и слабые стороны.\\n\\nGrid Search и Random Search:\\n\\nотлично параллелизуются;\\nне используют результаты предыдущих итераций.\\n\\n\\nБО и TPE:\\n\\nтрудно параллелизуются;\\nиспользуют результаты предыдущих итераций, при сходимости результаты лучше, чем у Random Search и Grid Search.\\n\\n\\n\\nВ алгоритме PBT была сделана попытка объединить сильные стороны обеих групп, что проиллюстрировано на картинке ниже:\\n\\n\\n\\nИсточник\\n\\n\\nВ процессе работы алгоритм обучает не одну модель, а целую популяцию P\\\\mathcal{P}P моделей — набор моделей одинакового типа, отличающихся только набором гиперпараметров:\\nP={(θi,hi)\\u2009∣\\u2009i=1,…,N},    \\\\mathcal{P} = \\\\{(\\\\theta_i, h_i) \\\\, | \\\\, i = 1, \\\\ldots, N \\\\}, \\nP={(θi\\u200b,hi\\u200b)∣i=1,…,N},где θi\\\\theta_iθi\\u200b и hih_ihi\\u200b — веса и гиперпараметры модели iii соответственно.\\nПредполагается также, что модели обучаются как-то итерационно, например градиентным спуском (но могут использоваться и безградиентные методы, такие как эволюционные стратегии). Изначально каждая модель в популяции имеет случайные веса и гиперпараметры. Каждая модель из популяции тренируется параллельно с остальными, и периодически качество каждой модели замеряется независимо от остальных.\\nКак только какая-то модель считается «созревшей» для обновления (например, прошла достаточное число шагов градиентного спуска или преодолела некоторый порог по качеству), у неё появляется шанс быть обновлённой относительно всей остальной популяции:\\n\\nпроцедура exploit(): если у модели низкое качество относительно популяции, то её веса заменяются на веса модели с более высоким качеством;\\nпроцедура explore(): если веса модели были перезаписаны, шаг explore добавляет случайный шум в параметры модели.\\n\\nПри таком подходе только лучшие пары моделей и гиперпараметров выживут и будут обновляться, что позволяет добиться более высокой утилизации ресурсов.\\n\\n\\n\\nИсточник\\n\\n\\nСтоит отметить, что наиболее оптимальный размер популяции, выявленный авторами в результате экспериментов, — от 20 до 40, что довольно много и не реализуется на обычном ноутбуке.\\nКрасивая гифка с демонстрацией работы алгоритма:\\n\\n\\n\\nИсточник\\n\\n\\nOpen-source-библиотеки\\nScikit-learn\\nВ библиотеке Scikit-learn есть реализации Grid Search и Random Search, что очень удобно, если вы используете модели из sklearn. Примеры их использования можно найти здесь.\\nHyperopt\\nВ библиотеке Hyperopt реализованы три метода оптимизации гиперпараметров:\\n\\nRandom Search\\nTPE\\nAdaptive TPE\\n\\nУ них есть небольшой туториал по тому, как начать пользоваться библиотекой. Кроме того, у них есть обёртка над sklearn, позволяющая работать с моделями оттуда: Hyperopt-sklearn.\\nOptuna\\nВ библиотеке Optuna реализованы те же методы оптимизации, что и в Hyperopt, но по многим параметрам она оказывается удобнее. Хорошее сравнение Optuna и Hyperopt можно найти здесь.\\nScikit-Optimize\\nВ библиотеке Scikit-Optimize реализованы алгоритмы байесовской оптимизации и Random Search. Кроме самих методов оптимизации библиотека предоставляет отличный инструментарий для различных визуализаций. Хорошее описание возможностей библиотеки можно найти тут.\\nKeras Tuner\\nБиблиотека Keras Tuner позволяет подбирать гиперпараметры для нейросеток, написанных на TensorFlow 2.0, и для обычных моделей из Scikit-learn. Доступные методы оптимизации — Random Search и Hyperband. Хороший гайд по использованию данной библиотеки можно найти тут.\\nSummary\\nСписок описанных методов не исчерпывает все существующие на данный момент методы оптимизации гиперпараметров: остались за кадром такие алгоритмы, как ASHA, Hyperband, BOHB. Хороший сравнительный обзор этих трёх алгоритмов можно найти здесь.\\nМы собрали все описанные выше алгоритмы в таблицу, чтобы вам было удобнее сравнивать их между собой. А к некоторым оставили дополнительные комментарии.\\nGrid Search. Хорошо работает, когда у вас совсем мало гиперпараметров либо вы смогли распараллелить его работу.\\n\\nСильные стороны:\\n\\nсамый простой для понимания и реализации;\\nтривиально распараллеливается.\\n\\n\\nСлабые стороны:\\n\\nне использует результаты других итераций;\\nограничен в выборе заданной сеткой;\\nдолго работает, если делает последовательный перебор по сетке. Нет гарантий на необходимое число итераций.\\n\\n\\n\\nВ защиту этого метода хочется сказать, что часто на практике приходится делать перебор гиперпараметров вообще вручную (если один инстанс вашей модели учится недели две и использует много ресурсов) либо по очень небольшой сетке. Так что метод вполне в ходу 😃\\nRandom Search. Метод представляет собой небольшое усложнение над Grid Search, но при этом оказывается намного более эффективным.\\n\\nСильные стороны:\\n\\nслучайный перебор по сетке позволяет находить оптимальные гиперпараметры более эффективно, чем Grid Search, в частности из-за того, что непрерывные параметры можно задать в виде распределения, а не перечислять значения заранее;\\nтривиально распараллеливается;\\nдопускает усиление за счёт использования квазислучайных распределений при семплировании.\\n\\n\\nСлабые стороны:\\n\\nне использует результаты других итераций;\\nограничен в выборе заданной сеткой, хотя и в некоторых случаях менее жёстко, чем Grid Search.\\n\\n\\n\\nБайесовская оптимизация\\n\\nСильные стороны:\\n\\nиспользует результаты предыдущих итераций;\\nможет моделировать внутренние зависимости между гиперпараметрами (за счёт работы с ними в едином подмножестве Rn\\\\mathbb{R}^nRn, где nnn — число гиперпараметров);\\nможет расширять заданные изначально границы множества поиска гиперпараметров;\\nдостигает более высокого качества, чем Random Search, если удалось провести достаточное количество итераций.\\n\\n\\nСлабые стороны:\\n\\nпаралеллится нетривиально;\\nв нераспараллеленном случае работает долго, так как для каждой итерации ему приходится заново строить вероятностную модель. В случае если такая модель — гауссовские процессы, сложность получается порядка n3n^3n3, где nnn — число гиперпараметров;\\nдля работы с категориальными гиперпараметрами нужны нетривиальные хаки.\\n\\n\\n\\nTree-structured Parzen Estimator\\n\\n\\nСильные стороны:\\n\\nиспользует результаты предыдущих итераций;\\nможет работать с зависимостями между гиперпараметрами, в которых один гиперпараметр не будет рассматриваться, если другой не примет какое-то определённое значение (например, число нейронов во втором слое нейросети нужно перебирать, если параметр «число слоёв» имеет значение не менее двух);\\nимеет линейную сложность по числу гиперпараметров (в отличие от БО);\\nне требует специальных хаков для работы с категориальными признаками, так как каждый гиперпараметр в этом алгоритме имеет своё отдельное одномерное распределение, и не нужно строить сложное совместное распределение всех гиперпараметров (как в БО);\\nдостигает высоких результатов по качеству, довольно часто используется в соревнованиях.\\n\\n\\n\\nСлабые стороны:\\n\\nне может моделировать неявные зависимости между гиперпараметрами (те, которые юзер не задал с помощью дерева);\\nхотя сложность и меньше, чем у БО, может работать довольно медленно даже на не очень большом числе гиперпараметров.\\n\\n\\n\\nPopulation Based Training\\n\\nСильные стороны:\\n\\nпараллельный by design;\\nможет использовать результаты предыдущих итераций.\\n\\n\\nСлабые стороны:\\n\\nдля эффективной работы нужно много воркеров (от 20 до 40), что нетривиально для реализации.\\n\\n\\n\\nПочитать по теме\\n\\nПримеры использования Grid Search от sklearn.\\nПримеры использования Random Search от sklearn.\\nХороший блог-пост о гиперпараметрах, в первом разделе которого есть интересные рассуждения про усиление Random Search с помощью квазислучайных распределений.\\nБлог-пост от DeepMind про предложенный ими алгоритм Population Based Training.\\nОригинальная статья, где был предложен алгоритм.\\nБлог-пост про эволюционные стратегии.\\n\\nА если вам интересно как следует разобраться в байесовской оптимизации (в частности, рассмотреть больше примеров вероятностных моделей и разных acquisition function), то вот полезный контент:\\n\\nОтличный туториал по различным методам оптимизации гиперпараметров, в частности по байесовской оптимизации.\\nСтатья-обзор, подробно объясняющая математические детали методов байесовской оптимизации и содержащая примеры их применения в ресёрче и индустрии.\\nВидеолекция Евгения Бурнаева на летней школе Deep | Bayes.\\nОригинальная статья, в которой были предложены методы TPE и байесовская оптимизация.\\nПример использования skopt (Scikit-Optimize) — нахождение лучших параметров для SVM с помощью байесовской оптимизации.\\nРеализация алгоритма байесовской оптимизации и примеры использования библиотечных реализаций.\\nПро гауссовские процессы с хорошими визуализациями.\\nБолее формально про гауссовские процессы, но с хорошими примерами на питоне.\\n\\nДля дальнейшего изучения метода TPE можно использовать следующие источники:\\n\\nОригинальная статья, в которой были предложены методы TPE и байесовская оптимизация.\\nБлог-пост про TPE и остальные методы тюнинга гиперпараметров от NeuPy. Там же можно найти пример применения TPE из Hyperopt.\\nОтличное объяснение того, что такое Parzen window density estimation.\\nОтличный туториал по различным методам оптимизации гиперпараметров (который уже был упомянут выше в разделе про байесовскую оптимизацию).\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф3.2. Кросс-валидацияКак строить надёжные оценки качества моделей и\\xa0никогда не\\xa0смешивать train и\\xa0testСледующий параграф4.1. Вероятностный подход в MLКак описать привычные модели на\\xa0языке статистики. Оптимизация функции потерь vs\\xa0оценка максимального правоподобияЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_45.html', 'title': 'Задача ранжирования'}, page_content='Задача ранжированияЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/510.1.Кластеризация10.2.Временные ряды10.3.Аналитика временных рядов10.4.Модели вида ARIMA10.5.Задача ранжированияПримерыМетрики качества ранжированияВещественная релевантностьДополнительные метрикиМетоды обучения ранжированиюПрактические советы11.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Задача ранжирования10.5. Задача ранжированияАвторыРоман ЛогиновЧеловек, который пользуется интернетом, часто начинает решение своих задач с поиска. Поисковая система по запросу помогает найти самую полезную для пользователя информацию, будь то поиск видео или научных статей. Для решения такой задачи необходимо отсортировать по полезности имеющуюся в базе информацию и выдать самую необходимую. Обычно такую сортировку называют ранжированием, полезность — релевантностью, а соответствующую задачу — задачей ранжирования.\\nОпишем задачу формально и введём обозначения.\\nD\\xa0−\\xa0коллекция\\xa0документов\\xa0(или\\xa0других\\xa0объектов)D\\\\ -\\\\  \\\\text{коллекция документов (или других объектов)}\\nD\\xa0−\\xa0коллекция\\xa0документов\\xa0(или\\xa0других\\xa0объектов)Q\\xa0−\\xa0множество\\xa0запросовQ\\\\ -\\\\  \\\\text{множество запросов}\\nQ\\xa0−\\xa0множество\\xa0запросов∀q∈Q\\xa0Dq∈D\\xa0−\\xa0набор\\xa0документов,\\xa0потенциально\\xa0релевантных\\xa0запросу\\\\forall q \\\\in Q\\\\ D_q \\\\in D\\\\ -\\\\  \\\\text{набор документов, потенциально релевантных запросу}\\n∀q∈Q\\xa0Dq\\u200b∈D\\xa0−\\xa0набор\\xa0документов,\\xa0потенциально\\xa0релевантных\\xa0запросуВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.ВступитьЗадача: Отсортировать документы внутри DqD_qDq\\u200b по убыванию релевантности запросу.\\nУже по формулировке задачи видно, что построение решения разбивается на несколько стадий. Сначала нужно сформировать набор кандидатов DqD_qDq\\u200b, а уже потом строить финальную сортировку. Подробнее об этом в разделе про методы ранжирования.\\nПримеры\\nWEB-поиск\\nКлассический пример задачи ранжирования — поиск информации в интернете. Пользователь задаёт запрос, и под этот запрос формируется выдача, в которой сайты (документы) расположены по убыванию полезности. В наших обозначениях:\\nD\\xa0−\\xa0база\\xa0веб-страниц,\\xa0проиндексированных\\xa0поисковой\\xa0системойD\\\\ -\\\\  \\\\text{база веб-страниц, проиндексированных поисковой системой}\\nD\\xa0−\\xa0база\\xa0веб-страниц,\\xa0проиндексированных\\xa0поисковой\\xa0системойQ\\xa0−\\xa0множество\\xa0запросов\\xa0пользователейQ\\\\ -\\\\  \\\\text{множество запросов пользователей}\\nQ\\xa0−\\xa0множество\\xa0запросов\\xa0пользователейЧтобы решать задачу ранжирования с помощью машинного обучения, необходимо иметь датасет с оценками асессоров. Обычно по парам (запрос, документ) собирают данные о том, насколько документ релевантен запросу. Такие оценки называются метками релевантности.\\nПоиск синонимов\\nЗадача подбора синонимов по слову тоже может восприниматься как задача ранжирования, поскольку похожесть слов друг на друга — неоднозначное свойство, и некоторые пары слов больше похожи, чем другие. Поэтому можно сортировать слова по похожести на слово из запроса. Если VVV — доступный словарь, то:\\nD=V\\xa0−\\xa0список\\xa0всех\\xa0словD = V\\\\ -\\\\  \\\\text{список всех слов}\\nD=V\\xa0−\\xa0список\\xa0всех\\xa0словQ=V\\xa0−\\xa0cлова,\\xa0для\\xa0которых\\xa0можно\\xa0искать\\xa0синонимыQ = V\\\\ -\\\\  \\\\text{cлова, для которых можно искать синонимы}\\nQ=V\\xa0−\\xa0cлова,\\xa0для\\xa0которых\\xa0можно\\xa0искать\\xa0синонимыДля построения моделей можно использовать датасет пар синонимичных слов. Обучив модель классификации, обычно мы получаем предсказатор вероятности положительного класса. Отсортировав слова по предсказанной вероятности синонимичности, мы решим задачу ранжирования.\\nРекомендательная система\\nРекомендательные системы встроены во многие онлайн сервисы. Если вы заходите на сервис с фильмами, система порекомендует фильмы для вас, если в социальную сеть — посты или новые видео, которые вас заинтересуют. Это тоже отбор самых полезных объектов, но «запрос» в данном случае — это сам пользователь.\\nD=I\\xa0−\\xa0список\\xa0айтемов,\\xa0доступных\\xa0на\\xa0сайте\\xa0(объявления,\\xa0фильмы)D = I\\\\ -\\\\  \\\\text{список айтемов, доступных на сайте (объявления, фильмы)}\\nD=I\\xa0−\\xa0список\\xa0айтемов,\\xa0доступных\\xa0на\\xa0сайте\\xa0(объявления,\\xa0фильмы)Q=U\\xa0−\\xa0пользователь\\xa0вместе\\xa0с\\xa0его\\xa0историейQ = U\\\\ -\\\\  \\\\text{пользователь вместе с его историей}\\nQ=U\\xa0−\\xa0пользователь\\xa0вместе\\xa0с\\xa0его\\xa0историейДля решения задачи построения рекомендаций используются свои методы, они рассмотрены в отдельной главе.\\nМетрики качества ранжирования\\nПредположим, мы решили задачу ранжирования. Обычно это делается обучением некоторой функции от запроса и документа aθ(q,d)a_\\\\theta(q, d)aθ\\u200b(q,d) (θ\\\\thetaθ — это параметры модели). Если такая функция готова, то выдача по запросу q∈Qq \\\\in Qq∈Q получается сортировкой множества DqD_qDq\\u200b по убыванию aθ(q,d)a_\\\\theta(q, d)aθ\\u200b(q,d).\\nЗдесь и далее, не нарушая общности, будем считать, что мы решаем задачу WEB-поиска. Выберем множество тестовых запросов QtQ_{t}Qt\\u200b, на которых оценим качество нашего решения.\\nДля формул метрик качества введём следующие обозначения:\\nTK(q)\\xa0−\\xa0первые\\xa0K\\xa0элементов\\xa0выдачи\\xa0по\\xa0запросу\\xa0qT_K(q)\\\\ -\\\\  \\\\text{первые } K \\\\text{ элементов выдачи по запросу } q \\nTK\\u200b(q)\\xa0−\\xa0первые\\xa0K\\xa0элементов\\xa0выдачи\\xa0по\\xa0запросу\\xa0qdq(k)\\xa0−\\xa0k\\xa0по\\xa0порядку\\xa0документ\\xa0в\\xa0выдачеd_q^{(k)}\\\\ -\\\\ k \\\\text{ по порядку документ в выдаче}\\ndq(k)\\u200b\\xa0−\\xa0k\\xa0по\\xa0порядку\\xa0документ\\xa0в\\xa0выдачеСоответственно, TK(q)={dq(1),…,dq(k)}T_K(q) = \\\\{ d_q^{(1)}, \\\\dots, d_q^{(k)} \\\\}TK\\u200b(q)={dq(1)\\u200b,…,dq(k)\\u200b}\\nБинарная релевантность\\nВеличина, которая обозначает, насколько документ подходит запросу, называется релевантностью. Способов измерить эту величину много. Обычно решение о том, релевантен документ запросу или нет, принимают асессоры — специальные люди, которые размечают данные для обучения ML-моделей. Собранные оценки называются метками релевантности, будем обозначать метку для запроса qqq и документа ddd за y(q,d)y(q, d)y(q,d).\\nДля начала введём метрики для случая бинарной релевантности, когда y(q,d)∈{0,1}y(q, d) \\\\in \\\\{0, 1\\\\}y(q,d)∈{0,1}.\\nPrecision / Recall\\nЕсли у релевантности есть всего 2 класса, то можно вспомнить метрики классификации и обобщить их для задачи ранжирования.\\nБудем считать, что ранжирующая модель считает релевантными те документы, которые попали в первые KKK элементов выдачи, то есть TK(q)T_K(q)TK\\u200b(q). Тогда можно вычислить метрики precision и recall в зависимости от KKK.\\nPrecision@K=число\\xa0релевантных\\xa0в\\xa0топеK=∑d∈TK(q)I{y(q,d)=1}KPrecision@K = \\\\frac{\\\\text{число релевантных в топе}}{K} = \\\\frac{\\\\sum\\\\limits_{d \\\\in T_K(q)} I\\\\{y(q, d) = 1\\\\}}{K} \\nPrecision@K=Kчисло\\xa0релевантных\\xa0в\\xa0топе\\u200b=Kd∈TK\\u200b(q)∑\\u200bI{y(q,d)=1}\\u200bRecall@K=число\\xa0релевантных\\xa0в\\xa0топевсего\\xa0релевантных=∑d∈TK(q)I{y(q,d)=1}min\\u2061(K,∑d∈DI{y(q,d)=1})Recall@K = \\\\frac{\\\\text{число релевантных в топе}}{\\\\text{всего релевантных}} = \\\\frac{\\\\sum\\\\limits_{d \\\\in T_K(q)} I\\\\{y(q, d) = 1\\\\}}{\\\\min(K, \\\\sum\\\\limits_{d \\\\in D} I\\\\{y(q, d) = 1\\\\})} \\nRecall@K=всего\\xa0релевантныхчисло\\xa0релевантных\\xa0в\\xa0топе\\u200b=min(K,d∈D∑\\u200bI{y(q,d)=1})d∈TK\\u200b(q)∑\\u200bI{y(q,d)=1}\\u200bВ случае с recall приходится брать минимум в знаменателе, поскольку в такой постановке модель не может выявить больше, чем KKK релевантных документов.\\nMean Average Precision\\nЗаметим, что метрики precision и recall хоть и показывают качество нашей ранжирующей системы, но совсем не смотрят на порядок элементов в TK(q)T_K(q)TK\\u200b(q). Чтобы его учесть, посмотрим на Precision по тем позициям, где стоят релевантные документы, и усредним их. Такая величина называется Average Precision.\\nAP(q)=1K∑k=1KPrecision@k⋅y(q,dq(k))AP(q) = \\\\frac{1}{K} \\\\sum\\\\limits_{k = 1}^K Precision@k \\\\cdot y(q, d_q^{(k)})\\nAP(q)=K1\\u200bk=1∑K\\u200bPrecision@k⋅y(q,dq(k)\\u200b)Теперь, чтобы получить труднопереводимую на русский язык метрику Mean Average Precision, нужно усреднить значения AP по всем запросам из набора.\\nMAP(q)=1∣Qt∣∑q∈QtAP(q)MAP(q) = \\\\frac{1}{|Q_t|} \\\\sum\\\\limits_{q \\\\in Q_t} AP(q)\\nMAP(q)=∣Qt\\u200b∣1\\u200bq∈Qt\\u200b∑\\u200bAP(q)Mean Reciprocal Rank\\nНазвание этой метрики переводится как средний обратный ранг. Ранжирование работает тем лучше, чем ближе к началу выдачи релевантный для пользователя документ. Для каждого запроса найдём позицию первого релевантного документа, возьмём обратное от этого числа и усредним по всем запросам.\\nMRR=1∣Q∣∑q∈Qmin\\u2061(i\\xa0∣\\xa0y(q,dq(i))=1)−1MRR = \\\\frac{1}{|Q|} \\\\sum\\\\limits_{q \\\\in Q} \\\\min\\\\left(i\\\\ |\\\\ y(q, d_q^{(i)}) = 1\\\\right)^{-1} \\nMRR=∣Q∣1\\u200bq∈Q∑\\u200bmin(i\\xa0∣\\xa0y(q,dq(i)\\u200b)=1)−1Обозначим релевантность kkk-го документа в выдаче как Rk=y(q,dq(k))R_k = y(q, d_q^{(k)})Rk\\u200b=y(q,dq(k)\\u200b).\\nТогда формулу можно переписать в следующем виде:\\nMRR=1∣Q∣∑q∈Q∑i∏k=1i−1(1−Rk)⋅Ri⋅1iMRR = \\\\frac{1}{|Q|} \\\\sum\\\\limits_{q \\\\in Q}\\\\sum\\\\limits_{i} \\\\prod\\\\limits_{k=1}^{i - 1}(1 - R_k) \\\\cdot R_i \\\\cdot \\\\frac{1}{i}\\nMRR=∣Q∣1\\u200bq∈Q∑\\u200bi∑\\u200bk=1∏i−1\\u200b(1−Rk\\u200b)⋅Ri\\u200b⋅i1\\u200bПроизведение в формуле будет равно 111 только в случае, когда iii документ релевантный, а все до него нет. В остальных случаях произведение равно 000.\\nВещественная релевантность\\nРассмотрим теперь метрики для случая, когда релевантность может принимать вещественные значения.\\nExpected Reciprocal Rank\\nПусть теперь Ri=P(q,dq(i))R_i = P(q, d_q^{(i)})Ri\\u200b=P(q,dq(i)\\u200b) — вероятность того, что документ dq(i)d_q^{(i)}dq(i)\\u200b релевантен. Например, если yiy_iyi\\u200b — метка релевантности, а YmaxY_{max}Ymax\\u200b — максимальное её значение, то можно определить Ri=2yi2YmaxR_i = \\\\frac{2^{y_i}}{2^{Y_{max}}}Ri\\u200b=2Ymax\\u200b2yi\\u200b\\u200b.\\nБудем считать, что пользователь листает выдачу документ за документом. В каждом он находит информацию, которая ему нужна, с вероятностью RiR_iRi\\u200b. Если информацию он нашел, то он заканчивает поиск.\\nВ такой модели хорошая система позволит пользователю найти информацию как можно быстрее. Но в отличие от бинарной релевантности позиция, где пользователь закончил поиск, теперь случайная величина, как и обратная позиция. Поэтому будем считать матожидание этой величины.\\nERR(q)=∑iP(пользователь\\xa0дошёл\\xa0до\\xa0док-та\\xa0и\\xa0остановился)⋅1iERR(q) = \\\\sum\\\\limits_i P(\\\\text{пользователь дошёл до док-та и остановился}) \\\\cdot \\\\frac{1}{i}\\nERR(q)=i∑\\u200bP(пользователь\\xa0дошёл\\xa0до\\xa0док-та\\xa0и\\xa0остановился)⋅i1\\u200bЕсли пользователь остановился на позиции iii, это значит, что на предыдущих документах задачу он не решил, а остановился именно на iii-ом. В модели эти события предполагаем независимыми, поэтому вероятности можно перемножить.\\nERR(q)=∑i∏k=1i−1(1−Rk)⋅Ri⋅1iERR(q) = \\\\sum\\\\limits_{i} \\\\prod\\\\limits_{k=1}^{i - 1}(1 - R_k) \\\\cdot R_i \\\\cdot \\\\frac{1}{i}\\nERR(q)=i∑\\u200bk=1∏i−1\\u200b(1−Rk\\u200b)⋅Ri\\u200b⋅i1\\u200bЧтобы получить финальную метрику, усредняем эту величину по всем запросам.\\nERR=1∣Q∣∑q∈Q∑i∏k=1i−1(1−Rk)⋅Ri⋅1iERR = \\\\frac{1}{|Q|} \\\\sum\\\\limits_{q \\\\in Q}\\\\sum\\\\limits_{i} \\\\prod\\\\limits_{k=1}^{i - 1}(1 - R_k) \\\\cdot R_i \\\\cdot \\\\frac{1}{i}\\nERR=∣Q∣1\\u200bq∈Q∑\\u200bi∑\\u200bk=1∏i−1\\u200b(1−Rk\\u200b)⋅Ri\\u200b⋅i1\\u200bЗаметим, что формула совпала с формулой для метрики MRR для бинарной релевантности.\\npFound\\nРассмотрим ещё одну метрику, которая основывается на модели поведения пользователя. Метрика pFound была придумана в Яндексе и некоторое время была основной для ранжирования.\\nПусть релевантность задаётся одним из классов y(q,d)∈Y={Not\\xa0Rel,Rel-,Rel+,Useful,Vital}y(q, d) \\\\in \\\\mathbb{Y} = \\\\{\\\\text{Not Rel}, \\\\text{Rel-}, \\\\text{Rel+}, \\\\text{Useful}, \\\\text{Vital}\\\\}y(q,d)∈Y={Not\\xa0Rel,Rel-,Rel+,Useful,Vital}\\nПо историческим данным считаются соответствующие вероятности найти нужное в документе в зависимости от класса Pq(d)={0,0.07,0.14,0.41,0.61}P_q(d) = \\\\{0, 0.07, 0.14, 0.41, 0.61\\\\}Pq\\u200b(d)={0,0.07,0.14,0.41,0.61}\\nОтличием от предыдущей модели является введённая константа Pbreak=0.15P_{break} = 0.15Pbreak\\u200b=0.15 — вероятность бросить искать информацию и листать выдачу.\\nПосчитаем вероятность PiP_iPi\\u200b того, что пользователь дошёл до позиции iii. Для этого он должен дойти до документа на позиции i−1i - 1i−1, не устать искать, и при этом не найти нужного в предыдущем документе. Опять же перемножаем вероятности.\\nP1=1;\\xa0Pi=Pi−1⋅(1−Pbreak)⋅(1−Pq(dq(i−1)))P_1 = 1;\\\\ P_i = P_{i - 1} \\\\cdot \\\\left(1 - P_{break}\\\\right) \\\\cdot \\\\left(1 - P_q(d_q^{(i - 1)})\\\\right)\\nP1\\u200b=1;\\xa0Pi\\u200b=Pi−1\\u200b⋅(1−Pbreak\\u200b)⋅(1−Pq\\u200b(dq(i−1)\\u200b))Теперь pFoundpFoundpFound — это вероятность найти нужное в выдаче\\npFound(q)=∑i=1nPi⋅Pq(dq(i))pFound(q) = \\\\sum\\\\limits_{i = 1}^n P_i \\\\cdot P_q(d_q^{(i)}) \\npFound(q)=i=1∑n\\u200bPi\\u200b⋅Pq\\u200b(dq(i)\\u200b)Чтобы теперь посчитать финальную метрику, усредняем pFoundpFoundpFound по всем запросам в тестовом множестве QtQ_tQt\\u200b.\\nnDCG\\nВведём метрику DCG (Discounted Cumulative Gain).\\nБудем считать, что релевантный документ в топе приносит некоторую пользу (gain) в зависимости от своей релевантности. При этом до низкого документа в выдаче могут и не долистать, поэтому он приносит меньше пользы, то есть она уменьшается. Будем дисконтировать пользу в зависимости от позиции (discount).\\nDCGn(q)=∑i=1nGq(dq(i))⋅D(i)DCG_n(q) = \\\\sum\\\\limits_{i = 1}^n G_q(d_q^{(i)}) \\\\cdot D(i) \\nDCGn\\u200b(q)=i=1∑n\\u200bGq\\u200b(dq(i)\\u200b)⋅D(i)Здесь Gq(d)G_q(d)Gq\\u200b(d) — функция пользы, а DDD — функция дисконтирования от позиции. Для этих функций возможны разные вариации, рассмотрим классический и упрощённый.\\nКлассический вариант: Gq(d)=2y(q,d)−1;\\xa0D(i)=1log\\u20612(i+1)G_q(d) = 2^{y(q, d) - 1};\\\\ D(i) = \\\\frac{1}{\\\\log_2(i + 1)}Gq\\u200b(d)=2y(q,d)−1;\\xa0D(i)=log2\\u200b(i+1)1\\u200b\\nУпрощённый вариант: Gq(d)=y(q,d);\\xa0D(i)=1i+1G_q(d) = y(q, d);\\\\ D(i) = \\\\frac{1}{i + 1}Gq\\u200b(d)=y(q,d);\\xa0D(i)=i+11\\u200b\\nИногда при реализации поисковой системы может быть понимание, как падает внимание пользователя с ростом позиции в зависимости от типа запроса. В этом случае функция дисконтирования может стать запросозависимой.\\nОднако низкое значение метрики DCGDCGDCG не всегда означает, что ранжирование отработало плохо. Могло быть так, что по запросу просто нет релевантных документов, или же их очень мало. Чтобы избавиться от этой проблемы, значение DCGDCGDCG нормируют на эту метрику при идеальном ранжировании, когда документы отсортированы по истинным значениям релевантности.\\nnDCG(q)=DCG(q)max\\u2061DCG(q)nDCG(q) = \\\\frac{DCG(q)}{\\\\max DCG(q)}\\nnDCG(q)=maxDCG(q)DCG(q)\\u200bКак и всегда, для получения метрики по набору запросов, считают среднее значение nDCG.nDCG.nDCG.\\nДополнительные метрики\\nДругие сигналы и экосистема\\nОписанные выше метрики были введены для агрегации релевантности документов в топе выдачи. Но те же рассуждения и формулы могут быть применены для других сигналов, сопоставляющих запрос и документ. Вы можете придумать любую инструкцию для асессоров и собрать разметку под вашу задачу. Одним из полезных сигналов может быть свежесть документа, которую можно понять и по времени создания документа.\\nОбычно в противовес релевантности смотрят на кликабельность элементов выдачи. Поисковым системам интересно получить больше кликов пользователей, тем не менее могут встречаться «кликбейтные» документы, которые побуждают кликать, но не решают на самом деле задачу пользователя. Кликабельность можно замерять как DCG предсказатора вероятности клика.\\nТакже важно следить за чистотой выдачи. Нужно не допускать в топ мошеннические документы, шокирующие документы и документы 18+ в поиске для детей. В качестве метрики можно замерять DCG или MAP «плохих» документов в топе.\\nРазнообразие и метрики рекомендаций\\nБывает полезно следить за тем, как документы в топе соотносятся друг с другом. В частности, нужно не допускать, чтобы все документы выдачи были с одного хоста и чтобы в них не было написано одно и то же. Для этого используются алгоритмы группировки и дедупликации.\\nТакже, если рассматривать поиск как решение задачи рекомендации документов, можно измерять метрики новизны и serendipity, про которые подробнее рассказано в главе о рекомендательных системах.\\nМетоды обучения ранжированию\\nРассмотрим некоторую модель aθ(q,d)a_\\\\theta(q, d)aθ\\u200b(q,d), по предсказаниям которой мы будем сортировать документы ddd по запросу qqq (здесь θ\\\\thetaθ — это параметры модели). Мы хотим обучить модель так, чтобы у неё было оптимальное значение одной из метрик ранжирования, например, NDCG. Заметим, что если совсем немного поменять θ\\\\thetaθ, то предсказания модели aθ(q,d)a_\\\\theta(q, d)aθ\\u200b(q,d) изменятся тоже несильно. Но небольшие изменения в предсказаниях могут не привести к изменению порядка документов. Тогда не изменится и метрика NDCG. Получается, что NDCG как функция от параметров θ\\\\thetaθ является кусочно постоянной, поэтому нельзя оптимизировать её напрямую.\\nНаша дальнейшая задача — представить методы, позволяющие получить модель, оптимальную по NDCG или другой аналогичной метрике ранжирования.\\nМетоды обучения ранжированию обычно делят на 3 типа:\\nПоточечный (pointwise) подход\\nВ этом подходе у нас известны некоторые оценки релевантности каждой пары запрос-документ, и модель учится предсказывать эти оценки. Взаимоотношения между разными документами внутри DqD_qDq\\u200b не рассматриваются.\\nПопарный (pairwise) подход\\nЗдесь во время обучения используют тройки (q,d1,d2)(q, d_1, d_2)(q,d1\\u200b,d2\\u200b), где d1,d2d_1, d_2d1\\u200b,d2\\u200b — документы из DqD_qDq\\u200b, причём d1d_1d1\\u200b релевантнее d2d_2d2\\u200b по запросу qqq. При этом модель всё равно может давать предсказания релевантности по паре (q,d)(q, d)(q,d).\\nСписочный (listwise) подход\\nВ данном подходе для обучения используются перестановки документов из DqD_qDq\\u200b. Например, асессорские оценки дают наилучшую известную сортировку. Для её получения нужно сначала показать на выдаче докумены с самой высокой оценкой, затем со следующей по порядку и т.д.\\nБудем рассматривать каждый из подходов по очереди.\\nПоточечный подход\\nСведение к регрессии и классификации\\nРассмотрим простейшую постановку задачи, в которой у нас есть набор запросов QQQ, для каждого запроса q∈Qq \\\\in Qq∈Q имеется набор документов DqD_qDq\\u200b, который необходимо отсортировать, а в качестве обучающей выборки известны асессорские оценки релевантности для некоторых пар запрос-документ (q,d)(q, d)(q,d). Будем обозначать множество возможных оценок Y\\\\mathbb{Y}Y, конкретную оценку — y(q,d)y(q, d)y(q,d).\\nПусть Y=R\\\\mathbb{Y} = \\\\mathbb{R}Y=R — множество действительных чисел. Тогда мы можем обучить любую модель регрессии на признаках пар (q,d)(q, d)(q,d) для предсказания оценок асессоров. Это может быть и линейная модель, и градиентный бустинг, и нейронная сеть. Обучать модель можно, например, оптимизируя MSE:\\n∑q∈Q∑d∈Dq(y^(q,d)−y(q,d))2→min\\u2061\\\\sum_{q \\\\in Q} \\\\sum_{d \\\\in D_q} (\\\\widehat{y}(q, d) - y(q, d)) ^ 2 \\\\rightarrow \\\\min \\nq∈Q∑\\u200bd∈Dq\\u200b∑\\u200b(y\\u200b(q,d)−y(q,d))2→minАналогично можно сводить задачу к классификации, если метки релевантности yyy бинарны или категориальны. Например, при шкале Y={0,1,2,3,4,5}\\\\mathbb{Y} = \\\\{0, 1, 2, 3, 4, 5\\\\}Y={0,1,2,3,4,5}. Оптимизировать в данном случае можно кросс-энтропийную функцию потерь.\\nPRank\\nВ случае классификации мы учим модель разделять классы, но никаким образом не задаём, что на метках имеется порядок. Мы не даём алгоритму никакой информации о том, что метка 4 находится между метками 5 и 3 и наоборот. Чтобы побороть эту проблему, была придумана модификация линейной модели, которая получила название PRank и была описана в статье  Pranking with ranking.\\nЕсли предположить, что метки релевантности — это целые числа от 000 до KKK, то есть Y={0,1,2,…,K}\\\\mathbb{Y} = \\\\{0, 1, 2, \\\\dots, K\\\\}Y={0,1,2,…,K}, то можно ввести пороги для значений ранжирующей функции, которые разделяют классы друг от друга.\\nВ качестве ранжирущей функции возьмём обычную линейную, то есть ⟨θ,x⟩\\\\langle\\\\theta, x\\\\rangle⟨θ,x⟩, где xxx — вектор признаков. Обозначим через b1,b2,…,bK−1b_1, b_2, \\\\dots, b_{K - 1}b1\\u200b,b2\\u200b,…,bK−1\\u200b границы классов. Они будут изменяться в процессе обучения. Также фиктивно введём bK=∞b_{K} = \\\\inftybK\\u200b=∞.\\nЧтобы предсказать класс, будем искать первую границу, которая больше вычисленной линейной функции:\\ny^(q,d)=min\\u2061{r\\xa0:\\xa0x⊤θ−br<0}\\\\widehat{y}(q, d) = \\\\min \\\\{r\\\\ :\\\\ x^\\\\top\\\\theta - b_r < 0\\\\} \\ny\\u200b(q,d)=min{r\\xa0:\\xa0x⊤θ−br\\u200b<0}Коротко опишем процесс обучения. Представим, что мы получили очередной объект и вычислили линейную функцию ⟨θ,x⟩\\\\langle\\\\theta, x\\\\rangle⟨θ,x⟩. Отметим на оси её значение и границы классов:\\n\\nЕсли предсказан ранг 1, а правильный класс для объекта 4, то необходимо, во-первых, обновить вектор θ\\\\thetaθ, а во-вторых, сдвинуть границы других классов в сторону получившегося предсказания.\\n\\nПосле осуществления сдвигов, предсказание на точке из обучающей выборки становится ближе к правильному.\\n\\nМы не будем приводить конкретные формулы для обновления обучаемых параметров; вы можете посмотреть их в статье.\\nПопарный подход\\nНачнём рассмотрение попарного подхода. Чтобы его применить, нужен датасет, состоящий из троек (q,d1,d2)(q, d_1, d_2)(q,d1\\u200b,d2\\u200b), где d1,d2∈Dqd_1, d_2\\\\in D_qd1\\u200b,d2\\u200b∈Dq\\u200b, причём известно, что по запросу qqq документ d1d_1d1\\u200b релевантнее, чем d2d_2d2\\u200b. Будем обозначать такое соотношение d2≺d1d_2 \\\\prec d_1d2\\u200b≺d1\\u200b.\\nЗамечание. Такие данные можно собирать с использованием асессорской Side-by-Side разметки, в которой асессоры отмечают, какой из двух предложенных документов релевантнее по заданному запросу. Также можно собирать данные с помощью пользовательских логов. Например, если пользователь пролистал первые 3 документа по запросу qqq, а решил свою задачу только на 4-ом, можно считать, что первые 3 документа были хуже. Пользовательских логов достаточно много, с их помощью можно обучать разные модели для ранжирования на основе кликов. Но надо помнить, что совсем не всегда те документы, на которые первоначально хочется кликнуть, релевантны и содержат необходимую информацию. Чтобы учесть это в модели, можно рассматривать только клики, после которых пользователь надолго остался на странице. Дополнительно можно смешивать модели на кликах с моделями на оценках асессоров, что поможет избежать проблемы смещения обучающей выборки в текущее ранжирование. Поясним, из-за чего может возникнуть эта проблема. Логи строятся по результатам взаимодействия пользователя с нашей ранжирующей моделью, и может оказаться так, что обучающая выборка будет состоять только из документов, уже попавших в топ выдачи.\\nКлассификатор на тройках\\nВ качестве самого простого решения снова рассмотрим сведение задачи ранжирования к уже известным задачам машинного обучения. А именно, будем решать задачу классификации троек (q,d1,d2)(q, d_1, d_2)(q,d1\\u200b,d2\\u200b). В качестве целевой переменной запишем 000, если лучше документ d1d_1d1\\u200b, и 111, если лучше d2d_2d2\\u200b. Собрав признаки для каждой тройки, можем обучить любой классификатор. Для этого можем взять и нейронную сеть, и линейную модель, и методы на основе деревьев.\\nЧтобы отсортировать документы внутри DqD_qDq\\u200b по запросу qqq, можем воспользоваться стандартным алгоритмом сортировки с компаратором, задаваемым предсказаниями нашего классификатора.\\nПроблема в том, что компаратор должен быть как минимум транзитивным. Гарантировать такое свойство для большинства моделей машинного обучения мы не можем.\\nRankingSVM\\nНо если ввести модель специальным образом, можно гарантировать транзитивность предсказаний. Снова воспользуемся линейной моделью.\\nЗадача. Найти вектор весов θ\\\\thetaθ, для которого ∀q∈Q\\xa0∀di,dj\\xa0:\\xa0di≺dj\\\\forall q \\\\in Q \\\\ \\\\forall d_i, d_j\\\\ :\\\\ d_i \\\\prec d_j∀q∈Q\\xa0∀di\\u200b,dj\\u200b\\xa0:\\xa0di\\u200b≺dj\\u200b было бы выполнено ⟨xi,θ⟩<⟨xj,θ⟩\\\\langle x_i, \\\\theta\\\\rangle < \\\\langle x_j, \\\\theta\\\\rangle⟨xi\\u200b,θ⟩<⟨xj\\u200b,θ⟩. Здесь xix_ixi\\u200b, xjx_jxj\\u200b – векторы признаков для пар (q,di)(q, d_i)(q,di\\u200b) и (q,dj)(q, d_j)(q,dj\\u200b) соответственно.\\nТак как требуется выполнение скалярных произведений вида ⟨xi−xj,θ⟩\\\\langle x_i - x_j, \\\\theta \\\\rangle⟨xi\\u200b−xj\\u200b,θ⟩, где xi−xjx_i - x_jxi\\u200b−xj\\u200b — вектор поэлементной разницы признаков, можно обучить линейную модель на парах, где признак пары – это как раз вектор xi−xjx_i - x_jxi\\u200b−xj\\u200b.\\nЕсли в качестве модели взять SVM, получится классический метод обучения ранжированию, который называется RankingSVM.\\nЧтобы сравнить 2 документа по запросу, надо сравнить ⟨xi−xj,θ⟩\\\\langle x_i - x_j, \\\\theta\\\\rangle⟨xi\\u200b−xj\\u200b,θ⟩ с нулём. Получается, что для двух одинаковых документов это выражение всегда 000. Кроме того, выполнена требуемая от компаратора транзитивность.\\nRankNet\\nПусть имеется обучающая выборка, в которой известны метки релевантности для пар запрос-документ.\\nX={xi}\\xa0–\\xa0признаковые\\xa0описания\\xa0(qi,di),Y={yi}\\xa0–\\xa0асессорские\\xa0оценки,Q={qi}\\xa0–\\xa0запросы.\\\\begin{align*}\\nX &= \\\\{x_i\\\\} \\\\ \\\\text{– признаковые описания $(q_i, d_i)$}, \\\\\\\\\\nY &= \\\\{y_i\\\\} \\\\ \\\\text{– асессорские оценки}, \\\\\\\\\\nQ &= \\\\{q_i\\\\} \\\\ \\\\text{– запросы}. \\\\\\\\\\n\\\\end{align*}\\nXYQ\\u200b={xi\\u200b}\\xa0–\\xa0признаковые\\xa0описания\\xa0(qi\\u200b,di\\u200b),={yi\\u200b}\\xa0–\\xa0асессорские\\xa0оценки,={qi\\u200b}\\xa0–\\xa0запросы.\\u200bВведём модель для ранжирующей функции y^θ(x)\\\\widehat{y}_\\\\theta(x)y\\u200bθ\\u200b(x), дифференцируемую по θ\\\\thetaθ. Например, это может быть линейная модель или нейронная сеть. Также возможно применение композиций деревьев. Ниже мы будем описывать оптимизационную процедуру для моделей, которые обучаются с помощью градиентного спуска; для деревьев потребуются другие методы.\\nБудем рассматривать события xi≻xjx_i \\\\succ x_jxi\\u200b≻xj\\u200b, то есть события вида «документ did_idi\\u200b релевантнее djd_jdj\\u200b по запросу qi=qj=qq_i = q_j = qqi\\u200b=qj\\u200b=q».\\nИз разметки асессоров мы можем задать вероятности таких событий.\\nQij={1,если\\xa0yi>yj0,если\\xa0yi<yj12,если\\xa0yi=yj Q_{ij} =\\n\\\\begin{cases}\\n    1, \\\\text{если } y_i > y_j \\\\\\\\\\n    0, \\\\text{если } y_i < y_j \\\\\\\\\\n    \\\\frac{1}{2}, \\\\text{если } y_i = y_j\\n\\\\end{cases}\\nQij\\u200b=⎩⎨⎧\\u200b1,если\\xa0yi\\u200b>yj\\u200b0,если\\xa0yi\\u200b<yj\\u200b21\\u200b,если\\xa0yi\\u200b=yj\\u200b\\u200bЕсли асессорских оценок на каждый документ несколько, можем по-разному агрегировать эти оценки в QijQ_{ij}Qij\\u200b. Если же доступна попарная разметка (какой из документов did_idi\\u200b и djd_jdj\\u200b релевантнее запросу qqq), то вероятностью можно назвать долю оценок, в которых did_idi\\u200b признан лучше, чем djd_jdj\\u200b.\\nДалее введём оценку вероятности этого события, порождённую моделью.\\nPij=11+e−σ(si−sj),\\xa0\\xa0si=y^θ(xi),\\xa0sj=y^θ(xj)P_{ij} = \\\\frac{1}{1 + e^{-\\\\sigma(s_i - s_j)}},\\\\ \\\\ s_i = \\\\widehat{y}_\\\\theta(x_i),\\\\  s_j = \\\\widehat{y}_\\\\theta(x_j)\\nPij\\u200b=1+e−σ(si\\u200b−sj\\u200b)1\\u200b,\\xa0\\xa0si\\u200b=y\\u200bθ\\u200b(xi\\u200b),\\xa0sj\\u200b=y\\u200bθ\\u200b(xj\\u200b)Для пары (xi,xj)(x_i, x_j)(xi\\u200b,xj\\u200b) рассмотрим случайную величину\\nξij=I[xi≻xj]\\\\xi_{ij} = \\\\mathbb{I}[x_i \\\\succ x_j]\\nξij\\u200b=I[xi\\u200b≻xj\\u200b]Согласно асессорам, ξij∼Bern(Qij)\\\\xi_{ij} \\\\sim Bern(Q_{ij})ξij\\u200b∼Bern(Qij\\u200b). Согласно модели, ξij∼Bern(Pij)\\\\xi_{ij} \\\\sim Bern(P_{ij})ξij\\u200b∼Bern(Pij\\u200b). Задача обучения в том, чтобы уравнять эти два распределения между собой для всех пар документов. Поэтому в качестве функции потерь будем использовать KL-дивергенцию. Введём для каждой пары лосс\\nCij=KL(Bern(Qij)\\u2009∣∣\\u2009Bern(Pij))=Qijlog\\u2061QijPij+(1−Qij)log\\u20611−Qij1−Pij=C_{ij} = \\\\text{KL}(Bern(Q_{ij})\\\\,\\\\vert\\\\vert\\\\, Bern(P_{ij})) = Q_{ij}\\\\log\\\\frac{Q_{ij}}{P_{ij}} + (1 - Q_{ij}) \\\\log\\\\frac{1 - Q_{ij}}{1 - P_{ij}} = \\nCij\\u200b=KL(Bern(Qij\\u200b)∣∣Bern(Pij\\u200b))=Qij\\u200blogPij\\u200bQij\\u200b\\u200b+(1−Qij\\u200b)log1−Pij\\u200b1−Qij\\u200b\\u200b==H(Qij)−Qijlog\\u2061Pij−(1−Qij)log\\u2061(1−Pij)= H(Q_{ij}) - Q_{ij}\\\\log P_{ij} - (1 - Q_{ij})\\\\log(1 - P_{ij}) \\n=H(Qij\\u200b)−Qij\\u200blogPij\\u200b−(1−Qij\\u200b)log(1−Pij\\u200b)Тут H(Qij)H(Q_{ij})H(Qij\\u200b) – энтропия, не зависящая от PijP_{ij}Pij\\u200b, а значит и от параметров модели θ\\\\thetaθ.\\nДля определённых выше QijQ_{ij}Qij\\u200b выполнены следующие свойства:\\nCij=log\\u2061(1+e−σ(si−sj)),если\\xa0yi>yjCij=log\\u2061(1+e−σ(sj−si)),если\\xa0yi<yjCij=log\\u20612,если\\xa0yi=yj\\xa0и\\xa0si≠sjCij=0,если\\xa0yi=yj\\xa0и\\xa0si=sj\\\\begin{align*}\\nC_{ij} &= \\\\log(1 + e^{-\\\\sigma(s_i - s_j)}), \\\\text{если $y_i > y_j$} \\\\\\\\\\nC_{ij} &= \\\\log(1 + e^{-\\\\sigma(s_j - s_i)}), \\\\text{если $y_i < y_j$} \\\\\\\\\\nC_{ij} &= \\\\log2, \\\\text{если $y_i = y_j$ и $s_i \\\\neq s_j$} \\\\\\\\\\nC_{ij} &= 0, \\\\text{если $y_i = y_j$ и $s_i = s_j$}\\n\\\\end{align*}\\nCij\\u200bCij\\u200bCij\\u200bCij\\u200b\\u200b=log(1+e−σ(si\\u200b−sj\\u200b)),если\\xa0yi\\u200b>yj\\u200b=log(1+e−σ(sj\\u200b−si\\u200b)),если\\xa0yi\\u200b<yj\\u200b=log2,если\\xa0yi\\u200b=yj\\u200b\\xa0и\\xa0si\\u200b\\ue020=sj\\u200b=0,если\\xa0yi\\u200b=yj\\u200b\\xa0и\\xa0si\\u200b=sj\\u200b\\u200bВидно, что для двух документов с разными истинными метками релевантности такой метод обучения штрафует модель за одинаковые предсказания. Это свойство очень полезно, поскольку в случае одинаковых предсказаний непонятно, в каком порядке располагать документы.\\nПолная функция потерь выглядит следующим образом:\\nQ(θ)=∑q∈Q∑i,j:qi=qj=qCij\\xa0→\\xa0min\\u2061θQ(\\\\theta) = \\\\sum_{q \\\\in Q}\\\\sum_{i, j: q_i = q_j = q} C_{ij} \\\\ \\\\rightarrow\\\\  \\\\min_{\\\\theta} \\nQ(θ)=q∈Q∑\\u200bi,j:qi\\u200b=qj\\u200b=q∑\\u200bCij\\u200b\\xa0→\\xa0θmin\\u200bМинимизировать её можно при помощи градиентного спуска или различных его модификаций.\\nθkt+1=θkt−η∑i,j:qi=qj=q∂Cij∂θk(θkt)=\\\\theta_k^{t + 1} = \\\\theta_k^t - \\\\eta \\\\sum_{i, j: q_i = q_j = q} \\\\frac{\\\\partial C_{ij}}{\\\\partial \\\\theta_k} (\\\\theta_k^t) = \\nθkt+1\\u200b=θkt\\u200b−ηi,j:qi\\u200b=qj\\u200b=q∑\\u200b∂θk\\u200b∂Cij\\u200b\\u200b(θkt\\u200b)==θkt−η∑i,j:qi=qj=q(∂Cij∂si∂si∂θk+∂Cij∂sj∂sj∂θk)= \\\\theta_k^t - \\\\eta  \\\\sum_{i, j: q_i = q_j = q}\\\\left(\\\\frac{\\\\partial C_{ij}}{\\\\partial s_i}\\\\frac{\\\\partial s_i}{\\\\partial \\\\theta_k} + \\\\frac{\\\\partial C_{ij}}{\\\\partial s_j}\\\\frac{\\\\partial s_j}{\\\\partial \\\\theta_k}\\\\right) \\n=θkt\\u200b−ηi,j:qi\\u200b=qj\\u200b=q∑\\u200b(∂si\\u200b∂Cij\\u200b\\u200b∂θk\\u200b∂si\\u200b\\u200b+∂sj\\u200b∂Cij\\u200b\\u200b∂θk\\u200b∂sj\\u200b\\u200b)Вычислим производные функции потерь по sis_isi\\u200b.\\nПусть yi>yjy_i > y_jyi\\u200b>yj\\u200b. Тогда:\\n∂Cij∂si=−σe−σ(si−sj)1+e−σ(si−sj)=−σ1+eσ(si−sj)\\\\frac{\\\\partial C_{ij}}{\\\\partial s_i} = \\\\frac{-\\\\sigma e^{-\\\\sigma(s_i - s_j)}}{1 + e^{-\\\\sigma(s_i - s_j)}} = \\\\frac{-\\\\sigma}{1 + e^{\\\\sigma(s_i - s_j)}}\\n∂si\\u200b∂Cij\\u200b\\u200b=1+e−σ(si\\u200b−sj\\u200b)−σe−σ(si\\u200b−sj\\u200b)\\u200b=1+eσ(si\\u200b−sj\\u200b)−σ\\u200b∂Cij∂sj=σe−σ(si−sj)1+e−σ(si−sj)=σ1+eσ(si−sj),\\\\frac{\\\\partial C_{ij}}{\\\\partial s_j} = \\\\frac{\\\\sigma e^{-\\\\sigma(s_i - s_j)}}{1 + e^{-\\\\sigma(s_i - s_j)}} = \\\\frac{\\\\sigma}{1 + e^{\\\\sigma(s_i - s_j)}},\\n∂sj\\u200b∂Cij\\u200b\\u200b=1+e−σ(si\\u200b−sj\\u200b)σe−σ(si\\u200b−sj\\u200b)\\u200b=1+eσ(si\\u200b−sj\\u200b)σ\\u200b,то есть ∂Cij∂si=−∂Cij∂sj\\\\frac{\\\\partial C_{ij}}{\\\\partial s_i} = -\\\\frac{\\\\partial C_{ij}}{\\\\partial s_j}∂si\\u200b∂Cij\\u200b\\u200b=−∂sj\\u200b∂Cij\\u200b\\u200b.\\nПодставим полученное выражение в производную функции потерь по весам.\\n∂Cij∂θk=∂Cij∂si∂si∂θk+∂Cij∂sj∂sj∂θk=−σ1+eσ(si−sj)⏟обозначаем\\xa0λij(∂si∂θk−∂sj∂θk)\\\\frac{\\\\partial C_{ij}}{\\\\partial \\\\theta_k} = \\\\frac{\\\\partial C_{ij}}{\\\\partial s_i}\\\\frac{\\\\partial s_i}{\\\\partial \\\\theta_k} + \\\\frac{\\\\partial C_{ij}}{\\\\partial s_j}\\\\frac{\\\\partial s_j}{\\\\partial \\\\theta_k} = \\\\underbrace{\\\\frac{-\\\\sigma}{1 + e^{\\\\sigma(s_i - s_j)}}}_\\\\text{обозначаем $\\\\lambda_{ij}$} \\\\left(\\\\frac{\\\\partial s_i}{\\\\partial \\\\theta_k} - \\\\frac{\\\\partial s_j}{\\\\partial \\\\theta_k}\\\\right)\\n∂θk\\u200b∂Cij\\u200b\\u200b=∂si\\u200b∂Cij\\u200b\\u200b∂θk\\u200b∂si\\u200b\\u200b+∂sj\\u200b∂Cij\\u200b\\u200b∂θk\\u200b∂sj\\u200b\\u200b=обозначаем\\xa0λij\\u200b1+eσ(si\\u200b−sj\\u200b)−σ\\u200b\\u200b\\u200b(∂θk\\u200b∂si\\u200b\\u200b−∂θk\\u200b∂sj\\u200b\\u200b)Аналогично получаем выражения для остальных случаев соотношения между yiy_iyi\\u200b и yjy_jyj\\u200b.\\nОпределим теперь\\nλi=∑j:\\xa0xi≻xjλij−∑j:\\xa0xi≺xjλij\\\\lambda_i = \\\\sum\\n\\\\limits_{j:\\\\ x_i \\\\succ x_j} \\\\lambda_{ij} - \\\\sum\\n\\\\limits_{j:\\\\ x_i \\\\prec x_j} \\\\lambda_{ij}λi\\u200b=j:\\xa0xi\\u200b≻xj\\u200b∑\\u200bλij\\u200b−j:\\xa0xi\\u200b≺xj\\u200b∑\\u200bλij\\u200bТогда получаем\\n∂Q∂θk=∑iλi∂si∂θk\\\\frac{\\\\partial Q}{\\\\partial \\\\theta_k} = \\\\sum\\\\limits_i \\\\lambda_i \\\\frac{\\\\partial s_i}{\\\\partial \\\\theta_k}\\n∂θk\\u200b∂Q\\u200b=i∑\\u200bλi\\u200b∂θk\\u200b∂si\\u200b\\u200bИтерацию градиентного спуска теперь можно записать в более простом виде:\\nθt+1=θt−η∑iλi∇θtsi\\\\theta^{t + 1} = \\\\theta^t - \\\\eta \\\\sum\\\\limits_i \\\\lambda_i \\\\nabla_{\\\\theta^t} s_i \\nθt+1=θt−ηi∑\\u200bλi\\u200b∇θt\\u200bsi\\u200bТаким образом, мы можем делать SGD не по парам документов, а по отдельным документам. Это увеличивает скорость сходимости.\\nПолучается, что λi\\\\lambda_{i}λi\\u200b зависит от номера документа и от попарных разностей скоров модели на документах si−sjs_i - s_jsi\\u200b−sj\\u200b, при этом не зависит от производных самих sss по параметру θ\\\\thetaθ. Введённые λi\\\\lambda_iλi\\u200b можно представить в виде стрелок, которые прикреплены к каждому документу в поисковой выдаче. Направление стрелки означает, куда мы хотим перенести документ, чтобы выросла нужная метрика, а длина – насколько сильно.\\n\\nLambdaRank\\nЗадача этого метода в том, чтобы соединить RankNet и наше желание напрямую оптимизировать введённые ранее кусочно постоянные метрики качества, например, NDCG. Обозначим через Z(q,y^)=Z(q,s)Z(q, \\\\widehat{y}) = Z(q, s)Z(q,y\\u200b)=Z(q,s) значение этой метрики для запроса qqq при ранжировании функцией y^\\\\widehat{y}y\\u200b. Попытаемся придумать гладкую попарную функцию потерь\\nQ‾=∑i,j\\u2009qi=qj=qC‾ij,\\\\overline{Q} = \\\\sum_{i, j\\\\, q_i = q_j = q}\\\\overline{C}_{ij},\\nQ\\u200b=i,jqi\\u200b=qj\\u200b=q∑\\u200bCij\\u200b,где\\nC‾ij=C‾ij(si−sj),\\\\overline{C}_{ij} = \\\\overline{C}_{ij}(s_i - s_j),\\nCij\\u200b=Cij\\u200b(si\\u200b−sj\\u200b),оптимизация которой была бы эквивалентна оптимизации ZZZ.\\nЗаметим, впрочем, что для обучения сама Q‾\\\\overline{Q}Q\\u200b нам не нужна, а нужны только производные, которые, как и в случае RankNet, можно записать в виде\\n∇θkC‾ij=λ‾ij(∇θksi−∇θksj)\\\\nabla_{\\\\theta_k}\\\\overline{C}_{ij} = \\\\overline{\\\\lambda}_{ij}(\\\\nabla_{\\\\theta_k}s_i - \\\\nabla_{\\\\theta_k}s_j)\\n∇θk\\u200b\\u200bCij\\u200b=λij\\u200b(∇θk\\u200b\\u200bsi\\u200b−∇θk\\u200b\\u200bsj\\u200b)В данном случае мы хотели бы задать λ‾ij\\\\overline{\\\\lambda}_{ij}λij\\u200b специальным образом: так, чтобы сдвиг в направлении антиградиента вёл к уменьшению метрики ZZZ. Ясно, что не любое выражение для λi\\\\lambda_iλi\\u200b может задавать градиент. Чтобы проверить существование функции потерь C‾\\\\overline{C}C, применим следующий частный случай леммы Пуанкаре:\\nЛемма. Пусть f1(θ1,…,θn),f2(θ1,…,θn),…,fn(θ1,…,θn)f_1(\\\\theta_1, \\\\dots, \\\\theta_n), f_2(\\\\theta_1, \\\\dots, \\\\theta_n), \\\\dots, f_n(\\\\theta_1, \\\\dots, \\\\theta_n)f1\\u200b(θ1\\u200b,…,θn\\u200b),f2\\u200b(θ1\\u200b,…,θn\\u200b),…,fn\\u200b(θ1\\u200b,…,θn\\u200b) – функции, такие что\\n∀i,j\\xa0\\xa0\\xa0∂fi∂θj=∂fj∂θi.\\\\forall i, j\\\\ \\\\ \\\\ \\\\frac{\\\\partial f_i}{\\\\partial \\\\theta_j} = \\\\frac{\\\\partial f_j}{\\\\partial \\\\theta_i}.\\n∀i,j\\xa0\\xa0\\xa0∂θj\\u200b∂fi\\u200b\\u200b=∂θi\\u200b∂fj\\u200b\\u200b.Тогда существует функция FFF, такая что ∀i\\xa0\\xa0∂F∂θi=fi\\\\forall i\\\\ \\\\ \\\\frac{\\\\partial F}{\\\\partial \\\\theta_i} = f_i∀i\\xa0\\xa0∂θi\\u200b∂F\\u200b=fi\\u200b.\\nЗначит, нужно ввести лямбды таким образом, чтобы совпадали смешанные производные.\\nОпределим yijy_{ij}yij\\u200b – функцию релевантности, в которой поменяли местами xix_ixi\\u200b и xjx_jxj\\u200b:\\nyij={y(xj),если\\xa0x=xiy(xi),если\\xa0x=xjy(x)\\xa0иначе y_{ij} =\\n\\\\begin{cases}\\n    y(x_j), \\\\text{если } x = x_i \\\\\\\\\\n    y(x_i), \\\\text{если } x = x_j \\\\\\\\\\n    y(x)\\\\  \\\\text{иначе}\\n\\\\end{cases}\\nyij\\u200b=⎩⎨⎧\\u200by(xj\\u200b),если\\xa0x=xi\\u200by(xi\\u200b),если\\xa0x=xj\\u200by(x)\\xa0иначе\\u200bОбозначим также через ΔZij=Z(q,y^ij)−Z(q,y^)\\\\Delta Z_{ij} = Z(q, \\\\widehat{y}_{ij}) - Z(q, \\\\widehat{y})ΔZij\\u200b=Z(q,y\\u200bij\\u200b)−Z(q,y\\u200b) приращение метрики при перестановке местами xix_ixi\\u200b и xjx_jxj\\u200b. В методе LambdaRank λij\\\\lambda_{ij}λij\\u200b определяется следующим образом:\\nλij=−σ1+eσ(si−sj)∣ΔZij∣,\\\\lambda_{ij} = \\\\frac{-\\\\sigma}{1 + e^{\\\\sigma(s_i - s_j)}} |\\\\Delta Z_{ij}|,\\nλij\\u200b=1+eσ(si\\u200b−sj\\u200b)−σ\\u200b∣ΔZij\\u200b∣,где σ\\\\sigmaσ – некоторая константа. Множитель ∣ΔZij∣\\\\vert\\\\Delta Z_{ij}\\\\vert∣ΔZij\\u200b∣ кусочно постоянен, так что не повлияет на градиент.\\nВопрос на подумать. Проверьте, что для указанных λij\\\\lambda_{ij}λij\\u200b действительно выполнено условие леммы Пуанкаре.\\nАвторы метода проверяли его для оптимизации NDCG. Они пытались случайными сдвигами параметра улучшить NDCG после оптимизации через LambdaRank. Доля успешных сдвигов оказалось очень мала, так что экспериментально подтверждается успешная оптимизация этим методом недифференцируемых метрик ранжирования.\\nLambdaMART\\nЭтот метод является конкретной реализацией подхода LambdaRank. В нём для предсказания y^θ(x)\\\\widehat{y}_{\\\\theta}(x)y\\u200bθ\\u200b(x) строится модель градиентного бустинга на решающих деревьях.\\nКаждое дерево обучается на градиент функции потерь предыдущей итерации алгоритма (и градиент мы как раз умеем считать, хотя саму функцию потерь – нет). Структура дерева определяется жадными по MSE разделениями.\\nПри этом размер шага (коэффициент, с которым берётся значение в следующем дереве) задаётся не один для всей модели, а подбирается во всех листах каждого дерева при помощи метода Ньютона.\\nБолее подробно о последних трёх методах можно почитать в оригинальной статье от Microsoft: From RankNet to LambdaRank to LambdaMART.\\nСписочный подход\\nSoftRank\\nВспомним, что основной проблемой, из-за которой невозможно оптимизировать NDCG и похожие метрики напрямую, является их кусочная линейность. Идея метода SoftRank — сгладить метрику, чтобы при небольших изменениях параметров модели она тоже изменялась. Для этого оценку релевантности документа будем рассматривать не как константу, а как случайную величину.\\nСглаживание метрики рассмотрим на примере. Пусть имеются документы d1d_1d1\\u200b, d2d_2d2\\u200b, d3d_3d3\\u200b для запроса qqq, а ранжирующая модель дала им соответственно оценки релевантности s1s_1s1\\u200b, s2s_2s2\\u200b, s3s_3s3\\u200b. Тогда, если это случайные величины, то они константны и их распределение вырождено. В связи с этим порядок документов на выдаче тоже определён однозначно, и первое место занимает документ с наибольшей оценкой.\\n\\nЧтобы поменять метрику, будем считать, что оценка релевантности документа did_idi\\u200b по нашей модели имеет распределение N(si,σ2)\\\\mathcal{N}(s_i, \\\\sigma^2)N(si\\u200b,σ2), где σ2\\\\sigma^2σ2 — гиперпараметр метода. Чтобы отсортировать документы, будем генерировать число из этого распределения и ранжировать по нему. Тогда может случиться так, что документ с самым большим sis_isi\\u200b окажется на последнем месте. Но с наибольшей вероятностью он всё равно будет первым. И распределение позиций документов на выдаче будет выглядеть примерно так:\\n\\nТеперь, чтобы получить метрику, осталось только в формуле NDCG заменить дискаунт на его математическое ожидание.\\nGsoft=1Gmax∑j=1Ng(dj)ED(j)G_{soft} = \\\\frac{1}{G_{max}} \\\\sum\\\\limits_{j = 1}^N g(d_j) \\\\mathbb{E} D(j) \\nGsoft\\u200b=Gmax\\u200b1\\u200bj=1∑N\\u200bg(dj\\u200b)ED(j)Можно заметить, что теперь небольшие сдвиги параметров будут менять распределение позиций документов, а значит и GsoftG_{soft}Gsoft\\u200b будет изменяться. Поэтому функция становится дифференцируемой, и её можно использовать как функцию потерь. При этом для того, чтобы вычислить распределение рангов, необходимо использовать отсортированный список документов. А значит, хоть напрямую оптимизируемый функционал и не зависит от перестановок, этот метод можно отнести к списочному подходу.\\nПодробнее об этом методе можно прочитать в статье.\\nListNet\\nСледующие 2 метода, которые мы рассмотрим, работают с перестановками документов для одного запроса. Если заданы оценки релевантности y(q,di)y(q, d_i)y(q,di\\u200b), i=1,…,Ni = 1, \\\\dots, Ni=1,…,N для запроса qqq, то они формируют распределение на перестановках.\\nПусть дана перестановка  π=(q1,…,qN)\\\\pi = (q_1, \\\\dots, q_N)π=(q1\\u200b,…,qN\\u200b). Определим её вероятность следующим образом:\\nP(π∣y)=∏j=1Ny(q,dj)∑k=jNy(q,dk)P(\\\\pi \\\\vert y) = \\\\prod\\\\limits_{j = 1}^N \\\\frac{y(q, d_j)}{\\\\sum\\\\limits_{k=j}^N y(q, d_k)} \\nP(π∣y)=j=1∏N\\u200bk=j∑N\\u200by(q,dk\\u200b)y(q,dj\\u200b)\\u200bТакая вероятностная модель называется моделью Люcа-Плакетта.\\nДля примера рассмотрим 3 документа и оценки yyy. Тогда вероятность перестановки ABCABCABC запишется так:\\nP(ABC∣y)=P(A\\xa0на\\xa0первом\\xa0месте)⋅P(ABC \\\\vert y) = P(A\\\\ \\\\text{на первом месте}) \\\\cdot\\nP(ABC∣y)=P(A\\xa0на\\xa0первом\\xa0месте)⋅P(B\\xa0на\\xa0втором\\xa0месте\\xa0∣\\xa0A\\xa0на\\xa0первом\\xa0месте)⋅P(B\\\\ \\\\text{на втором месте}\\\\ \\\\vert\\\\text{ A на первом месте})\\\\cdot\\nP(B\\xa0на\\xa0втором\\xa0месте\\xa0∣\\xa0A\\xa0на\\xa0первом\\xa0месте)⋅P(C\\xa0на\\xa0третьем\\xa0месте\\xa0∣A\\xa0на\\xa0первом\\xa0месте\\xa0и\\xa0B\\xa0на\\xa0втором\\xa0месте)=P(C\\\\text{ на третьем месте}\\\\ \\\\vert A\\\\ \\\\text{на первом месте и}\\\\ B\\\\ \\\\text{на втором месте})=\\nP(C\\xa0на\\xa0третьем\\xa0месте\\xa0∣A\\xa0на\\xa0первом\\xa0месте\\xa0и\\xa0B\\xa0на\\xa0втором\\xa0месте)==y(A)y(A)+y(B)+y(C)⋅y(B)y(B)+y(C)⋅y(C)y(C)= \\\\frac{y(A)}{y(A) + y(B) + y(C)} \\\\cdot \\\\frac{y(B)}{y(B) + y(C)} \\\\cdot \\\\frac{y(C)}{y(C)} \\n=y(A)+y(B)+y(C)y(A)\\u200b⋅y(B)+y(C)y(B)\\u200b⋅y(C)y(C)\\u200bАналогично методу RankNet, мы имеем распределение, которое можно сформировать из оценок асессоров yyy и из оценок модели y^\\\\widehat{y}y\\u200b. А значит можно в качестве функции потерь использовать KL-дивергенцию между этими распределениями.\\nKL(P(π∣y)\\u2009∣∣\\u2009P(π∣y^θ))→min\\u2061θ,\\\\text{KL}(P(\\\\pi | y)\\\\,\\\\vert\\\\vert\\\\, P(\\\\pi | \\\\widehat{y}_\\\\theta)) \\\\rightarrow \\\\min\\\\limits_{\\\\theta},\\nKL(P(π∣y)∣∣P(π∣y\\u200bθ\\u200b))→θmin\\u200b,где θ\\\\thetaθ — параметр модели.\\nОднако различных перестановок получается N!N!N!, а значит, даже чтобы просто вычислить дивергенцию Кульбака-Лейблера, потребуется немало времени, не говоря уже про оптимизацию этой функции потерь.\\nПоэтому вместо того, чтобы рассматривать вероятность полной перестановки, смотрят на распределение индекса первого документа на выдаче.\\nPy^(j)=y^(q,dj)∑d∈Dqy^(q,d)P_{\\\\widehat{y}}(j) = \\\\frac{\\\\widehat{y}(q, d_j)}{\\\\sum\\\\limits_{d \\\\in D_q} \\\\widehat{y}(q, d)} \\nPy\\u200b\\u200b(j)=d∈Dq\\u200b∑\\u200by\\u200b(q,d)y\\u200b(q,dj\\u200b)\\u200bPy(j)=y(q,dj)∑d∈Dqy(q,d)P_{y}(j) = \\\\frac{y(q, d_j)}{\\\\sum\\\\limits_{d \\\\in D_q} y(q, d)} \\nPy\\u200b(j)=d∈Dq\\u200b∑\\u200by(q,d)y(q,dj\\u200b)\\u200bОптимизируют KL-дивергенцию между этими распределениями:\\nKL(Py\\u2009∣∣\\u2009Py^)→min\\u2061θ\\\\text{KL}(P_y\\\\,\\\\vert\\\\vert\\\\, P_{\\\\widehat{y}}) \\\\rightarrow \\\\min\\\\limits_{\\\\theta} \\nKL(Py\\u200b∣∣Py\\u200b\\u200b)→θmin\\u200bListMLE\\nВ этом методе рассматривают вероятность одной «правильной» перестановки. В данном случае под правильной перестановкой πq\\\\pi_qπq\\u200b понимается та, которая получается, если упорядочить документы из DqD_qDq\\u200b по убыванию асессорских оценок yyy. Логично, что хорошая модель должна давать большую вероятность такой перестановке, поэтому именно она и максимизируется.\\nP(πq∣y^θ)→max\\u2061θP(\\\\pi_q | \\\\widehat{y}_\\\\theta) \\\\rightarrow \\\\max\\\\limits_\\\\theta\\nP(πq\\u200b∣y\\u200bθ\\u200b)→θmax\\u200bВсё сводится к поиску оценки максимального правдоподобия, поэтому метод и называется ListMLE.\\nПрактические советы\\nПопулярные признаки\\nВыше показано, как можно построить модель, если уже известны признаки для пар запрос-документ (q,d)(q, d)(q,d), а также, как можно собрать целевые переменные для обучения. Но какие стоит взять признаки, чтобы получить хорошую модель?\\nПризнаки для моделей ранжирования можно разделить на 3 типа: запросные, документные и запросно-документные. Первые зависят только от запросы, вторые только от документа, а третьи от всей пары, то есть для их вычисления необходимо сопоставить запрос и документ.\\nКонечно, наибольший интерес представляют запросно-документные факторы. Но и другие группы факторов могут быть полезны.\\nЗапросными являются, например, следующие факторы:\\n\\nКоличество слов в запросе\\nЯзык запроса\\nСтрана, из которой задали запрос\\nЗначение классификаторов\\n\\nP(запрос\\xa0про\\xa0машинное\\xa0обучение)P(\\\\text{запрос про машинное обучение})P(запрос\\xa0про\\xa0машинное\\xa0обучение)\\nP(запрос\\xa0пиратский)P(\\\\text{запрос пиратский})P(запрос\\xa0пиратский)\\n\\n\\n\\nМногие модели могут подстроиться под эти факторы и отдельно обучиться под различные значения запросных признаков. Например, так модель может по-разному реагировать на запросы из разных стран.\\nTF-IDF\\nЧтобы сопоставить запрос и документ, можно использовать TF-IDF слов запроса. Например, можно просуммировать его по всем словам из запроса и получить фактор для ранжирования. Подробно о том, как считать TF-IDF, можно прочитать в главе про NLP.\\nDSSM и другие нейросетевые факторы\\nЗапросно-документные факторы можно получать, «соединяя» векторные представления запроса и документа. Классическим способом такого соединения является простой подсчёт косинуса угла между векторами.\\nЕсли обучить модель, которая для пар, где документ релевантен запросу, выдаёт вектора, похожие на сонаправленные, то скалярное произведение становится оценкой релевантности. Если оно близко к единице, векторы сонаправлены, а значит документ подходит запросу. При этом обычно нормируют векторы на выходе, чтобы косинус и скалярное произведение были одним и тем же.\\nВ качестве модели эмбеддинга можно использовать полносвязную нейронную сеть. На вход такой сети можно подать BagOfWords вектор или же вектор TfIdf. Эти векторы большой размерности нейросеть преобразует в меньшие. Обычно размер выходного слоя выбирают, балансируя между качеством и ресурсами, необходимыми на расчёт и хранение выхода сети.\\nЭтот подход был назван Deep Structured Semantic Models и описан в статье от Microsoft. Идею можно применять в целом в любой задаче ранжирования для своих типов «документов».\\nПосмотреть, как она была воплощена в Web поиске для поиска по смыслу, можно в блоге Яндекса. Если вкратце, у модели следующие особенности:\\n\\nНа входе у модели не все тексты, а только заголовок документа и запрос;\\nДля уменьшения размера входа текст разбит на буквенные триграммы, вектор на входе - это Bag Of Trigrams;\\nАрхитектура обработки запроса и документа разная;\\nОсобый способ генерации негативных примеров.\\n\\nСхематически архитектура модели показана на рисунке ниже:\\n\\nКонечно, чтобы заставить простую архитектуру давать хорошее качество, нужно экспериментировать с методами сбора данных и улучшения качества, которые описаны в других главах учебника. Однако при наличии соответствующих мощностей можно улучшать качество, изменяя архитектуру обработки текста. В частности, модели на основе трансформеров (например, BERT) улучшают качество. Это же касается и косинуса, то есть соединительной части. Вполне можно вместо него использовать полносвязную сеть или даже трансформерную архитектуру.\\nТрансформеры, которые по-отдельности обрабатывают сущности запроса и документа, в Яндексе названы split-моделями и более подробно описаны в том же блоге на Хабре.\\nМетафичи\\nВ предыдущем пункте мы уже ввели фактор для модели, который сам является значением модели. Мы улучшили качество с помощью стекинга DSSM и итоговой модели. Аналогичным образом можно использовать предсказания моделей, обученных на разные таргеты и разными способами: их можно добавить к другим фичам для итоговой модели.\\nК сожалению, если факторов-моделей (метафичей) много, такая модель не будет удовлетворять требованиям по времени работы. В этом случае можно прибегнуть к дистилляции знаний большой модели в более компактную.\\nФичи, зависящие от времени\\nИзвестно, что модель машинного обучения работает хорошо (а точнее, ожидаемо) только в случае, когда при её применении распределения данных и факторов похожи на те, которые использовались при обучении. Но в продакшн-системах постоянное выполнение этого свойства невозможно обеспечить.\\nПоэтому главный совет: настраивайте мониторинги качества ваших ML-систем для того, чтобы не пропускать моменты поломки. Качество может снизиться как из-за изменений в логике других сервисов, на которые вы полагаетесь при вычислении факторов, так и из-за появления новых трендов. Например, это происходит при появлении новых тем, о которых раньше не было запросов. Показательный случай — пандемия вируса COVID-19, который стал резко появляться среди запросов пользователей.\\nНо бывают факторы, которые зависят от времени сами по себе. Так, для ранжирования новых документов может быть полезно знать возраст документа. Со временем распределение этого фактора сдвигается вправо, поскольку многие старые документы не удаляются из базы. Получается, фактор старого документа меняется, а релевантность нет. Появление фичей вне ожидаемых значений может привести к непредсказуемому поведению модели. Так что их нужно применять с осторожностью, лучше в тех моделях, которые можно быстро обучить и обновить в продакшне. Ещё лучшим решением будет преобразовать или отнормировать фичи таким образом, чтобы их распределение не менялось так кардинально. Наш фактор с возрастом документа можно преобразовать в индикатор того, что возраст документа меньше одного дня. Тогда только один раз за историю документа этот фактор будет изменён, при этом распределение этого фактора в документах изо дня в день будет похожим.\\nМногостадийное ранжирование\\nПредставьте, что вы смогли обучить сложную модель, применив все описанные выше подходы: добавили метафичи, которые сами по себе являются формулами, обучили DSSM, BERT, или даже более тяжёлую нейросеть.\\nНаконец, вам пришёл запрос от пользователя, и вы намерены отсортировать все документы в базе по оценке релевантности, которую даёт ваша модель. Если вы ранжируете 1000, 10000 или даже 100000 документов, это ещё может получиться, и пользователь дождётся ответа. Но что делать, если в вашей базе миллионы, а то и миллиарды документов?\\nКонечно же вам на помощь могут прийти распределённые системы, и разбив документы по разным инстансам сервиса, который рассчитывает прогноз, вы ускорите получение ответа. Но даже с учётом распределённых вычислений быстро вычислить BERT миллиард раз будет либо очень дорого, либо очень долго. Поэтому применяется подход многостадийного ранжирования.\\nТяжёлые модели применяются не сразу. Сначала можно ограничиться применением самых простых оценок релевантности. Как пример, можно просто взять TF-IDF или BM25. Простой моделью отсекаются самые нерелевантные документы, а прошедшие дальше уже сортируются с помощью более ресурсоёмкой и продвинутой модели.\\nВ зависимости от количества документов в базе вы можете соединить столько уровней, сколько нужно, для получения приемлемого времени ответа. Конечно, количество параметров такой системы возрастает в несколько раз, но это делает возможным быстрое взаимодействие с пользователем.\\nГотовые решения\\nЕсли вы разрабатываете продукт, для которого требуется поиск по текстовым документам, для начала вы можете воспользоваться готовыми решениями.\\nОдним из самых популярных сервисов для поиска является Sphinx. Этот сервис позволяет индексировать текстовые документы и сохранять их в базу данных (как SQL, так и NoSQL). Через специальный SQL-подобный язык запросов он позволяет получать списки релевантных документов и сопутствующие им данные. Таким образом можно доставать только документы, подходящие по заданным фильтрам, отсортированные по релевантности. Это может быть полезно, например, для реализации поиска по интернет-магазину.\\nБолее того, получив некоторый топ выдачи, вы можете переранжировать его, сразу используя сложную модель или учитывая другие потребности ваших пользователей.\\nДругие альтернативы для текстового поиска можно посмотреть в статье.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф10.4. Модели вида ARIMAСледующий параграф11.1. Обучение с подкреплениемЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_53.html', 'title': 'Ландшафт функции потерь'}, page_content=\"Ландшафт функции потерьЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/613.1.Введение в теорию глубокого обучения13.2.Обобщающая способность – классическая теория13.3.PAC-байесовские оценки риска13.4.Сети бесконечной ширины13.5.Ландшафт функции потерьВсе минимумы достаточно широкой нелинейной сети глобальныОбобщения13.6.Implicit bias14.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Ландшафт функции потерь13.5. Ландшафт функции потерьАвторыГоликов ЕвгенийЗадача обучения параметрической модели fθf_\\\\thetafθ\\u200b ставится как задача минимизации эмпирического риска\\nR^m(θ)=Ex,y∈Smr(y,fθ(x)),\\\\hat R_m(\\\\theta) = \\\\mathbb{E}_{x,y \\\\in S_m} r(y, f_\\\\theta(x)),\\nR^m\\u200b(θ)=Ex,y∈Sm\\u200b\\u200br(y,fθ\\u200b(x)),где SmS_mSm\\u200b – выборка размера mmm, а rrr – функция риска, например, r(y,y^)=I[y≠y^]r(y, \\\\hat{y}) = \\\\mathbb{I}[y \\\\neq \\\\hat y]r(y,y^\\u200b)=I[y\\ue020=y^\\u200b]. Часто интересующая нас функция риска не дифференцируема по второму аргументу, что делает градиентную оптимизацию неприменимой. По этой причине вместо исходной функции риска rrr вводят её дифференцируемый выпуклый суррогат, то есть некоторую выпуклую и дифференцируемую по второму аргументу функцию ℓ≥r\\\\ell \\\\geq rℓ≥r. Новый функционал эмпирического риска имеет вид\\nL^m(θ)=Ex,y∈Smℓ(y,fθ(x)).\\\\hat{\\\\mathcal{L}}_m(\\\\theta) = \\\\mathbb{E}_{x,y \\\\in S_m} \\\\ell(y, f_\\\\theta(x)).\\nL^m\\u200b(θ)=Ex,y∈Sm\\u200b\\u200bℓ(y,fθ\\u200b(x)).Если fθf_\\\\thetafθ\\u200b дифференцируема по θ\\\\thetaθ, то из дифференцируемости ℓ\\\\ellℓ следует дифференцируемость L^m\\\\hat{\\\\mathcal{L}}_mL^m\\u200b, что делает возможной градиентную оптимизацию. А если fθf_\\\\thetafθ\\u200b выпукла по θ\\\\thetaθ, то из выпуклости ℓ\\\\ellℓ следует выпуклость L^m\\\\hat{\\\\mathcal{L}}_mL^m\\u200b, что даёт гарантии на сходимость градиентного спуска в глобальный минимум.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nУвы, в общем случае нейронные сети не выпуклы как функции своих весов. Это можно увидеть на простом примере. Пусть fθ(x)=uvxf_\\\\theta(x) = u v xfθ\\u200b(x)=uvx, где uuu, vvv и xxx – скаляры, а θ=(u,v)\\\\theta = (u,v)θ=(u,v). Гессиан fff как функции θ\\\\thetaθ в любой точке равен (0\\u2005\\u200axx\\u2005\\u200a0)\\\\binom{0 \\\\; x}{x \\\\; 0}(x00x\\u200b); его собственные числа равны xxx и (−x)(-x)(−x), что и означает, что для любого ненулевого xxx функция fff не выпукла.\\nТаким образом, даже для выпуклой ℓ\\\\ellℓ функция потерь L^m\\\\hat{\\\\mathcal{L}}_mL^m\\u200b нейронной сети не обязана быть выпуклой функцией весов.\\nУ невыпуклых функций могут быть минимумы, не являющиеся глобальными, в которых может «застревать» градиентный спуск. Тем не менее, на практике часто оказывается, что градиентный спуск всегда находит точку со сколь угодно близким к глобальному минимуму значением функции потерь.\\nЭто наблюдение приводит к гипотезе, что, хотя поверхность функции потерь не обязана быть выпуклой, все её минимумы глобальны для используемых нами сетей и тех наборов данных, на которых мы их обучаем.\\nИзвестны два случая, для которых эту гипотезу удаётся доказать. Первый – это линейные сети. Второй – это достаточно широкие нелинейные сети (ширина одного из слоёв не меньше числа примеров в выборке). К сожалению, оба примера нереалистичны: выразительная способность линейных сетей не выше, чем у обыкновенной линейной модели, а ширина реальных нейронных сетей не настолько велика (порядка 10310^3103 нейронов против 10610^6106 примеров в ImageNet), причём улучшить оценку на ширину в общем случае невозможно, см. Q. Nguyen A note on connectivity of sublevel sets in deep learning. Возможно, для получения лучших оценок исследователям предстоит научиться учитывать структуру данных обучающей выборки.\\nИсторически первое доказательство глобальности всех локальных минимумов линейной сети содержится в работе Deep learning without poor local minima. Более простое доказательство в немного более общем случае можно найти в работе Depth creates no bad local minima. Ещё более простое доказательство есть в работе Deep linear networks with arbitrary loss: All local minima are global, но оно подходит только для сетей без боттлнеков (nl≥min\\u2061(n0,nL+1)n_l \\\\geq \\\\min(n_0,n_{L+1})nl\\u200b≥min(n0\\u200b,nL+1\\u200b) ∀l∈1,…,L\\\\forall l \\\\in 1,\\\\ldots,L∀l∈1,…,L). Последнее подробно разобрано в конспекте лекций автора этого параграфа.\\nЗдесь мы разберём только второй случай (достаточно широкие нелинейные сети) как потенциально более перспективный.\\nВсе минимумы достаточно широкой нелинейной сети глобальны\\nРассмотрим нейронную сеть с одним скрытым слоем:\\nf(x)=W1x1(x)∈Rn2,x1(x)=ϕ(h1(x))∈Rn1,h1(x)=W0x∈Rn1,x∈Rn0,f(x) = W_1 x_1(x) \\\\in \\\\mathbb{R}^{n_2},\\n\\\\quad\\nx_1(x) = \\\\phi(h_1(x)) \\\\in \\\\mathbb{R}^{n_1},\\n\\\\quad\\nh_1(x) = W_0 x \\\\in \\\\mathbb{R}^{n_1},\\n\\\\quad\\nx \\\\in \\\\mathbb{R}^{n_0},\\nf(x)=W1\\u200bx1\\u200b(x)∈Rn2\\u200b,x1\\u200b(x)=ϕ(h1\\u200b(x))∈Rn1\\u200b,h1\\u200b(x)=W0\\u200bx∈Rn1\\u200b,x∈Rn0\\u200b,где функция активации ϕ\\\\phiϕ применяется поэлементно. Рассмотрим набор данных (X,Y)(X,Y)(X,Y) размера mmm, где X∈Rn0×mX \\\\in \\\\mathbb{R}^{n_0 \\\\times m}X∈Rn0\\u200b×m, а Y∈Rn2×mY \\\\in \\\\mathbb{R}^{n_2 \\\\times m}Y∈Rn2\\u200b×m. Применяя соотношения выше к этому набору, получим следующие значения выходов слоёв:\\nY^=W1X1∈Rn2×m,X1=ϕ(H1)∈Rn1×m,H1=W0X∈Rn1×m,X∈Rn0×m.\\\\hat Y = W_1 X_1 \\\\in \\\\mathbb{R}^{n_2 \\\\times m},\\n\\\\quad\\nX_1 = \\\\phi(H_1) \\\\in \\\\mathbb{R}^{n_1 \\\\times m},\\n\\\\quad\\nH_1 = W_0 X \\\\in \\\\mathbb{R}^{n_1 \\\\times m},\\n\\\\quad\\nX \\\\in \\\\mathbb{R}^{n_0 \\\\times m}.\\nY^=W1\\u200bX1\\u200b∈Rn2\\u200b×m,X1\\u200b=ϕ(H1\\u200b)∈Rn1\\u200b×m,H1\\u200b=W0\\u200bX∈Rn1\\u200b×m,X∈Rn0\\u200b×m.Поставим задачу оптимизации квадратичной функции потерь\\nL(W0:1)=∣∣Y−Y^∣∣F2→min\\u2061W0:1,\\\\mathcal{L}(W_{0:1}) = \\\\vert\\\\vert Y - \\\\hat Y \\\\vert\\\\vert_F^2 \\\\to \\\\min_{W_{0:1}},\\nL(W0:1\\u200b)=∣∣Y−Y^∣∣F2\\u200b→W0:1\\u200bmin\\u200b,где ∣∣⋅∣∣F\\\\vert\\\\vert\\\\cdot\\\\vert\\\\vert_F∣∣⋅∣∣F\\u200b – норма Фробениуса.\\nТеорема 1 (On the local minima free condition of backpropagation learning) Если ϕ\\\\phiϕ аналитична, ограничена и не тождественно равна нулю, ширина скрытого слоя n1n_1n1\\u200b не меньше mmm и все столбцы матрицы XXX различны, то все локальные минимумы L(W0:1)\\\\mathcal{L}(W_{0:1})L(W0:1\\u200b) глобальны.\\nДоказательство. Пусть W0:1∗W_{0:1}^*W0:1∗\\u200b – локальный минимум L(W0:1)\\\\mathcal{L}(W_{0:1})L(W0:1\\u200b), и пусть H1∗H_1^*H1∗\\u200b, X1∗X_1^*X1∗\\u200b – соответствующие ему скрытые представления. Тогда W1∗W_1^*W1∗\\u200b – локальный минимум LW0∗(W1)=∣∣Y−W1X1∗∣∣F2\\\\mathcal{L}_{W_0^*}(W_1) = \\\\vert\\\\vert Y - W_1 X_1^* \\\\vert\\\\vert_F^2LW0∗\\u200b\\u200b(W1\\u200b)=∣∣Y−W1\\u200bX1∗\\u200b∣∣F2\\u200b.\\nЗадача оптимизации LW0∗(W1)\\\\mathcal{L}_{W_0^*}(W_1)LW0∗\\u200b\\u200b(W1\\u200b) выпуклая, поэтому W1∗W_1^*W1∗\\u200b – глобальный минимум LW0∗(W1)\\\\mathcal{L}_{W_0^*}(W_1)LW0∗\\u200b\\u200b(W1\\u200b).\\nЕсли rkX1∗=m\\\\text{rk} X_1^* = mrkX1∗\\u200b=m, то система Yi=W1,iX1∗Y_i = W_{1,i} X_1^*Yi\\u200b=W1,i\\u200bX1∗\\u200b, где W1,iW_{1,i}W1,i\\u200b – неизвестная матрица, гарантировано имеет решение для каждого i∈1…,n2i \\\\in 1\\\\ldots,n_2i∈1…,n2\\u200b. Следовательно,\\nL(W0:1∗)=LW0∗(W1∗)=min\\u2061LW0∗(W1)=0,\\\\mathcal{L}(W_{0:1}^*) = \\\\mathcal{L}_{W_0^*}(W_1^*) = \\\\min \\\\mathcal{L}_{W_0^*}(W_1) = 0,\\nL(W0:1∗\\u200b)=LW0∗\\u200b\\u200b(W1∗\\u200b)=minLW0∗\\u200b\\u200b(W1\\u200b)=0,а значит, W0:1∗W_{0:1}^*W0:1∗\\u200b – глобальный минимум L(W0:1)\\\\mathcal{L}(W_{0:1})L(W0:1\\u200b).\\nЗаметим, что для выполнения равенства rkX1∗=m\\\\text{rk} X_1^* = mrkX1∗\\u200b=m необходимо n1≥mn_1 \\\\geq mn1\\u200b≥m.\\nПусть теперь rkX1∗<m\\\\text{rk} X_1^* < mrkX1∗\\u200b<m. Если тем не менее min\\u2061LW0∗(W1∗)=0\\\\min \\\\mathcal{L}_{W_0^*}(W_1^*) = 0minLW0∗\\u200b\\u200b(W1∗\\u200b)=0, то W0:1∗W_{0:1}^*W0:1∗\\u200b – по-прежнему глобальный минимум L(W0:1)\\\\mathcal{L}(W_{0:1})L(W0:1\\u200b). Пусть\\nL(W0,1∗)=LW0∗(W1∗)=min\\u2061LW0∗(W1)>0.\\\\mathcal{L}(W_{0,1}^*) = \\\\mathcal{L}_{W_0^*}(W_1^*) = \\\\min \\\\mathcal{L}_{W_0^*}(W_1) > 0.\\nL(W0,1∗\\u200b)=LW0∗\\u200b\\u200b(W1∗\\u200b)=minLW0∗\\u200b\\u200b(W1\\u200b)>0.Докажем, что W0,1∗W_{0,1}^*W0,1∗\\u200b не может быть локальным минимумом L\\\\mathcal{L}L до тех пор, пока выполнены условия следующей леммы, которую мы докажем позже:\\nЛемма 1. Если n1≥mn_1 \\\\geq mn1\\u200b≥m, функция ϕ\\\\phiϕ аналитична, ограничена и не тождественно равна нулю, а все столбцы матрицы XXX различны, то лебегова мера множества {W0∈Rn1×n0:\\u2005\\u200arkX1<m}\\\\{W_0 \\\\in \\\\mathbb{R}^{n_1 \\\\times n_0}: \\\\; \\\\text{rk} X_1 < m\\\\}{W0\\u200b∈Rn1\\u200b×n0\\u200b:rkX1\\u200b<m} равна нулю.\\nТак как L(W0,1∗)>0\\\\mathcal{L}(W_{0,1}^*) > 0L(W0,1∗\\u200b)>0 и L\\\\mathcal{L}L – непрерывна как функция от W0,1W_{0,1}W0,1\\u200b, существует ϵ>0\\\\epsilon > 0ϵ>0, для которого\\n∀W0,1∈Bϵ(W0,1∗):L(W0,1)>0,\\\\forall W_{0,1} \\\\in B_\\\\epsilon(W_{0,1}^*):\\\\quad\\\\mathcal{L}(W_{0,1}) > 0,\\n∀W0,1\\u200b∈Bϵ\\u200b(W0,1∗\\u200b):L(W0,1\\u200b)>0,где через Bϵ(W0,1∗)B_\\\\epsilon(W_{0,1}^*)Bϵ\\u200b(W0,1∗\\u200b) мы обозначили ϵ\\\\epsilonϵ-окрестность точки W0,1∗W_{0,1}^*W0,1∗\\u200b в пространстве весов.\\nИз леммы 1 следует, что для любого δ>0\\\\delta > 0δ>0 найдётся W0′∈Bδ(W0∗)W_0' \\\\in B_\\\\delta(W_0^*)W0′\\u200b∈Bδ\\u200b(W0∗\\u200b), для которого rkX1′=m\\\\text{rk} X_1' = mrkX1′\\u200b=m. Возьмём δ∈(0,ϵ)\\\\delta \\\\in (0,\\\\epsilon)δ∈(0,ϵ). Для соответствующего W0′W_0'W0′\\u200b имеем L(W0′,W1∗)>0\\\\mathcal{L}(W_0',W_1^*) > 0L(W0′\\u200b,W1∗\\u200b)>0; при этом rkX1′=m\\\\text{rk} X_1' = mrkX1′\\u200b=m. Как было отмечено выше, задача минимизации LW0′(W1)\\\\mathcal{L}_{W_0'}(W_1)LW0′\\u200b\\u200b(W1\\u200b) выпуклая, и оптимум её равен нулю, так как rkX1′=m\\\\text{rk} X_1' = mrkX1′\\u200b=m. Поэтому градиентный спуск, применённый к LW0′\\\\mathcal{L}_{W_0'}LW0′\\u200b\\u200b и стартующий в W1∗W_1^*W1∗\\u200b, сойдётся в некоторую точку W1∗,′W_1^{*,\\\\prime}W1∗,′\\u200b, для которой LW0′(W1∗,′)=0\\\\mathcal{L}_{W_0'}(W_1^{*,\\\\prime}) = 0LW0′\\u200b\\u200b(W1∗,′\\u200b)=0.\\nМы знаем, что в нашей эпсилон-окрестности функция потерь положительна, значит, найденная точка находится вне её: (W0′,W1∗,′)∉Bϵ(W0,1∗)(W_0',W_1^{*,\\\\prime}) \\\\notin B_\\\\epsilon(W_{0,1}^*)(W0′\\u200b,W1∗,′\\u200b)∈/Bϵ\\u200b(W0,1∗\\u200b).\\nТаким образом, найдётся ϵ>0\\\\epsilon > 0ϵ>0 такое, что для любых δ∈(0,ϵ)\\\\delta \\\\in (0,\\\\epsilon)δ∈(0,ϵ) существует пара (W0′,W1∗)∈Bδ(W0,1∗)(W_0',W_1^*) \\\\in B_\\\\delta(W_{0,1}^*)(W0′\\u200b,W1∗\\u200b)∈Bδ\\u200b(W0,1∗\\u200b) такая, что градиентный спуск, примененный к L\\\\mathcal{L}L, стартующий в (W0′,W1∗)(W_0',W_1^*)(W0′\\u200b,W1∗\\u200b) и действующий только на W1W_1W1\\u200b, сходится в точку (W0′,W1∗,′)∉Bϵ(W0,1∗)(W_0',W_1^{*,\\\\prime}) \\\\notin B_\\\\epsilon(W_{0,1}^*)(W0′\\u200b,W1∗,′\\u200b)∈/Bϵ\\u200b(W0,1∗\\u200b).\\nОчевидно, что если «для любых δ∈(0,ϵ)\\\\delta \\\\in (0,\\\\epsilon)δ∈(0,ϵ)» заменить на «для любых δ>0\\\\delta > 0δ>0», утверждение выше останется верным. Это означает, что динамика градиентного спуска, действующего только на W1W_1W1\\u200b, не устойчива по Ляпунову в точке W0,1∗W_{0,1}^*W0,1∗\\u200b. Следовательно, W0,1∗W_{0,1}^*W0,1∗\\u200b не может быть точкой минимума (иначе градиентный спуск был бы устойчив), а значит, условие L(W0,1∗)>0\\\\mathcal{L}(W_{0,1}^*) > 0L(W0,1∗\\u200b)>0 невыполнимо в условиях леммы 1. Таким образом, все локальные минимумы L\\\\mathcal{L}L глобальны. Теорема 1 доказана.\\nДоказательство леммы 1. Пусть ImI_mIm\\u200b – наборов индексов из 1,…,n11,\\\\ldots,n_11,…,n1\\u200b длины mmm. Рассмотрим X1,Im∈Rm×mX_{1,I_m} \\\\in \\\\mathbb{R}^{m \\\\times m}X1,Im\\u200b\\u200b∈Rm×m – подматрицу матрицы X1X_1X1\\u200b, состоящую из строк X1X_1X1\\u200b, проиндексированных набором ImI_mIm\\u200b. В терминах ImI_mIm\\u200b условие rkX1<m\\\\text{rk} X_1 < mrkX1\\u200b<m эквивалентно det\\u2061X1,Im=0\\\\det X_{1,I_m} = 0detX1,Im\\u200b\\u200b=0 ∀Im\\\\forall I_m∀Im\\u200b.\\nТак как ϕ\\\\phiϕ аналитична, а определитель – аналитическая функция элементов матрицы, det\\u2061X1,Im\\\\det X_{1,I_m}detX1,Im\\u200b\\u200b – аналитическая функция от W0W_0W0\\u200b для любого ImI_mIm\\u200b.\\nНам понадобится следующая лемма, доказательство которой вы можете найти в The loss surface of deep and wide neural networks (лемма 4.3):\\nЛемма 2. В условиях леммы 1, найдётся W0W_0W0\\u200b, для которого rkX1=m\\\\text{rk} X_1 = mrkX1\\u200b=m.\\nИз леммы 2 и эквивалентности выше следует, что найдётся W0W_0W0\\u200b, такой что для некоторого ImI_mIm\\u200b имеет место неравенство det\\u2061X1,Im≠0\\\\det X_{1,I_m} \\\\neq 0detX1,Im\\u200b\\u200b\\ue020=0. Так как определитель X1,mX_{1,m}X1,m\\u200b – аналитическая функция W0W_0W0\\u200b, а всякая не тождественно нулевая аналитическая функция принимает значение ноль лишь на множестве меры ноль по Лебегу, то лебегова мера множества {W0:\\u2005\\u200adet\\u2061X1,Im=0}\\\\{W_0: \\\\; \\\\det X_{1,I_m} = 0\\\\}{W0\\u200b:detX1,Im\\u200b\\u200b=0} равна нулю. Таким образом, лемма 1 доказана.\\nОбобщения\\nПри доказательстве теоремы 1 мы воспользовались следующими условиями:\\n\\nВсе обучающие примеры (столбцы матрицы XXX) различны;\\nЧисло скрытых слоёв LLL равно одному;\\nШирина (последнего) скрытого слоя не меньше числа примеров: nL≥mn_L \\\\geq mnL\\u200b≥m;\\nФункция активации ϕ\\\\phiϕ аналитична, ограничена и не тождественно равна нулю;\\nФункция ошибки квадратична.\\n\\nМожем ли мы ослабить какие-то из них?\\nЕсли какие-то из примеров совпадают и соответствующие метки также одинаковы, теорема обобщается тривиально. Если же метки не совпадают, то нулевая ошибка, вообще говоря, недостижима. Тем не менее, доказательство меняется по большому счёту лишь в том, что вместо mmm будет фигурировать число различных примеров.\\nРассмотрим сеть с LLL скрытыми слоями, действующую на набор данных X0X_0X0\\u200b размера mmm:\\nY^=WLXL∈RnL+1×m,Xl=ϕ(Hl)∈Rnl×m,Hl=Wl−1Xl−1∈Rnl×m∀l∈[L],X0∈Rn0×m.\\\\hat Y = W_L X_L \\\\in \\\\mathbb{R}^{n_{L+1} \\\\times m},\\n\\\\quad\\nX_l = \\\\phi(H_l) \\\\in \\\\mathbb{R}^{n_l \\\\times m},\\n\\\\quad\\nH_l = W_{l-1} X_{l-1} \\\\in \\\\mathbb{R}^{n_l \\\\times m} \\\\quad \\\\forall l \\\\in [L],\\n\\\\quad\\nX_0 \\\\in \\\\mathbb{R}^{n_0 \\\\times m}.\\nY^=WL\\u200bXL\\u200b∈RnL+1\\u200b×m,Xl\\u200b=ϕ(Hl\\u200b)∈Rnl\\u200b×m,Hl\\u200b=Wl−1\\u200bXl−1\\u200b∈Rnl\\u200b×m∀l∈[L],X0\\u200b∈Rn0\\u200b×m.Для обобщения теоремы 1 на глубокие сети с широким последним скрытым слоем, достаточно обобщить лемму 1. Например, можно воспользоваться следующим результатом (лемма 4.4 из The loss surface of deep and wide neural networks)\\nЛемма 3. Пусть ϕ\\\\phiϕ аналитична, ограничена и не тождественно равна нулю, и пусть l∈1,…,Ll \\\\in 1,\\\\ldots,Ll∈1,…,L. Тогда если nl≥mn_l \\\\geq mnl\\u200b≥m и все строки матрицы X0X_0X0\\u200b различны, то лебегова мера множества {W0:l−1:\\u2005\\u200arkXl<m}\\\\{W_{0:l-1}: \\\\; \\\\text{rk} X_l < m\\\\}{W0:l−1\\u200b:rkXl\\u200b<m} равна нулю.\\nТретье предположение можно попытаться ослабить с двух сторон.\\nВо-первых, можно требовать меньшего числа нейронов в скрытом слое. В общем случае этот подход не работает: в статье A note on connectivity of sublevel sets in deep learning доказывается, что mmm – это наименьшая ширина, при которой теорема выполняется для набора данных общего вида с различными примерами. Тем не менее, для реальных нейронных сетей градиентный спуск нередко находит глобальный минимум, хотя их ширина часто гораздо меньше размера набора данных, на которых они обучаются. Возможно, оценки на минимальную ширину удастся улучшить, если учесть структуру данных: например, если все примеры разбиваются на подмножества с элементами, находящимися близко друг к другу и имеющими одинаковые метки.\\nВо-вторых, можно предположить, что самым широким является не последний скрытый слой, а один из промежуточных: nl≥mn_l \\\\geq mnl\\u200b≥m для некоторого l<Ll < Ll<L. Но тогда задача LW0:l−1∗(Wl:L)\\\\mathcal{L}_{W_{0:l-1}^*}(W_{l:L})LW0:l−1∗\\u200b\\u200b(Wl:L\\u200b) не выпукла, а значит, из rkXl∗=m\\\\text{rk} X_l^* = mrkXl∗\\u200b=m не следует, что L(W0:L∗)=0\\\\mathcal{L}(W_{0:L}^*) = 0L(W0:L∗\\u200b)=0, и градиентный спуск, действующий на Wl:LW_{l:L}Wl:L\\u200b, не обязан сходиться в точку, в которой L=0\\\\mathcal{L} = 0L=0 (он может застрять в локальном минимуме). Тем не менее, поставив ряд дополнительных условий, теорему 1 можно обобщить:\\nТеорема 2. Пусть W0:L∗W_{0:L}^{\\\\ast}W0:L∗\\u200b – локальный минимум L(W0:L)=∥Y^−Y∥F2\\\\mathcal{L}(W_{0:L}) = \\\\| \\\\hat Y - Y \\\\|_F^2L(W0:L\\u200b)=∥Y^−Y∥F2\\u200b и выполнены следующие условия:\\n\\nϕ\\\\phiϕ аналитична, ограничена, не тождественно равна нулю;\\nпроизводная ϕ\\\\phiϕ нигде не обращается в ноль;\\nnl≥mn_l \\\\geq mnl\\u200b≥m;\\nrkWl′∗=nl′+1\\\\text{rk} W_{l'}^* = n_{l'+1}rkWl′∗\\u200b=nl′+1\\u200b ∀l′∈{l+1,…,L}\\\\forall l' \\\\in \\\\{l+1,\\\\ldots,L\\\\}∀l′∈{l+1,…,L};\\ndet\\u2061(∇Wl+1:L2L(W0:L∗))≠0\\\\det(\\\\nabla^2_{W_{l+1:L}} \\\\mathcal{L}(W_{0:L}^*)) \\\\neq 0det(∇Wl+1:L\\u200b2\\u200bL(W0:L∗\\u200b))\\ue020=0.\\n\\nТогда W0:L∗W_{0:L}^*W0:L∗\\u200b – глобальный минимум L(W0:L)\\\\mathcal{L}(W_{0:L})L(W0:L\\u200b).\\nУсловие 4 необходимо, чтобы из rkXl∗=m\\\\text{rk} X_l^* = mrkXl∗\\u200b=m следовало L(W0:L∗)=0\\\\mathcal{L}(W_{0:L}^*) = 0L(W0:L∗\\u200b)=0. Отметим, что из условия 4 также следует, что nl≥nl+1n_l \\\\geq n_{l+1}nl\\u200b≥nl+1\\u200b ∀l′>L\\\\forall l' > L∀l′>L, то есть нейронная сеть должна сужаться, начиная со следующего после самого широкого слоя.\\nУсловие 5 необходимо, чтобы в случае rkXl∗<m\\\\text{rk} X_l^* < mrkXl∗\\u200b<m построить малое возмущение минимума W0:L∗W_{0:L}^*W0:L∗\\u200b, которое снова является минимумом, но для которого rkXl=m\\\\text{rk} X_l = mrkXl\\u200b=m; невырожденный гессиан позволяет применить для этого теорему об обратной функции.\\nЕсли функция активации ϕ\\\\phiϕ не аналитична, то лемма 3 неверна. В самом деле, для однородной ϕ\\\\phiϕ (например, для ReLU или leaky ReLU) паттерны активаций, ϕ′(H1)\\\\phi'(H_1)ϕ′(H1\\u200b), не меняются при малом возмущении весов. Значит, мы, вообще говоря, не можем найти такое малое возмущение, для которого ранг X1X_1X1\\u200b будет полным.\\nВместо малого возмущения в работе On Connected Sublevel Sets in Deep Learning явно строятся пути в пространстве весов, на которых функция потерь не возрастает и достигает нуля. Если такой путь можно построить из произвольной точки, то все (строгие) локальные минимумы глобальны. Оказывается, что для построения такого пути аналитичность функции активации не требуется. Более того, при определённых условиях можно доказать, что из любых двух точек в пространстве весов можно построить соответствующие пути так, чтобы они сходились в одной точке. Это значит, что множество подуровня L−1((−∞,E))\\\\mathcal{L}^{-1}((-\\\\infty, \\\\mathcal{E}))L−1((−∞,E)) связно при любом E>0\\\\mathcal{E} > 0E>0 – эффект, впервые эмпирически обнаруженный в работах Loss surfaces, mode connectivity, and fast ensembling of DNNs и Essentially No Barriers in Neural Network Energy Landscape.\\nСвязность множеств подуровня сильнее глобальности всех строгих минимумов. В самом деле, если бы существовал строгий локальный минимум уровня E>0\\\\mathcal{E} > 0E>0, то для достаточно малого ϵ>0\\\\epsilon > 0ϵ>0 ему бы соответствовала отдельная связная компонента множества подуровня E+ϵ\\\\mathcal{E} + \\\\epsilonE+ϵ. С другой стороны, если все локальные минимумы глобальны, но изолированы, то множество подуровня ϵ\\\\epsilonϵ несвязно для достаточно малого ϵ>0\\\\epsilon > 0ϵ>0.\\nВместо того, чтобы строить пути, на которых функция потерь достигает нуля, можно строить пути, на которых функция потерь достигает сколь угодно малого значения ϵ>0\\\\epsilon > 0ϵ>0. Это позволяет обобщить результат на функции потерь ℓ(y,y^)\\\\ell(y,\\\\hat y)ℓ(y,y^\\u200b), для которых минимум по второму аргументу, ответу сети, не достигается. Пример такой функции – кросс-энтропия. Так мы приходим к следующей теореме:\\nТеорема (On Connected Sublevel Sets in Deep Learning). Пусть выполнены следующие условия:\\n\\nϕ(R)=R\\\\phi(\\\\mathbb{R}) = \\\\mathbb{R}ϕ(R)=R, ϕ\\\\phiϕ строго монотонна и не найдётся ненулевых (λi,ai)i=1p(\\\\lambda_i,a_i)_{i=1}^p(λi\\u200b,ai\\u200b)i=1p\\u200b с ai≠aja_i \\\\neq a_jai\\u200b\\ue020=aj\\u200b ∀i≠j\\\\forall i \\\\neq j∀i\\ue020=j, таких что ∀x∈R\\\\forall x \\\\in \\\\mathbb{R}∀x∈R ϕ(x)=∑i=1pλiϕ(x−ai)\\\\phi(x) = \\\\sum_{i=1}^p \\\\lambda_i \\\\phi(x - a_i)ϕ(x)=∑i=1p\\u200bλi\\u200bϕ(x−ai\\u200b);\\nℓ(y,y^)\\\\ell(y,\\\\hat y)ℓ(y,y^\\u200b) выпукла по второму аргументу и inf\\u2061y^ℓ(y,y^)=0\\\\inf_{\\\\hat y} \\\\ell(y,\\\\hat y) = 0infy^\\u200b\\u200bℓ(y,y^\\u200b)=0 для любого yyy;\\nСуществует l∈{1,…,L}l \\\\in \\\\{1,\\\\ldots,L\\\\}l∈{1,…,L}, для которого rkXl=m\\\\text{rk} X_l = mrkXl\\u200b=m и nl′>nl′+1n_{l'} > n_{l'+1}nl′\\u200b>nl′+1\\u200b для всех l′∈{l+1,…,L}l' \\\\in \\\\{l+1,\\\\ldots,L\\\\}l′∈{l+1,…,L}.\\n\\nТогда если L(W0:L)=∑j=1mℓ(yj,fW0:L(xj))\\\\mathcal{L}(W_{0:L}) = \\\\sum_{j=1}^m \\\\ell(y_j, f_{W_{0:L}}(x_j))L(W0:L\\u200b)=∑j=1m\\u200bℓ(yj\\u200b,fW0:L\\u200b\\u200b(xj\\u200b)), то\\n\\nДля каждого ϵ>0\\\\epsilon > 0ϵ>0 найдутся веса W0:LW_{0:L}W0:L\\u200b, для которых L(W0:L)<ϵ\\\\mathcal{L}(W_{0:L}) < \\\\epsilonL(W0:L\\u200b)<ϵ;\\nМножество подуровня L−1((−∞,E))\\\\mathcal{L}^{-1}((-\\\\infty, \\\\mathcal{E}))L−1((−∞,E)) связно для каждого E>0\\\\mathcal{E} > 0E>0.\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанЗагрузка...Сообщить об ошибкеПредыдущий параграф13.4. Сети бесконечной шириныСледующий параграф13.6. Implicit biasЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_28.html', 'title': 'Нейросети для облаков точек'}, page_content=\"Нейросети для облаков точекЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/56.1.Свёрточные нейросети6.2.Нейросети для работы с последовательностями6.3.Трансформеры6.4.Графовые нейронные сети6.5.Нейросети для облаков точекСенсоры для получения облаков точекАрхитектуры для обработки облаков точек7.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Нейросети для облаков точек6.5. Нейросети для облаков точекАвторыФилатов АртёмОблако точек — это неструктурированный набор векторов вида (x,y,z)(x, y, z)(x,y,z). Опционально каждой точке может также соответствовать вектор fff, который содержит дополнительные признаки, например, RGB цвет точки, интенсивность полученного сенсором сигнала и т.д.\\n\\nОблака точек возникают во многих задачах из реального трёхмерного мира и позволяют нам понять, как он выглядит. Например, беспилотный автомобиль воспринимает окружающие его объекты в виде облака точек и строит между ними безопасный маршрут. Современные смартфоны с помощью сенсоров, которые возвращают облака точек, способны восстановить точную геометрию окружающего пространства.\\nИнтуитивно кажется, что облака точек похожи на трёхмерные изображения, но есть важное отличие: облако точек — это неупорядоченный, неструктурированный массив, и чтобы извлечь из него пространственную структуру, нужно ещё постараться.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nПоэтому, мы не сможем напрямую использовать для работы с облаками точек архитектуры, созданные для изображений. В этом параграфе мы расскажем о том, как извлекать признаки из облаков точек и какие end-to-end архитектуры существуют для популярных задач: классификации и сегментации.\\nНо прежде, чем мы переедем к описанию конкретных архитектур, давайте разберёмся, откуда берутся облака точек.\\nСенсоры для получения облаков точек\\nLiDAR\\nОдин из самых популярных сенсоров для получения облаков точек — это LiDAR. Принцип его действия состоит в следующем: посылается луч, он отражается от поверхности и возвращается на детектор. Зная скорость света и время, за которое луч вернулся, мы можем посчитать расстояние до объекта.\\n\\nБлагодаря используемой технологии у каждой точки помимо позиции (x,y,z)(x, y, z)(x,y,z) будет известна ещё интенсивность. Она показывает, насколько сильным был отклик от поверхности. Например, от стекол или снега мы можем ожидать околонулевой мощности отражения, в то время как от дорожных знаков отражение будет очень сильным. Таким образом, интенсивность может оказаться важным признаком при построении предсказательной модели над облаками.\\nБольшим преимуществом лидара является способность получать отклик на расстоянии 200 метров и больше при сохранении высокой точности.\\nКамера RGB-D\\nRGB-D камера — это камера, которая способна возвращать глубину каждого пикселя помимо RGB значения. Зная глубину и математическую модель камеры, мы можем восстановить облако точек. Большим преимуществом такого сенсора является наличие цвета для каждой точки. Это может быть полезно при построении моделей сегментации, детекции и так далее.\\nRGB-D камеры, как правило, обладают меньшей точностью, чем LiDAR. Также такие сенсоры не позволяют оценивать глубину дальше 10-15 метров.\\nRGB-камеры\\nОблако точек можно получить и с помощью обычной камеры, если воспользоваться алгоритмами компьютерного зрения для вычисления глубины. Существуют алгоритмы оценки глубины по одному кадру (monocular depth estimation), оценки глубины по нескольким кадрам (multi view stereo), восстановление облака точек окружающих объектов с помощью движения камеры (Structure From Motion). В зависимости от алгоритма, камеры и среды качество итоговых облаков может различаться. В каждом отдельном практическом приложении нужно смотреть, насколько тот или иной сенсор (или метод получения облака точек) подходит для задачи.\\nАрхитектуры для обработки облаков точек\\nВ этой части мы рассмотрим различные подходы к построению нейросетей для работы с облаками точек. Мы обсудим детали архитектур, их минусы и плюсы, а также варианты использования этих архитектур в приложениях.\\nPointNet\\nДавайте подумаем как, именно мы можем обрабатывать облака точек.\\nВо-первых мы можем применять некоторую функцию, например, MLP (полносвязную нейросеть) к вектору признаков каждой точки нашего облака.\\nПроблема с таким подходом в том, что мы работаем не с облаком, а с отдельными точками. У такой архитектуры не будет пространственного контекста. В терминах CNN, receptive field каждой точки будет равен 1 — это эквивалентно свёрточной архитектуре, где все ядра размера 1x1.\\nЗначит нам нужен некий механизм, с помощью которого точки смогут обмениваться информацией друг с другом. Таких механизмов можно придумать много: это и построение графа над облаком, и пересылка сообщений между вершинами, это может быть механизм внимания.\\nАвторы архитектуры PointNet предложили максимально простой механизм: имея для каждой точки некоторый вектор признаков fi∈Rpf_i \\\\in \\\\mathbb{R}^pfi\\u200b∈Rp, давайте применим к этим векторам некоторую глобальную агрегацию AGGR, например, GlobalAveragePooling или GlobalMaxPooling:\\nAGGR:\\xa0(fii=1n)↦f∈Rp\\\\text{AGGR}:\\\\ ({f_i}_{i=1}^n) \\\\mapsto f \\\\in \\\\mathbb{R}^p\\nAGGR:\\xa0(fi\\u200bi=1n\\u200b)↦f∈RpТаким образом из набора векторов для точек мы получим общий вектор признаков, описывающий всё облако.\\n\\nПолученный вектор описывает облако в целом, но точки при таком подходе по-прежнему не обмениваются информацией: ни одна из них не «знает», что происходит вокруг.\\nЧтобы исправить недостатки двух описанных выше подходов, объединим их, рассмотрев следующий базовый «слой» преобразования облака:\\n\\nАгрегируем векторы признаков точек fif_ifi\\u200b, получаем общий вектор признаков fff для облака в целом;\\nДля каждой точки конкатенируем её вектор признаков с вектором признаков облака и строим новый вектор признаков gi=concat(fi,f)g_i = \\\\text{concat}(f_i, f)gi\\u200b=concat(fi\\u200b,f);\\nПрименим MLP к вектору признаков каждой точки: fi′=MLP(gi)f'_i = \\\\text{MLP}(g_i)fi′\\u200b=MLP(gi\\u200b).\\n\\nТакие слои можно применять последовательно, формируя сколь угодно глубокую архитектуру.\\nТеперь, если мы хотим решить задачу классификации над облаком, мы можем взять очередной вектор fff, полученный после агрегации всех векторов признаков точек, применить к нему MLP, получить логиты и обучать полученную сеть на cross-entropy loss.\\nЕсли же мы хотим решить задачу сегментации (то есть классификации для каждой точки), мы можем точно после очередного слоя применить MLP с необходимыми размерами к каждому вектору fif_ifi\\u200b и получить логиты.\\nАвторы архитектуры PointNet предложили ещё одно небольшое усложнение описанной выше архитектуры. Давайте внимательно посмотрим на схему из статьи:\\n\\nСиний прямоугольник наверху соответствует одному описанному выше «слою», но вместо одного MLP здесь последовательность двух MLP, между которыми вставлена операция feature transform. Эта операция состоит в следующем:\\n\\nс помощью дополнительной сети по облаку в целом строим матрицу;\\nэта матрица умножается на вектор признаков каждой из точек.\\n\\nТем самым точки дополнительно обмениваются информацией. Во многих последующих статьях эту часть не включают, и практика показывает, что для PointNet это не ключевой элемент архитектуры, и им можно пренебречь.\\nPointNet — это достаточно простая архитектура. Она использует геометрию облака, потому что координаты точек входят в векторы признаков, но работает не с локальной окрестностью точки, а со всем облаком в целом. Если мы обратимся к аналогиям с изображениями, то такая архитектура будет эквивалентна CNN со свёртками размера всего изображения. Такая архитектура с трудом улавливает локальные паттерны и детали, и именно это будет основным направлением дальнейшего улучшения этой архитектуры.\\nPointNet++\\nПродолжением статьи PointNet стала статья PointNet++ от той же научной группы. Новая архитектура развивает идею локальности, о который мы писали в предыдущем параграфе.\\nДавайте подумаем, каким образом мы можем получить локальные, зависящие только от окрестности точки операции над облаком точек. Первым делом нам необходимо понять, что такое локальность. В облаке точек очевидным решением кажется определить окрестность точки исходя из евклидового расстояния между точками — как шар некоторого радиуса — и проводить операции внутри этого шара.\\nКакие операции мы будем проводить внутри шара? У нас уже есть PointNet, который по произвольному набору точек может построить вектор, описывающий этот набор в целом. С его помощью мы можем описать многослойную архитектуру, напоминающую энкодер свёрточной сети, каждый слой которой будет выглядеть следующим образом:\\n\\nсэмплируем NNN ключевых точек в облаке, где NNN меньше размера облака (как правило в несколько раз);\\nвокруг каждой ключевой точки фиксируем шар радиуса RRR;\\nдля каждого шара находим все точки, которые в нём лежат;\\nзапускаем PointNet с одними и теми же весами для каждого шара;\\nполучаем новое облако из NNN точек, где каждой точке присвоен вектор признаков, полученный из PointNet.\\n\\nДалее мы можем повторять эту процедуру, пока у нас не останется одна точка, то есть один вектор признаков для всего облака в целом. Полученный эмбединг можно использовать для решения различных задач.\\n\\nОбсудим детали реализации этой архитектуры.\\nВ качестве алгоритма сэмплирования предлагается Farthest Point Sampling (FPS). Он заключается в том, что мы жадно сэмплируем точку, которая максимально удалена от текущего насэмплированного множества. Этот процесс мы повторяем, пока не наберём достаточное количество точек. Благодаря FPS мы можем в некоторой степени гарантировать, что покроем равномерно все облако и не пропустим какие-то мелкие, но удалённые от основного облака объекты. В этом ценное отличие FPS от случайного сэмплирования, при котором детали, содержащие мало точек, могут быть легко потеряны.\\nВыбор радиуса шара зависит, как правило, от плотности облака. Стоит посмотреть, сколько примерно точек попадает в каждую окрестность, и исходя из этого фиксировать радиус. Также этот параметр можно подобрать с помощью валидации. Для удобства реализации фиксируют не только радиус, но и максимальное количество объектов внутри шара.\\nЭто делается для того, чтобы при реализации архитектуры можно было манипулировать тензорами, избегая итерации по всем точкам каждого шара. Например, мы фиксируем количество шаров NNN, количество точек внутри шара KKK и размерность вектора признаков для каждой точки PPP. Тогда размер тензора будет: N×K×PN \\\\times K \\\\times PN×K×P. Если в окрестности шара оказалось больше точек, чем мы заранее определили, то возьмем ближайшие KKK. В случае, если точек в тензоре меньше, чем KKK, тензор паддится нулями до нужного размера.\\nВнутри каждого слоя веса PointNet одинаковы для всех шаров. На окрестностях с одинаковой локальной структурой мы хотим получать одинаковые результаты, поэтому мы не можем использовать абсолютные значения (x,y,z)(x, y, z)(x,y,z).\\nВ этом плане мы хотим, чтобы наши слои были похожи на свёртки, результат работы которых тоже не зависят от положения пикселя на изображении. Чтобы этого добиться, координаты (x,y,z)(x, y, z)(x,y,z) каждой точки преобразуются в локальные координаты (x−x‾,y−y‾,z−z‾)(x - \\\\overline{x}, y - \\\\overline{y}, z - \\\\overline{z})(x−x,y−y\\u200b,z−z), где (x‾,y‾,z‾)(\\\\overline{x}, \\\\overline{y}, \\\\overline{z})(x,y\\u200b,z) — координаты центра шара.\\nОтдельно стоит обсудить, как получить поточечные признаки для решения задачи сегментации. Нам предстоит обратить энкодер и получить архитектуру декодера. Для этого каждый раз, когда мы делаем downsampling облака, то есть переход от вектора для каждой точки к общему вектору для некоторой окрестности, мы будем запоминать, какие точки принадлежали к какому шару.\\nВ процессе upsampling, то есть перехода от вектора окрестности к поточечным векторам, мы будем конкатенировать вектор окрестности с исходными векторами признаков точек. После каждой операции upsampling мы можем запускать MLP, чтобы точка, получив информацию с более глубокого уровня, могла извлечь оттуда информацию.\\nВоксельные архитектуры\\nМы разобрали архитектуру PointNet++, которая построена на обработке локальных окрестностей точек, определённых исходя из евклидового расстояния. У этой архитектуры есть свои проблемы.\\nВо-первых, поиск ближайших соседей и FPS — это медленные процедуры. В результате могут возникнуть проблемы, когда мы захотим использовать такую сеть в real-time приложениях.\\nВо-вторых, в зависимости от расстояния до сенсора у нас может меняться паттерн разреженности: если вблизи сенсора в шаре радиуса ϵ\\\\epsilonϵ мы можем найти в среднем 100 точек, то, как правило, чем больше расстояние до наблюдателя — тем меньше точек мы будем находить в таком же объёме. Это усложняет обучение и может привести к тому, что с расстоянием качество будет сильно деградировать.\\nДавайте попробуем напрямую применить свёрточные нейронные сети в нашей задаче. Идея следующая: возьмём прямоугольный параллелепипед, который накрывает интересующую нас область пространства. Далее разобьём этот прямоугольный параллелепипед на значительно меньшие прямоугольные параллелипипеды одинакового размера. Назовем эти прямоугольные параллелепипеды вокселями. В результате внутри каждого вокселя окажется некоторый набор точек. Нам нужно каким-то образом превратить точки внутри каждого вокселя в один вектор. После этого мы можем применить обычные свёрточные сети и решать любые задачи.\\nЧтобы из набора точек внутри вокселя получить вектор признаков, мы можем просто применить PointNet. Таким образом, мы получим тензор размера H×W×L×PH\\\\times W \\\\times L \\\\times PH×W×L×P, где первые три размерности пространственные, а последняя размерность — размерность вектора признаков.\\nДалее мы можем использовать 3D-свёртки для того, чтобы обработать этот тензор. Но проблема в том, что ядро в 3D-свёртках имеет на одну размерность больше, чем в 2D-свёртках, что делает их дорогими для вычисления.\\n\\nВ статьях были предложены различные подходы для того, чтобы ускорить эту архитектуру. Часто размерность по высоте и размерность признаков объединяют в одну: тензор размерности W×L×H×PW\\\\times L\\\\times H\\\\times PW×L×H×P превращается в тензор размерности W×L×(H×P)W\\\\times L\\\\times (H\\\\times P)W×L×(H×P), как будто мы смотрим на него сверху (bird’s eye view). Для работы с ними мы можем использовать 2D-свёртки.\\n\\nЕщё одним популярным трюком является использование высокой степени разреженности этого тензора: большое количество «столбиков» не содержат ни одной точки. Давайте возьмём все «столбики», содержащие хотя бы одну точку; предположим, что их BBB штук. Соберём из них тензор размера B×(H×P)B\\\\times (H\\\\times P)B×(H×P) и применим к нему линейный слой, который уменьшит число каналов, то есть высоту «столбиков».\\nПолучится тензор B×NB\\\\times NB×N для выбранного нами N<H×PN < H\\\\times PN<H×P. Его столбцы мы снова расставим на их исходные места, получив тензор W×L×NW\\\\times L\\\\times NW×L×N. «Столбики» этого тензора, которые соответствуют пустым «столбикам» исходного тензора W×L×(H×P)W\\\\times L\\\\times (H\\\\times P)W×L×(H×P), мы заполним нулями.\\nВоксельный подход очень популярен в задаче детекции, потому что позволяет напрямую переиспользовать некоторые подходы к 2D детектированию. Но у него есть и недостатки. Например, нельзя сколько угодно плотно покрыть вокселями сколь угодно большой объём, потому что это напрямую влияет на размер тензора, а значит — на время работы, что может быть критично в real-time приложениях. В итоге мы можем потерять какие-то важные удалённые объекты или не справиться с мелкими деталями.\\nАрхитектуры с цилиндрической проекцией\\nОсновная проблема воксельных архитектур — это неспособность обрабатывать облака произвольного размера и дальности. У вас может быть самый лучший LiDAR в мире, который видит на 300 метров во все стороны, но если ваша архитектура не будет способна обрабатывать полученное облако точек в режиме реального времени, то такой сенсор бесполезен.\\nЧтобы преодолеть эту проблему, мы можем воспользоваться тем фактом, что лидар сканирует окрестность из одной точки. Давайте окружим сенсор виртуальным цилиндром и спроецируем все точки на этот цилиндр. Далее цилиндр можно развернуть в прямоугольник и получить представление с которым могут работать свёрточные сети.\\nОбычно точки параметризуют двумя углами:\\n\\npitch — наклон по вертикали,\\nyaw — угол по горизонтали.\\n\\nМы можем дискретизовать эти углы и таким образом получить для каждой точки координаты пикселя, в который она должна быть помещена. В каналы нашего тензора мы можем записать: расстояние до точки, интенсивность сигнала, абсолютную высоту точки. Далее такое представление может быть обработано любой 2D архитектурой.\\n\\nВажно отметить, что такое представление не будет работать для других методов получения облаков точек. Лидар сканирует окрестность из одной точки, и потому в направлении каждого луча у нас будет только одна точка. Это означает, что мы не видим за препятствиями. В то время облака, полученные, например, с помощью Structure from Motion, в направлении одного луча могут содержать несколько точек, что лишает нас возможности без потерь спроецировать всё облако на 2D холст.\\nТакое представление снимает проблему с производительностью, так как размер обрабатываемых данных не зависит от разброса облака. Но, к сожалению, появляются новые проблемы.\\nВо-первых, объекты, которые находятся далеко друг от друга в трёхмерном пространстве, могут оказаться рядом в этой проекции — в итоге, информация о таких объектах будет смешиваться, и в результате границы в задаче сегментации могут получиться размытыми. Воксельные- или PointNet-архитектуры не имеют этой проблемы — в них далёкие друг от друга объекты не будут так сильно взаимодействовать.\\nВо-вторых, объекты при таком представлении будут иметь разные размеры в зависимости от расстояния, и модели придётся адаптироваться к этому. Воксельные и PointNet архитектуры, опять же, лишены этой проблемы, так как их представления инвариантны к переносу.\\nОбе эти проблемы могут приводить к существенной деградации качества. Именно поэтому архитектуры, построенные на цилиндрической проекции, часто проигрывают другим архитектурам по качеству.\\nОбъединение информации с облака и изображения\\nВ некоторых приложениях у нас может быть несколько различных сенсоров: например, лидар и камера. Тогда возникает задача: построить архитектуру, которая использует информацию с обоих сенсоров.\\nКонечно, мы бы могли отдельно подготовить архитектуру для работы с изображением, отдельно архитектуру для работы с облаком и затем каким-то образом объединить результаты. Минус такого подхода в том, что нам требуется в два раза больше вычислительных мощностей, а нейронные сети никак не делятся информацией на промежуточных слоях. Возможно, нейронная сеть могла бы извлечь более богатые представления из объединенных данных. Например, разрешить некоторую неопределённость в облаке с помощью изображений.\\nОказывается, что задача смешивания различных сенсоров очень непростая, и в литературе долгое время не могли для задачи 3D детекции получить результаты, которые бы превосходили только лидарный подход. Мы обсудим идеи, как именно можно смешивать изображения и 3D информацию.\\nПрежде чем мы перейдем к описанию подходов, давайте обсудим одну важную техническую деталь. Смешивание информации с разных сенсоров всегда предполагает, что сенсоры скалиброваны. Это означает что нам известна матрица перехода из системы координат одного сенсора в систему координат другого. Таким образом, у нас есть необходимая информация, для того, чтобы спроецировать лидарную точку на плоскость изображения.\\nАрхитектура, которая первой показала действительно значительный прирост качества при смешивании сенсоров, называется PointPainting. Идея была следующей:\\n\\nДелаем сегментацию изображения;\\nДля каждой лидарной точки найдём соответствующий ей пиксель на изображении и присвоим этой точке соответствующий класс из сегментации;\\nДля архитектуры, которая работает с облаком точек, этот класс будет еще одним дополнительным признаком на входе.\\n\\nВажно отметить, что сопоставление точки класса не будет идеальным, например, из-за того, что камера и лидар находятся не в одной точке.\\n\\nИдея очень простая, но при этом позволяет сильно улучшить метрики. Проблема в том, что данный подход не решает проблему возросших вычислений при обработке данных с двух сенсоров. Более того, мы передаем в 3D архитектуру лишь одно число для каждой точки и теряем много информации об изображении.\\nДругой подход был описан в статье LaserNet++. Авторы используют цилиндрическую проекцию для обработки облака. Перед началом обработки они пропускают изображение через небольшую свёрточную сеть, получая в результате некоторый вектор признаков в каждом пикселе.\\nДалее для каждой лидарной точки находим соответствующий пиксель на изображении и добавляем его признаки к признакам точки. Отличие от PointPainting в том, что в данном подходе обе нейронные сети обучаются end-to-end. Это позволяет нам извлечь более богатое представление из изображения.\\nВ статье авторы сообщают о росте метрик на in-house наборе данных. К сожалению, на открытых данных этот подход не смог продемонстрировать превосходства над lidar-only моделями.\\n\\nВ этом параграфе мы разобрали основные идеи работы с облаками точек. Верхнеуровнево все архитектуры можно разделить на следующие группы:\\n\\nОблако как граф (PointNet и многие другие архитектуры);\\nВокселизация (VoxelNet, PointPillars и другие);\\nПроекция лидарного облака на 2D поверхность (RangerNet, LaserNet, LaserNet++ и другие).\\n\\nКаждый подход имеет свои плюсы и минусы и выбор конкретного подхода должен быть обусловлен решаемой задачей.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф6.4. Графовые нейронные сетиСледующий параграф7.1. Обучение представленийЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_69.html', 'title': 'Энтропия и семейство экспоненциальных распределений'}, page_content=\"Энтропия и семейство экспоненциальных распределенийЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/716.1.Матричное дифференцирование16.2.Матричная факторизация16.3.Вероятностные распределения16.4.Многомерные распределения16.5.Независимость и условные распределения вероятностей16.6.Параметрические оценки16.7.Энтропия и семейство экспоненциальных распределенийЭнтропияПринцип максимальной энтропииЭкспоненциальное семейство распределенийГлавная/Хендбуки/Учебник по машинному обучению/Энтропия и семейство экспоненциальных распределений16.7. Энтропия и семейство экспоненциальных распределенийАвторыСергей ЛыткинЭнтропия\\nИнформативность наблюдений\\nПредставим, что вы пронаблюдали значение xxx некоторой случайной величины ξ\\\\xiξ. Как измерить количество информации h(x)h(x)h(x), которое вы при этом получили? Следующие соображения кажутся в этом плане вполне естественными:\\n\\nчем выше вероятность P(ξ=x)\\\\mathbb P(\\\\xi = x)P(ξ=x), тем более ожидаемо появление значения xxx и, соответственно, менее информативно;\\nи наоборот, наблюдение маловероятного значения xxx обычно даёт обильную пищу для размышлений и повышает h(x)h(x)h(x);\\nпри наблюдении двух независимых реализаций xxx и yyy случайной величины ξ\\\\xiξ логично складывать полученную информацию: h((x,y))=h(x)+h(y)h((x, y)) = h(x) + h(y)h((x,y))=h(x)+h(y).\\n\\nУказанные соображения наводят на мысль, что информацию следует считать убывающей функцией от вероятности: h(x)=g(P(ξ=x))h(x) = g\\\\big(\\\\mathbb P(\\\\xi = x)\\\\big)h(x)=g(P(ξ=x)). Кроме того, функция ggg должна превращать произведение в сумму, поскольку для независимых случайных величин ξ\\\\xiξ и η\\\\etaη\\nравенство h((x,y))=h(x)+h(y)h((x, y)) = h(x) + h(y)h((x,y))=h(x)+h(y) влечёт\\ng(P(ξ=x,η=y))=g(P(ξ=x)P(η=y))=g(P(ξ=x))+g(P(η=y)).   g\\\\big(\\\\mathbb P(\\\\xi = x, \\\\eta=y)\\\\big) = g\\\\big(\\\\mathbb P(\\\\xi = x) \\\\mathbb P(\\\\eta = y)\\\\big) = g\\\\big(\\\\mathbb P(\\\\xi = x)\\\\big) + g\\\\big(\\\\mathbb P(\\\\eta = y)\\\\big).\\ng(P(ξ=x,η=y))=g(P(ξ=x)P(η=y))=g(P(ξ=x))+g(P(η=y)).На самом деле выбор тут небогат. Единственная непрерывная функция, обладающая такими свойствами, — это логарифм: g(p)=−log\\u2061pg(p) = -\\\\log pg(p)=−logp. Основание логарифма может быть любым числом больше единицы. Поскольку информацию измеряют в битах и байтах, в теории информации обычно предпочитают двоичные логарифмы. Однако для вычислений удобнее использовать натуральный логарифм, и по умолчанию мы будем подразумевать именно его (кстати, соответствующую единицу информации называют «нат»).\\nЭнтропия Шеннона\\nСреднее количество информации, которое несёт в себе значение дискретной случайной ξ\\\\xiξ с распределением вероятностей pk=P(ξ=k)p_k= \\\\mathbb P(\\\\xi = k)pk\\u200b=P(ξ=k), вычисляется по формулеВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nHξ=E(g(p(ξ)))=−E(log\\u2061p(ξ))=−∑kpklog\\u2061pk.    \\\\mathbb H\\\\xi = \\\\mathbb E \\\\big(g(p(\\\\xi))\\\\big) =  -\\\\mathbb E \\\\big(\\\\log p(\\\\xi)\\\\big)=-\\\\sum\\\\limits_k p_k \\\\log p_k.\\nHξ=E(g(p(ξ)))=−E(logp(ξ))=−k∑\\u200bpk\\u200blogpk\\u200b.Это так называемая энтропия (Шэннона).\\nНебольшое математическое замечаниеПри вычислении энтропии регулярно можно встретить выражение 0⋅log\\u206100\\\\cdot \\\\log 00⋅log0 с не вполне очевидным значением. Поскольку lim\\u2061t→0+tlog\\u2061t=0\\\\lim \\\\limits_{t\\\\to 0+} t\\\\log t = 0t→0+lim\\u200btlogt=0, по определению полагаем 0⋅log\\u20610=0.0\\\\cdot \\\\log 0 = 0.0⋅log0=0.\\nПример. Рассмотрим схему Бернулли с вероятностью «успеха» ppp. Энтропия её результата равна\\nHξ=−(1−p)log\\u2061(1−p)−plog\\u2061p,ξ∼Bern(p).\\\\mathbb H\\\\xi = -(1 - p)\\\\log(1 - p) - p\\\\log p, \\\\quad \\\\xi \\\\sim \\\\mathrm{Bern}(p).\\nHξ=−(1−p)log(1−p)−plogp,ξ∼Bern(p).Давайте посмотрим на график этой функции:\\n\\nМинимальное значение (нулевое) энтропия принимает при p=0p = 0p=0 или p=1p=1p=1. Исход такого вырожденного эксперимента заранее известен, и чтобы сообщить кому-то о его результате,  достаточно 000 бит информации. Иначе говоря, можно вообще ничего не передавать, и так всё предельно ясно.\\nМаксимальное значение энтропии достигается в точке 12\\\\frac1221\\u200b, что вполне соответствует тому, что при p=12p=\\\\frac12p=21\\u200b предсказать исход эксперимента сложнее всего.\\nУпражнение. Найдите энтропию геометрического распределения с вероятностью «успеха» ppp: ξ∼Geom(p)\\\\xi \\\\sim \\\\mathrm{Geom}(p)ξ∼Geom(p),\\nP(ξ=k)=p(1−p)k−1,k∈N,0<p⩽1.\\\\mathbb P(\\\\xi = k) = p(1-p)^{k-1}, \\\\quad k \\\\in \\\\mathbb N, \\\\quad 0 < p \\\\leqslant 1.\\nP(ξ=k)=p(1−p)k−1,k∈N,0<p⩽1.Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)По определению энтропии имеем\\nHξ=−∑k=1∞p(1−p)k−1(log\\u2061p+(k−1)log\\u2061(1−p))=\\\\mathbb H\\\\xi = -\\\\sum\\\\limits_{k=1}^\\\\infty p(1-p)^{k-1} (\\\\log p + (k-1)\\\\log(1-p)) =\\nHξ=−k=1∑∞\\u200bp(1−p)k−1(logp+(k−1)log(1−p))==−log\\u2061p−plog\\u2061(1−p)∑k=1∞k(1−p)k.= -\\\\log p -p\\\\log(1-p)\\\\sum\\\\limits_{k=1}^\\\\infty k(1-p)^k.\\n=−logp−plog(1−p)k=1∑∞\\u200bk(1−p)k.Дифференцированием геометрической прогрессии находим\\n∑k=1∞kxk−1=∑k=1∞(xk)′=ddx∑k=0∞xk=(11−x)′=1(1−x)2;\\\\sum\\\\limits_{k=1}^\\\\infty kx^{k-1} = \\\\sum\\\\limits_{k=1}^\\\\infty (x^k)' = \\\\frac d{dx}\\\\sum\\\\limits_{k=0}^\\\\infty x^k = \\\\Big(\\\\frac 1{1-x}\\\\Big)' = \\\\frac 1{(1-x)^2};\\nk=1∑∞\\u200bkxk−1=k=1∑∞\\u200b(xk)′=dxd\\u200bk=0∑∞\\u200bxk=(1−x1\\u200b)′=(1−x)21\\u200b;подставляя сюда x=1−px = 1-px=1−p, окончательно получаем, что\\nHξ=−log\\u2061p−p(1−p)log\\u2061(1−p)⋅1p2=−log\\u2061p−(1−p)log\\u2061(1−p)p.    \\\\mathbb H\\\\xi = -\\\\log p -p(1-p)\\\\log(1-p)\\\\cdot \\\\frac 1{p^2} = -\\\\log p - \\\\frac{(1-p)\\\\log(1-p)}p.\\nHξ=−logp−p(1−p)log(1−p)⋅p21\\u200b=−logp−p(1−p)log(1−p)\\u200b.Вспомним, что случайная величина ξ∼Geom(p)\\\\xi\\\\sim\\\\mathrm{Geom}(p)ξ∼Geom(p) равна количеству независимых испытаний с вероятностью «успеха» ppp до появления первого «успеха». При p=1p=1p=1 энтропия Hξ\\\\mathbb H\\\\xiHξ минимальна и равна нулю, ведь в этом случае геометрическое распределение становится вырожденным: «успех» наступает сразу же с вероятностью 111. А вот при p→0+p\\\\to 0+p→0+ серия неудачных испытаний может быть сколь угодно длинной; распределение становится всё более неопределённым и «размазанным» по своему носителю, и его энтропия стремится к +∞+\\\\infty+∞. График энтропии как функции от ppp выглядит так:\\n\\nСледующие свойства энтропии дискретной случайной величины ξ\\\\xiξ вытекают прямо из определения:\\n\\nнеотрицательность: Hξ⩾0\\\\mathbb H\\\\xi \\\\geqslant 0Hξ⩾0;\\nHξ=0\\u2005\\u200a⟺\\u2005\\u200aP(ξ=a)=1\\\\mathbb H\\\\xi = 0 \\\\iff \\\\mathbb P(\\\\xi = a) = 1Hξ=0⟺P(ξ=a)=1 при некотором a∈Ra\\\\in\\\\mathbb Ra∈R (нулевую энтропию имеют вырожденные распределения и только они);\\nHξ⩽log\\u2061n\\\\mathbb H\\\\xi \\\\leqslant \\\\log nHξ⩽logn, если случайная величина имеет конечный носитель мощности nnn.\\n\\nПоследнее свойство выводится из неравенства Йенсена. Применяя его к выпуклой вверх логарифмической функции, с учётом нормировки условия ∑k=1npk=1\\\\sum\\\\limits_{k=1}^n p_k = 1k=1∑n\\u200bpk\\u200b=1 получаем\\n−∑k=1npklog\\u2061pk=∑k=1npklog\\u20611pk⩽log\\u2061(∑k=1npk⋅1pk)=log\\u2061n.    -\\\\sum\\\\limits_{k=1}^n p_k\\\\log p_k = \\\\sum\\\\limits_{k=1}^n p_k\\\\log \\\\frac 1p_k \\\\leqslant\\n    \\\\log\\\\bigg(\\\\sum\\\\limits_{k=1}^n p_k\\\\cdot \\\\frac 1{p_k}\\\\bigg) = \\\\log n.\\n−k=1∑n\\u200bpk\\u200blogpk\\u200b=k=1∑n\\u200bpk\\u200blogp1\\u200bk\\u200b⩽log(k=1∑n\\u200bpk\\u200b⋅pk\\u200b1\\u200b)=logn.Вопрос на подумать. Итак, всякое распределение с носителем {1,2,…,n}\\\\{1, 2, \\\\ldots, n\\\\}{1,2,…,n} имеет энтропию не больше log\\u2061n\\\\log nlogn. А у какого распределения она в точности равна log\\u2061n\\\\log nlogn?\\nОтвет (не открывайте сразу, сначала подумайте самостоятельно)Такое произойдёт, если все вероятности pk=1np_k = \\\\frac 1npk\\u200b=n1\\u200b, 1⩽k⩽n1\\\\leqslant k \\\\leqslant n1⩽k⩽n, т.е. распределение равномерное.\\nДифференциальная энтропия\\nЧтобы вычислить энтропию непрерывной случайной величины ξ\\\\xiξ, надо, как водится, сумму заменить на интеграл, и получится формула дифференциальной энтропии:\\nHξ=−∫pξ(x)log\\u2061pξ(x)\\u2009dx.    \\\\mathbb H\\\\xi = -\\\\int p_{\\\\xi}(x) \\\\log p_{\\\\xi}(x)\\\\, dx.\\nHξ=−∫pξ\\u200b(x)logpξ\\u200b(x)dx.Замечание. В дальнейшем мы будем использовать одинаковый термин энтропия как для дискретных, так и для непрерывных случайных величин, для краткости опуская слово дифференциальная в последнем случае. Кроме того, энтропию распределения ppp, заданного через pmf или pdf, будем обозначать H[p]\\\\mathbb H[p]H[p]. Такое обозначение позволяет избежать привязки к случайной величине там, где это излишне. Если ξ∼p(x)\\\\xi \\\\sim p(x)ξ∼p(x), то обозначения Hξ\\\\mathbb H\\\\xiHξ и H[p]\\\\mathbb H[p]H[p] эквивалентны. Также отметим, что энтропию можно записать в виде математического ожидания:\\nH[p]=Eξ∼p(x)log\\u20611p(ξ).  \\\\mathbb H[p] = \\\\mathbb E_{\\\\xi \\\\sim p(x)} \\\\log\\\\frac 1{p(\\\\xi)}.\\nH[p]=Eξ∼p(x)\\u200blogp(ξ)1\\u200b.Пример. Найдём энтропию нормального распределения N(μ,σ2)\\\\mathcal N(\\\\mu, \\\\sigma^2)N(μ,σ2). Его плотность равна p(x)=12πσe−(x−μ)22σ2p(x) = \\\\frac 1{\\\\sqrt{2\\\\pi} \\\\sigma} e^{-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}}p(x)=2π\\u200bσ1\\u200be−2σ2(x−μ)2\\u200b, следовательно,\\nH[p]=∫−∞+∞p(x)(12ln\\u2061(2πσ2)+(x−μ)22σ2)dx=\\\\mathbb H[p] = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty}p(x)\\\\Big(\\\\frac 12\\\\ln\\\\big(2\\\\pi\\\\sigma^2\\\\big) + \\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}\\\\Big)dx =\\nH[p]=−∞∫+∞\\u200bp(x)(21\\u200bln(2πσ2)+2σ2(x−μ)2\\u200b)dx==12ln\\u2061(2πσ2)+12πσ∫−∞+∞(x−μ)22σ2e−(x−μ)22σ2\\u2009dx.= \\\\frac 12\\\\ln\\\\big(2\\\\pi\\\\sigma^2\\\\big)  + \\\\frac 1{\\\\sqrt{2\\\\pi} \\\\sigma}\\\\int\\\\limits_{-\\\\infty}^{+\\\\infty}\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}e^{-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}}\\\\,dx.\\n=21\\u200bln(2πσ2)+2π\\u200bσ1\\u200b−∞∫+∞\\u200b2σ2(x−μ)2\\u200be−2σ2(x−μ)2\\u200bdx.Делая в последнем интеграле замену t=−(x−μ)22σ2t = -\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}t=−2σ2(x−μ)2\\u200b, получаем, что\\nH[p]=12ln\\u2061(2πσ2)+12π∫0+∞2te−t\\u2009dt=12ln\\u2061(2πσ2)+Γ(32)π.\\\\mathbb H[p] = \\\\frac 12\\\\ln\\\\big(2\\\\pi\\\\sigma^2\\\\big) + \\\\frac 1{\\\\sqrt{2\\\\pi}}\\\\int\\\\limits_{0}^{+\\\\infty}\\\\sqrt{2t}e^{-t}\\\\,dt =\\\\frac 12\\\\ln\\\\big(2\\\\pi\\\\sigma^2\\\\big) + \\\\frac{\\\\Gamma\\\\big(\\\\frac 32\\\\big)}{\\\\sqrt \\\\pi}.\\nH[p]=21\\u200bln(2πσ2)+2π\\u200b1\\u200b0∫+∞\\u200b2t\\u200be−tdt=21\\u200bln(2πσ2)+π\\u200bΓ(23\\u200b)\\u200b.По свойству гамма-функции Γ(32)=12Γ(12)=π2\\\\Gamma\\\\big(\\\\frac 32\\\\big) = \\\\frac 12 \\\\Gamma\\\\big(\\\\frac 12\\\\big) = \\\\frac{\\\\sqrt \\\\pi}2Γ(23\\u200b)=21\\u200bΓ(21\\u200b)=2π\\u200b\\u200b. Таким образом,\\nH[p]=12ln\\u2061(2πσ2)+12.    \\\\mathbb H[p] = \\\\frac 12\\\\ln\\\\big(2\\\\pi\\\\sigma^2\\\\big) + \\\\frac 12.\\nH[p]=21\\u200bln(2πσ2)+21\\u200b.Как видно, энтропия гауссовского распределения N(μ,σ2)\\\\mathcal N(\\\\mu, \\\\sigma^2)N(μ,σ2) не ограничена ни сверху, ни снизу:\\nlim\\u2061σ→+∞12ln\\u2061(2πσ2)=+∞,lim\\u2061σ→0+12ln\\u2061(2πσ2)=−∞.    \\\\lim\\\\limits_{\\\\sigma\\\\to +\\\\infty} \\\\frac 12\\\\ln\\\\big(2\\\\pi\\\\sigma^2\\\\big) = +\\\\infty, \\\\quad\\n    \\\\lim\\\\limits_{\\\\sigma\\\\to 0+} \\\\frac 12\\\\ln\\\\big(2\\\\pi\\\\sigma^2\\\\big) = -\\\\infty.\\nσ→+∞lim\\u200b21\\u200bln(2πσ2)=+∞,σ→0+lim\\u200b21\\u200bln(2πσ2)=−∞.И да, в отличие от энтропии дискретного распределения дифференциальная энтропия может быть отрицательной. Это связано с тем, что плотность может принимать значения больше единицы, и поэтому математическое ожидание её логарифма с обратным знаком может оказаться меньше нуля. В частности, с нормальным распределением так происходит, если σ2<12πe\\\\sigma^2 < \\\\frac 1{2\\\\pi e}σ2<2πe1\\u200b.\\nУпражнение. Найдите энтропию показательного распределения Exp(λ)\\\\mathrm{Exp}(\\\\lambda)Exp(λ).\\nРешение (не открывайте сразу, попробуйте сначала решить самостоятельно)Плотность случайной величины ξ∼Exp(λ)\\\\xi \\\\sim \\\\mathrm{Exp}(\\\\lambda)ξ∼Exp(λ) равна pξ(x)=λe−λxp_{\\\\xi}(x) = \\\\lambda e^{-\\\\lambda x}pξ\\u200b(x)=λe−λx, x⩾0x \\\\geqslant 0x⩾0. Таким образом,\\nHξ=H[pξ]=∫0+∞pξ(x)(λx−log\\u2061λ)\\u2009dx=∫0+∞λ2xe−λx\\u2009dx−log\\u2061λ.    \\\\mathbb H\\\\xi = \\\\mathbb H[p_\\\\xi] = \\\\int\\\\limits_{0}^{+\\\\infty} p_{\\\\xi}(x)(\\\\lambda x - \\\\log \\\\lambda)\\\\, dx =\\n    \\\\int\\\\limits_{0}^{+\\\\infty} \\\\lambda^2 xe^{-\\\\lambda x}\\\\,dx - \\\\log\\\\lambda.\\nHξ=H[pξ\\u200b]=0∫+∞\\u200bpξ\\u200b(x)(λx−logλ)dx=0∫+∞\\u200bλ2xe−λxdx−logλ.С помощью замены t=λxt = \\\\lambda xt=λx находим, что\\nHξ=∫0+∞te−t\\u2009dt−log\\u2061λ=Γ(2)−log\\u2061λ=1−log\\u2061λ.    \\\\mathbb H\\\\xi = \\\\int\\\\limits_{0}^{+\\\\infty} te^{-t}\\\\,dt - \\\\log\\\\lambda = \\\\Gamma(2) - \\\\log\\\\lambda = 1 - \\\\log\\\\lambda.\\nHξ=0∫+∞\\u200bte−tdt−logλ=Γ(2)−logλ=1−logλ.KL-дивергенция\\nВ задачах машинного обучения истинное распределение p(x)p(x)p(x), из которого приходят наблюдения, обычно неизвестно, и его пытаются приблизить распределением q(x)q(x)q(x) из некоторого класса модельных распределений. Дивергенция Кульбака-Лейблера (KL-дивергенция, относительная энтропия) позволяет оценить расстояние между распределениями ppp и qqq:\\nKL(p∣∣q)=∑kpklog\\u2061pkqk\\\\mathbb{KL}(p\\\\vert\\\\vert q) = \\\\sum\\\\limits_k p_k\\\\log\\\\frac{p_k}{q_k}\\nKL(p∣∣q)=k∑\\u200bpk\\u200blogqk\\u200bpk\\u200b\\u200bв дискретном случае и\\nKL(p∣∣q)=∫p(x)log\\u2061p(x)q(x)dx\\\\mathbb{KL}(p\\\\vert\\\\vert q) = \\\\int p(x)\\\\log \\\\frac{p(x)}{q(x)}dx\\nKL(p∣∣q)=∫p(x)logq(x)p(x)\\u200bdxв непрерывном. KL-дивергенцию можно представить в виде разности:\\nKL(p∣∣q)=∫p(x)log\\u20611q(x)dx−∫p(x)log\\u20611p(x)dx=\\\\mathbb{KL}(p\\\\vert\\\\vert q) = \\\\int p(x)\\\\log \\\\frac 1{q(x)}dx - \\\\int p(x)\\\\log\\\\frac 1{p(x)}dx =\\nKL(p∣∣q)=∫p(x)logq(x)1\\u200bdx−∫p(x)logp(x)1\\u200bdx==Eξ∼p(x)1log\\u2061q(ξ)⏟кросс-энтропия−Eξ∼p(x)1log\\u2061p(ξ)⏟энтропия.=\\\\underbrace{\\\\mathbb E_{\\\\xi \\\\sim p(x)} \\\\frac 1{\\\\log q(\\\\xi)}}_{\\\\text{кросс-энтропия}} - \\\\underbrace{\\\\mathbb E_{\\\\xi \\\\sim p(x)} \\\\frac1{\\\\log p(\\\\xi)}}_{\\\\text{энтропия}}.\\n=кросс-энтропияEξ∼p(x)\\u200blogq(ξ)1\\u200b\\u200b\\u200b−энтропияEξ∼p(x)\\u200blogp(ξ)1\\u200b\\u200b\\u200b.Здесь вычитаемое – это уже знакомая нам энтропия распределения p(x)p(x)p(x), которая показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины ξ∼p(x)\\\\xi \\\\sim p(x)ξ∼p(x). Уменьшаемое носит название кросс-энтропии распределений p(x)p(x)p(x) и q(x)q(x)q(x).\\nКросс-энтропию можно интерпретировать как среднее число бит для кодирования значения случайной величины ξ∼p(x)\\\\xi \\\\sim p(x)ξ∼p(x) алгоритмом, оптимизированным для кодирования случайной величины η∼q(x)\\\\eta \\\\sim q(x)η∼q(x). Иными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя длина кодов для значений ppp, если при настройке алгоритма кодирования вместо ppp использовать qqq. Подробнее об этом вы можете почитать, например, в данном посте.\\nДивергенция Кульбака-Лейблера в некотором роде играет роль расстояния между распределениями. В частности, KL(p∣∣q)⩾0\\\\mathbb{KL}(p\\\\vert\\\\vert q)\\\\geqslant 0KL(p∣∣q)⩾0, причём дивергенция равна нулю, только если распределения совпадают почти всюду (для дискретных и непрерывных распределений это означает, что они просто тождественны). Но при этом она не является симметричной: вообще говоря, KL(p∣∣q)≠KL(q∣∣p)\\\\mathbb{KL}(p\\\\vert\\\\vert q)\\\\ne \\\\mathbb{KL}(q\\\\vert\\\\vert p)KL(p∣∣q)\\ue020=KL(q∣∣p).\\nУпражнение. Пользуясь неравенством ln\\u2061(1+t)⩽t\\\\ln (1+t) \\\\leqslant tln(1+t)⩽t, t>−1t > -1t>−1, докажите неотрицательность KL-дивергенции.\\nПопробуйте доказать самостоятельно, прежде чем смотреть решениеУказанное неравенство можно переписать в виде −ln\\u2061t⩾t−1-\\\\ln t \\\\geqslant t - 1−lnt⩾t−1, t>0t > 0t>0, поэтому\\nKL(p∣∣q)=−∫p(x)ln\\u2061q(x)p(x)dx⩾  \\\\mathbb{KL}(p\\\\vert\\\\vert q) = -\\\\int p(x)\\\\ln{\\\\frac{q(x)}{p(x)}}dx \\\\geqslant \\nKL(p∣∣q)=−∫p(x)lnp(x)q(x)\\u200bdx⩾⩾∫p(x)(q(x)p(x)−1)dx=∫q(x)dx−∫p(x)dx=0.  \\\\geqslant \\\\int p(x)\\\\Big(\\\\frac{q(x)}{p(x)} - 1\\\\Big)dx = \\\\int q(x)dx - \\\\int p(x)dx = 0.\\n⩾∫p(x)(p(x)q(x)\\u200b−1)dx=∫q(x)dx−∫p(x)dx=0.Пример. С помощью KL-дивергенции измерим расстояние между двумя гауссианами p(x)=N(x∣μ1,σ12)p(x) = \\\\mathcal N(x \\\\vert \\\\mu_1, \\\\sigma_1^2)p(x)=N(x∣μ1\\u200b,σ12\\u200b) и q(x)=N(x∣μ2,σ22)q(x) = \\\\mathcal N(x \\\\vert \\\\mu_2, \\\\sigma_2^2)q(x)=N(x∣μ2\\u200b,σ22\\u200b).\\nПодставляя явные выражения для плотностей\\np(x)=12πσ1e−(x−μ1)22σ12\\xa0и\\xa0q(x)=12πσ2e−(x−μ2)22σ22,p(x) = \\\\frac 1{\\\\sqrt{2\\\\pi}\\\\sigma_1} e^{-\\\\frac{(x-\\\\mu_1)^2}{2\\\\sigma_1^2}}\\\\text{ и }\\nq(x) = \\\\frac 1{\\\\sqrt{2\\\\pi}\\\\sigma_2} e^{-\\\\frac{(x-\\\\mu_2)^2}{2\\\\sigma_2^2}},\\np(x)=2π\\u200bσ1\\u200b1\\u200be−2σ12\\u200b(x−μ1\\u200b)2\\u200b\\xa0и\\xa0q(x)=2π\\u200bσ2\\u200b1\\u200be−2σ22\\u200b(x−μ2\\u200b)2\\u200b,находим\\nln\\u2061p(x)q(x)=ln\\u2061σ2σ1+(x−μ2)22σ22−(x−μ1)22σ12=\\\\ln \\\\frac{p(x)}{q(x)} = \\\\ln \\\\frac{\\\\sigma_2}{\\\\sigma_1} + \\\\frac{(x-\\\\mu_2)^2}{2\\\\sigma_2^2} -\\\\frac{(x-\\\\mu_1)^2}{2\\\\sigma_1^2} =\\nlnq(x)p(x)\\u200b=lnσ1\\u200bσ2\\u200b\\u200b+2σ22\\u200b(x−μ2\\u200b)2\\u200b−2σ12\\u200b(x−μ1\\u200b)2\\u200b==ln\\u2061σ2σ1+(12σ22−12σ12)(x−μ1)2+μ1−μ2σ22(x−μ1)+(μ1−μ2)22σ22.=\\\\ln \\\\frac{\\\\sigma_2}{\\\\sigma_1} + \\\\Big(\\\\frac 1{2\\\\sigma_2^2} - \\\\frac 1{2\\\\sigma_1^2}\\\\Big)(x-\\\\mu_1)^2 + \\\\frac{\\\\mu_1 - \\\\mu_2}{\\\\sigma_2^2}(x-\\\\mu_1) + \\\\frac{(\\\\mu_1 - \\\\mu_2)^2}{2\\\\sigma_2^2}.\\n=lnσ1\\u200bσ2\\u200b\\u200b+(2σ22\\u200b1\\u200b−2σ12\\u200b1\\u200b)(x−μ1\\u200b)2+σ22\\u200bμ1\\u200b−μ2\\u200b\\u200b(x−μ1\\u200b)+2σ22\\u200b(μ1\\u200b−μ2\\u200b)2\\u200b.Из свойств нормального распределения вытекает, что\\n∫−∞+∞p(x)(x−μ1)\\u2009dx=0,∫−∞+∞p(x)(x−μ1)2\\u2009dx=σ12.    \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty}p(x) (x-\\\\mu_1)\\\\,dx = 0, \\\\quad\\n    \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty}p(x)  (x-\\\\mu_1)^2\\\\,dx = \\\\sigma_1^2.\\n−∞∫+∞\\u200bp(x)(x−μ1\\u200b)dx=0,−∞∫+∞\\u200bp(x)(x−μ1\\u200b)2dx=σ12\\u200b.Таким образом,\\nKL(p∣∣q)=Eξ∼p(x)ln\\u2061p(ξ)q(ξ)=ln\\u2061σ2σ1+σ12+(μ1−μ2)22σ22−12.\\\\mathbb{KL}(p\\\\vert\\\\vert q) = \\\\mathbb E_{\\\\xi \\\\sim p(x)} \\\\ln \\\\frac{p(\\\\xi)}{q(\\\\xi)} = \\\\ln \\\\frac{\\\\sigma_2}{\\\\sigma_1} + \\\\frac{\\\\sigma_1^2 + (\\\\mu_1 - \\\\mu_2)^2}{2\\\\sigma_2^2} - \\\\frac 12.\\nKL(p∣∣q)=Eξ∼p(x)\\u200blnq(ξ)p(ξ)\\u200b=lnσ1\\u200bσ2\\u200b\\u200b+2σ22\\u200bσ12\\u200b+(μ1\\u200b−μ2\\u200b)2\\u200b−21\\u200b.Как и должно быть, полученное выражение равно нулю, если гауссианы совпадают. При равных дисперсиях σ12=σ22=σ2\\\\sigma_1^2 = \\\\sigma_2^2 = \\\\sigma^2σ12\\u200b=σ22\\u200b=σ2 получаем, что KL(p∣∣q)=(μ1−μ2)22σ2\\\\mathbb{KL}(p\\\\vert\\\\vert q) = \\\\frac{(\\\\mu_1 - \\\\mu_2)^2}{2\\\\sigma^2}KL(p∣∣q)=2σ2(μ1\\u200b−μ2\\u200b)2\\u200b. Это выражение остаётся прежним, если поменять местами μ1\\\\mu_1μ1\\u200b и μ2\\\\mu_2μ2\\u200b, поэтому в этом случае KL(p∣∣q)=KL(q∣∣p)\\\\mathbb{KL}(p\\\\vert\\\\vert q) = \\\\mathbb{KL}(q\\\\vert\\\\vert p)KL(p∣∣q)=KL(q∣∣p). Если же σ1≠σ2\\\\sigma_1 \\\\ne \\\\sigma_2σ1\\u200b\\ue020=σ2\\u200b, то выражение для KL(q∣∣p)\\\\mathbb{KL}(q\\\\vert\\\\vert p)KL(q∣∣p) явно отличается от KL(p∣∣q)\\\\mathbb{KL}(p\\\\vert\\\\vert q)KL(p∣∣q), что лишний раз показывает несимметричность KL-дивергенции.\\nУпражнение. Найдите дивергенцию Кульбака-Лейблера двух показательных распределений p(x)=Exp(x∣λ)p(x) = \\\\mathrm{Exp}(x \\\\vert \\\\lambda)p(x)=Exp(x∣λ) и q(x)=Exp(x∣μ)q(x) = \\\\mathrm{Exp}(x \\\\vert \\\\mu)q(x)=Exp(x∣μ).\\nПопробуйте решить самостоятельно, прежде чем смотреть решениеИмеем p(x)=λe−λxp(x) = \\\\lambda e^{-\\\\lambda x}p(x)=λe−λx, q(x)=μe−μxq(x) = \\\\mu e^{-\\\\mu x}q(x)=μe−μx, x⩾0x\\\\geqslant 0x⩾0, откуда\\nln\\u2061p(x)q(x)=ln\\u2061λμ+(μ−λ)x.\\\\ln \\\\frac{p(x)}{q(x)} =  \\\\ln \\\\frac \\\\lambda\\\\mu + (\\\\mu-\\\\lambda)x.\\nlnq(x)p(x)\\u200b=lnμλ\\u200b+(μ−λ)x.Следовательно,\\nKL(p∣∣q)=ln\\u2061λμ+(μ−λ)∫0+∞λxe−λx\\u2009dx=  \\\\mathbb{KL}(p\\\\vert\\\\vert q) = \\\\ln \\\\frac \\\\lambda\\\\mu + (\\\\mu-\\\\lambda)\\\\int\\\\limits_0^{+\\\\infty} \\\\lambda x e^{-\\\\lambda x}\\\\,dx =\\nKL(p∣∣q)=lnμλ\\u200b+(μ−λ)0∫+∞\\u200bλxe−λxdx==ln\\u2061λμ+μ−λλ∫0+∞te−tdt=ln\\u2061λμ+μλ−1.  =\\\\ln\\\\frac \\\\lambda\\\\mu + \\\\frac{\\\\mu-\\\\lambda}{\\\\lambda}\\\\int\\\\limits_0^{+\\\\infty} te^{-t}dt = \\\\ln \\\\frac \\\\lambda\\\\mu + \\\\frac \\\\mu \\\\lambda - 1.\\n=lnμλ\\u200b+λμ−λ\\u200b0∫+∞\\u200bte−tdt=lnμλ\\u200b+λμ\\u200b−1.И здесь KL-дивергенция равна нулю при λ=μ\\\\lambda = \\\\muλ=μ.\\nКросс-энтропия\\nПри определении KL-дивергенции мы уже встречались с кросс-энтропией\\nH[p,q]=−Eξ∼p(x)log\\u2061q(ξ)    \\\\mathbb H[p, q] = -\\\\mathbb E_{\\\\xi \\\\sim p(x)} \\\\log q(\\\\xi)\\nH[p,q]=−Eξ∼p(x)\\u200blogq(ξ)В зависимости от типа распределений кросс-энтропия вычисляется по формуле\\nH[p,q]=−∑kpklog\\u2061qk\\xa0или\\xa0H[p,q]=−∫−∞+∞p(x)log\\u2061q(x)\\u2009dx.    \\\\mathbb H[p, q] = -\\\\sum\\\\limits_k p_k \\\\log q_k \\\\text{ или  }\\n    \\\\mathbb H[p, q] = -\\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p(x) \\\\log q(x)\\\\, dx.\\nH[p,q]=−k∑\\u200bpk\\u200blogqk\\u200b\\xa0или\\xa0H[p,q]=−−∞∫+∞\\u200bp(x)logq(x)dx.Поскольку\\nKL(p∣∣q)=H[p,q]−H[p],    \\\\mathbb{KL}(p\\\\vert\\\\vert q) = \\\\mathbb H[p, q] - \\\\mathbb H[p],\\nKL(p∣∣q)=H[p,q]−H[p],задача минимизации KL-дивергенции между неизвестным распределением данных p(x)p(x)p(x) и модельным распределением q(x)q(x)q(x) эквивалентна задаче минимизации кросс-энтропии. Разница между ними равна энтропии распределения p(x)p(x)p(x), которая, очевидно, не зависит от q(x)q(x)q(x).\\nВ машинном обучении кросс-энтропию часто используют в качестве функции потерь в задаче классификации на K>1K>1K>1 классов. Истинное распределение на каждом обучающем объекте задаётся с помощью one hot encoding и является вырожденным:\\ny=(y1,…,yK),yk∈{0,1},∑k=1Kyk=1.\\\\boldsymbol y = (y_1, \\\\ldots, y_K), \\\\quad y_k \\\\in \\\\{0,1\\\\}, \\\\quad \\\\sum\\\\limits_{k=1}^K y_k = 1.\\ny=(y1\\u200b,…,yK\\u200b),yk\\u200b∈{0,1},k=1∑K\\u200byk\\u200b=1.Классификатор обычно выдаёт вероятности принадлежности каждому из классов,\\ny^=(y^1,…,y^K),y^k=P(класс\\xa0k).\\\\boldsymbol{\\\\widehat y} = (\\\\widehat y_1, \\\\ldots, \\\\widehat y_K), \\\\quad \\\\widehat y_k = \\\\mathbb P(\\\\text{класс }k).\\ny\\u200b=(y\\u200b1\\u200b,…,y\\u200bK\\u200b),y\\u200bk\\u200b=P(класс\\xa0k).Функция потерь на одном объекте полагается равной кросс-энтропии между истинным и предсказанным распределениями:\\nL(y,y^)=−∑k=1Kyklog\\u2061y^k.    \\\\mathcal L(\\\\boldsymbol y, \\\\boldsymbol {\\\\widehat y}) = -\\\\sum\\\\limits_{k=1}^K y_k \\\\log \\\\widehat y_k.\\nL(y,y\\u200b)=−k=1∑K\\u200byk\\u200blogy\\u200bk\\u200b.И это вполне логично: чем ближе модельное распределение к истинному, тем меньше наши потери. В идеале L(y,y^)=0\\\\mathcal L(\\\\boldsymbol y, \\\\boldsymbol {\\\\widehat y}) = 0L(y,y\\u200b)=0, если y=y^\\\\boldsymbol y =  \\\\boldsymbol {\\\\widehat y}y=y\\u200b.\\nЧтобы вычислить функцию потерь по обучающей выборке из NNN объектов с метками y(i)\\\\boldsymbol y^{(i)}y(i), обычно берут усреднённную кросс-энтропию\\n1N∑i=1NL(y(i),y^(i))=−1N∑i=1N∑k=1Kyk(i)log\\u2061y^k(i).    \\\\frac 1N \\\\sum\\\\limits_{i=1}^N\\\\mathcal L(\\\\boldsymbol y^{(i)}, \\\\boldsymbol{\\\\widehat y}^{(i)})\\n    =-\\\\frac 1N\\\\sum\\\\limits_{i=1}^N\\\\sum\\\\limits_{k=1}^K y^{(i)}_k \\\\log \\\\widehat y^{(i)}_k.\\nN1\\u200bi=1∑N\\u200bL(y(i),y\\u200b(i))=−N1\\u200bi=1∑N\\u200bk=1∑K\\u200byk(i)\\u200blogy\\u200bk(i)\\u200b.Принцип максимальной энтропии\\nВ параграфе про оценки параметров были описаны различные свойства параметрических оценок и методы их получения, например, метод моментов или метод максимального правдоподобия. В принципе, если мы уже выбрали для наших данных X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b некоторое параметрическое семейство Fθ(x)F_\\\\theta(x)Fθ\\u200b(x), моделирующее их распределение, восстановить его параметры чаще всего можно по выборочному среднему X‾n=1n∑k=1nXk\\\\overline{X}_n = \\\\frac{1}{n}\\\\sum\\\\limits_{k = 1}^n X_kXn\\u200b=n1\\u200bk=1∑n\\u200bXk\\u200b и/или выборочной дисперсии S‾n=1n∑k=1n(Xk−X‾n)2\\\\overline S_n = \\\\frac{1}{n} \\\\sum\\\\limits_{k = 1}^n (X_k - \\\\overline{X}_n)^2Sn\\u200b=n1\\u200bk=1∑n\\u200b(Xk\\u200b−Xn\\u200b)2.\\nА теперь представим, что мы посчитали эти (или какие-то другие) статистики, а семейство распределений пока не выбрали. Как же совершить этот судьбоносный выбор? Давайте посмотрим на следующие три семейства и подумаем, в каком из них мы бы стали искать распределение, зная его истинные матожидание и дисперсию?\\n\\nПочему-то хочется сказать, что в первом. Почему? Второе не симметрично – но что нас может заставить подозревать, что интересующее нас распределение не симметрично? С третьим проблема в том, что, выбирая его, мы добавляем дополнительную информацию как минимум о том, что у распределения конечный носитель. А с чего бы? У нас такой инфомации вроде бы нет.\\nОбщая идея такова: мы будем искать распределение, которое удовлетворяет только явно заданным нами ограничениям и не отражает никакого дополнительного знания о нём. Таким образом, искомое распределение должно обладать максимальной неопределённостью при заданных ограничениях, или, говоря более научно, иметь максимально возможную энтропию. В самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше – тем более «\\u200eпроизвольное» распределение, по крайней мере, в теории. В этом и заключается принцип максимальной энтропии для выбора модели машинного обучения.\\nКак мы уже видели выше, среди распределений с конечным носителем максимальную энтропию имеет равномерное распределение. Примеры геометрического и нормального распределения показывают, что энтропия распределений с бесконечным носителем (счётным или континуальным) может быть сколь угодно большой, и среди них нет какого-то одного распределения с максимальной энтропией. Однако в более узком классе распределений с фиксированным средним и/или дисперсией найти распределение с максимальной энтропией, как правило, можно.\\nПример. Покажем, что среди распределений на множестве натуральных чисел N\\\\mathbb NN и математическим ожиданием μ>1\\\\mu > 1μ>1 максимальную энтропию имеет геометрическое распределение.\\nДля минимизации энтропии H[p]=−∑n=1∞pnlog\\u2061pn\\\\mathbb H[p] = -\\\\sum\\\\limits_{n=1}^\\\\infty p_n \\\\log p_nH[p]=−n=1∑∞\\u200bpn\\u200blogpn\\u200b с учётом ограничений\\n∑n=1∞pn=1,∑n=1∞npn=μ\\\\sum\\\\limits_{n=1}^\\\\infty p_n = 1, \\\\quad \\\\sum\\\\limits_{n=1}^\\\\infty np_n = \\\\mu\\nn=1∑∞\\u200bpn\\u200b=1,n=1∑∞\\u200bnpn\\u200b=μвоспользуемся методом множителей Лагранжа, согласно которому требуется минимизировать функцию Лагранжа\\nL(p,a,b)=∑n=1∞pnlog\\u2061pn−a(∑n=1∞pn−1)−b(∑n=1∞npn−μ).    \\\\mathcal L(p, a, b) = \\\\sum\\\\limits_{n=1}^\\\\infty p_n \\\\log p_n - a\\\\Big(\\\\sum\\\\limits_{n=1}^\\\\infty p_n - 1\\\\Big) - b\\\\Big(\\\\sum\\\\limits_{n=1}^\\\\infty np_n - \\\\mu\\\\Big).\\nL(p,a,b)=n=1∑∞\\u200bpn\\u200blogpn\\u200b−a(n=1∑∞\\u200bpn\\u200b−1)−b(n=1∑∞\\u200bnpn\\u200b−μ).Приравняем к нулю частные производные по pnp_npn\\u200b:\\n∂L∂pn=1+log\\u2061pn−a−bn=0.    \\\\frac{\\\\partial \\\\mathcal L}{\\\\partial p_n} = 1 + \\\\log p_n - a - bn = 0.\\n∂pn\\u200b∂L\\u200b=1+logpn\\u200b−a−bn=0.Отсюда следует, что pn=ea−1+bn=αβnp_n = e^{a-1 + bn} = \\\\alpha \\\\beta^npn\\u200b=ea−1+bn=αβn, так что распределение действительно получается геометрическое. Параметры α\\\\alphaα и β\\\\betaβ найдём из уравнений\\n1=∑n=1∞pn=∑n=1∞αβn=αβ1−β,    1 = \\\\sum\\\\limits_{n=1}^\\\\infty p_n  = \\\\sum\\\\limits_{n=1}^\\\\infty \\\\alpha \\\\beta^n = \\\\frac{\\\\alpha\\\\beta}{1-\\\\beta},\\n1=n=1∑∞\\u200bpn\\u200b=n=1∑∞\\u200bαβn=1−βαβ\\u200b,μ=∑n=1∞npn=αβ∑n=1∞nβn−1=αβ(1−β)2.    \\\\mu = \\\\sum\\\\limits_{n=1}^\\\\infty np_n  = \\\\alpha \\\\beta\\\\sum\\\\limits_{n=1}^\\\\infty n \\\\beta^{n-1} = \\\\frac{\\\\alpha\\\\beta}{(1-\\\\beta)^2}.\\nμ=n=1∑∞\\u200bnpn\\u200b=αβn=1∑∞\\u200bnβn−1=(1−β)2αβ\\u200b.Деля первое уравнение на второе, получаем 1μ=1−β\\\\frac 1 \\\\mu = 1 - \\\\betaμ1\\u200b=1−β, или β=1−1μ\\\\beta = 1 - \\\\frac 1\\\\muβ=1−μ1\\u200b. Далее из первого уравнения находим α=1−ββ=1μ−1\\\\alpha = \\\\frac {1-\\\\beta}{\\\\beta} = \\\\frac 1{\\\\mu -1}α=β1−β\\u200b=μ−11\\u200b. Итак,\\npn=1μ−1(1−1μ)n=1μ(1−1μ)n−1,p_n = \\\\frac 1{\\\\mu -1} \\\\Big(1 - \\\\frac 1\\\\mu\\\\Big)^n = \\\\frac 1\\\\mu \\\\Big(1 - \\\\frac 1\\\\mu\\\\Big)^{n-1},\\npn\\u200b=μ−11\\u200b(1−μ1\\u200b)n=μ1\\u200b(1−μ1\\u200b)n−1,а это и есть геометрическое распределение с параметром 1μ\\\\frac 1\\\\muμ1\\u200b.\\nУ непрерывных распределений возможны более интересные комбинации из ограничений на носитель и параметры. И конечно же, первую скрипку среди распределений с максимальной энтропией играет гауссовское распределение.\\nПример. Докажем, что среди распределений на R\\\\mathbb RR c математическим ожиданием μ\\\\muμ и дисперсией σ2\\\\sigma^2σ2 наибольшую энтропию имеет нормальное распределение N(μ,σ2)\\\\mathcal{N}(\\\\mu,\\\\sigma^2)N(μ,σ2).\\nПусть p(x)p(x)p(x) – некоторое распределение со средним μ\\\\muμ и дисперсией σ2\\\\sigma^2σ2, q(x)∼N(μ,σ2)q(x)\\\\sim\\\\mathcal{N}(\\\\mu, \\\\sigma^2)q(x)∼N(μ,σ2). Как было показано выше, H[q]=12log\\u2061(2πσ2)+12\\\\mathbb H[q] = \\\\frac12\\\\log(2\\\\pi\\\\sigma^2) + \\\\frac12H[q]=21\\u200blog(2πσ2)+21\\u200b. Запишем дивергенцию Кульбака-Лейблера:\\nKL(p∣∣q)=∫−∞+∞p(x)log\\u2061p(x)dx−∫−∞+∞p(x)log\\u2061q(x)dx=\\\\mathbb KL(p\\\\vert\\\\vert q) = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p(x)\\\\log{p(x)}dx - \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p(x)\\\\log{q(x)}dx =\\nKL(p∣∣q)=−∞∫+∞\\u200bp(x)logp(x)dx−−∞∫+∞\\u200bp(x)logq(x)dx==−H[p]−∫−∞+∞p(x)(−12log\\u2061(2πσ2)−12σ2(x−μ)2)dx== -\\\\mathbb H[p] - \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p(x)\\\\left(-\\\\frac12\\\\log(2\\\\pi\\\\sigma^2) - \\\\frac 1{2\\\\sigma^2}(x - \\\\mu)^2\\\\right)dx =\\n=−H[p]−−∞∫+∞\\u200bp(x)(−21\\u200blog(2πσ2)−2σ21\\u200b(x−μ)2)dx==−H[p]+12log\\u2061(2πσ2)∫−∞+∞p(x)\\u2009dx+12σ2∫−∞+∞(x−μ)2p(x)\\u2009dx⏟=V[p]=σ2== - \\\\mathbb H[p] +\\\\frac12\\\\log(2\\\\pi\\\\sigma^2)\\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} p(x)\\\\,dx + \\\\frac1{2\\\\sigma^2}\\\\underbrace{\\\\int\\\\limits_{-\\\\infty}^{+\\\\infty}(x - \\\\mu)^2p(x)\\\\,dx}_{=\\\\mathbb{V}[p]=\\\\sigma^2} =\\n=−H[p]+21\\u200blog(2πσ2)−∞∫+∞\\u200bp(x)dx+2σ21\\u200b=V[p]=σ2−∞∫+∞\\u200b(x−μ)2p(x)dx\\u200b\\u200b==−H[p]+12log\\u2061(2πσ2)+12=H[q]−H[p].= - \\\\mathbb H[p] + \\\\frac12\\\\log(2\\\\pi\\\\sigma^2) + \\\\frac12 = \\\\mathbb H[q] - \\\\mathbb H[p].\\n=−H[p]+21\\u200blog(2πσ2)+21\\u200b=H[q]−H[p].Так как KL-дивергенция всегда неотрицательна, получаем, что H[p]⩽H[q]\\\\mathbb H[p]\\\\leqslant \\\\mathbb H[q]H[p]⩽H[q] при любом распределении ppp, удовлетворяющем заданным ограничениям.\\nМожно показать, что максимальную энтропию среди многомерных распределений с вектором средних μ\\\\boldsymbol \\\\muμ и матрицей ковариаций Σ\\\\boldsymbol \\\\SigmaΣ имеет также гауссовское распределение N(μ,Σ)\\\\mathcal N(\\\\boldsymbol \\\\mu, \\\\boldsymbol \\\\Sigma)N(μ,Σ).\\nУпражнение. Докажите, что среди распределений на отрезке [a,b][a,b][a,b] максимальную энтропию имеет равномерное распределение U[a,b]U[a,b]U[a,b].\\nПопробуйте доказать самостоятельно, прежде чем смотреть решение.Пусть q(x)∼U[a,b]q(x) \\\\sim U[a,b]q(x)∼U[a,b], тогда для произвольного распределения ppp на отрезке [a,b][a,b][a,b] имеем\\nH[p,q]=−∫abp(x)log\\u2061q(x)\\u2009dx=∫abp(x)log\\u2061(b−a)\\u2009dx=log\\u2061(b−a).  \\\\mathbb H[p,q] = - \\\\int\\\\limits_a^b p(x)\\\\log q(x)\\\\,dx = \\\\int\\\\limits_a^b p(x) \\\\log(b-a)\\\\,dx = \\\\log(b-a).\\nH[p,q]=−a∫b\\u200bp(x)logq(x)dx=a∫b\\u200bp(x)log(b−a)dx=log(b−a).В частности, отсюда следует, что H[q]=log\\u2061(b−a)\\\\mathbb H[q] = \\\\log(b-a)H[q]=log(b−a). Расписывая теперь KL-дивергенцию, получаем\\nKL(p∣∣q)=H[p,q]−H[p]=log\\u2061(b−a)−H[p]=H[q]−H[p]⩾0,\\\\mathbb{KL}(p\\\\vert\\\\vert q) = \\\\mathbb H[p,q] - \\\\mathbb H[p] = \\\\log(b-a) - \\\\mathbb H[p] = \\\\mathbb H[q] - \\\\mathbb H[p] \\\\geqslant 0,\\nKL(p∣∣q)=H[p,q]−H[p]=log(b−a)−H[p]=H[q]−H[p]⩾0,что и требовалось доказать.\\nУпражнение. Докажите, что среди распределений на промежутке [0,+∞)[0, +\\\\infty)[0,+∞) с математическим ожиданием λ>0\\\\lambda > 0λ>0 максимальную энтропию имеет показательное распределение Exp(1λ)\\\\mathrm{Exp\\\\big(\\\\frac 1\\\\lambda)}Exp(λ1\\u200b).\\nПопробуйте доказать самостоятельно, прежде чем смотреть решение.Согласно результату одного из предыдущих упражнений H[q]=log\\u2061λ+1\\\\mathbb H[q] = \\\\log \\\\lambda + 1H[q]=logλ+1, если q(x)∼Exp(1λ)q(x) \\\\sim \\\\mathrm{Exp}\\\\big(\\\\frac 1\\\\lambda)q(x)∼Exp(λ1\\u200b), т.е. q(x)=1λe−xλq(x) = \\\\frac 1\\\\lambda e^{-\\\\frac x\\\\lambda}q(x)=λ1\\u200be−λx\\u200b, x⩾0x\\\\geqslant 0x⩾0. Далее, для произвольного распределения p(x)p(x)p(x) на [0,+∞)[0, +\\\\infty)[0,+∞) со средним λ\\\\lambdaλ имеем\\nKL(p∣∣q)=H[p,q]−H[p]=−∫0+∞p(x)(−log\\u2061λ−xλ)\\u2009dx−H[p]=\\\\mathbb{KL}(p\\\\vert\\\\vert q) = \\\\mathbb H[p,q] - \\\\mathbb H[p] = - \\\\int\\\\limits_0^{+\\\\infty} p(x)\\\\Big(-\\\\log\\\\lambda - \\\\frac x\\\\lambda\\\\Big) \\\\,dx - \\\\mathbb H[p]=\\nKL(p∣∣q)=H[p,q]−H[p]=−0∫+∞\\u200bp(x)(−logλ−λx\\u200b)dx−H[p]==log\\u2061λ+1λ∫0+∞xp(x)\\u2009dx−H[p]=log\\u2061λ+1−H[p]=H[q]−H[p]⩾0.=\\\\log\\\\lambda + \\\\frac 1\\\\lambda \\\\int\\\\limits_0^{+\\\\infty} xp(x)\\\\, dx  - \\\\mathbb H[p] = \\\\log\\\\lambda + 1 - \\\\mathbb H[p] =  \\\\mathbb H[q]- \\\\mathbb H[p] \\\\geqslant 0.\\n=logλ+λ1\\u200b0∫+∞\\u200bxp(x)dx−H[p]=logλ+1−H[p]=H[q]−H[p]⩾0.Как выяснилось, многие классические распределения имеют максимальную энтропию при весьма естественных ограничениях. Но как быть, если даны не эти конкретные, а какие-то другие ограничения? Есть ли какой-нибудь надёжный алгоритм вывода распределения с максимальной энтропией, позволяющий избежать случайных озарений и гаданий на кофейной гуще? Оказывается, что при некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса.\\nЭкспоненциальное семейство распределений\\nГоворят, что параметрическое семейство распределений относится к экспоненциальному классу, если его pdf (или pmf) может быть представлена в виде\\np(x∣θ)=g(x)h(θ)exp\\u2061(θTu(x))=g(x)exp\\u2061(θTu(x)−A(θ)),p(x\\\\vert \\\\boldsymbol\\\\theta) = \\\\frac{g(x)}{h(\\\\boldsymbol\\\\theta)}\\\\exp(\\\\boldsymbol \\\\theta^T \\\\boldsymbol u(x)) = g(x)\\\\exp(\\\\boldsymbol \\\\theta^T \\\\boldsymbol u(x) - A(\\\\boldsymbol \\\\theta)),\\np(x∣θ)=h(θ)g(x)\\u200bexp(θTu(x))=g(x)exp(θTu(x)−A(θ)),где\\n\\nθ∈Rm\\\\boldsymbol\\\\theta \\\\in\\\\mathbb R^mθ∈Rm – вектор натуральных параметров распределения;\\ng(x)g(x)g(x) — неотрицательная функция (base measure), часто равная единице;\\nh(θ)>0h(\\\\boldsymbol\\\\theta) > 0h(θ)>0 — нормализатор (partition), обеспечивающий суммируемость pmf или интегрируемость pdf в единицу:\\n\\nh(θ)=∫g(x)exp\\u2061(θTu(x))dx;h(\\\\boldsymbol\\\\theta) = \\\\int g(x)\\\\exp(\\\\boldsymbol \\\\theta^T \\\\boldsymbol u(x))dx;\\nh(θ)=∫g(x)exp(θTu(x))dx;\\nA(θ)=ln\\u2061h(θ)A(\\\\boldsymbol \\\\theta) = \\\\ln h(\\\\boldsymbol\\\\theta)A(θ)=lnh(θ) — log-partition;\\nu(x)∈Rm\\\\boldsymbol u(x)\\\\in\\\\mathbb R^mu(x)∈Rm — вектор достаточных статистик распределения.\\n\\nПример. Покажем, что нормальное распределение N(x∣μ,σ2)\\\\mathcal N(x\\\\vert \\\\mu, \\\\sigma^2)N(x∣μ,σ2) принадлежит экспоненциальному классу. Оно имеет два параметра, поэтому такую же размерность имеют θ\\\\boldsymbol \\\\thetaθ и вектор-функция u\\\\boldsymbol uu.\\nРаспишем плотность:\\n12πσexp\\u2061(−(x−μ)22σ2)=12πσexp\\u2061(−12σ2x2+μσ2x+μ22σ2)=exp\\u2061(−12σ2x2+μσ2x)2πσexp\\u2061(−μ22σ2).\\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}\\\\exp\\\\left(-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}\\\\right) =\\n\\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}\\\\exp\\\\left(-\\\\frac 1{2\\\\sigma^2}x^2 + \\\\frac{\\\\mu}{\\\\sigma^2}x + \\\\frac{\\\\mu^2}{2\\\\sigma^2}\\\\right)=\\n\\\\frac{\\\\exp\\\\left(-\\\\frac1{2\\\\sigma^2}x^2 + \\\\frac{\\\\mu}{\\\\sigma^2}x\\\\right)}{\\\\sqrt{2\\\\pi}\\\\sigma\\\\exp\\\\left(-\\\\frac{\\\\mu^2}{2\\\\sigma^2}\\\\right)}.\\n2π\\u200bσ1\\u200bexp(−2σ2(x−μ)2\\u200b)=2π\\u200bσ1\\u200bexp(−2σ21\\u200bx2+σ2μ\\u200bx+2σ2μ2\\u200b)=2π\\u200bσexp(−2σ2μ2\\u200b)exp(−2σ21\\u200bx2+σ2μ\\u200bx)\\u200b.Положим g(x)=12πg(x) = \\\\frac 1{\\\\sqrt{2\\\\pi}}g(x)=2π\\u200b1\\u200b, u(x)=(x,x2)\\\\boldsymbol u(x) = (x, x^2)u(x)=(x,x2),\\nθ=(θ1,θ2),θ1=μσ2,θ2=−12σ2.\\\\boldsymbol \\\\theta = (\\\\theta_1, \\\\theta_2),\\\\quad \\\\theta_1 = \\\\frac{\\\\mu}{\\\\sigma^2},\\\\quad \\\\theta_2 = -\\\\frac1{2\\\\sigma^2}.\\nθ=(θ1\\u200b,θ2\\u200b),θ1\\u200b=σ2μ\\u200b,θ2\\u200b=−2σ21\\u200b.Остаётся выразить функцию h(θ)=σexp\\u2061(−μ22σ2)h(\\\\boldsymbol\\\\theta) = \\\\sigma\\\\exp\\\\left(-\\\\frac{\\\\mu^2}{2\\\\sigma^2}\\\\right)h(θ)=σexp(−2σ2μ2\\u200b) через θ1\\\\theta_1θ1\\u200b и θ2\\\\theta_2θ2\\u200b.\\nУпражнение. Выразите partition h(θ)h(\\\\boldsymbol\\\\theta)h(θ) и log-partition A(θ)A(\\\\boldsymbol\\\\theta)A(θ) через θ\\\\boldsymbol\\\\thetaθ и запишите плотность нормального распределения в экспоненциальном виде.\\nОтветh(θ)=−2θ2exp\\u2061(θ124θ2),A(θ)=−θ124θ2−12ln\\u2061(−2θ2),h(\\\\boldsymbol\\\\theta)=\\\\sqrt{-2\\\\theta_2} \\\\exp\\\\Big(\\\\frac{\\\\theta_1^2}{4\\\\theta_2}\\\\Big), \\\\quad  A(\\\\boldsymbol\\\\theta) = -\\\\frac{\\\\theta_1^2}{4\\\\theta_2} - \\\\frac 12\\\\ln(-2\\\\theta_2),\\nh(θ)=−2θ2\\u200b\\u200bexp(4θ2\\u200bθ12\\u200b\\u200b),A(θ)=−4θ2\\u200bθ12\\u200b\\u200b−21\\u200bln(−2θ2\\u200b),N(x∣μ,σ2)=12π⋅1h(θ)exp\\u2061(θTu(x))=12πexp\\u2061(θTu(x)−A(θ)).\\\\mathcal N(x \\\\vert \\\\mu, \\\\sigma^2) = \\\\frac1{\\\\sqrt {2\\\\pi}}\\\\cdot \\\\frac 1{h(\\\\boldsymbol\\\\theta)}\\\\exp(\\\\boldsymbol\\\\theta^T \\\\boldsymbol u(x)) = \\\\frac1{\\\\sqrt {2\\\\pi}} \\\\exp(\\\\boldsymbol \\\\theta^T \\\\boldsymbol u(x) - A(\\\\boldsymbol \\\\theta)).\\nN(x∣μ,σ2)=2π\\u200b1\\u200b⋅h(θ)1\\u200bexp(θTu(x))=2π\\u200b1\\u200bexp(θTu(x)−A(θ)).Пример. Покажем, что распределение Бернулли Bern(p)\\\\mathrm{Bern}(p)Bern(p) принадлежит экспоненциальному классу. Его pmf P(ξ=x∣p)\\\\mathbb P(\\\\xi =x \\\\vert p)P(ξ=x∣p) можно записать как\\npx(1−p)1−x=exp\\u2061(xlog\\u2061p+(1−x)log\\u2061(1−p))=(1−p)exp\\u2061(xlog\\u2061p1−p).  p^x(1 - p)^{1 - x} = \\\\exp\\\\big(x\\\\log{p} + (1 - x)\\\\log(1 - p)\\\\big)=\\n  (1-p) \\\\exp\\\\Big(x\\\\log\\\\frac p{1-p}\\\\Big).\\npx(1−p)1−x=exp(xlogp+(1−x)log(1−p))=(1−p)exp(xlog1−pp\\u200b).Параметр здесь один, поэтому натуральный параметр θ\\\\thetaθ тоже один: θ=log\\u2061p1−p\\\\theta = \\\\log\\\\frac p{1-p}θ=log1−pp\\u200b. Такая функция от ppp называется функцией логитов и активно участвует в построении модели логистической регрессии. Остальные функции положим равными u(x)=xu(x) = xu(x)=x, g(x)=1g(x)=1g(x)=1, h(θ)=11−ph(\\\\theta) = \\\\frac 1{1-p}h(θ)=1−p1\\u200b. Остаётся выразить partition через θ\\\\thetaθ:\\nlog\\u2061p1−p=log\\u2061(−1+11−p)=θ\\u2005\\u200a⟺\\u2005\\u200a11−p=1+eθ.  \\\\log\\\\frac p{1-p} = \\\\log\\\\Big(-1 + \\\\frac 1{1-p}\\\\Big) = \\\\theta \\\\iff \\\\frac 1{1-p} = 1 + e^\\\\theta. \\nlog1−pp\\u200b=log(−1+1−p1\\u200b)=θ⟺1−p1\\u200b=1+eθ.Итак, h(θ)=1+eθh(\\\\theta) = 1+e^\\\\thetah(θ)=1+eθ, и экспоненциальный вид распределения Бернулли записывается как\\n11+eθexp\\u2061(θx)=exp\\u2061(θx−log\\u2061(1+eθ)).  \\\\frac 1{1+e^\\\\theta}\\\\exp(\\\\theta x) = \\\\exp\\\\big(\\\\theta x - \\\\log(1 + e^\\\\theta)\\\\big).\\n1+eθ1\\u200bexp(θx)=exp(θx−log(1+eθ)).Вопрос на подумать. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках U[a,b]U[a, b]U[a,b]? Казалось бы, да, ведь\\np(x)=1b−aI[a,b](x)exp\\u2061(0).p(x) = \\\\frac{1}{b - a}\\\\mathbb{I}_{[a,b]}(x)\\\\exp(0).\\np(x)=b−a1\\u200bI[a,b]\\u200b(x)exp(0).В чём может быть подвох?\\nПопробуйте определить сами, прежде чем смотреть ответ.Нет, не принадлежит. Давайте вспомним, как звучало определение экспоненциального семейства. Возможно, вас удивило, что там было написано не «распределение относится», а «семейство распределений относится». Это важно: ведь семейство определяется именно различными значениями θ\\\\thetaθ, и если нас интересует семейство равномерных распределений на отрезках, определяемое параметрами aaa и bbb, то они не могут быть в функции g(x)g(x)g(x), они должны быть под экспонентой, а экспонента ни от чего не может быть равна индикатору.\\nПри этом странное и не очень полезное семейство с нулём параметров, состоящее из одинокого распределения U[0,1]U[0,1]U[0,1] можно считать относящимся к экспоненциальному классу: ведь для него формула\\np(x)=I[0,1](x)exp\\u2061(0)p(x) = \\\\mathbb{I}_{[0,1]}(x)\\\\exp(0)\\np(x)=I[0,1]\\u200b(x)exp(0)будет работать.\\nК экспоненциальным семействам относятся многие непрерывные и дискретные распределения из часто встречающихся в теории и на практике, в том числе\\n\\nнормальное N(μ,σ2)\\\\mathcal N(\\\\mu, \\\\sigma^2)N(μ,σ2);\\nраспределение Пуассона Pois(λ)\\\\mathrm{Pois}(\\\\lambda)Pois(λ);\\nэкспоненциальное Exp(λ)\\\\mathrm{Exp}(\\\\lambda)Exp(λ);\\nбиномиальное Bin(n,p)\\\\mathrm{Bin}(n, p)Bin(n,p);\\nгеометрическое Geom(p)\\\\mathrm{Geom}(p)Geom(p);\\nбета-распределение;\\nгамма-распределение;\\nраспределение Дирихле.\\n\\nКак выглядят натуральные параметры, достаточные статистики и нормализаторы этих и других распределений из экспоненциального класса, можно посмотреть на википедии.\\nК экспоненциальным семействам не относятся, например, равномерное распределение U[a,b]U[a,b]U[a,b], ttt-распределение Стьюдента, распределение Коши, смесь нормальных распределений.\\nДифференцирование log-partition\\nЕсли распределение p(x∣θ)p(x\\\\vert \\\\boldsymbol\\\\theta)p(x∣θ) принадлежит экспоненциальному классу,\\np(x∣θ)=g(x)h(θ)exp\\u2061(θTu(x))=g(x)exp\\u2061(θTu(x)−A(θ)),p(x\\\\vert \\\\boldsymbol\\\\theta) = \\\\frac{g(x)}{h(\\\\boldsymbol\\\\theta)}\\\\exp(\\\\boldsymbol \\\\theta^T \\\\boldsymbol u(x)) = g(x)\\\\exp(\\\\boldsymbol \\\\theta^T \\\\boldsymbol u(x) - A(\\\\boldsymbol \\\\theta)),\\np(x∣θ)=h(θ)g(x)\\u200bexp(θTu(x))=g(x)exp(θTu(x)−A(θ)),то моменты его достаточных статистик u(x)\\\\boldsymbol u(x)u(x) могут быть получены дифференцированием функции A(θ)=log\\u2061h(θ)A(\\\\boldsymbol \\\\theta) = \\\\log h(\\\\boldsymbol\\\\theta)A(θ)=logh(θ).\\nУтверждение. ∇θlog\\u2061h(θ)=Eξ∼p(x∣θ)u(ξ)\\\\nabla_{\\\\boldsymbol\\\\theta}\\\\log h(\\\\boldsymbol \\\\theta) = \\\\mathbb{E}_{\\\\xi \\\\sim p(x\\\\vert \\\\boldsymbol \\\\theta)} \\\\boldsymbol u(\\\\xi)∇θ\\u200blogh(θ)=Eξ∼p(x∣θ)\\u200bu(ξ).\\nДоказательство.\\nПо правилу дифференцирования сложной функции имеем\\n∇θlog\\u2061h(θ)=∇θh(θ)h(θ).    \\\\nabla_{\\\\boldsymbol\\\\theta}\\\\log{h(\\\\boldsymbol\\\\theta)} = \\\\frac{\\\\nabla_{\\\\boldsymbol\\\\theta}{h(\\\\boldsymbol\\\\theta)}}{h(\\\\boldsymbol\\\\theta)}.\\n∇θ\\u200blogh(θ)=h(θ)∇θ\\u200bh(θ)\\u200b.Нормализатор h(θ)h(\\\\boldsymbol\\\\theta)h(θ) записывается в виде интеграла\\nh(θ)=∫g(x)exp\\u2061(θTu(x))dx,  h(\\\\boldsymbol\\\\theta) = \\\\int g(x)\\\\exp\\\\left(\\\\boldsymbol\\\\theta^T\\\\boldsymbol u(x)\\\\right)dx,\\nh(θ)=∫g(x)exp(θTu(x))dx,который мы продифференцируем внесением градиента внутрь под знак интеграла:\\n∇θh(θ)=∇θ∫g(x)exp\\u2061(θTu(x))dx=\\\\nabla_{\\\\boldsymbol\\\\theta} h(\\\\boldsymbol\\\\theta) = \\\\nabla_{\\\\boldsymbol\\\\theta} \\n \\\\int g(x)\\\\exp\\\\left(\\\\boldsymbol\\\\theta^T\\\\boldsymbol u(x)\\\\right)dx =\\n∇θ\\u200bh(θ)=∇θ\\u200b∫g(x)exp(θTu(x))dx==∫g(x)∇θexp\\u2061(θTu(x))dx=∫g(x)u(x)exp\\u2061(θTu(x))dx. =\\\\int g(x)\\\\nabla_{\\\\boldsymbol\\\\theta}  \\n \\\\exp\\\\left(\\\\boldsymbol\\\\theta^T\\\\boldsymbol u(x)\\\\right)dx = \\n \\\\int g(x)\\\\boldsymbol u(x)\\n \\\\exp\\\\big(\\\\boldsymbol\\\\theta^T\\\\boldsymbol u(x)\\\\big)dx.\\n=∫g(x)∇θ\\u200bexp(θTu(x))dx=∫g(x)u(x)exp(θTu(x))dx.Таким образом,\\n∇θh(θ)h(θ)=∫u(x)g(x)h(θ)exp\\u2061(θTu(x))⏟p(x∣θ)dx=Eξ∼p(x∣θ)u(ξ).  \\\\frac{\\\\nabla_{\\\\boldsymbol\\\\theta}{h(\\\\boldsymbol\\\\theta)}}{h(\\\\boldsymbol\\\\theta)}\\n    =\\\\int \\\\boldsymbol u(x)\\\\underbrace{\\\\frac{g(x)}{h(\\\\boldsymbol \\\\theta)}\\\\exp(\\\\boldsymbol \\\\theta^T\\\\boldsymbol u(x))}_{p(x\\\\vert\\\\boldsymbol \\\\theta)}dx = \\\\mathbb{E}_{\\\\xi \\\\sim p(x\\\\vert \\\\boldsymbol \\\\theta)} \\\\boldsymbol u(\\\\xi).\\nh(θ)∇θ\\u200bh(θ)\\u200b=∫u(x)p(x∣θ)h(θ)g(x)\\u200bexp(θTu(x))\\u200b\\u200bdx=Eξ∼p(x∣θ)\\u200bu(ξ).Замечание о дифференцировании под знаком интеграломИспользованный в доказательстве приём внесения градиента под знак интеграла называют также правилом Лейбница. Этот же метод используется для почленного дифференцирования ряда, что может быть полезно в случае дискретного распределения p(x∣θ)p(x\\\\vert \\\\boldsymbol\\\\theta)p(x∣θ). В математическом анализе имеется ряд теорем, обеспечивающих применимость правила Лейбница, однако, мы не будем на них останавливаться. Будем считать, что все рассматриваемые экспоненциальные семейства таковы, что применение правила Лейбница законно.\\nЕсли ui(x)=xiu_i(x) = x^iui\\u200b(x)=xi, то в соответствии с только что доказанным частная производная ∂A(θ)∂θi\\\\frac{\\\\partial A(\\\\boldsymbol \\\\theta)}{\\\\partial \\\\theta_i}∂θi\\u200b∂A(θ)\\u200b даёт iii-й момент распределения p(x∣θ)p(x\\\\vert \\\\boldsymbol\\\\theta)p(x∣θ).\\nУпражнение. Вычислите производные по натуральным параметрам от log-partition для распределения Бернулли Bern(x∣p)\\\\mathrm{Bern}(x\\\\vert p)Bern(x∣p) и нормального распределения N(μ,σ2)\\\\mathcal N(\\\\mu, \\\\sigma^2)N(μ,σ2) и проверьте, что они совпадают со значениями соответствующих моментов.\\nРешениеДля ξ∼Bern(x∣p)\\\\xi \\\\sim \\\\mathrm{Bern}(x\\\\vert p)ξ∼Bern(x∣p) имеем\\nA(θ)=log\\u2061(1+eθ),11−p=1+eθ,  A(\\\\theta) = \\\\log(1 + e^\\\\theta), \\\\quad \\\\frac 1{1-p} = 1 + e^\\\\theta,\\nA(θ)=log(1+eθ),1−p1\\u200b=1+eθ,поэтому\\nA′(θ)=eθ1+eθ=1−11+eθ=1−1+p=p=Eξ.  A'(\\\\theta) = \\\\frac{e^\\\\theta}{1+e^\\\\theta} = 1 - \\\\frac 1{1+e^\\\\theta} = 1 -1 + p = p = \\\\mathbb E\\\\xi.\\nA′(θ)=1+eθeθ\\u200b=1−1+eθ1\\u200b=1−1+p=p=Eξ.Для гауссовской случайной величины η∼N(x∣μ,σ2)\\\\eta\\\\sim\\\\mathcal N(x \\\\vert\\\\mu, \\\\sigma^2)η∼N(x∣μ,σ2) получаем\\nA(θ1,θ2)=−θ124θ2−12ln\\u2061(−2θ2),  A(\\\\theta_1, \\\\theta_2) =  -\\\\frac{\\\\theta_1^2}{4\\\\theta_2} - \\\\frac 12\\\\ln(-2\\\\theta_2),\\nA(θ1\\u200b,θ2\\u200b)=−4θ2\\u200bθ12\\u200b\\u200b−21\\u200bln(−2θ2\\u200b),θ1=μσ2,  \\\\quad \\\\theta_1 = \\\\frac{\\\\mu}{\\\\sigma^2},\\nθ1\\u200b=σ2μ\\u200b,θ2=−12σ2,  \\\\quad \\\\theta_2 = -\\\\frac1{2\\\\sigma^2},\\nθ2\\u200b=−2σ21\\u200b,и, значит,\\n∂A∂θ1=−θ12θ2=μ=Eη,  \\\\frac{\\\\partial A}{\\\\partial\\\\theta_1} = -\\\\frac{\\\\theta_1}{2\\\\theta_2} = \\\\mu = \\\\mathbb E\\\\eta,\\n∂θ1\\u200b∂A\\u200b=−2θ2\\u200bθ1\\u200b\\u200b=μ=Eη,∂A∂θ2=θ124θ22−12θ2=μ2+σ2=Eη2.  \\\\frac{\\\\partial A}{\\\\partial\\\\theta_2} = \\\\frac{\\\\theta_1^2}{4\\\\theta_2^2} - \\\\frac 1{2\\\\theta_2} = \\\\mu^2 + \\\\sigma^2 = \\\\mathbb E\\\\eta^2.\\n∂θ2\\u200b∂A\\u200b=4θ22\\u200bθ12\\u200b\\u200b−2θ2\\u200b1\\u200b=μ2+σ2=Eη2.Кстати, можно продифференцировать ещё раз и доказать, что\\n∇θ2log\\u2061h(θ)=cov(u(ξ),u(ξ)).  \\\\nabla^2_{\\\\boldsymbol\\\\theta}\\\\log{h(\\\\boldsymbol\\\\theta)} = \\\\mathrm{cov}(\\\\boldsymbol u(\\\\xi), \\\\boldsymbol u(\\\\xi)).\\n∇θ2\\u200blogh(θ)=cov(u(ξ),u(ξ)).ДоказательствоВ предыдущий раз мы доказали, что\\n∇θA(θ)=∫u(x)g(x)exp\\u2061(θTu(x)−A(θ))\\u2009dx.\\\\nabla_{\\\\boldsymbol\\\\theta}A(\\\\boldsymbol\\\\theta) = \\\\int \\\\boldsymbol u(x)g(x)\\\\exp(\\\\boldsymbol \\\\theta^T\\\\boldsymbol u(x) - A(\\\\boldsymbol \\\\theta))\\\\,dx.\\n∇θ\\u200bA(θ)=∫u(x)g(x)exp(θTu(x)−A(θ))dx.Теперь возьмём ещё раз градиент по θ\\\\boldsymbol \\\\thetaθ от этого выражения:\\n∇θ2A(θ)=∫u(x)g(x)∇θexp\\u2061(θTu(x)−A(θ))\\u2009dx=\\\\nabla^2_{\\\\boldsymbol\\\\theta}A(\\\\boldsymbol\\\\theta) = \\\\int \\\\boldsymbol u(x)g(x)\\\\nabla_{\\\\boldsymbol\\\\theta}\\\\exp\\\\big(\\\\boldsymbol \\\\theta^T\\\\boldsymbol u(x) - A(\\\\boldsymbol \\\\theta)\\\\big)\\\\,dx\\n=\\n∇θ2\\u200bA(θ)=∫u(x)g(x)∇θ\\u200bexp(θTu(x)−A(θ))dx==∫u(x)g(x)(u(x)−∇θA(θ))Texp\\u2061(θTu(x)−A(θ))\\u2009dx== \\\\int \\\\boldsymbol u(x)g(x)\\\\big(\\\\boldsymbol u(x) - \\\\nabla_{\\\\boldsymbol\\\\theta}A(\\\\boldsymbol \\\\theta)\\\\big)^T \\\\exp\\\\big(\\\\boldsymbol \\\\theta^T\\\\boldsymbol u(x) - A(\\\\boldsymbol \\\\theta)\\\\big)\\\\,dx=\\n=∫u(x)g(x)(u(x)−∇θ\\u200bA(θ))Texp(θTu(x)−A(θ))dx==∫u(x)g(x)(u(x)−Eu)Texp\\u2061(θTu(x)−A(θ))\\u2009dx==\\\\int \\\\boldsymbol u(x)g(x)\\\\big(\\\\boldsymbol u(x) -  \\\\mathbb{E} \\\\boldsymbol u\\\\big)^T \\\\exp\\\\big(\\\\boldsymbol \\\\theta^T\\\\boldsymbol u(x) - A(\\\\boldsymbol \\\\theta)\\\\big)\\\\,dx =\\n=∫u(x)g(x)(u(x)−Eu)Texp(θTu(x)−A(θ))dx==EuuT−Eu(Eu)T=cov(u(ξ),u(ξ)).= \\\\mathbb{E} \\\\boldsymbol u \\\\boldsymbol u^T - \\\\mathbb{E} \\\\boldsymbol u \\\\big(\\\\mathbb{E} \\\\boldsymbol u\\\\big)^T = \\\\mathrm{cov}(\\\\boldsymbol u(\\\\xi), \\\\boldsymbol u(\\\\xi)).\\n=EuuT−Eu(Eu)T=cov(u(ξ),u(ξ)).В последней выкладке для краткости обозначено Eu=Eξ∼p(x∣θ)u(ξ)\\\\mathbb E \\\\boldsymbol u = \\\\mathbb{E}_{\\\\xi \\\\sim p(x\\\\vert \\\\boldsymbol \\\\theta)} \\\\boldsymbol u(\\\\xi)Eu=Eξ∼p(x∣θ)\\u200bu(ξ).\\nMLE для семейства из экспоненциального класса\\nВозможно, вас удивил странный и на первый взгляд не очень естественный вид p(x∣θ)p(x\\\\vert \\\\boldsymbol \\\\theta)p(x∣θ). Но всё не просто так: оказывается, что оценка максимального правдоподобия параметров распределений из экспоненциального класса устроена очень интригующе.\\nЗапишем функцию правдоподобия i.i.d. выборки x1,…,xnx_1,\\\\ldots,x_nx1\\u200b,…,xn\\u200b:\\np(x1,…,xn∣θ)=h(θ)−n⋅(∏i=1ng(xi))⋅exp\\u2061(θT∑i=1nu(xi)).p(x_1, \\\\ldots, x_n\\\\vert\\\\boldsymbol \\\\theta) = h(\\\\boldsymbol \\\\theta)^{-n}\\\\cdot\\\\bigg(\\\\prod_{i=1}^ng(x_i)\\\\bigg)\\\\cdot\\\\exp\\\\bigg(\\\\boldsymbol \\\\theta^T \\\\sum_{i=1}^n \\\\boldsymbol u(x_i)\\\\bigg).\\np(x1\\u200b,…,xn\\u200b∣θ)=h(θ)−n⋅(i=1∏n\\u200bg(xi\\u200b))⋅exp(θTi=1∑n\\u200bu(xi\\u200b)).Её логарифм равен\\nlog\\u2061p(x1,…,xn∣θ)=−nlog\\u2061h(θ)+∑i=1nlog\\u2061g(xi)+θT∑i=1nu(xi).\\\\log p(x_1, \\\\ldots, x_n\\\\vert\\\\boldsymbol \\\\theta) = -n\\\\log{h(\\\\boldsymbol \\\\theta)} + \\\\sum_{i=1}^n\\\\log{g(x_i)} + \\\\boldsymbol \\\\theta^T\\\\sum_{i=1}^n \\\\boldsymbol u(x_i).\\nlogp(x1\\u200b,…,xn\\u200b∣θ)=−nlogh(θ)+i=1∑n\\u200blogg(xi\\u200b)+θTi=1∑n\\u200bu(xi\\u200b).Дифференцируя по θ\\\\boldsymbol \\\\thetaθ, получаем\\n∇θlog\\u2061p(x1,…,xn∣θ)=−n∇θlog\\u2061h(θ)+∑i=1nu(xi).\\\\nabla_{\\\\boldsymbol\\\\theta}\\\\log p(x_1, \\\\ldots, x_n\\\\vert\\\\boldsymbol\\\\theta) = -n\\\\nabla_{\\\\boldsymbol\\\\theta}\\\\log{h(\\\\boldsymbol\\\\theta)} + \\\\sum_{i=1}^n \\\\boldsymbol u(x_i).\\n∇θ\\u200blogp(x1\\u200b,…,xn\\u200b∣θ)=−n∇θ\\u200blogh(θ)+i=1∑n\\u200bu(xi\\u200b).Приравнивая ∇θlog\\u2061p(x1,…,xn∣θ)\\\\nabla_{\\\\boldsymbol\\\\theta}\\\\log p(x_1,\\\\ldots, x_n\\\\vert\\\\boldsymbol\\\\theta)∇θ\\u200blogp(x1\\u200b,…,xn\\u200b∣θ) к нулю и пользуясь равенством ∇θlog\\u2061h(θ)=Eξ∼p(x∣θ)u(ξ)\\\\nabla_{\\\\boldsymbol\\\\theta}\\\\log{h(\\\\boldsymbol\\\\theta)} = \\\\mathbb{E}_{\\\\xi \\\\sim p(x\\\\vert \\\\boldsymbol \\\\theta)} \\\\boldsymbol u(\\\\xi)∇θ\\u200blogh(θ)=Eξ∼p(x∣θ)\\u200bu(ξ), находим\\nEξ∼p(x∣θ)u(ξ)=1n∑i=1nu(xi).\\\\mathbb{E}_{\\\\xi \\\\sim p(x\\\\vert \\\\boldsymbol \\\\theta)} \\\\boldsymbol u(\\\\xi) = \\\\frac1n \\\\sum_{i=1}^n \\\\boldsymbol u(x_i).\\nEξ∼p(x∣θ)\\u200bu(ξ)=n1\\u200bi=1∑n\\u200bu(xi\\u200b).Таким образом, теоретические матожидания всех компонент ui(ξ)u_i(\\\\xi)ui\\u200b(ξ) должны совпадать с их эмпирическими оценками, а метод максимального правдоподобия совпадает с методом моментов для Eui(ξ)\\\\mathbb{E}u_i(\\\\xi)Eui\\u200b(ξ) в качестве моментов. И в следующем пункте выяснится, что распределения из экспоненциальных семейств обладают максимальной энтропией среди тех, что имеют заданные моменты Eui(ξ)\\\\mathbb{E}u_i(\\\\xi)Eui\\u200b(ξ).\\nТеорема Купмана—Питмана—Дармуа\\nТеперь мы наконец готовы сформулировать одно из самых любопытных свойств семейств экспоненциального класса.\\nВ следующей теореме мы опустим некоторые не очень обременительные условия регулярности. Просто считайте, что для хороших дискретных и абсолютно непрерывных распределений, с которыми вы в основном и будете сталкиваться, это так.\\nТеорема. Пусть параметр θ∈Rm\\\\boldsymbol\\\\theta\\\\in R^mθ∈Rm распределения pθ(x)=1h(θ)exp\\u2061(θTu(x))p_{\\\\boldsymbol\\\\theta}(x) = \\\\frac1{h(\\\\boldsymbol\\\\theta)}\\\\exp\\\\big(\\\\boldsymbol\\\\theta^T\\\\boldsymbol u(x)\\\\big)pθ\\u200b(x)=h(θ)1\\u200bexp(θTu(x)) выбран так, что\\nEξ∼pθ(x)u(ξ)=α\\\\mathbb{E}_{\\\\xi \\\\sim p_{\\\\boldsymbol\\\\theta}(x)} \\\\boldsymbol u(\\\\xi) = \\\\boldsymbol\\\\alpha\\nEξ∼pθ\\u200b(x)\\u200bu(ξ)=αдля некоторого фиксированного α∈Rm\\\\boldsymbol \\\\alpha \\\\in\\\\mathbb R^mα∈Rm. Тогда распределение pθ(x)p_{\\\\boldsymbol\\\\theta}(x)pθ\\u200b(x) обладает наибольшей энтропией среди распределений qqq с тем же носителем, для которых Eξ∼q(x)u(ξ)=α\\\\mathbb{E}_{\\\\xi \\\\sim q(x)} \\\\boldsymbol u(\\\\xi) = \\\\boldsymbol\\\\alphaEξ∼q(x)\\u200bu(ξ)=α.\\nИдея обоснования через оптимизацию.Мы приведём рассуждение для дискретного случая; в абсолютно непрерывном рассуждения будут по сути теми же, только там придётся дифференцировать не по переменных, а по функциям, и мы решили не ввергать читателя в мир вариационного исчисления.\\nВ дискретном случае у нас есть счётное семейство точек x1,x2,…x_1, x_2,\\\\ldotsx1\\u200b,x2\\u200b,…, и распределение определяется счётным набором вероятностей pip_ipi\\u200b принимать значение xix_ixi\\u200b. Мы будем решать задачу\\n{−∑jpjlog\\u2061pj⟶max\\u2061,∑jpjui(xj)=αi,i=1,…,m,∑jpj=1,pj⩾0.\\\\begin{cases}\\n-\\\\sum_j p_j\\\\log{p_j}\\\\longrightarrow\\\\max,\\\\\\\\\\n\\\\sum_jp_ju_i(x_j) = \\\\alpha_i, i = 1,\\\\ldots,m,\\\\\\\\\\n\\\\sum_jp_j = 1,\\\\\\\\\\np_j\\\\geqslant 0.\\n\\\\end{cases}\\n⎩⎨⎧\\u200b−∑j\\u200bpj\\u200blogpj\\u200b⟶max,∑j\\u200bpj\\u200bui\\u200b(xj\\u200b)=αi\\u200b,i=1,…,m,∑j\\u200bpj\\u200b=1,pj\\u200b⩾0.\\u200bДля решения этой оптимизационной задачи нам понадобится обобщение метода множителей Лагранжа, известное также как теорема Каруша—Куна—Таккера. В данном случае задача сводится к минимизации лагранжиана\\nL=∑jpjlog\\u2061pj+∑iθi(αi−∑jpjui(xj))+θ0(∑jpj−1)−∑jλjpj\\\\mathcal{L} = \\\\sum_j p_j\\\\log{p_j} + \\\\sum_i\\\\theta_i\\\\left(\\\\alpha_i - \\\\sum_jp_ju_i(x_j)\\\\right)+\\\\theta_0\\\\left(\\\\sum_jp_j - 1\\\\right) - \\\\sum_j\\\\lambda_jp_j\\nL=j∑\\u200bpj\\u200blogpj\\u200b+i∑\\u200bθi\\u200b(αi\\u200b−j∑\\u200bpj\\u200bui\\u200b(xj\\u200b))+θ0\\u200b(j∑\\u200bpj\\u200b−1)−j∑\\u200bλj\\u200bpj\\u200bпри имеющихся ограничениях и условиях дополняющей нежёсткости λjpj=0\\\\lambda_jp_j = 0λj\\u200bpj\\u200b=0.\\nПриравняем частные производные по pjp_jpj\\u200b к нулю:\\n∂L∂pj=log\\u2061pj+1−∑iθiui(xj)+θ0−λj=0.\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial p_j} = \\\\log{p_j} + 1 - \\\\sum_i\\\\theta_iu_i(x_j) + \\\\theta_0 - \\\\lambda_j = 0.\\n∂pj\\u200b∂L\\u200b=logpj\\u200b+1−i∑\\u200bθi\\u200bui\\u200b(xj\\u200b)+θ0\\u200b−λj\\u200b=0.Отсюда получаем, что\\npj=exp\\u2061(θTu(xj))exp\\u2061(λj−θ0−1).p_j = \\\\frac{\\\\exp(\\\\boldsymbol \\\\theta^T \\\\boldsymbol u(x_j))}{\\\\exp\\\\left(\\\\lambda_j - \\\\theta_0 - 1\\\\right)}.\\npj\\u200b=exp(λj\\u200b−θ0\\u200b−1)exp(θTu(xj\\u200b))\\u200b.Числитель уже ровно такой, как и должен быть у распределения из экспоненциального класса; разберёмся со знаменателем. Поскольку pj>0p_j >0pj\\u200b>0 (ведь тут сплошные экспоненты), λj=0\\\\lambda_j = 0λj\\u200b=0 при всех jjj. Параметр θ0\\\\theta_0θ0\\u200b находится из условия ∑jpj=1\\\\sum_jp_j = 1∑j\\u200bpj\\u200b=1, а точнее, выражается через остальные θi\\\\theta_iθi\\u200b, что позволяет записать знаменатель в виде h(θ)h(\\\\boldsymbol \\\\theta)h(θ).\\nИдея доказательства «в лоб».Как и следовало ожидать, оно ничем не отличается от того, как мы доказывали максимальность энтропии у равномерного или нормального распределения. Пусть q(x)q(x)q(x) – ещё одно распределение, для которого\\n∫ui(x)q(x)dx=∫ui(x)pθ(x)dx\\\\int u_i(x)q(x)dx = \\\\int u_i(x)p_{\\\\boldsymbol\\\\theta}(x)dx\\n∫ui\\u200b(x)q(x)dx=∫ui\\u200b(x)pθ\\u200b(x)dxдля всех i=1,…,mi = 1,\\\\ldots,mi=1,…,m. Тогда\\n0⩽KL(q∣∣p)=∫q(x)log\\u2061(q(x)pθ(x))dx=0\\\\leqslant \\\\mathbb{KL}(q\\\\vert\\\\vert p) = \\\\int q(x)\\\\log\\\\left(\\\\frac{q(x)}{p_{\\\\boldsymbol\\\\theta}(x)}\\\\right)dx = \\n0⩽KL(q∣∣p)=∫q(x)log(pθ\\u200b(x)q(x)\\u200b)dx==∫q(x)log\\u2061q(x)dx⏟−H[q]−∫q(x)log\\u2061pθ(x)dx==\\\\underbrace{\\\\int q(x)\\\\log{q(x)}dx}_{-\\\\mathbb H[q]} - \\\\int q(x)\\\\log{p_{\\\\boldsymbol\\\\theta}(x)}dx=\\n=−H[q]∫q(x)logq(x)dx\\u200b\\u200b−∫q(x)logpθ\\u200b(x)dx==−H[q]−∫q(x)(−log\\u2061h(θ)+∑iθiui(x))dx==-\\\\mathbb H[q] - \\\\int q(x)\\\\left(-\\\\log{h(\\\\boldsymbol \\\\theta)} + \\\\sum_i\\\\theta_iu_i(x)\\\\right)dx =\\n=−H[q]−∫q(x)(−logh(θ)+i∑\\u200bθi\\u200bui\\u200b(x))dx==−H[q]−log\\u2061h(θ)∫q(x)dx⏟=1=∫pθ(x)dx−∑iθi∫q(x)ui(x)dx⏟=∫pθ(x)ui(x)dx==-\\\\mathbb H[q] - \\\\log{h(\\\\theta)}\\\\underbrace{\\\\int q(x)dx}_{=1=\\\\int p_{\\\\boldsymbol\\\\theta}(x)dx} - \\\\sum_i\\\\theta_i\\\\underbrace{\\\\int q(x)u_i(x)dx}_{=\\\\int p_{\\\\boldsymbol\\\\theta}(x)u_i(x)dx} =\\n=−H[q]−logh(θ)=1=∫pθ\\u200b(x)dx∫q(x)dx\\u200b\\u200b−i∑\\u200bθi\\u200b=∫pθ\\u200b(x)ui\\u200b(x)dx∫q(x)ui\\u200b(x)dx\\u200b\\u200b==−H[q]−∫p(x)(−log\\u2061h(θ)+∑iθiui(x))dx==-\\\\mathbb H[q] - \\\\int p(x)\\\\left(-\\\\log{h(\\\\boldsymbol\\\\theta)} + \\\\sum_i\\\\theta_iu_i(x)\\\\right)dx =\\n=−H[q]−∫p(x)(−logh(θ)+i∑\\u200bθi\\u200bui\\u200b(x))dx==−H[q]+∫pθ(x)log\\u2061pθ(x)dx=−H[q]+H[pθ].=-\\\\mathbb H[q] + \\\\int p_{\\\\boldsymbol\\\\theta}(x)\\\\log{p_{\\\\boldsymbol\\\\theta}(x)}dx = -\\\\mathbb H[q] + \\\\mathbb H[p_{\\\\boldsymbol\\\\theta}].\\n=−H[q]+∫pθ\\u200b(x)logpθ\\u200b(x)dx=−H[q]+H[pθ\\u200b].Таким образом, H[pθ]⩾H[q]\\\\mathbb H[p_{\\\\boldsymbol\\\\theta}]\\\\geqslant \\\\mathbb H[q]H[pθ\\u200b]⩾H[q], что и требовалось доказать.\\nВыше мы уже находили обладающее максимальной энтропией распределение на множестве натуральных чисел с заданным математическим ожиданием μ>1\\\\mu>1μ>1. Таковым оказалось геометрическое распределение Geom(1μ)\\\\mathrm{Geom}\\\\big(\\\\frac 1\\\\mu\\\\big)Geom(μ1\\u200b).\\nТеорема Купмана—Питмана—Дармуа позволяет сделать это гораздо быстрее.\\nВ данном случае у нас лишь одна функция u1(x)=xu_1(x) = xu1\\u200b(x)=x, которая соответствует фиксации математического ожидания Eξ\\\\mathbb E\\\\xiEξ. Искомое дискретное распределение имеет вид\\npk=P(ξ=k)=1h(θ)exp\\u2061(θk)=1h(θ)(eθ)k.p_k =\\\\mathbb P(\\\\xi = k) = \\\\frac1{h(\\\\theta)}\\\\exp(\\\\theta k) =  \\\\frac1{h(\\\\theta)} \\\\big(e^{\\\\theta}\\\\big)^k.\\npk\\u200b=P(ξ=k)=h(θ)1\\u200bexp(θk)=h(θ)1\\u200b(eθ)k.Это уже похоже на геометрическое распределение с параметром p=1−eθp = 1 - e^{\\\\theta}p=1−eθ. Его математическое ожидание равно 1p\\\\frac 1pp1\\u200b, что по условию должно равняться μ\\\\muμ. Итак, наше распределение с максимальной этропией выглядит так:\\npk=1μ(1−1μ)k−1,k∈N.p_k = \\\\frac1{\\\\mu}\\\\left(1 - \\\\frac1{\\\\mu}\\\\right)^{k-1},\\\\quad k\\\\in\\\\mathbb N.\\npk\\u200b=μ1\\u200b(1−μ1\\u200b)k−1,k∈N.Пример. Среди распределений на всей вещественной прямой с заданным математическим ожиданием μ\\\\muμ найдём распределение с максимальной энтропией.\\nА сможете ли вы его найти? Решение под катом.Теория говорит нам, что его плотность должна иметь вид\\np(x)=1h(θ)exp\\u2061(θx),p(x) = \\\\frac1{h(\\\\theta)}\\\\exp\\\\left(\\\\theta x\\\\right),\\np(x)=h(θ)1\\u200bexp(θx),но интеграла экспоненты не существует, то есть применение «в лоб» теоремы провалилось. И неспроста: если даже рассмотреть все нормально распределённые случайные величины со средним μ\\\\muμ, их энтропии, равные 12+12log\\u2061(2πσ2)\\\\frac12 + \\\\frac12\\\\log(2\\\\pi\\\\sigma^2)21\\u200b+21\\u200blog(2πσ2), не ограничены сверху, то есть величины с наибольшей энтропией не существует.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф16.5. Независимость и условные распределения вероятностейПредыдущий параграф16.6. Параметрические оценкиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_68.html', 'title': 'Параметрические оценки'}, page_content='Параметрические оценкиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/716.1.Матричное дифференцирование16.2.Матричная факторизация16.3.Вероятностные распределения16.4.Многомерные распределения16.5.Независимость и условные распределения вероятностей16.6.Параметрические оценкиПредельные теоремыСвойства параметрических оценокМетоды оценки параметров16.7.Энтропия и семейство экспоненциальных распределенийГлавная/Хендбуки/Учебник по машинному обучению/Параметрические оценки16.6. Параметрические оценкиАвторыСергей ЛыткинРазличные типы распределений, описанные в предыдущих параграфах, применяются в качестве теоретических моделей в задачах, связанных со случайностью и неопределённостью. Однако на практике далеко не всегда ясно, какое именно распределение моделирует имеющиеся в наличии данные. А если из каких-либо соображений тип распределения всё же установлен, то следующая задача — оценить параметры этого распределения, например, среднее и/или дисперсию в случае гауссовского распределения N(μ,σ2)\\\\mathcal N(\\\\mu, \\\\sigma^2)N(μ,σ2).\\nПодобными обратными по отношению к теории вероятностей задачами занимается математическая статистика. Типичный пример статистической задачи: по числовой выборке X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b оценить параметры распределения, из которого они были получены. Обычно предполагается, что выборка i.i.d. (independent and identically distributed), то есть представляет собой независимые реализации случайной величины с одним и тем же распределением. Параметр этого определения θ\\\\thetaθ может быть числом или вектором; оценку этого параметра по выборке X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b обычно обозначают θ^(X1,…,Xn)\\\\widehat \\\\theta(X_1, \\\\ldots, X_n)θ(X1\\u200b,…,Xn\\u200b) или просто θ^\\\\widehat \\\\thetaθ.\\nПредельные теоремы\\nКак правило, чем больше размер выборки, тем более информативны параметрические оценки вида θ^(X1,…,Xn)\\\\widehat \\\\theta(X_1, \\\\ldots, X_n)θ(X1\\u200b,…,Xn\\u200b). Теоретические свойства таких оценок при n→∞n\\\\to\\\\inftyn→∞ устанавливаются с помощью предельных теорем теории вероятностей.\\nЗакон больших чисел\\nВнимательный читатель мог обратить внимание, что в ряде примеров из предыдущих параграфов параметры некоторых распределений почему-то молчаливо подменялись средними значениями. Так мы поступили в задаче о показе рекламы, взяв в качестве параметра пуассоновского распределение среднее количество кликов пользователей. Фактически мы оценили неизвестный параметра λ\\\\lambdaλ средним по выборке:\\nλ^=1n∑k=1nXk.\\\\widehat\\\\lambda = \\\\frac 1n \\\\sum\\\\limits_{k=1}^n X_k.\\nλ=n1\\u200bk=1∑n\\u200bXk\\u200b.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.ВступитьВ общем-то это кажется логичным, поскольку λ=Eξ\\\\lambda = \\\\mathbb E\\\\xiλ=Eξ, если ξ∼Pois(λ)\\\\xi \\\\sim \\\\mathrm{Pois}(\\\\lambda)ξ∼Pois(λ). Однако у такой оценки есть также мощное теоретическое обоснование.\\nТеорема (Закон больших чисел, ЗБЧ). Пусть X1,X2,…X_1, X_2, \\\\dotsX1\\u200b,X2\\u200b,… – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием μ\\\\muμ. Тогда для любого ε>0\\\\varepsilon > 0ε>0\\nlim\\u2061n→∞P(∣X‾n−μ∣>ε)=0,\\xa0где\\xa0X‾n=1n∑k=1nXk.  \\\\lim\\\\limits_{n \\\\to \\\\infty} \\\\mathbb{P}(|\\\\overline{X}_n - \\\\mu| > \\\\varepsilon) = 0, \\\\text{ где } \\\\overline{X}_n = \\\\frac{1}{n}\\\\sum\\\\limits_{k = 1}^n X_k.\\nn→∞lim\\u200bP(∣Xn\\u200b−μ∣>ε)=0,\\xa0где\\xa0Xn\\u200b=n1\\u200bk=1∑n\\u200bXk\\u200b.Таким образом, чем больше размер выборки nnn, тем менее вероятно отклонение выборочного среднего X‾n\\\\overline{X}_nXn\\u200b от истинного среднего μ\\\\muμ на любое число ε>0\\\\varepsilon > 0ε>0.\\nЗакон больших чисел особенно легко обосновать для случая конечных дисперсий: VXk=σ2<+∞\\\\mathbb V X_k = \\\\sigma^2 < +\\\\inftyVXk\\u200b=σ2<+∞. Имеем\\nEX‾n=1n∑k=1nEXk=μ,VX‾n=1n2∑k=1nVXk=σ2n.    \\\\mathbb E \\\\overline{X}_n = \\\\frac 1n \\\\sum\\\\limits_{k=1}^n \\\\mathbb EX_k = \\\\mu, \\\\quad\\n    \\\\mathbb V \\\\overline{X}_n = \\\\frac 1{n^2} \\\\sum\\\\limits_{k=1}^n \\\\mathbb VX_k = \\\\frac{\\\\sigma^2}n.\\nEXn\\u200b=n1\\u200bk=1∑n\\u200bEXk\\u200b=μ,VXn\\u200b=n21\\u200bk=1∑n\\u200bVXk\\u200b=nσ2\\u200b.Отсюда видно, что lim\\u2061n→∞VX‾n=0\\\\lim\\\\limits_{n\\\\to\\\\infty} \\\\mathbb V \\\\overline{X}_n = 0n→∞lim\\u200bVXn\\u200b=0, поэтому при больших nnn распределение случайной величины VX‾n\\\\mathbb V \\\\overline{X}_nVXn\\u200b всё больше похоже на распределение, сосредоточенное в одной лишь точке μ\\\\muμ. Формально же утверждение ЗБЧ получается с помощью неравенства Чебышева:\\nP(∣X‾n−μ∣>ε)⩽VX‾nε=σ2nε→0,n→∞.    \\\\mathbb{P}\\\\big(\\\\vert \\\\overline{X}_n - \\\\mu\\\\vert > \\\\varepsilon\\\\big) \\\\leqslant \\\\frac{\\\\mathbb{V} \\\\overline{X}_n}{\\\\varepsilon} = \\\\frac{\\\\sigma^2}{n \\\\varepsilon} \\\\to 0, \\\\quad n\\\\to\\\\infty.\\nP(∣Xn\\u200b−μ∣>ε)⩽εVXn\\u200b\\u200b=nεσ2\\u200b→0,n→∞.Закон больших чисел допускает следующее усиление.\\nТеорема (Усиленный закон больших чисел, УЗБЧ). Пусть X1,X2,…X_1, X_2, \\\\dotsX1\\u200b,X2\\u200b,… – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием μ\\\\muμ. Тогда выборочное среднее X‾n\\\\overline{X}_nXn\\u200b почти наверное сходится к μ\\\\muμ, т.е.\\nP(lim\\u2061n→∞X‾n=μ)=1\\\\mathbb P\\\\big(\\\\lim\\\\limits_{n\\\\to\\\\infty}\\\\overline{X}_n = \\\\mu\\\\big) = 1P(n→∞lim\\u200bXn\\u200b=μ)=1.\\nЗамечание о типах сходимостей случайных величинПоследовательность случайных величин (Xn)(X_n)(Xn\\u200b), n∈Nn\\\\in\\\\mathbb Nn∈N, сходится к случайной величине XXX\\n\\nпо распределению, Xn→DXX_n \\\\stackrel{D}{\\\\to} XXn\\u200b→DX, если FXn(x)→FX(x)F_{X_n}(x) \\\\to F_X(x)FXn\\u200b\\u200b(x)→FX\\u200b(x);\\nпо вероятности, Xn→PXX_n \\\\stackrel{P}{\\\\to} XXn\\u200b→PX, если lim\\u2061n→∞P(∣Xn−X∣>ε)=0\\\\lim\\\\limits_{n \\\\to \\\\infty} \\\\mathbb{P}(\\\\vert X_n - X\\\\vert > \\\\varepsilon) = 0n→∞lim\\u200bP(∣Xn\\u200b−X∣>ε)=0 для любого ε>0\\\\varepsilon > 0ε>0;\\nпочти наверное, Xn→п.н.XX_n \\\\stackrel{\\\\text{п.н.}}{\\\\to} XXn\\u200b→п.н.X, если P(lim\\u2061n→∞Xn=X)=1\\\\mathbb P\\\\big(\\\\lim\\\\limits_{n\\\\to\\\\infty}X_n = X\\\\big) = 1P(n→∞lim\\u200bXn\\u200b=X)=1;\\nв среднем квадратичном, Xn→L2XX_n \\\\stackrel{L_2}{\\\\to} XXn\\u200b→L2\\u200bX, если lim\\u2061n→∞E(Xn−X)2=0\\\\lim\\\\limits_{n \\\\to \\\\infty} \\\\mathbb E(X_n - X)^2=0n→∞lim\\u200bE(Xn\\u200b−X)2=0.\\n\\nИзвестно, что\\n\\nиз сходимости по вероятности вытекает сходимость по распределению, Xn→PX\\u2005\\u200a⟹\\u2005\\u200aXn→DXX_n \\\\stackrel{P}{\\\\to} X  \\\\implies X_n \\\\stackrel{D}{\\\\to} XXn\\u200b→PX⟹Xn\\u200b→DX;\\nиз сходимость почти наверное следует сходимость по вероятности, Xn→п.н.X\\u2005\\u200a⟹\\u2005\\u200aXn→PXX_n \\\\stackrel{\\\\text{п.н.}}{\\\\to} X  \\\\implies X_n \\\\stackrel{P}{\\\\to} XXn\\u200b→п.н.X⟹Xn\\u200b→PX;\\nсходимость в среднем квадратичном влечёт сходимость по вероятности, Xn→L2X\\u2005\\u200a⟹\\u2005\\u200aXn→PXX_n \\\\stackrel{L_2}{\\\\to} X  \\\\implies X_n \\\\stackrel{P}{\\\\to} XXn\\u200b→L2\\u200bX⟹Xn\\u200b→PX.\\n\\nА вот из сходимости по вероятности, вообще говоря, не следует сходимость почти наверное (контрпример можно посмотреть здесь).\\nЗакон больших чисел утверждает, что выборочное среднее сходится по вероятности к истинному среднему. А согласно УЗБЧ имеет место более сильный тип сходимости — почти наверное.\\nТеорема Муавра-Лапласа\\nДоска Гальтона иллюстрирует биномиальное распределение. До поворота на ее дне лежит множество маленьких шариков. Сразу после переворота шарики проходят через 10 рядов гладких круглых препятствий. Преодоление каждого препятствия можно рассматривать как испытание Бернулли: с равными вероятностями шарик может пойти как налево, так и направо. Поэтому финальное положение шарика в одной из 10 корзин является приблизительной реализацией биномиального распределения Bin(10,0.5)\\\\mathrm{Bin}(10, 0.5)Bin(10,0.5).\\n\\nУже при n=10n=10n=10 биномиальное распределение напоминает нормальное. И действительно, чем больше nnn, тем лучше дискретная случайная величина ξ∼Bin(n,p)\\\\xi \\\\sim \\\\mathrm{Bin}(n, p)ξ∼Bin(n,p) аппроксимируется непрерывной гауссианой N(np,np(1−p))\\\\mathcal N\\\\big(np, np(1-p)\\\\big)N(np,np(1−p)).\\nТеорема Муавра-Лапласа. Пусть ξ∼Bin(n,p)\\\\xi \\\\sim \\\\mathrm{Bin}(n, p)ξ∼Bin(n,p), q=1−pq=1-pq=1−p, тогда\\nlim\\u2061n→∞P(a<ξ−npnpq⩽b)=12π∫abe−x22\\u2009dx.    \\\\lim\\\\limits_{n\\\\to\\\\infty} \\\\mathbb P\\\\Big(a < \\\\frac{\\\\xi - np}{\\\\sqrt{npq}} \\\\leqslant b\\\\Big) = \\\\frac 1{\\\\sqrt{2\\\\pi}} \\\\int\\\\limits_a^b e^{-\\\\frac{x^2}2}\\\\,dx.\\nn→∞lim\\u200bP(a<npq\\u200bξ−np\\u200b⩽b)=2π\\u200b1\\u200ba∫b\\u200be−2x2\\u200bdx.Из теоремы Муавра-Лапласа вытекает, что при больших nnn вероятность попадания биномиальной случайной величины ξ∼Bin(n,p)\\\\xi \\\\sim \\\\mathrm{Bin}(n, p)ξ∼Bin(n,p) в заданный интервал можно оценить как\\nP(A<ξ⩽B)≈Φ(B−npnpq)−Φ(A−npnpq).    \\\\mathbb P(A < \\\\xi \\\\leqslant B) \\\\approx \\\\Phi\\\\Big(\\\\frac{B - np}{\\\\sqrt{npq}}\\\\Big) - \\\\Phi\\\\Big(\\\\frac{A - np}{\\\\sqrt{npq}}\\\\Big).\\nP(A<ξ⩽B)≈Φ(npq\\u200bB−np\\u200b)−Φ(npq\\u200bA−np\\u200b).где Φ(z)\\\\Phi(z)Φ(z) — функция распределения стандартного нормального распределения.\\nЦентральная предельная теорема\\nПри выводе закона больших чисел мы видели, что выборочное среднее X‾n\\\\overline X_nXn\\u200b имеет среднее μ\\\\muμ и дисперсию σ2n\\\\frac{\\\\sigma^2} nnσ2\\u200b. Но как именно выглядит распределение случайной величины X‾n\\\\overline X_nXn\\u200b при увеличении nnn? Оказывается, что оно становится всё больше похоже на N(μ,σ2n)\\\\mathcal N\\\\big(\\\\mu, \\\\frac{\\\\sigma^2} n\\\\big)N(μ,nσ2\\u200b). Вот как, например, выглядят нормализованные гистограммы 500050005000 выборочных средних, построенных по i.i.d. выборкам X1,…,Xn∼Bin(30,0.3)X_1, \\\\ldots, X_n  \\\\sim \\\\mathrm{Bin}(30, 0.3)X1\\u200b,…,Xn\\u200b∼Bin(30,0.3) для разных значений nnn:\\n\\nЭти гистограммы и впрямь очень напоминают гауссианы, и это прямое следствие следующей теоремы.\\nЦентральная предельная теорема, ЦПТ. Пусть X1,X2,…X_1, X_2, \\\\dotsX1\\u200b,X2\\u200b,… – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием μ\\\\muμ и дисперсией σ2\\\\sigma^2σ2. Тогда\\nZn:=n(X‾n−μ)σ≈N(0,1)\\xa0при\\xa0n≫1.  Z_n := \\\\frac{\\\\sqrt n(\\\\overline X_n - \\\\mu)}{\\\\sigma} \\\\approx \\\\mathcal N(0, 1) \\\\text{ при } n \\\\gg 1. \\nZn\\u200b:=σn\\u200b(Xn\\u200b−μ)\\u200b≈N(0,1)\\xa0при\\xa0n≫1.Точнее говоря, lim\\u2061n→∞P(Zn⩽z)=Φ(z)\\\\lim\\\\limits_{n\\\\to\\\\infty}\\\\mathbb P(Z_n \\\\leqslant z) = \\\\Phi(z)n→∞lim\\u200bP(Zn\\u200b⩽z)=Φ(z). Таким образом, случайная величина ZnZ_nZn\\u200b сходится по распределению к N(0,1)\\\\mathcal N(0,1)N(0,1): Zn→DN(0,1)Z_n \\\\stackrel{D}{\\\\to} \\\\mathcal N(0, 1)Zn\\u200b→DN(0,1).\\nЕсли применить центральную предельную теорему к бернуллиевским случайным величинам с вероятностью успеха ppp, то вновь получим теорему Муавра-Лапласа.\\nСвойства параметрических оценок\\nОценивать параметры можно по-разному, хочется делать это хорошо. Ценные свойства оценок, которые обычно желательны – это несмещенность и состоятельность.\\nНесмещённость\\nКаждый элемент i.i.d выборки X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b можно рассматривать как значение случайной величины из некоторого распределения с неизвестным параметром θ\\\\thetaθ.\\nА раз так, то всякую оценку этого параметра θ^(X1,…,Xn)\\\\widehat\\\\theta(X_1, \\\\dots, X_n)θ(X1\\u200b,…,Xn\\u200b) также можно считать случайной величиной, у которой можно пытаться вычислять математическое ожидание, например.\\nОценка θ^(X1,…,Xn)\\\\widehat\\\\theta(X_1, \\\\dots, X_n)θ(X1\\u200b,…,Xn\\u200b) параметра θ\\\\thetaθ называется несмещенной, если Eθ^=θ\\\\mathbb{E}\\\\widehat\\\\theta = \\\\thetaEθ=θ. Несмещённость оценки означает, что она в среднем будет равна истинному значению параметра.\\nИнтуитивно можно представлять себе несмещённость следующим образом: если мы нагенерим большое количество выборок X1(i),X2(i),…,Xn(i)X_1^{(i)}, X_2^{(i)}, \\\\dots, X_n^{(i)}X1(i)\\u200b,X2(i)\\u200b,…,Xn(i)\\u200b, 1⩽i⩽N1\\\\leqslant i \\\\leqslant N1⩽i⩽N, и для каждой посчитаем оценку θ^(i)\\\\widehat \\\\theta^{(i)}θ(i), то в среднем получится более или менее истинное значение параметра θ\\\\thetaθ: 1N∑i=1Nθ^(i)≈θ\\\\frac 1N\\\\sum\\\\limits_{i=1}^N \\\\widehat \\\\theta^{(i)} \\\\approx \\\\thetaN1\\u200bi=1∑N\\u200bθ(i)≈θ.\\nПростейший пример несмещённой оценки среднего значения θ\\\\thetaθ даёт выборочное среднее X‾n=1n∑nXk\\\\overline{X}n = \\\\frac{1}{n}\\\\sum\\\\limits^n X_kXn=n1\\u200b∑n\\u200bXk\\u200b, поскольку\\nEX‾n=1n∑k=1nEXk=1n⋅nθ=θ.    \\\\mathbb E \\\\overline{X}_n = \\\\frac 1n \\\\sum\\\\limits_{k = 1}^n \\\\mathbb E X_k = \\\\frac 1n\\\\cdot n\\\\theta = \\\\theta.\\nEXn\\u200b=n1\\u200bk=1∑n\\u200bEXk\\u200b=n1\\u200b⋅nθ=θ.Медианой выборки X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b называется средний член вариационного ряда, состоящего из отсортированных по возрастанию элементов выборки:\\nX(1)⩽X(2)⩽…⩽X(n).    X_{(1)} \\\\leqslant X_{(2)} \\\\leqslant \\\\ldots \\\\leqslant X_{(n)}.\\nX(1)\\u200b⩽X(2)\\u200b⩽…⩽X(n)\\u200b.Если nnn нечётно, n=2m+1n=2m+1n=2m+1, то есть ровно один элемент в середине вариационного ряда, именно он называется медианой: med(X1,…,Xn)=X(m)=X(n+12)\\\\mathrm{med}(X_1,\\\\ldots, X_n) = X_{(m)} = X_{\\\\big(\\\\frac{n+1}2\\\\big)}med(X1\\u200b,…,Xn\\u200b)=X(m)\\u200b=X(2n+1\\u200b)\\u200b. При чётном n=2mn=2mn=2m в качестве медианы берут среднее двух центральных элементов вариационного ряда:\\nmed(X1,…,Xn)=12(X(m)+X(m+1))=12(X(n2)+X(n2+1)).\\\\mathrm{med}(X_1,\\\\ldots, X_n) = \\\\frac 12(X_{(m)}+ X_{(m+1)}) = \\\\frac 12 \\\\big(X_{(\\\\frac n2)} + X_{(\\\\frac n2 + 1)}\\\\big).\\nmed(X1\\u200b,…,Xn\\u200b)=21\\u200b(X(m)\\u200b+X(m+1)\\u200b)=21\\u200b(X(2n\\u200b)\\u200b+X(2n\\u200b+1)\\u200b).Упражнение.  Дана i.i.d. выборка X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b из равномерного распределения U[0,2θ]U[0,2\\\\theta]U[0,2θ]. Докажите, что выборочная медиана даёт несмещённую оценку медианы распределения U[0,2θ]U[0,2\\\\theta]U[0,2θ].\\nРешение (не открывайте сразу, попробуйте сначала решить самостоятельно)Если ξ∼U[0,2θ]\\\\xi \\\\sim U[0,2\\\\theta]ξ∼U[0,2θ], то Eξ=med(ξ)=θ\\\\mathbb E\\\\xi = \\\\mathrm{med}(\\\\xi) = \\\\thetaEξ=med(ξ)=θ. В секции про бета-распределение была найдена плотность kkk-й порядковой статистики, посчитанной по выборке из равномерного распределения на [0,1][0,1][0,1]:\\np(x)=n!(k−1)!(n−k)!xk−1(1−x)n−k,0⩽x⩽1.  p(x) = \\\\frac{n!}{(k-1)!(n-k)!} x^{k-1}(1-x)^{n-k}, \\\\quad 0 \\\\leqslant x \\\\leqslant 1.\\np(x)=(k−1)!(n−k)!n!\\u200bxk−1(1−x)n−k,0⩽x⩽1.Чтобы получить отсюда плотность kkk-й порядковой статистики X(k)X_{(k)}X(k)\\u200b для нашей выборки из U[0,2θ]U[0, 2\\\\theta]U[0,2θ], сделаем линейную замену t=2θxt = 2\\\\theta xt=2θx. Тогда\\npX(k)(t)=12θn!(k−1)!(n−k)!(t2θ)k−1(1−t2θ)n−k,  p_{X_{(k)}}(t) = \\\\frac 1{2\\\\theta}\\\\frac{n!}{(k-1)!(n-k)!} \\\\Big(\\\\frac t{2\\\\theta}\\\\Big)^{k-1}\\\\Big(1-\\\\frac t{2\\\\theta}\\\\Big)^{n-k},\\npX(k)\\u200b\\u200b(t)=2θ1\\u200b(k−1)!(n−k)!n!\\u200b(2θt\\u200b)k−1(1−2θt\\u200b)n−k,0⩽t⩽2θ.    0 \\\\leqslant t \\\\leqslant 2\\\\theta.\\n0⩽t⩽2θ.Рассмотрим два случая. Если n=2m+1n = 2m+1n=2m+1, то выборочная медиана равна X(m+1)X_{(m+1)}X(m+1)\\u200b, и\\nEX(m+1)=(2m+1)!m!(m+1)!∫02θ(t2θ)m+1(1−t2θ)m\\u2009dt.    \\\\mathbb E X_{(m+1)} = \\\\frac{(2m+1)!}{m!(m+1)!}\\\\int\\\\limits_0^{2\\\\theta} \\\\Big(\\\\frac t{2\\\\theta}\\\\Big)^{m+1} (1-\\\\frac t{2\\\\theta}\\\\Big)^m\\\\,dt.\\nEX(m+1)\\u200b=m!(m+1)!(2m+1)!\\u200b0∫2θ\\u200b(2θt\\u200b)m+1(1−2θt\\u200b)mdt.Возвращаясь к переменной x=t2θx= \\\\frac t{2\\\\theta}x=2θt\\u200b, находим\\nEX(m+1)=2θ(2m)!m!(m−1)!∫01xm+1(1−x)m\\u2009dx=    \\\\mathbb E X_{(m+1)} = 2\\\\theta\\\\frac{(2m)!}{m!(m-1)!}\\\\int\\\\limits_0^1 x^{m+1} (1-x)^m\\\\,dx =\\nEX(m+1)\\u200b=2θm!(m−1)!(2m)!\\u200b0∫1\\u200bxm+1(1−x)mdx==2θ(2m+1)!m!(m+1)!B(m+2,m+1)=2θ(2m+1)!m!m!(m+1)!m!(2m+2)!=θ.    = 2\\\\theta\\\\frac{(2m+1)!}{m!(m+1)!} B(m+2, m+1)\\n    =2\\\\theta\\\\frac{(2m+1)!}{m! m!} \\\\frac{(m+1)!m!}{(2m+2)!} = \\\\theta. \\n=2θm!(m+1)!(2m+1)!\\u200bB(m+2,m+1)=2θm!m!(2m+1)!\\u200b(2m+2)!(m+1)!m!\\u200b=θ.Если же n=2mn = 2mn=2m, то нам потребуется найти E(12(X(m)+X(m+1)))\\\\mathbb E \\\\big(\\\\frac 12(X_{(m)} + X_{(m+1)})\\\\big)E(21\\u200b(X(m)\\u200b+X(m+1)\\u200b)). Используя ту же самую замену x=t2θx= \\\\frac t{2\\\\theta}x=2θt\\u200b, получаем\\nEX(m)=2θ(2m)!m!(m−1)!∫01xm(1−x)m\\u2009dx=    \\\\mathbb E X_{(m)} = 2\\\\theta\\\\frac{(2m)!}{m!(m-1)!}\\\\int\\\\limits_0^1 x^{m} (1-x)^m\\\\,dx =\\nEX(m)\\u200b=2θm!(m−1)!(2m)!\\u200b0∫1\\u200bxm(1−x)mdx==2θ(2m)!m!(m−1)!B(m+1,m+1)=2θ(2m)!m!(m−1)!m!m!(2m+1)!=2θm2m+1;    =2\\\\theta\\\\frac{(2m)!}{m!(m-1)!} B(m+1, m+1) \\n    =2\\\\theta\\\\frac{(2m)!}{m!(m-1)!}\\\\frac{m!m!}{(2m+1)!} = \\\\frac {2\\\\theta m}{2m+1};\\n=2θm!(m−1)!(2m)!\\u200bB(m+1,m+1)=2θm!(m−1)!(2m)!\\u200b(2m+1)!m!m!\\u200b=2m+12θm\\u200b;EX(m+1)=2θ(2m)!m!(m−1)!∫01xm+1(1−x)m−1\\u2009dx=    \\\\mathbb E X_{(m+1)} = 2\\\\theta\\\\frac{(2m)!}{m!(m-1)!}\\\\int\\\\limits_0^1 x^{m+1} (1-x)^{m-1}\\\\,dx =\\nEX(m+1)\\u200b=2θm!(m−1)!(2m)!\\u200b0∫1\\u200bxm+1(1−x)m−1dx==2θ(2m)!m!(m−1)!B(m+2,m)=2θ(2m)!m!(m−1)!(m+1)!(m−1)!(2m+1)!=2θ(m+1)2m+1.    =2\\\\theta\\\\frac{(2m)!}{m!(m-1)!} B(m+2, m) \\n    =2\\\\theta\\\\frac{(2m)!}{m!(m-1)!}\\\\frac{(m+1)!(m-1)!}{(2m+1)!} = \\\\frac {2\\\\theta(m+1)}{2m+1}.\\n=2θm!(m−1)!(2m)!\\u200bB(m+2,m)=2θm!(m−1)!(2m)!\\u200b(2m+1)!(m+1)!(m−1)!\\u200b=2m+12θ(m+1)\\u200b.Следовательно,\\nE(12(X(m)+X(m+1)))=12(EX(m)+EX(m+1))=θ(m2m+1+m+12m+1)=θ.\\\\mathbb E \\\\Big(\\\\frac 12(X_{(m)} + X_{(m+1)})\\\\Big) = \\\\frac 12\\\\Big(\\\\mathbb E X_{(m)} + \\\\mathbb EX_{(m+1)}\\\\Big) = \\\\theta \\\\Big(\\\\frac m{2m+1} + \\\\frac {m+1}{2m+1}\\\\Big) = \\\\theta.\\nE(21\\u200b(X(m)\\u200b+X(m+1)\\u200b))=21\\u200b(EX(m)\\u200b+EX(m+1)\\u200b)=θ(2m+1m\\u200b+2m+1m+1\\u200b)=θ.Итак, выборочная медиана — несмещённая оценка как медианы, так и среднего распределения U[0,2θ]U[0,2\\\\theta]U[0,2θ].\\nВ некоторых случаях оценка θ^n=θ^(X1,…,Xn)\\\\widehat\\\\theta_n = \\\\widehat\\\\theta(X_1, \\\\dots, X_n)θn\\u200b=θ(X1\\u200b,…,Xn\\u200b) смещена, но с ростом nnn это смещение нивелируется. Если lim\\u2061n→∞Eθ^n=θ\\\\lim\\\\limits_{n\\\\to\\\\infty} \\\\mathbb E\\\\widehat\\\\theta_n = \\\\thetan→∞lim\\u200bEθn\\u200b=θ, то оценка θ^n\\\\widehat\\\\theta_nθn\\u200b называется асимптотически несмещённой.\\nУпражнение. Пусть X1,…,Xn∼U[0,θ]X_1, \\\\ldots, X_n \\\\sim U[0, \\\\theta]X1\\u200b,…,Xn\\u200b∼U[0,θ] — i.i.d. выборка. Оценим параметр θ\\\\thetaθ как максимальное значение выборки:\\nθ^n=X(n)=max\\u2061{X1,…,Xn}.\\\\widehat \\\\theta_n = X_{(n)} = \\\\max\\\\{X_1, \\\\ldots, X_n\\\\}.\\nθn\\u200b=X(n)\\u200b=max{X1\\u200b,…,Xn\\u200b}.Является ли эта оценка несмещённой? Асимптотически несмещённой?\\nРешение (не открывайте сразу, попробуйте сначала решить самостоятельно)В силу свойства i.i.d. для 0⩽x⩽θ0\\\\leqslant x \\\\leqslant \\\\theta0⩽x⩽θ имеем\\nP(X(n)⩽x)=P(X1⩽x,…,Xn⩽x)=∏k=1nP(X1⩽x)=(xθ)n.\\\\mathbb P(X_{(n)}\\\\leqslant x) = \\\\mathbb P(X_1 \\\\leqslant x, \\\\ldots, X_n \\\\leqslant x) = \\n\\\\prod\\\\limits_{k=1}^n \\\\mathbb P(X_1 \\\\leqslant x) = \\\\Big(\\\\frac x\\\\theta\\\\Big)^n.\\nP(X(n)\\u200b⩽x)=P(X1\\u200b⩽x,…,Xn\\u200b⩽x)=k=1∏n\\u200bP(X1\\u200b⩽x)=(θx\\u200b)n.Следовательно, плотность случайной величины θ^n=X(n)\\\\widehat \\\\theta_n = X_{(n)}θn\\u200b=X(n)\\u200b равна nxn−1θnn\\\\frac {x^{n-1}}{\\\\theta^n}nθnxn−1\\u200b, и поэтому\\nEθ^n=nθn∫0θxn\\u2009dx=nθn+1.    \\\\mathbb E \\\\widehat \\\\theta_n = \\\\frac n{\\\\theta^n} \\\\int\\\\limits_0^\\\\theta x^n\\\\,dx = \\\\frac {n\\\\theta}{n+1}.\\nEθn\\u200b=θnn\\u200b0∫θ\\u200bxndx=n+1nθ\\u200b.Отсюда видно, что оценка смещённая. Однако lim\\u2061n→∞nθn+1=θ\\\\lim\\\\limits_{n\\\\to\\\\infty} \\\\frac {n\\\\theta}{n+1} = \\\\thetan→∞lim\\u200bn+1nθ\\u200b=θ, так что оценка θ^n\\\\widehat \\\\theta_nθn\\u200b асимптотически несмещённая. Чтобы получить несмещённость в чистом виде, можно взять оценку θ~n=n+1nX(n)\\\\tilde \\\\theta_n = \\\\frac{n+1}n X_{(n)}θ~n\\u200b=nn+1\\u200bX(n)\\u200b.\\nСостоятельность\\nОценка θ^n=θ^(X1,…,Xn)\\\\widehat\\\\theta_n = \\\\widehat\\\\theta(X_1, \\\\dots, X_n)θn\\u200b=θ(X1\\u200b,…,Xn\\u200b) называется состоятельной, если она сходится по вероятности к θ\\\\thetaθ, θ^n→Pθ\\\\widehat\\\\theta_n \\\\stackrel{P}{\\\\to} \\\\thetaθn\\u200b→Pθ, то есть\\nlim\\u2061n→∞P(∣θ^n−θ∣>ε)=0\\xa0для\\xa0любого\\xa0ε>0.  \\\\lim\\\\limits_{n \\\\to \\\\infty} \\\\mathbb{P}(|\\\\widehat\\\\theta_n - \\\\theta| > \\\\varepsilon) = 0 \\\\text{ для любого } \\\\varepsilon > 0.\\nn→∞lim\\u200bP(∣θn\\u200b−θ∣>ε)=0\\xa0для\\xa0любого\\xa0ε>0.Cостоятельность означает, что с ростом размера выборки всё менее вероятны хоть сколько нибудь значимые отклонения оценки от истинного значения параметра.\\nЕсли i.i.d. выборка X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b получена из распределения с конечным математическим ожиданием θ\\\\thetaθ, то в силу закона больших чисел выборочное среднее X‾n\\\\overline{X}_nXn\\u200b является состоятельной оценкой для θ\\\\thetaθ.\\nСостоятельность оценки – независимое от несмещенности свойство: оценки могут быть состоятельными, но не несмещенными и наоборот. Например, оценка θ^n=X(n)\\n\\\\widehat \\\\theta_n = X_{(n)}θn\\u200b=X(n)\\u200b из предыдущего упражнения оказалась смещённой, однако, она состоятельна:\\nP(∣X(n)−θ∣>ε)=P(X(n)<θ+ε)=    \\\\mathbb P(\\\\vert X_{(n)} - \\\\theta\\\\vert > \\\\varepsilon) = \\\\mathbb P(X_{(n)} < \\\\theta + \\\\varepsilon) = \\nP(∣X(n)\\u200b−θ∣>ε)=P(X(n)\\u200b<θ+ε)==(θ−εθ)n=(1−εθ)n→0,n→∞.    = \\\\Big(\\\\frac{\\\\theta - \\\\varepsilon}{\\\\theta}\\\\Big)^n = \\\\Big(1 - \\\\frac \\\\varepsilon\\\\theta\\\\Big)^n \\\\to 0, n\\\\to\\\\infty.\\n=(θθ−ε\\u200b)n=(1−θε\\u200b)n→0,n→∞.Упражнение. Приведите пример несмещённой оценки, не являющейся состоятельной.\\nИмея i.i.d. выборку X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b из невырожденного распределения с конечным средним θ\\\\thetaθ, оценим это среднее как θ^=X1\\\\widehat \\\\theta = X_1θ=X1\\u200b. Эта оценка, очевидно, несмещённая: Eθ^=EX1=θ\\\\mathbb E \\\\widehat \\\\theta = \\\\mathbb EX_1 = \\\\thetaEθ=EX1\\u200b=θ. Состоятельной, однако, она не является, ведь выражение\\nP(∣θ^−θ∣>ε)=P(∣X1−θ∣>ε)    \\\\mathbb P(\\\\vert \\\\widehat \\\\theta - \\\\theta\\\\vert > \\\\varepsilon) = \\\\mathbb P(\\\\vert X_1 - \\\\theta\\\\vert > \\\\varepsilon)\\nP(∣θ−θ∣>ε)=P(∣X1\\u200b−θ∣>ε)никоим образом не зависит от nnn. Следовательно, состоятельность оценки θ^\\\\widehat \\\\thetaθ означала бы, что P(∣X1−θ∣>ε)=0\\\\mathbb P(\\\\vert X_1 - \\\\theta\\\\vert > \\\\varepsilon)=0P(∣X1\\u200b−θ∣>ε)=0 для любого ε>0\\\\varepsilon >0ε>0. Такое возможно только для вырожденного распределения, сосредоточенного в одной лишь точке θ\\\\thetaθ: P(X1=θ)=1\\\\mathbb P(X_1 = \\\\theta) = 1P(X1\\u200b=θ)=1.\\nBias-variance decomposition\\nСмещение (bias) оценки θ^≡θ^n=θ^(X1,…,Xn)\\\\widehat{\\\\theta}\\\\equiv\\\\widehat{\\\\theta}_n = \\\\widehat{\\\\theta}(X_1,\\\\ldots,X_n)θ≡θn\\u200b=θ(X1\\u200b,…,Xn\\u200b) определяется как\\nbias(θ^)=Eθ^−θ.\\\\mathrm{bias}(\\\\widehat{\\\\theta}) = \\\\mathbb{E}\\\\widehat{\\\\theta} - \\\\theta.\\nbias(θ)=Eθ−θ.Смещение показывает, насколько оценка в среднем отклоняется от истинного значения. Оценка θ^n\\\\widehat{\\\\theta}_nθn\\u200b\\n\\nнесмещённая, если bias(θ^n)=0\\\\mathrm{bias}(\\\\widehat{\\\\theta}_n) = 0bias(θn\\u200b)=0;\\nасимптотически несмещённая, если lim\\u2061n→∞bias(θ^n)=0\\\\lim\\\\limits_{n\\\\to\\\\infty}\\\\mathrm{bias}(\\\\widehat{\\\\theta}_n) = 0n→∞lim\\u200bbias(θn\\u200b)=0.\\n\\nСреднеквадратичной ошибкой (mean squared error, MSE) оценки называется величина\\nMSE(θ^)=E(θ^−θ)2.\\\\mathrm{MSE}(\\\\widehat{\\\\theta}) = \\\\mathbb{E}(\\\\widehat{\\\\theta} - \\\\theta)^2.\\nMSE(θ)=E(θ−θ)2.Смещение, дисперсия и среднеквадратичная ошибка связаны между собой следующим соотношением (bias-variance decomposition):\\nMSE(θ^)=bias2(θ^)+V(θ^).\\\\mathrm{MSE}(\\\\widehat{\\\\theta}) = \\\\text{bias}^2(\\\\widehat{\\\\theta}) + \\\\mathbb{V}(\\\\widehat{\\\\theta}).\\nMSE(θ)=bias2(θ)+V(θ).ДоказательствоИмеем\\nMSE(θ^)=E(θ^−θ)2=E(θ^−Eθ^+Eθ^−θ)2=\\\\mathrm{MSE}(\\\\widehat{\\\\theta}) = \\\\mathbb{E}(\\\\widehat{\\\\theta} - \\\\theta)^2 =\\n\\\\mathbb{E}\\\\big(\\\\widehat{\\\\theta} - \\\\mathbb E\\\\widehat\\\\theta + \\\\mathbb E\\\\widehat\\\\theta  - \\\\theta\\\\big)^2=\\nMSE(θ)=E(θ−θ)2=E(θ−Eθ+Eθ−θ)2==E(θ^−Eθ^)2+2E(θ^−Eθ^)(Eθ^−θ)+E(Eθ^−θ)2==\\\\mathbb{E}\\\\big(\\\\widehat{\\\\theta} - \\\\mathbb E\\\\widehat\\\\theta\\\\big)^2 + 2\\\\mathbb{E}(\\\\widehat{\\\\theta} - \\\\mathbb E\\\\widehat\\\\theta)(\\\\mathbb E\\\\widehat\\\\theta - \\\\theta) + \\n\\\\mathbb{E}\\\\big(\\\\mathbb E\\\\widehat\\\\theta - \\\\theta\\\\big)^2 =\\n=E(θ−Eθ)2+2E(θ−Eθ)(Eθ−θ)+E(Eθ−θ)2==V(θ^)+2(Eθ^−Eθ^)(Eθ^−θ)+bias2(θ^).  = \\\\mathbb{V}(\\\\widehat{\\\\theta})  + 2\\\\big(\\\\mathbb{E}\\\\widehat{\\\\theta} - \\\\mathbb E\\\\widehat\\\\theta\\\\big)\\\\big(\\\\mathbb E\\\\widehat\\\\theta - \\\\theta\\\\big) + \\\\mathrm{bias}^2(\\\\widehat{\\\\theta}).\\n=V(θ)+2(Eθ−Eθ)(Eθ−θ)+bias2(θ).Среднее слагаемое здесь равно нулю, откуда и вытекает доказываемое равенство.\\nУпражнение. Докажите, что оценка θ^n\\\\widehat{\\\\theta}_nθn\\u200b состоятельная, если она асимптотически несмещённая и lim\\u2061n→∞V(θ^n)=0\\\\lim\\\\limits_{n\\\\to\\\\infty}\\\\mathbb{V}(\\\\widehat{\\\\theta}_n) = 0n→∞lim\\u200bV(θn\\u200b)=0.\\nРешение (не открывайте сразу, попробуйте сначала решить самостоятельно)С помощью неравенства Маркова получаем, что\\nP(∣θ^n−θ∣>ε)⩽P((θ^n−θ)2⩾ε2)⩽E(θ^n−θ)2ε2=MSE(θ^n)ε2.\\\\mathbb{P}\\\\big(\\\\vert\\\\widehat{\\\\theta}_n - \\\\theta\\\\vert > \\\\varepsilon\\\\big)\\\\leqslant  \\\\mathbb{P}\\\\big((\\\\widehat{\\\\theta}_n - \\\\theta)^2 \\\\geqslant \\\\varepsilon^2\\\\big)\\\\leqslant \\\\frac{\\\\mathbb{E}(\\\\widehat{\\\\theta}_n - \\\\theta)^2}{\\\\varepsilon^2} = \\\\frac{\\\\mathrm{MSE}(\\\\widehat\\\\theta_n)}{\\\\varepsilon^2}.\\nP(∣θn\\u200b−θ∣>ε)⩽P((θn\\u200b−θ)2⩾ε2)⩽ε2E(θn\\u200b−θ)2\\u200b=ε2MSE(θn\\u200b)\\u200b.По условию оба слагаемых в формуле bias-variance decomposition стремятся к нулю,\\nMSE(θ^n)=bias2(θ^n)+V(θ^n)→0,n→∞,\\\\mathrm{MSE}(\\\\widehat\\\\theta_n) = \\\\mathrm{bias}^2(\\\\widehat{\\\\theta}_n) + \\\\mathbb{V}(\\\\widehat{\\\\theta}_n) \\\\to 0, \\\\quad n\\\\to \\\\infty,\\nMSE(θn\\u200b)=bias2(θn\\u200b)+V(θn\\u200b)→0,n→∞,и поэтому lim\\u2061n→∞P(∣θ^n−θ∣>ε)=0\\\\lim\\\\limits_{n\\\\to\\\\infty}\\\\mathbb{P}\\\\big(\\\\vert\\\\widehat{\\\\theta}_n - \\\\theta\\\\vert > \\\\varepsilon\\\\big) = 0n→∞lim\\u200bP(∣θn\\u200b−θ∣>ε)=0 при любом фиксированном ε>0\\\\varepsilon > 0ε>0.\\nТаким образом, если lim\\u2061n→∞MSE(θ^n)=0\\\\lim\\\\limits_{n\\\\to\\\\infty}\\\\mathrm{MSE}(\\\\widehat{\\\\theta}_n) = 0n→∞lim\\u200bMSE(θn\\u200b)=0, то оценка θ^n\\\\widehat{\\\\theta}_nθn\\u200b параметра θ\\\\thetaθ асимптотически несмещённая и состоятельная.\\nАсимптотическая нормальность\\nСтандартным отклонением оценки θ^n\\\\widehat{\\\\theta}_nθn\\u200b параметра θ\\\\thetaθ называется корень из дисперсии:\\nse(θ^n)=Vθ^n.    \\\\mathrm{se}(\\\\widehat{\\\\theta}_n) = \\\\sqrt{\\\\mathbb V \\\\widehat{\\\\theta}_n}.\\nse(θn\\u200b)=Vθn\\u200b\\u200b.Оценка θ^n\\\\widehat{\\\\theta}_nθn\\u200b асимптотически нормальна, если θ^n−θse(θ^n)→DN(0,1)\\\\frac{\\\\widehat{\\\\theta}_n - \\\\theta}{\\\\mathrm{se}(\\\\widehat{\\\\theta}_n)} \\\\stackrel{D}{\\\\to} \\\\mathcal N(0,1)se(θn\\u200b)θn\\u200b−θ\\u200b→DN(0,1), т.е.\\nlim\\u2061n→∞P(θ^n−θse(θ^n)⩽z)=Φ(z).    \\\\lim\\\\limits_{n\\\\to\\\\infty}\\\\mathbb P\\\\Big(\\\\frac{\\\\widehat{\\\\theta}_n - \\\\theta}{\\\\mathrm{se}(\\\\widehat{\\\\theta}_n)} \\\\leqslant z\\\\Big) = \\\\mathbb \\\\Phi(z).\\nn→∞lim\\u200bP(se(θn\\u200b)θn\\u200b−θ\\u200b⩽z)=Φ(z).Согласно центральной предельной теореме выборочное среднее i.i.d. выборки из распределения с конечными средним μ\\\\muμ и дисперсией σ2\\\\sigma^2σ2 является асимптотически нормальной оценкой параметра μ\\\\muμ.\\nЭффективность\\nПусть θ^\\\\widehat{\\\\theta}θ и θ~\\\\tilde{\\\\theta}θ~ — несмещённые оценки параметра θ\\\\thetaθ. Оценка θ^\\\\widehat{\\\\theta}θ эффективнее оценки θ~\\\\tilde{\\\\theta}θ~, если Vθ^<Vθ~\\\\mathbb V\\\\widehat{\\\\theta} < \\\\mathbb V\\\\tilde{\\\\theta}Vθ<Vθ~. Такое определение эффективности вполне логично, ведь чем меньше дисперсия несмещённой оценки, тем меньше у неё шансов удалиться куда-то далеко от истинного значения параметра.\\nПример. Пусть X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b — i.i.d. выборка из распределения U[0,2θ]U[0, 2\\\\theta]U[0,2θ]. Какая оценка параметра θ\\\\thetaθ эффективнее: выборочное среднее или медиана?\\nНесмещённость оценок θ^=X‾n\\\\widehat{\\\\theta} = \\\\overline X _nθ=Xn\\u200b и θ~=med(X1,…,Xn)\\\\tilde{\\\\theta} = \\\\mathrm{med}(X_1, \\\\ldots, X_n)θ~=med(X1\\u200b,…,Xn\\u200b) уже была показана выше.\\nНайдём дисперсию наших оценок. Диспресия случайной величины ξ∼U[0,2θ]\\\\xi \\\\sim U[0, 2\\\\theta]ξ∼U[0,2θ] равна Vξ=θ23\\\\mathbb V \\\\xi = \\\\frac{\\\\theta^2}{3}Vξ=3θ2\\u200b, следовательно, Vθ^=θ23n\\\\mathbb V \\\\widehat \\\\theta = \\\\frac{\\\\theta^2}{3n}Vθ=3nθ2\\u200b.\\nНайти дисперсию медианы несколько сложнее. Ограничимся случаем n=2m+1n = 2m+1n=2m+1. Тогда θ~=X(m+1)\\\\tilde{\\\\theta} = X_{(m+1)}θ~=X(m+1)\\u200b, и\\nEθ~2=EX(m+1)=12θ(2m+1)!(m!)2∫02θx2(x2θ)m(1−x2θ)m\\u2009dx.    \\\\mathbb E\\\\tilde{\\\\theta}^2 =  \\\\mathbb E X_{(m+1)} = \\\\frac 1{2\\\\theta}\\\\frac{(2m+1)!}{(m!)^2} \\\\int\\\\limits_0^{2\\\\theta} x^2 \\\\Big(\\\\frac x{2\\\\theta}\\\\Big)^{m}\\\\Big(1-\\\\frac x{2\\\\theta}\\\\Big)^{m}\\\\,dx.\\nEθ~2=EX(m+1)\\u200b=2θ1\\u200b(m!)2(2m+1)!\\u200b0∫2θ\\u200bx2(2θx\\u200b)m(1−2θx\\u200b)mdx.С помощью замены t=x2θt = \\\\frac x{2\\\\theta}t=2θx\\u200b отсюда находим, что\\nEθ~2=(2m+1)!(m!)2∫014θ2tm+2(1−t)m\\u2009\\\\mathbb E\\\\tilde{\\\\theta}^2 = \\\\frac{(2m+1)!}{(m!)^2}\\\\int\\\\limits_0^1 4\\\\theta^2t^{m+2}(1-t)^m\\\\,\\nEθ~2=(m!)2(2m+1)!\\u200b0∫1\\u200b4θ2tm+2(1−t)mdt=4θ2(2m+1)!(m!)2B(m+3,m+1)=dt =4\\\\theta^2 \\\\frac{(2m+1)!}{(m!)^2} B(m+3, m+1) =\\ndt=4θ2(m!)2(2m+1)!\\u200bB(m+3,m+1)==4θ2(2m+1)!(m!)2(m+2)!m!(2m+3)!=2θ2m+22m+3=θ2+θ2n+3.=4\\\\theta^2 \\\\frac{(2m+1)!}{(m!)^2} \\\\frac{(m+2)!m!}{(2m+3)!} = 2\\\\theta^2 \\\\frac{m+2}{2m+3} = \\\\theta^2 + \\\\frac{\\\\theta^2}{n+3}.\\n=4θ2(m!)2(2m+1)!\\u200b(2m+3)!(m+2)!m!\\u200b=2θ22m+3m+2\\u200b=θ2+n+3θ2\\u200b.Следовательно, Vθ~=θ2n+3\\\\mathbb V\\\\tilde{\\\\theta} = \\\\frac{\\\\theta^2}{n+3}Vθ~=n+3θ2\\u200b, что при n>1n>1n>1 больше, чем\\nVθ^=θ23n\\\\mathbb V \\\\widehat \\\\theta = \\\\frac{\\\\theta^2}{3n}Vθ=3nθ2\\u200b, так что выборочное среднее эффективнее\\nмедианы (примерно в 3\\\\sqrt 33\\u200b раз при больших nnn, если считать по отношению стандартных отклонений).\\nНесмотря на то что в плане эффективности среднее оказалось предпочтительнее в этом примере,\\nв статистике медиану любят за бОльшую устойчивость к выбросам.\\nНиже приведён scatter-plot, по которому можно наглядно оценить меру разброса среднего и медианы выборки из равномерного распределения на отрезке [0,2θ][0, 2\\\\theta][0,2θ] для θ=5\\\\theta = 5θ=5. Для построения этого графика были взяты 200200200 i.i.d. выборок из U[0,10]U[0, 10]U[0,10] размера n=10,100,1000,10000n=10, 100, 1000, 10000n=10,100,1000,10000, и для каждого nnn посчитаны выборочное среднее и медиана. Эти статистики и задают координаты точки на графике. Разумеется, чем больше значение nnn, тем кучнее локализованы точки вокруг среднего значения θ=5\\\\theta = 5θ=5, совпадающего в данном случае с медианой. Как видно, облако точек сосредоточено вдоль прямой y=θ+3(x−θ)y = \\\\theta + \\\\sqrt 3(x - \\\\theta)y=θ+3\\u200b(x−θ).\\n\\nВыборочная дисперсия\\nКак мы уже убедились, выборочное среднее X‾n=1n∑k=1nXk\\\\overline{X}_n = \\\\frac{1}{n}\\\\sum\\\\limits_{k = 1}^n X_kXn\\u200b=n1\\u200bk=1∑n\\u200bXk\\u200b представляет собой несмещённую и состоятельную оценку для математического ожидания. Можно ли то же самое сказать про выборочную дисперсию\\nS‾n=1n∑k=1n(Xk−X‾n)2    \\\\overline S_n = \\\\frac{1}{n} \\\\sum\\\\limits_{k = 1}^n (X_k - \\\\overline{X}_n)^2\\nSn\\u200b=n1\\u200bk=1∑n\\u200b(Xk\\u200b−Xn\\u200b)2в предположении, что i.i.d. выборка X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b состоит из реализаций случайной величины ξ\\\\xiξ с конечными моментами Eξ=θ1\\\\mathbb E\\\\xi = \\\\theta_1Eξ=θ1\\u200b и Eξ2=θ2\\\\mathbb E\\\\xi^2 = \\\\theta_2Eξ2=θ2\\u200b?\\nПрежде всего раскроем скобки и перепишем S‾n\\\\overline S_nSn\\u200b в виде\\nS‾n=1n∑k=1n(Xk2−2XkX‾n+(X‾n)2)=    \\\\overline S_n  = \\\\frac{1}{n} \\\\sum\\\\limits_{k = 1}^n \\\\big(X_k^2 - 2X_k \\\\overline{X}_n + (\\\\overline{X}_n)^2\\\\big) = \\nSn\\u200b=n1\\u200bk=1∑n\\u200b(Xk2\\u200b−2Xk\\u200bXn\\u200b+(Xn\\u200b)2)==1n∑k=1nXk2−2(X‾n)2+(X‾n)2=X2‾n−(X‾n)2,    = \\\\frac{1}{n} \\\\sum\\\\limits_{k = 1}^n X_k^2 - 2 (\\\\overline{X}_n)^2 + (\\\\overline{X}_n)^2 = \\n    \\\\overline{X^2}_n - (\\\\overline{X}_n)^2,\\n=n1\\u200bk=1∑n\\u200bXk2\\u200b−2(Xn\\u200b)2+(Xn\\u200b)2=X2n\\u200b−(Xn\\u200b)2,где X2‾n=1n∑k=1nXk2\\\\overline{X^2}_n  = \\\\frac 1n\\\\sum\\\\limits_{k = 1}^n X_k^2X2n\\u200b=n1\\u200bk=1∑n\\u200bXk2\\u200b — выборочное среднее, построенное по выборке X12,…,Xn2X_1^2, \\\\ldots, X_n^2X12\\u200b,…,Xn2\\u200b. Оно несмещённое, поэтому EX2‾n=θ2\\\\mathbb E \\\\overline{X^2}_n  = \\\\theta_2EX2n\\u200b=θ2\\u200b. Заметим также, что\\n(X‾n)2=1n2(∑k=1nXk)2=1n2∑k=1nXk2+2n2∑1⩽i<j⩽nXiXj,    (\\\\overline{X}_n)^2 = \\\\frac 1{n^2} \\\\Big(\\\\sum\\\\limits_{k=1}^n X_k\\\\Big)^2 =\\n    \\\\frac 1{n^2} \\\\sum\\\\limits_{k=1}^n X_k^2 + \\\\frac 2{n^2} \\\\sum\\\\limits_{1\\\\leqslant i < j\\\\leqslant n} X_iX_j,\\n(Xn\\u200b)2=n21\\u200b(k=1∑n\\u200bXk\\u200b)2=n21\\u200bk=1∑n\\u200bXk2\\u200b+n22\\u200b1⩽i<j⩽n∑\\u200bXi\\u200bXj\\u200b,откуда в силу независимости XiX_iXi\\u200b и XjX_jXj\\u200b при i≠ji\\\\ne ji\\ue020=j получаем\\nE(X‾n)2=1nEX2‾n+2n2∑1⩽i<j⩽nEXiEXj=θ2n+n−1nθ12.    \\\\mathbb E(\\\\overline{X}_n)^2 = \\\\frac 1n \\\\mathbb E\\\\overline{X^2}_n + \\\\frac 2{n^2} \\\\sum\\\\limits_{1\\\\leqslant i < j\\\\leqslant n} \\\\mathbb E X_i \\\\mathbb E X_j=\\n    \\\\frac{\\\\theta_2}n + \\\\frac{n-1}n\\\\theta_1^2.\\nE(Xn\\u200b)2=n1\\u200bEX2n\\u200b+n22\\u200b1⩽i<j⩽n∑\\u200bEXi\\u200bEXj\\u200b=nθ2\\u200b\\u200b+nn−1\\u200bθ12\\u200b.Итак,\\nES‾n=θ2−θ2n−n−1nθ12=n−1nVξ.    \\\\mathbb E\\\\overline S_n = \\\\theta_2- \\\\frac {\\\\theta_2}n - \\\\frac{n-1}n\\\\theta_1^2 = \\\\frac{n-1}n \\\\mathbb V\\\\xi.\\nESn\\u200b=θ2\\u200b−nθ2\\u200b\\u200b−nn−1\\u200bθ12\\u200b=nn−1\\u200bVξ.Таким образом, оценка дисперсии S‾n\\\\overline S_nSn\\u200b смещённая (хотя и асимптотически несмещённая). По этой причине для оценки дисперсии часто используют аналогичную несмещённую оценку\\nσ‾n=nn−1S‾n=1n−1∑k=1n(Xk−X‾n)2,    \\\\overline \\\\sigma_n = \\\\frac n{n-1}\\\\overline S_n = \\\\frac{1}{n-1} \\\\sum\\\\limits_{k = 1}^n (X_k - \\\\overline{X}_n)^2,\\nσn\\u200b=n−1n\\u200bSn\\u200b=n−11\\u200bk=1∑n\\u200b(Xk\\u200b−Xn\\u200b)2,которую также называют выборочной дисперсией.\\nОбоснуем теперь состоятельность оценки S‾n=X2‾n−(X‾n)2\\\\overline S_n = \\\\overline{X^2}_n - (\\\\overline{X}_n)^2Sn\\u200b=X2n\\u200b−(Xn\\u200b)2. Согласно закону больших чисел X2‾n→Pθ2\\\\overline{X^2}_n \\\\stackrel{P}{\\\\to} \\\\theta_2X2n\\u200b→Pθ2\\u200b,\\nX‾n→Pθ1\\\\overline{X}_n \\\\stackrel{P}{\\\\to} \\\\theta_1Xn\\u200b→Pθ1\\u200b. Здесь нам потребуется пара свойств сходимости по вероятности.\\nУпражнение. Пусть ξn→Pξ\\\\xi_n \\\\stackrel{P}{\\\\to} \\\\xiξn\\u200b→Pξ, ηn→Pη\\\\eta_n \\\\stackrel{P}{\\\\to} \\\\etaηn\\u200b→Pη. Докажите, что ξn+ηn→Pξ+η\\\\xi_n + \\\\eta_n\\\\stackrel{P}{\\\\to} \\\\xi + \\\\etaξn\\u200b+ηn\\u200b→Pξ+η.\\nРешение (не открывайте сразу, попробуйте сначала решить самостоятельно)Зафиксируем некоторое ε>0\\\\varepsilon > 0ε>0. Поскольку ∣ξn−ξ∣+∣ηn−η∣⩾∣ξn+ηn−ξ−η∣\\\\vert \\\\xi_n - \\\\xi\\\\vert + \\\\vert \\\\eta_n - \\\\eta\\\\vert \\\\geqslant \\\\vert \\\\xi_n + \\\\eta_n - \\\\xi - \\\\eta\\\\vert∣ξn\\u200b−ξ∣+∣ηn\\u200b−η∣⩾∣ξn\\u200b+ηn\\u200b−ξ−η∣, то\\nP(∣ξn+ηn−ξ−η∣>ε)⩽P(∣ξn−ξ∣+∣ηn−η∣>ε).    \\\\mathbb P\\\\big(\\\\vert \\\\xi_n + \\\\eta_n - \\\\xi - \\\\eta\\\\vert > \\\\varepsilon\\\\big) \\\\leqslant\\n    \\\\mathbb P\\\\big(\\\\vert \\\\xi_n - \\\\xi\\\\vert  + \\\\vert \\\\eta_n - \\\\eta\\\\vert  > \\\\varepsilon\\\\big).\\nP(∣ξn\\u200b+ηn\\u200b−ξ−η∣>ε)⩽P(∣ξn\\u200b−ξ∣+∣ηn\\u200b−η∣>ε).Далее, если ∣ξn−ξ∣+∣ηn−η∣>ε\\\\vert \\\\xi_n - \\\\xi\\\\vert + \\\\vert \\\\eta_n - \\\\eta\\\\vert  > \\\\varepsilon∣ξn\\u200b−ξ∣+∣ηn\\u200b−η∣>ε, то выполняется хотя бы одно из неравенств ∣ξn−ξ∣>ε2\\\\vert \\\\xi_n - \\\\xi\\\\vert >\\\\frac \\\\varepsilon 2∣ξn\\u200b−ξ∣>2ε\\u200b и ∣ηn−η∣>ε2\\\\vert \\\\eta_n - \\\\eta\\\\vert >\\\\frac \\\\varepsilon 2∣ηn\\u200b−η∣>2ε\\u200b. Следовательно,\\nP(∣ξn+ηn−ξ−η∣>ε)⩽P(∣ξn−ξ∣>ε2)+P(∣ηn−η∣>ε2).    \\\\mathbb P\\\\big(\\\\vert \\\\xi_n + \\\\eta_n - \\\\xi - \\\\eta\\\\vert > \\\\varepsilon\\\\big) \\\\leqslant \\\\mathbb P\\\\Big(\\\\vert \\\\xi_n - \\\\xi\\\\vert >\\\\frac \\\\varepsilon 2\\\\Big) + \\\\mathbb P\\\\Big(\\\\vert \\\\eta_n - \\\\eta\\\\vert >\\\\frac \\\\varepsilon 2\\\\Big).\\nP(∣ξn\\u200b+ηn\\u200b−ξ−η∣>ε)⩽P(∣ξn\\u200b−ξ∣>2ε\\u200b)+P(∣ηn\\u200b−η∣>2ε\\u200b).Но последние две вероятности стремятся к нулю, так как ξn→Pξ\\\\xi_n \\\\stackrel{P}{\\\\to} \\\\xiξn\\u200b→Pξ и ηn→Pη\\\\eta_n \\\\stackrel{P}{\\\\to} \\\\etaηn\\u200b→Pη. Следовательно, последовательность случайных величин ξn+ηn\\\\xi_n + \\\\eta_nξn\\u200b+ηn\\u200b сходится по вероятности к ξ+η\\\\xi + \\\\etaξ+η.\\nУпражнение. Пусть ξn→Pξ\\\\xi_n \\\\stackrel{P}{\\\\to} \\\\xiξn\\u200b→Pξ. Докажите, что ξn2→Pξ2\\\\xi_n^2 \\\\stackrel{P}{\\\\to} \\\\xi^2ξn2\\u200b→Pξ2.\\nРешение (не открывайте сразу, попробуйте сначала решить самостоятельно)Пусть 0<ε<10 < \\\\varepsilon < 10<ε<1 и δ>0\\\\delta > 0δ>0. Выберем число M>1M>1M>1 так, что P(∣ξ∣>M)<δ\\\\mathbb P(\\\\vert\\\\xi\\\\vert > M) < \\\\deltaP(∣ξ∣>M)<δ. Если ∣ξn−ξ∣⩽εM\\\\vert \\\\xi_n - \\\\xi\\\\vert \\\\leqslant \\\\frac\\\\varepsilon M∣ξn\\u200b−ξ∣⩽Mε\\u200b и ∣ξ∣⩽M\\\\vert \\\\xi \\\\vert \\\\leqslant M∣ξ∣⩽M, то\\n∣ξn2−ξ2∣=∣ξn−ξ∣⋅∣ξn+ξ∣=∣ξn−ξ∣⋅∣ξn−ξ+2ξ∣⩽    \\\\vert \\\\xi_n^2 - \\\\xi^2\\\\vert = \\\\vert \\\\xi_n - \\\\xi\\\\vert \\\\cdot\\\\vert \\\\xi_n + \\\\xi\\\\vert = \\\\vert \\\\xi_n - \\\\xi\\\\vert \\\\cdot\\\\vert \\\\xi_n - \\\\xi + 2\\\\xi\\\\vert  \\\\leqslant\\n∣ξn2\\u200b−ξ2∣=∣ξn\\u200b−ξ∣⋅∣ξn\\u200b+ξ∣=∣ξn\\u200b−ξ∣⋅∣ξn\\u200b−ξ+2ξ∣⩽⩽εM(∣ξn−ξ∣+2∣ξ∣)⩽εM(1+2M)<3ε.    \\\\leqslant\\\\frac\\\\varepsilon M(\\\\vert \\\\xi_n - \\\\xi\\\\vert + 2\\\\vert \\\\xi\\\\vert) \\\\leqslant \\\\frac\\\\varepsilon M(1+ 2M) < 3\\\\varepsilon.\\n⩽Mε\\u200b(∣ξn\\u200b−ξ∣+2∣ξ∣)⩽Mε\\u200b(1+2M)<3ε.Следовательно,\\nP(∣ξn2−ξ2∣>3ε)⩽P(∣ξn−ξ∣>εM)+P(∣ξ∣>M)<2δ.    \\\\mathbb P\\\\big(\\\\vert \\\\xi_n^2 - \\\\xi^2\\\\vert > 3\\\\varepsilon\\\\big) \\\\leqslant  \\\\mathbb P \\\\Big(\\\\vert \\\\xi_n - \\\\xi\\\\vert > \\\\frac \\\\varepsilon M\\\\Big) + \\\\mathbb P(\\\\vert \\\\xi\\\\vert > M) < 2\\\\delta.\\nP(∣ξn2\\u200b−ξ2∣>3ε)⩽P(∣ξn\\u200b−ξ∣>Mε\\u200b)+P(∣ξ∣>M)<2δ.Последнее неравенство выполняется для всех достаточно больших nnn, при которых первое слагаемое меньше δ\\\\deltaδ; этого же всегда можно достичь за счёт увеличения nnn, поскольку по условию ξn→Pξ\\\\xi_n \\\\stackrel{P}{\\\\to} \\\\xiξn\\u200b→Pξ. В силу произвольности δ\\\\deltaδ отсюда заключаем, что\\nlim\\u2061n→∞P(∣ξn2−ξ2∣>3ε)=0,\\\\lim\\\\limits_{n\\\\to\\\\infty} \\\\mathbb P\\\\big(\\\\vert \\\\xi_n^2 - \\\\xi^2\\\\vert > 3\\\\varepsilon\\\\big) = 0,\\nn→∞lim\\u200bP(∣ξn2\\u200b−ξ2∣>3ε)=0,то есть последовательность ξn2\\\\xi_n^2ξn2\\u200b сходится по вероятности к случайной величине ξ2\\\\xi^2ξ2.\\nПользуясь результатами этих упражнений, заключаем, что\\n(X‾n)2→Pθ12\\\\big(\\\\overline{X}_n\\\\big)^2 \\\\stackrel{P}{\\\\to} \\\\theta_1^2(Xn\\u200b)2→Pθ12\\u200b и\\nS‾n→Pθ2−θ12=Vξ\\\\overline{S}_n \\\\stackrel{P}{\\\\to} \\\\theta_2 - \\\\theta_1^2 = \\\\mathbb V\\\\xiSn\\u200b→Pθ2\\u200b−θ12\\u200b=Vξ, и, стало быть, оценка S‾n\\\\overline S_nSn\\u200b состоятельна.\\nМетоды оценки параметров\\nДо этого мы обсуждали разные приятные свойства оценок, а теперь рассмотрим некоторые методы, позволяющие систематически получать по выборке оценки параметров с нужными свойствами.\\nМетод моментов\\nПусть выборка X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b получена сэмплированием из некоторого семейства распределений Fθ(x)F_{\\\\boldsymbol \\\\theta}(x)Fθ\\u200b(x) с параметрами θ=(θ1,…,θm)\\\\boldsymbol \\\\theta = (\\\\theta_1, \\\\ldots, \\\\theta_m)θ=(θ1\\u200b,…,θm\\u200b). Метод моментов для оценки этих параметров заключается в приравнивании выборочных моментов\\nXk‾n=1n∑j=1nXjk    \\\\overline{X^k}_n = \\\\frac 1n\\\\sum\\\\limits_{j=1}^n X_j^k\\nXkn\\u200b=n1\\u200bj=1∑n\\u200bXjk\\u200bк теоретическим\\nαk(θ)=∫−∞+∞xkdFθ(x). \\\\alpha_k(\\\\boldsymbol \\\\theta) = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} x^k dF_{\\\\boldsymbol \\\\theta}(x).\\nαk\\u200b(θ)=−∞∫+∞\\u200bxkdFθ\\u200b(x).Решая полученную систему уравнений αk(θ)=Xk‾n\\\\alpha_k(\\\\boldsymbol \\\\theta) = \\\\overline{X^k}_nαk\\u200b(θ)=Xkn\\u200b, 1⩽k⩽m1\\\\leqslant k \\\\leqslant m1⩽k⩽m, находим оценки параметров θ^k\\\\widehat \\\\theta_kθk\\u200b.\\nПример. Оценим параметры нормального распределения N(μ,σ2)\\\\mathcal{N}(\\\\mu, \\\\sigma^2)N(μ,σ2) с помощью метода моментов.\\nПопробуйте сделать сами, прежде чем смотреть решение.Теоретические моменты равны\\nα1=μ,α2=σ2+μ2.\\\\alpha_1 = \\\\mu,\\\\quad\\\\alpha_2 = \\\\sigma^2 + \\\\mu^2.\\nα1\\u200b=μ,α2\\u200b=σ2+μ2.Запишем систему:\\nμ=X‾n,\\\\mu = \\\\overline X_n,\\nμ=Xn\\u200b,σ2+μ2=X2‾n.\\\\sigma^2 + \\\\mu^2 = \\\\overline {X^2}_n.\\nσ2+μ2=X2n\\u200b.Из неё очевидным образом находим μ^=X‾n\\\\widehat \\\\mu =  \\\\overline X_nμ\\u200b=Xn\\u200b,\\nσ2^=X2‾n−(X‾n)2=1n∑k=1n(Xk−X‾n)2.\\\\widehat{\\\\sigma^2} =  \\\\overline {X^2}_n - \\\\big( \\\\overline X_n\\\\big)^2=\\n\\\\frac1n \\\\sum\\\\limits_{k=1}^n\\\\big(X_k -  \\\\overline X_n \\\\big)^2.\\nσ2=X2n\\u200b−(Xn\\u200b)2=n1\\u200bk=1∑n\\u200b(Xk\\u200b−Xn\\u200b)2.Как видно, оценки по методу моментов в данном случае совпадают с выборочными средним и дисперсией.\\nУпражнение. Оцените по методу моментов параметры aaa и bbb для выборки X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b из U[a,b]U[a, b]U[a,b].\\nОтветРешая систему уравнений α1=a+b2=X‾n\\\\alpha_1 = \\\\frac{a+b}2 = \\\\overline X_nα1\\u200b=2a+b\\u200b=Xn\\u200b, α2=a2+ab+b23=X2‾n\\\\alpha_2 = \\\\frac{a^2+ab + b^2}3 = \\\\overline{X^2}_nα2\\u200b=3a2+ab+b2\\u200b=X2n\\u200b, находим\\na^=X‾n−3(X2‾n−X‾n2),b^=X‾n+3(X2‾n−X‾n2)    \\\\widehat a =  \\\\overline X_n - \\\\sqrt{3 (\\\\overline {X^2}_n -  \\\\overline X_n^2)}, \\\\quad\\n    \\\\widehat b =  \\\\overline X_n + \\\\sqrt{3 (\\\\overline {X^2}_n -  \\\\overline X_n^2)}\\na=Xn\\u200b−3(X2n\\u200b−Xn2\\u200b)\\u200b,b=Xn\\u200b+3(X2n\\u200b−Xn2\\u200b)\\u200bТаким образом, согласно методу моментов оценки для границ отрезка отстоят от выборочного среднего на выборочное стандартное отклонение, помноженное на 3\\\\sqrt 33\\u200b.\\nПри некоторых условиях на регулярность семейства распределений Fθ(x)F_{\\\\boldsymbol \\\\theta}(x)Fθ\\u200b(x) оценка по методу моментов получается состоятельной и асимптотически нормальной.\\nМетод максимального правдоподбия\\nПусть, как обычно, выборка X1,…,Xn∼Fθ(x)X_1, \\\\ldots, X_n \\\\sim F_\\\\theta(x)X1\\u200b,…,Xn\\u200b∼Fθ\\u200b(x).\\nПравдоподобие (функция правдоподобия, likelihood) выборки X1,…,…XnX_1,\\\\ldots, \\\\ldots X_nX1\\u200b,…,…Xn\\u200b — это просто её совместная pmf или pdf. Вне зависимости от типа распределения будем обозначать правдоподобие как\\nL(θ)≡L(X1,…,Xn∣θ)=p(X1,…,Xn∣θ).\\\\mathcal L(\\\\theta) \\\\equiv L(X_1, \\\\ldots, X_n \\\\vert \\\\theta) = p(X_1, \\\\ldots, X_n \\\\vert \\\\theta).\\nL(θ)≡L(X1\\u200b,…,Xn\\u200b∣θ)=p(X1\\u200b,…,Xn\\u200b∣θ).Если выборка i.i.d., то функция правдоподобия распадается в произведение одномерных функций:\\nL(X1,…,Xn∣θ)=∏k=1np(Xk∣θ).L(X_1, \\\\ldots, X_n \\\\vert \\\\theta) = \\\\prod\\\\limits_{k=1}^n  p(X_k\\\\vert \\\\theta). \\nL(X1\\u200b,…,Xn\\u200b∣θ)=k=1∏n\\u200bp(Xk\\u200b∣θ).Оценка максимального правдоподобия (maximum likelihood estimation, MLE) максимизирует правдоподобие:\\nθ^ML=arg\\u2061max\\u2061θL(θ)    \\\\widehat \\\\theta_{\\\\mathrm{ML}} =  \\\\arg \\\\max\\\\limits_{\\\\theta} \\\\mathcal L(\\\\theta)\\nθML\\u200b=argθmax\\u200bL(θ)Поскольку максимизировать сумму проще, чем произведение, обычно переходят к логарифму правдоподобия (log-likelihood). Это особенно удобно в случае i.i.d. выборки, тогда\\nθ^ML=arg\\u2061max\\u2061θlog\\u2061L(θ)=arg\\u2061max\\u2061θ∑k=1nlog\\u2061p(Xk∣θ).    \\\\widehat \\\\theta_{\\\\mathrm{ML}} =  \\\\arg \\\\max\\\\limits_{\\\\theta} \\\\log \\\\mathcal L(\\\\theta) =\\n    \\\\arg \\\\max\\\\limits_{\\\\theta} \\\\sum\\\\limits_{k=1}^n \\\\log p(X_k\\\\vert \\\\theta).\\nθML\\u200b=argθmax\\u200blogL(θ)=argθmax\\u200bk=1∑n\\u200blogp(Xk\\u200b∣θ).Пример. В результате nnn подбрасываний монеты выпало kkk «орлов» и n−kn-kn−k «решек».\\nОценим вероятность выпадения «орла» методом максимального правдоподобия.\\nПусть ppp — вероятность выпадения «орла», тогда правдоподобие равно\\nL(p)=pk(1−p)n−k.\\\\mathcal L(p)=p^k (1-p)^{n-k}.\\nL(p)=pk(1−p)n−k.Дифференцируя логарифм правдоподобия\\nlog\\u2061L(p)=klog\\u2061p+(n−k)log\\u2061(1−p)\\\\log \\\\mathcal L(p) = k\\\\log p + (n-k)\\\\log(1-p)\\nlogL(p)=klogp+(n−k)log(1−p)и приравнивая к нулю производную, находим\\nkp=n−k1−p\\u2005\\u200a⟺\\u2005\\u200ak(1−p)=(n−k)p\\u2005\\u200a⟺\\u2005\\u200ap=kn.    \\\\frac kp = \\\\frac{n-k}{1-p} \\\\iff k(1-p) = (n-k)p \\\\iff p = \\\\frac kn.\\npk\\u200b=1−pn−k\\u200b⟺k(1−p)=(n−k)p⟺p=nk\\u200b.Нетрудно убедиться, что это точка максимума. Итак, оценка максимального правдоподобия p^ML=kn\\\\widehat p_{\\\\mathrm{ML}} = \\\\frac knp\\u200bML\\u200b=nk\\u200b вероятности «успеха» в схеме Бернулли вполне ожидаемо оказалась равна доле «успехов» в серии из nnn испытаний.\\nУпражнение. Пусть i.i.d. выборка X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b взята из пуассоновского распределения с параметром λ\\\\lambdaλ. Найдите его оценку максимального правдоподобия.\\nОтветλ^ML=X‾n=1n∑k=1nXk.\\\\widehat \\\\lambda_{ML} = \\\\overline X_n = \\\\frac 1n \\\\sum\\\\limits_{k=1}^n X_k.\\nλML\\u200b=Xn\\u200b=n1\\u200bk=1∑n\\u200bXk\\u200b.Методом максимального правдоподобия можно оценить сразу несколько параметров.\\nПример. Найдём MLE-оценки параметров распределения N(μ,τ)\\\\mathcal N(\\\\mu, \\\\tau)N(μ,τ) по i.i.d. выборке X1,…,XnX_1, \\\\ldots, X_nX1\\u200b,…,Xn\\u200b.\\nЗапишем правдоподобие:\\nL(μ,τ)=∏k=1n12πτexp\\u2061−(Xk−μ)22τ.  \\\\mathcal L(\\\\mu, \\\\tau) = \\\\prod\\\\limits_{k = 1}^n \\\\frac{1}{\\\\sqrt{2\\\\pi\\\\tau}} \\\\exp{\\\\frac{-(X_k - \\\\mu)^2}{2\\\\tau}}.\\nL(μ,τ)=k=1∏n\\u200b2πτ\\u200b1\\u200bexp2τ−(Xk\\u200b−μ)2\\u200b.Перейдём к log-likelihood:\\nlog\\u2061L(μ,τ)=−n2(log\\u2061τ+ln\\u20612π)−12τ∑k=1n(Xk−μ)2.  \\\\log \\\\mathcal L(\\\\mu, \\\\tau) = -\\\\frac{n}{2}(\\\\log{\\\\tau} + \\\\ln{2\\\\pi}) - \\\\frac{1}{2\\\\tau} \\\\sum\\\\limits_{k = 1}^n (X_k - \\\\mu)^2.\\nlogL(μ,τ)=−2n\\u200b(logτ+ln2π)−2τ1\\u200bk=1∑n\\u200b(Xk\\u200b−μ)2.Приравняем частные производные по μ\\\\muμ и τ\\\\tauτ к нулю:\\n∂log\\u2061L∂μ=1τ∑k=1N(Xk−μ)=0,  \\\\frac{\\\\partial \\\\log \\\\mathcal L}{\\\\partial \\\\mu} = \\\\frac{1}{\\\\tau}\\\\sum\\\\limits_{k = 1}^N (X_k - \\\\mu) = 0,\\n∂μ∂logL\\u200b=τ1\\u200bk=1∑N\\u200b(Xk\\u200b−μ)=0,∂log\\u2061L∂τ=−nτ+1τ2∑k=1n(Xk−μ)2=0,  \\\\frac{\\\\partial \\\\log \\\\mathcal L}{\\\\partial \\\\tau} = -\\\\frac{n}{\\\\tau} + \\\\frac{1}{\\\\tau^2}\\\\sum\\\\limits_{k = 1}^n (X_k - \\\\mu)^2 = 0,\\n∂τ∂logL\\u200b=−τn\\u200b+τ21\\u200bk=1∑n\\u200b(Xk\\u200b−μ)2=0,откуда μ^ML=X‾n\\\\widehat\\\\mu_{\\\\mathrm{ML}} = \\\\overline{X}_nμ\\u200bML\\u200b=Xn\\u200b – выборочное среднее, τ^ML=1n∑k=1nXk2−(X‾n)2\\\\widehat\\\\tau_{\\\\mathrm{ML}} = \\\\frac{1}{n} \\\\sum\\\\limits_{k = 1}^n X_k^2 - (\\\\overline{X}_n)^2τML\\u200b=n1\\u200bk=1∑n\\u200bXk2\\u200b−(Xn\\u200b)2 – выборочная дисперсия.\\nУпражнение. Пусть i.i.d. выборка X1,…,Xn∼U[a,b]X_1, \\\\ldots, X_n \\\\sim U[a, b]X1\\u200b,…,Xn\\u200b∼U[a,b]. Найдите оценки максимального правдоподобия для параметров aaa и bbb.\\nРешение (не открывайте сразу, попробуйте сначала решить самостоятельно)Оказывается, при поиске MLE не всегда надо дифференцировать. Правдоподобие здесь имеет вид\\nL(X1,…,Xn∣a,b)=1(b−a)n∏k=1nI(Xk∈[a,b]).    L(X_1, \\\\ldots, X_n\\\\vert a, b) = \\\\frac 1{(b-a)^n}\\\\prod\\\\limits_{k=1}^n \\\\mathbb I(X_k \\\\in [a, b]).\\nL(X1\\u200b,…,Xn\\u200b∣a,b)=(b−a)n1\\u200bk=1∏n\\u200bI(Xk\\u200b∈[a,b]).При фиксированных иксах и bbb это выражение максимально при a=X(1)a = X_{(1)}a=X(1)\\u200b: ведь если взять чуть больше, то произведение индикаторов обнулится, если меньше — то правдоподобие уменьшится за счёт увеличения (b−a)n(b-a)^n(b−a)n. По аналогичным соображениям b^ML=X(n)\\\\widehat b_{\\\\mathrm{ML}} = X_{(n)}bML\\u200b=X(n)\\u200b.\\nСвойства оценки максимального правдоподобия\\n\\nсостоятельность: θ^ML→Pθ\\\\widehat \\\\theta_{\\\\mathrm{ML}} \\\\stackrel{P}{\\\\to} \\\\thetaθML\\u200b→Pθ;\\nинвариантность относительно параметризации: если θ^ML\\\\widehat \\\\theta_{\\\\mathrm{ML}}θML\\u200b — MLE-оценка для θ\\\\thetaθ, то φ(θ^ML)\\\\varphi\\\\left( \\\\hat{\\\\theta}_{ML} \\\\right)φ(θ^ML\\u200b) — MLE-оценка для φ(θ)\\\\varphi(\\\\theta)φ(θ);\\nасимптотическая нормальность: θ^ML−θse^→DN(0,1)\\\\frac{\\\\widehat \\\\theta_{\\\\mathrm{ML}} - \\\\theta}{\\\\widehat{\\\\mathrm{se}}} \\\\stackrel{D}{\\\\to} \\\\mathcal N(0,1)seθML\\u200b−θ\\u200b→DN(0,1);\\nасимптотическая оптимальность: при достаточно больших nnn оценка\\nθ^ML\\\\widehat \\\\theta_{\\\\mathrm{ML}}θML\\u200b имеет минимальную дисперсию.\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф16.5. Независимость и условные распределения вероятностейСледующий параграф16.7. Энтропия и семейство экспоненциальных распределенийЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_29.html', 'title': 'Обучение представлений'}, page_content='Обучение представленийЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/27.1.Обучение представленийНейронные сети и выучивания представленийSupervised обучениеSelf-supervised обучениеПослесловиеПочитать по теме7.2.Дистилляция знаний8.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Обучение представлений7.1. Обучение представленийАвторыАшуха АрсенийОбъекты, с которыми работает машинное обучение, очень разнообразны и часто состоят из большого количества низкоуровневых сигналов: это может быть цвет отдельного пикселя, амплитуда звукового сигнала в отдельно взятый момент времени или буква в тексте.\\nКаждый из таких слабых сигналов в отдельности несёт крайне мало информации про объект, но все вместе слабые сигналы складываются в музыку, изображение или текст.\\nВ сыром виде такие объекты сложно анализировать, поэтому наша цель — находить хорошие представления данных, удобные для анализа и решения разных задач. Любой способ построения признакового описания объекта мы будем назвать алгоритмом построения представлений, а настройку такого алгоритма по данным — обучением.\\nТак, все методы понижения размерности, например, SVD — это методы обучения представлений, а методы обучения представлений часто можно проинтерпретировать как методы понижения размерности.\\nПредставим, что нам хочется уметь искать похожие музыкальные треки и использовать эту технологию в музыкальном сервисе для функции «играть похожие треки».Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nКаждый трек в нашей базе хранится в формате WAV с частотой 44kHz и длится 3 минуты. Другими словами, трек будет описываться вектором из 7920000 чисел (44000 Hz * 60 секунд * 3 минуты).\\nОднако небольшие изменения треков (сдвиг по времени, изменение громкости) могут соответствовать существенному изменению положения вектора представлений в пространстве. Поэтому простые расстояния в таком пространстве, вероятно, не будут отражать представление людей о схожести треков. Искать похожие треки, используя такие, сырые представления, проблематично, и необходимо научится строить представления, с помощью которых будет удобно решать разные высокоуровневые задачи.\\nВыученные представления или модели для вычисления представлений можно использовать для:\\n\\nПоиска изображений: по представлению изображения искать похожие изображения.\\nРекомендаций: по представлениям пользователя определять наиболее интересные фильмы или товары.\\nЧат-ботов: по представлению диалога уметь продолжать диалог, отвечать на вопросы и так далее.\\nСистем видеосвязи: уметь восстанавливать кадры по компактным представлениям, сохраняя высокое качество картинки.\\nи многого другого.\\n\\nДля каждой из этих задач будут свои хорошо отлаженные трюки, однако ключевых идей, которые часто переиспользуются, повторяются и модифицируются, не очень много. Мы постараемся рассказать про такие идеи.\\nПредупреждение: перед прочтением этой главы стоит освежить в памяти главу про\\nнейронные сети.\\nНейронные сети и выучивания представлений\\nНейронные сети можно рассматривать, как механизм автоматического выучивания представлений, поэтому современные методы выучивания представлений существенно сконцентрированы на использовании нейросетей.\\nНапомним, что нейронная сеть состоит из набора дифференцируемых преобразований, примененных друг за другом к объекту xxx для получения предсказания целевой переменной yyy. Обычно преобразования содержат обучаемые параметры, которые настраиваются в процессе обучения по данным.\\nПреобразования в литературе часто называют слоями. Результат применения преобразования к его входу мы будем называть скрытыми представлениями или активациями.\\nRepresentations\\nАктивации любого слоя можно использовать как представления объекта. Представления с разной глубины нейронной сети будут обладать разными свойствами.\\nРассмотрим свёрточные нейронные сети для изображений. Активации первых слоёв обычно видят только маленькие части исходной картинки, другими словами, имеют маленький receptive field. Такие активации могут реагировать — принимать большие значения — только на низкоуровневые детали, маленькие фрагменты изображения.\\nПо мере увеличения глубины receptive field становится больше, а активации начинают реагировать на более высокоуровневые абстракции, такие как формы и части объектов. Активации последних слоёв имеют большой receptive field и реагируют на уже на объекты и группы объектов.\\n\\n\\n\\nИсточник\\n\\n\\nНа изображении ниже показаны части картинок (патчи), каждая группа из 9 изображений максимизирует значение определенной активации в обученной нейронной сети. Размер патча зависит от receptive field активации, а максимизация ведется по датасету реальных изображений:  выбираем топ-9 патчей из датасета по значению активации.\\nДля активаций, взятых с ранних слоев, нейроны реагируют на низкоуровневые детали. По мере увеличения глубины нейроны начинают реагировать на более высокоуровневые объекты.\\n\\n\\n\\nИсточник\\n\\n\\nБольшая часть методов, которые мы рассмотрим ниже, за исключением матричной факторизации (в зависимости от того, как на это взглянуть), будут использовать активации нейросети в качестве представлений. Поэтому, обучить представления и обучить нейросеть это почти синонимы. Большинство отличий будет состоять в том, как эти нейронные сети обучаются и какую архитектуру имеют.\\nДообучение\\nНейронные сети можно обучать из случайной инициализации, а можно стартовать с вектора весов, обученного на внешнем датасете.\\nК примеру, если вы решаете задачу классификации изображений, часто инициализация части вашей нейронной сети весами, предобученными на популярном датасете ImageNet, ускорит и улучшит обучение.\\nТакой процесс называется fine-tuning («дообучение» / «файнтюнинг»):\\n\\n\\n\\nИсточник\\n\\n\\nКак можно усложнять эту схему:\\n\\nДобавлять в модель много новых, обучающихся с нуля слоёв (на картинке мы добавляем один, но можно и больше);\\nНе обязательно копировать все слои, можно копировать только сколько-то первых.\\nДообучать как все веса модели, так и какую-то часть. К примеру, можно заморозить скопированные слои и дообучать только новые части модели.\\nДля файнтюнинга часто используют постепенное увеличение (warm-up) learning rate на первых эпохах обучения. Это позволяет сетке «привыкнуть» к новой задаче и архитектуре. Пример:\\n\\n\\n\\n\\nИсточник\\n\\n\\nPrior\\nВ некотором смысле хорошая инициализация работает как праер на функции, которые могут быть выучены после дообучения.\\nПоэтому дообучение часто требует в разы меньше данных, чем обучение со случайной инициализации.\\nSupervised обучение\\nОбучение представлений через решение supervised задачи\\nОбучение представлений через решение supervised задач — это простой и популярный способ обучения представлений. Рассмотрим его на примере задачи поиска изображений (image retrieval)\\nЗадача: Рассмотрим задачу поиска изображений. Каждое изображение хочется закодировать вектором признаков (представлением) так, чтобы вектора признаков похожих изображений были близки.\\nПри чем тут обучение представлений? Image retrieval часто рассматривается как задача обучения представлений. Хочется получить алгоритм, который по изображению выдаст вектор (представление объекта) так, чтобы близость векторов по какой-то простой (скажем, евклидовой) метрике означала схожесть объектов.\\nИдея: Возьмём активации с последнего слоя из нейросети, предобученной на большом размеченном датасете.\\nДля задач зрения почти всегда имеется ввиду предобучение на задаче классификации. Также мы предполагаем, что высокоуровневая разметка собрана человеком.\\nПочему такие представления могут адекватно решать задачу поиска изображений? Классификационная сеть будет неявно поощрять, чтобы у похожих изображений векторы активаций тоже были близки. К примеру, перед последнем слоем классификационной сети активации кошек и собак должны быть распознаны линейными классификаторами — активации картинок одного класса должны быть близки друг к другу. А за счет похожих паттернов визуально похожие коты будут находиться ближе друг к другу, чем непохожие.\\nРешение\\nДля начала нам нужно обучить нейросеть на большом размеченном датасете картинок/текста/звука/... (pretext problem)\\nОбычно лучше всего работает предобучение на задачах классификации. Почему так происходит? Пока непонятно. Возможно, это связано с тем, что для классификации удобнее собирать датасеты, а возможно это хорошие свойства задачи или CrossEntropy функции потерь.\\n\\nДля изображений часто используется предобучение на датасете ImageNet — классификация на 1000 классов, 1.3M изображений в обучающей выборке, ~ 150 GiB.\\nДля текстов, обычно решают задачу языкового моделирования, на набирающем популярность датасет The Pile ~ 825 GiB.\\n\\nЗатем мы дообучаем нейросеть на более похожем на нашу задачу размеченном датасете  (если такой есть; если нет пропускаем этот шаг).\\nПосле — оставляем только первые LLL слоёв. Aктивации слоя LLL берём в качестве представлений объектов\\nАгрегируем активации по пространственным координатам, чтобы получить вектор для каждого объекта. Часто используется покомпонентное среднее или максимум (скажем, глобальный пулинг для изображений).\\nНаконец, используем признаки или предобученные веса для решения целевой задачи (downstream problem).\\nОб алгоритме\\nSupervised подход можно применять для различных типов данных. Всё, что нужно — это большой размеченный датасет, хоть и отдалённо, но похожий на целевые данные. Для музыки это может быть задача классификации жанра, для видео — задача классификация действий, для текста — классификация тематик.\\nДостоинства и недостатки\\n👍 Благодаря выученным представлениям мы сможем решать целевую задачу, не имея для неё большого датасета;\\n👎 Нужен большой размеченный датасет, близкий для целевой задачи;\\n👎 Оптимальные представления для датасета, на котором мы предобучаемся, могут сколь угодно плохо подойти для целевой задачи. К примеру, представления, полученные на ImageNet, плохо подойдут для медицинских изображений (Raghu2019).\\nЭксперименты\\nНа рисунке ниже показан пример того, как представления помогают решать задачу поиска. Запрос находится в самом левом столбике. Зеленым отмечены верно найденные изображения, красным — неверно найденные, синим — изображения из стоп-листа.\\nКак видно, система вполне неплохо решает задачу поиска изображений. Подробнее про такой подход для поиска изображений можно почитать в работах  (Babenko 2014, Babenko 2015).\\n\\n\\n\\nИсточник\\n\\n\\nСоветы\\nСтатья Big Transfer (BiT): General Visual Representation Learning (Kolesnikov at el. 2019) даёт ряд важных советов, о том, на что именно стоит обратить внимание при supervised обучении c целью переноса представлений и весов модели на новые задачи.\\nРассмотрим их подробнее.\\n\\n\\nБольшие и разнообразные датасеты: Увеличение размера pretex-датасета вносит существенный вклад в качество решения downstream задачи. Авторы продемонстрировали, что при предобучении переход от 1М изображений (ImageNet) к 14М изображений (ImageNet21k) к 300M изображений (JFT), стабильно улучшает качество дообучения на новой задаче с маленьким числом размеченных примеров. Да, тут мы заходим на территорию, когда ImageNet рассматривается как маленький датасет.\\n\\n\\nБольшие pretext модели: Увеличение датасета при недостаточном размере модели может навредить. Нужно одновременно иметь большие модели и большие датасеты. Одно из возможных объяснений такое: с увеличением датасета модель должна предсказывать правильные ответы на трейне для огромного числа точек. При этом нельзя работать очень плохо хоть на каких-то точках, ведь когда гибкости недостаточно, моделька настраивается «так себе» во многих областях пространства, что может ухудшить финальное качество алгоритма.\\n\\n\\nДолгое обучение: Большие модели требуют много шагов оптимизации.\\n\\n\\nОговоркаБольшие модели часто учатся на десятках, сотнях или даже тысячах вычислителей, таких как GPU и TPU, и требуют много памяти для обучения. Как правило, это происходит из-за большого размера моделей: на один вычислитель часто помещаются только маленькие минибатчи (1-10 примеров). Маленькие минибатчи работают плохо с популярной Batch Normalization или требуют дорогой синхорнизации между вычислителями.\\nВ статье (Kolesnikov at el. 2019) Batch Normalization заменяется на Group Normalization, которая позволяет использовать батчи маленького размера на каждом отдельном вычислительном девайсе. Group Normalization  используется в комбинации c  Weight Standardization , что позволяет улучшить обучение.\\nОбучение метрических эмбедингов с использование разметки (triplet loss)\\nМотивация: После supervised обучения расстояния между эмбеддингами не обязаны хорошо отражать треубуемую для решения нашей задачи «похожесть». Поэтому хочется, чтобы «похожесть» моделировалась известным расстоянием (к примеру евклидовым).\\nДля этого была предложена триплетная фунция потерь или triplet loss (Schroff at el. 2015). Триплетный лосс обучается на тройках объектов (якорный объект, негативный к якорному, позитивный к якорному). Информация о позитивных и негативных объектах – это один из видов разметки. Этот лосс может использоваться как для обучения с нуля, так и для дообучения.\\nОтметим, что объекты не обязательно должны быть из одного домена: к примеру, якорный объект может быть картинкой, а позитивные и негативые объекты текстами. Таким образом, мы сможем находить «подходящие» тексты к картинкам и наоборот.\\nКак будет устроено обучение\\n\\nрассмотрим тройки объектов (obji,posi,negi)(\\\\text{obj}_i,  \\\\text{pos}_i, \\\\text{neg}_i)(obji\\u200b,posi\\u200b,negi\\u200b), где posi\\\\text{pos}_iposi\\u200b — позитивный пример к obji\\\\text{obj}_iobji\\u200b, negi\\\\text{neg}_inegi\\u200b — негативный пример к obji\\\\text{obj}_iobji\\u200b\\nбудем притягивать emb(obji)emb(\\\\text{obj}_i)emb(obji\\u200b) и emb(posi)emb(\\\\text{pos}_i)emb(posi\\u200b) и отталкивать emb(obji)emb(\\\\text{obj}_i)emb(obji\\u200b) и emb(negi)emb(\\\\text{neg}_i)emb(negi\\u200b)\\nодним из популярных лоссов для решения такой задачи является triplet loss:\\n\\nL=∑iN[∥embθ(obji)−embθ(posi)∥22−∥embθ(obji)−embθ(negi)∥22+α]+.L=\\\\sum_i^N\\\\left[\\\\left\\\\|emb_\\\\theta(\\\\text{obj}_i)-emb_\\\\theta(\\\\text{pos}_i)\\\\right\\\\|_2^2 - \\\\left\\\\|emb_\\\\theta(\\\\text{obj}_i)-emb_\\\\theta(\\\\text{neg}_i)\\\\right\\\\|_2^2+\\\\alpha\\\\right]_+.\\nL=i∑N\\u200b[∥embθ\\u200b(obji\\u200b)−embθ\\u200b(posi\\u200b)∥22\\u200b−∥embθ\\u200b(obji\\u200b)−embθ\\u200b(negi\\u200b)∥22\\u200b+α]+\\u200b.\\nembembemb представляется нейронной сетью embθ(⋅)emb_\\\\theta(\\\\cdot)embθ\\u200b(⋅)\\nα\\\\alphaα — параметер зазора — в некотором смысле усложняет задачу:\\n\\nпри alpha=0alpha=0alpha=0 достаточно, чтобы позитивный эмбединг был ближе якорному, чем негативный;\\nс параметром alpha=0.5alpha=0.5alpha=0.5 мы начинаем требовать, чтобы позитивный был ближе, чем негативный, как минимум на 0.50.50.5.\\n\\n\\nлосс LLL оптимизируем по параметрам θ\\\\thetaθ.\\n\\n\\nПочему бы просто не притягивать позитивные примеры? Нужны ли нам негативные?Если будем только притягивать, то любой константный вектор будет хорошим решением. К примеру emb(x)=[1,…,1]emb(x) = [1,\\\\dots, 1]emb(x)=[1,…,1].\\nАлгоритм формирования троек\\nОбучение с триплет лоссом сильно зависит от алгоритма формирования троек.\\nЕсли формировать тройки случайно, то большинство троек будут слишком легкими, не информативными. Негативные объекты будет слишком легко отличить от позитивных, поэтому обучающего сигнала от таких троек будет мало.\\nПоэтому хочется собрать наиболее сложные тройки из всех объектов в датасете или минибатче.\\nТакой процесс называется hard negative/positive mining и часто используется для обучения с триплетной функцией потерь.\\n🧪 Примеры:\\nПримеры\\n\\nДиалоговая система:\\n\\nobji\\\\text{obj}_iobji\\u200b — фраза;\\nposi\\\\text{pos}_iposi\\u200b — подходящий ответ;\\nnegi\\\\text{neg}_inegi\\u200b — ответ не в тему.\\n\\n\\nВерификация лица:\\n\\nobji\\\\text{obj}_iobji\\u200b — лицо которое хотим верифицировать;\\nposi\\\\text{pos}_iposi\\u200b — тот же человек, что и в obji\\\\text{obj}_iobji\\u200b, но с других ракукрсов, в другом освещении, ...;\\nnegi\\\\text{neg}_inegi\\u200b — лица других людей.\\n\\n\\n\\n\\nДостоинства и недостатки\\n👍 Обучение метрических эмбедингов (metric learning), в отличие от supervised подхода, использует информацию о метрике, что позволяет выучить более релевантные представления для целевой задачи.\\n👎 Все еще требует разметки (на тройки объектов).\\n👎 Обучение с триплетным лоссом часто ведет себя нестабильно (еще нестабильнее, чем обучение нейросетей для других задач).\\nSelf-supervised обучение\\nВ этом разделе мы хотим показать, что нейронные сети и представления можно предобучать без рукотвороной разметки.\\nМотивация Мы разобрали supervised предобучение нейронных сетей и их использование для извлечения признаков. Однако supervised подходы не всегда эффективны. Supervised обучение требует больших размеченных датасетов.\\nРазметка данных — это трудоёмкий и дорогой процесс, на выходе от которого всё равно получается шумная, и зачастую смещенная разметка. Поэтому от ручной разметки данных хочется уйти или хотя бы постараться её минимизировать.\\nЭтого можно добиться, если научиться использовать неразмеченные данные для предобучения. Неразмеченные данные генерируются в огромном количестве, и их значительно проще собирать. Это позволит нам обучаться на  огромных коллекциях данных, размер которых был бы недостижим при необходимости сбора разметки.\\nТакже в каждом объекте, изображении, звуке или тексте содержится в разы больше информации, которую можно учитывать при обучении, чем закодировано в одном таргете.\\nК примеру, один из тысячи классов можно закодировать всего десятью битами, а изображение содержит мегабайты внутренней полезной информации, котрую можно использовать для обучения. Поэтому подходы, которые могут обучаться без разметки, но с использованием внтурненнией информации, потенциально могут выучивать более хорошие представления, чем supervised подходы.\\n💡Основная идея self-supervised обучения — обучение через решение синтетических supervised задач (pretext problems), источником разметки в которых является сам объект (текст, изображение, или видео). Отсюда и приставка \"self\" в названии подхода.\\nПримеры pretext задач\\n\\nпредсказание объекта по его компактному описанию;\\nпредсказание слова по контексту;\\nпредсказание закрытой части изображения по открытой;\\nпредсказание будущих кадров по прошлым.\\n\\nЕсли всё это кажется вам supervised-задачами, вы правы! Приставлка self- означает отсутствие внешней разметки.\\nПризнаки и веса, выученные для решения, казалось бы, бесполезных pretext задач, на практике работают как очень хороший претрейнинг для решения supervised задач (downstream problems). Это позволяет достигать отличного качества, используя в сотни раз меньше размеченных данных по сравнению с чисто supervised подходами.\\nОсталось ответить на вопрос: какие pretext задачи использовать?\\nУниверсального ответа нет, но оказывается, что многие pretext задачи используют контекст для обучения. Подробнее об этом расскажем далее.\\nИспользование контекста для обучения\\nПочему контекст так важен для обучения? Обучение людей, как и обучение алгоритмов, неразрывно связано с использованием контекста.\\nПри изучении иностранного языка часто прибегают к упражнениям вида «Вставте правильные слова в текст». Чтобы выполнить такое упражнение, человеку нужно учитывать контекст и предсказывать значения незнакомых слов, если это необходимо для понимания текста.\\n\\nПредложенная профессором Южно-Калифорнийского университета Стивином Крашенйном «гипотезы входного материала» (input hypothesis) предполагают, что для эффективного изучения языка человеку нужно читать и слушать текст, который немного превышает его текущий уровень. Скажем, содержит 10-15% незнакомых слов, но при этом остается понятным. Такой способ обучения требует восстановления значения незнакомых слов из контекста.\\nВизуальный контекст также широко используется при обучении детей. Вы можете помнить упражнения, в которых нужно было найти лишний предмет, закончить рисунок или раскрасить изображение. Такие задания требуют учета визуального контекста для решения задачи: важно уметь понимать принадлежность разных рисунков к одной группе, генерировать изображение, наблюдая только некоторую его часть и так далее.\\n\\nSelf-supervised обучение представлений и моделей глубокого обучения использует похожие идеи обучения из контекста.\\nК примеру, модель  word2vec (Mikolov et al. 2013) и BERT (Devlin et al. 2018) выучивают эмбединги слов, решая задачу предсказания слов по контексту. С философией word2vec вы уже познакомились в параграфе про нейросети для работы с последовательностями.\\nА некоторые модели для картинок решают пазлы (Doersch, Noroozi et al. 2017), дополняют изображения или звуки (van den Oord et al. 2018), раскрашивают фотографии (Zhang et al. 2016) и ищут похожие объекты (Chen et al. 2020).\\nПоследнюю из них — SimCLR – мы подробно разберём ниже.\\nSelf-supervised предобучение для изображений\\nSimCLR — метод, который первым продемонстрировал, что self-supervised предобучение может достигать того же качества что и supervised обучение.\\nОн основан на контрастивной функции потерь (contrastive loss), и в некотором смысле решает задачу поиска похожих объектов.\\nТакже мы разберем метод self-supervised предобучения для vision tranformer, который, в некотором смысле, дорисовывает картинку, а также демонстрирует, что методы self-supervised предобучения для изображений и текстов во многом похожи.\\nСтоит отметить, что pretext задачи, которые мы будем обсуждать ниже, не являются «серебряной пулей».\\nИзвестно, что такие задачи работают как хороший претрейнинг. Другими словами, позволяют получить хорошее качество для некоторых downstream задач (классификация, детекция, сегментация) после дообучения на небольшом количестве размеченных примеров.\\nХорошего понимания, почему эти методы работают, в области пока нет. Скорее всего, разные типы задач (downstream problems) будут требовать разных методов претрейнинга, но это мы поймём только в ближайшие несколько лет.\\nA Simple Framework for Contrastive Learning of Visual Representations (SimCLR)\\nSimCLR решает синтетическую задачу поиска похожих  изображений. Вот как он работает на верхнем уровне:\\n\\nДля каждого изображения в минибатче генерируются две аугментации;\\nВыбирается одно из изображений; одна из его аугментаций считается запросом, вторая — позитивным ответом, аугментации остальных объектов — негативными примерами;\\nЦель модели — для каждого «запроса» найти позитивный пример.\\nВыученные веса могут быть использованны для «дообучения»/«файнтюнинга» сети под финальную задачу.\\n\\n\\n\\n\\n    Иллюстрация задачи SimCLR для одного запросса из минибатча\\n  \\n\\nА вот как работает SimCLR на низком уровне\\n\\n\\nИсточник\\n\\n\\nЛосс\\nSimCLR оптимизирует контрастив лосс (contrastive loss), который фактически является кросс энтропией на positive-negative разметке:\\nLpositivequery=−logexp\\u2061(sim(emb(query),emb(positive)))∑z∈Neg(query)exp\\u2061(sim(emb(query),emb(z))),L_{positive}^{query}=-log\\\\frac{\\\\exp\\\\left(sim(emb(query),emb(positive))\\\\right)}{\\\\sum_{z\\\\in\\\\mathrm{Neg}(query)}\\\\exp\\\\left(sim(emb(query),emb(z))\\\\right)},\\nLpositivequery\\u200b=−log∑z∈Neg(query)\\u200bexp(sim(emb(query),emb(z)))exp(sim(emb(query),emb(positive)))\\u200b,где sim(⋅,⋅)sim(\\\\cdot,\\\\cdot)sim(⋅,⋅) — это косинусное расстояние, a лосс LLL работает следующим образом:\\n\\nКонтрастивная функция потерь LpositivequeryL_{positive}^{query}Lpositivequery\\u200b притягивает друг к другу эмбединги запроса emb(query)emb(\\\\color{#5180e6}{\\\\textit{query}})emb(query) и позитивного примера emb(positive)emb(\\\\color{#72bd44}{\\\\textit{positive}})emb(positive), в то же самое время отталкивая эмбединги негативных примеров emb(z)emb(\\\\color{#fd0007}{z})emb(z);\\nМаксимум sim(query,positive)sim(\\\\color{#5180e6}{\\\\textit{query}}, \\\\color{#72bd44}{\\\\textit{positive}})sim(query,positive) будет достигается в точке query=positive\\\\color{#5180e6}{\\\\textit{query}}=\\\\color{#72bd44}{\\\\textit{positive}}query=positive, поэтому эмбединги аугментаций одной и той же картинки будут притягиваться;\\nЗнаменатель требует, чтобы негативные эмбединги были далеко от запроса.\\n\\nИнтуиция: На контрастивную функцию потерь можно смотреть как на поиск ответа по запросу, который ведется только среди всех эмбедингов в текущем минибатче. Такая задача требует сохранения информации про контент на изображении (что, вообще говоря, не очень просто) и в то же время понижения размерности, так как эмбединги f(⋅)f(\\\\cdot)f(⋅) обычно имеют сравнительно низкую размерность.\\nРазмер минибатча: Размер минибатча влияет на количество отрицательных примеров. Чем больше отрицательных примеров — тем более сложную задачу мы ставим перед нейросетью. Существует некоторый баланс между сложностью задачи и качеством выученных представлений. Слишком простые задачи (то есть маленькие батчи) обычно не позволяют выучить хороших представлений: простая задача может хорошо решаться даже с помощью «плохих» представлений. Поэтому SimCLR обучается хорошо только на очень больших мини-батчах (с тысячами примеров).\\nЧто нам нужно иметь перед началом обучения\\n\\nнеразмеченный датасет изображений X=x0,...,xNX = {x_0, ..., x_N}X=x0\\u200b,...,xN\\u200b\\nоперацию аугментации изображения aug(xj)aug(x_j)aug(xj\\u200b)\\nэнкодер fθ(⋅):Image→RMf_\\\\theta(\\\\cdot): Image \\\\to R^Mfθ\\u200b(⋅):Image→RM (типичные значения M~2048)\\nпроекция gψ(⋅):RM→RKg_\\\\psi(\\\\cdot): R^M \\\\to R^Kgψ\\u200b(⋅):RM→RK  (типичные значения K~128)\\n\\n✍️ В примере сверху emb(x)=g⊙f(x)emb (x) = g \\\\odot f (x)emb(x)=g⊙f(x)\\nКак мы обучаемся\\n\\n\\nСемплируем мини-батч объектов Х^∼X\\\\hat{Х} \\\\sim XХ^∼X;\\n\\n\\nДля каждого объекта в минибатче Х^\\\\hat{Х}Х^:\\n— Cемплируем две аугментации vi,v′i=aug(Х^i),aug(Х^i){v}_i, {v^\\\\prime}_i = aug(\\\\hat{Х}_i), aug(\\\\hat{Х}_i)vi\\u200b,v′i\\u200b=aug(Х^i\\u200b),aug(Х^i\\u200b);\\n— Вычисляем эмбединги yi,yi′=fθ(vi),fθ(vi′)y_i, y^\\\\prime_i =  f_\\\\theta(v_i), f_\\\\theta(v^\\\\prime_i)yi\\u200b,yi′\\u200b=fθ\\u200b(vi\\u200b),fθ\\u200b(vi′\\u200b);\\n— Вычисляем проекции  zi,zi′=gψ(vi),gψ(vi′)z_i, z^\\\\prime_i =  g_\\\\psi(v_i), g_\\\\psi(v^\\\\prime_i)zi\\u200b,zi′\\u200b=gψ\\u200b(vi\\u200b),gψ\\u200b(vi′\\u200b);\\n\\n\\nВычисляем contrastive loss L=∑ilzi′zi+∑ilzizi′\\\\mathcal{L}=\\\\sum_i l^{z_i}_ {z^\\\\prime_i} + \\\\sum_i l^{z^\\\\prime_i}_{z_i}L=∑i\\u200blzi′\\u200bzi\\u200b\\u200b+∑i\\u200blzi\\u200bzi′\\u200b\\u200b, используя sim(⋅,⋅)=uTv∥∥u∥∥\\xa0∥∥v∥∥sim(\\\\cdot,\\\\cdot) = \\\\frac{u^Tv}{\\\\|\\\\|u\\\\|\\\\|\\\\ \\\\|\\\\|v\\\\|\\\\|}sim(⋅,⋅)=∥∥u∥∥\\xa0∥∥v∥∥uTv\\u200b.\\nlpq=−logexp\\u2061(sim(q,p))∑zi,zi′≠q[exp\\u2061(sim(q,zj))+exp\\u2061(sim(q,zj′))]l_p^q=-log\\\\frac{\\\\exp(sim(q,p))}{\\\\sum_{z_i,z_i\\'\\\\neq q}[\\\\exp(sim(q,z_j))+\\\\exp(sim(q,z_j\\'))]}\\nlpq\\u200b=−log∑zi\\u200b,zi′\\u200b\\ue020=q\\u200b[exp(sim(q,zj\\u200b))+exp(sim(q,zj′\\u200b))]exp(sim(q,p))\\u200b— В L\\\\mathcal{L}L два слагаемых из-за того, что в паре (изображение, аугментация), вообще говоря, любой элемент можно выбрать в качестве запроса (другой тогда будет позитивным примером). Тем самым из одного мини-батча картинок мы можем сделать два мини-батча для обучения SimCLR.\\n— Функция потерь L\\\\mathcal{L}L вычисляется для низкоразмерных проекций zi,zi′∈RKz_i, z^\\\\prime_i  \\\\in R^Kzi\\u200b,zi′\\u200b∈RK.\\n\\n\\nДелаем шаг по градиенту ∇θ,ψL\\\\nabla_{\\\\theta,\\\\psi} \\\\mathcal{L}∇θ,ψ\\u200bL, повторяем с шага 1 пока не сойдёмся;\\n\\n\\nИспользуем fθ(⋅)f_{\\\\theta}(\\\\cdot)fθ\\u200b(⋅) для генерации эмбедингов или файнтюнинга под supervised задачу.\\n\\n\\nПочему это вообще работает?\\nТочно никто не знает, но приведем следующую гипотезу:\\nКонтрастивная функция потерь требует различать аугментации разных изображений. При этом эмбеддинги должны содержать информацию о контенте изображения, чтобы осуществлять поиск аугментаций одинаковых изображений по ключу. Этот процесс позволяет создать представления изображений, сохраняющие достаточно много информации про контент, чтобы решать не только задачу поиска аугментаций, но и другие задачи.\\nРезультаты\\nПретрейнинг, который мы обсудили выше, позволяет эффективно дообучать модели и получать качество, сравнимое с supervised обучением, используя в 100 раз меньше размеченных примеров.\\n\\nОговорка в том, что эти результаты получены второй весрсией метода SimCLRv2 (Chen at. el, 2020).\\nSimCLRv2 TLDR; модели больше, глубже сеть проекции, улучшение качества происходит за счет дистиляции.\\nНемного вопросов и ответов про SimCLRКакие аугментации выбрать?\\nАвторы предлагают использовать resize random crop, random flip, color distortions, Gaussian blur. Такая комбинация была найдена небольшим перебором. Для разных данных оптимальный набор аугментаций может получаться разным.\\n\\nСложно ли такое учится?\\nДа, но сложности в основном технические. Вы быстро оказываетесь наедине c размером батча 2048 на ImageNet (не забудьте ещё, что на каждую картинку 2 аугментации, поэтому реальный размер батча 4048). Даже с не самой большой сеткой, например, ResNet50 (25М параметров) приходится использовать не один десяток ГПУ с 32 Гб памяти в каждой и долго ждать, пока всё обучится. Процесс чтения 2k картинок с диска тоже может занимать намного больше времени, чем вы предпологали, а одновременное вычисление 4k аугментаций быстро создает bottleneck в CPU. Но если у вас много GPU, батч успевает грузиться быстро, и аугментации не упираются в CPU, так что основные сложности позади.\\nМожно ли обойтись без contrastive loss/негативных примеров?\\nДа можно: так делают авторы статьи \"Bootstrap your own latent: A new approach to self-supervised Learning\" (Grill et al.). Но пока метод достаточно новый, и его рано добавлять в учебник. Если вам очень хочется узнать про этот метод, рекомендуем посмотреть разбор с анализом на канале Yannic Kilcher.\\nVision Transformer и BERT-like обучение\\nОдна из самых популярных self-supervised задач в NLP — это предсказание замаскированных токенов (masked tokens prediction Devlin at el. 2019). При обучении такая модель (обычно transformer) видит текст, в котором некоторые токены заменены на специальный токен [MASK]; задача модели — правильно предсказать замаскированные токены по контексту.\\nОказывается, такой претрейнинг позволяет очень хорошо адаптировать модель для решения разных задач, таких как классификация текстов, используя при этом мало размеченных примеров.\\nОговоркаНа самом деле, модель будет способна не только предсказывать замаскированные токены, но и для оригинальных предсказывать «более логичные», на её взгляд замены: например, с её помощью можно исправлять опечатки. Это можно использовать на этапе обучения модели, чтобы показывать ей, что токена [MASK] может и не быть в тексте. Это может оказаться полезным на этапе файнтюнинга, где уже нет маскировки токенов.\\nПодробнее про Transformer, BERT, и masked tokens prediction можно прочитать в курсе NLP for You.\\nМожно ли использовать такой self-supervised подход для изображений? Оказывается, что да! В этом помогает vision transformer.\\nВ последнее время модели на основе vision transformer (ViT) (Dosovitskiy at el. 2020) бурно развиваются и  компьютерного зрения.\\n\\nВ supervised режиме для задачи классификации vision transformer обучается следующим образом:\\n\\nИзображение нарезается на квадратные патчи одинакового размера;\\nЗатем патчи вытягиваются в последовательность;\\nКаждый патч вытягивается в столбец пикселей;\\nКаждый стобец проецируется обучаемой матрицей;\\nК каждому вектору с шага 4 добавляются positional encoding (без позиционных эмбеддингов трансформер не учитывает позицию токена в последовательности, а positional encoding кодирует позицию токена и позволяют трансформеру учитывать эту информацию);\\nВекторы с шага 5 подаются в трансформер;\\nКлассификационный токен на выходе предсказывает распределение на классы;\\nВычисляется кросс-энтропия, делается шаг по её градиенту.\\n\\nViT не используют локальные операции, такие как свёртки. Как следствие, такие модели требуют заметно больше данных и параметров для обучения (300M изображений по сравнению со стандартным размером размеченного датасета 1.3М). Но оказывается, что BERT-like self-supervised обучение применимо и для моделей vision transformer, и позволяет обучать их без использовния гиганских датасетов.\\nКакие self-supervised задачи на замаскированных патчах решают авторы статьи:\\n\\nПредсказание среднего цвета в замаскированном патче;\\nПредсказание патча низкого разрешения и одновременное предсказание цвета;\\nПредсказание патча высокого разрешения разрешение с использованием L2L^2L2-лосса.\\n\\nВо всех случаях обучается и файнтюнится вся сеть целиком. Этот интересный пример показывает, что pretext-задачи придуманные, для NLP-сетей могут быть применены и к задачам зрения.\\nПослесловие\\nГлубинное обучение — в существенной степени наука о представлениях сложных объектов. В этом параграфе мы лишь слегка затронули несколько важных тем: supervised предобучение, self-supervised предобучение, и metric learning. Self-supervised предобучение — это важный новый раздел глубинного обучения, который,  вероятно, поможет серьезно сократить количество необходимой разметки во многих приложениях. Генеративные модели VAE/inverse-GANs также широко используются для получения и обработки представлений. О них вы сможете прочитать в следующих параграфах.\\nПочитать по теме\\n\\nContrastive Representation Learning, Lilian Weng, May 2021.\\nSelf-Supervised Representation Learning, Lilian Weng, Nov 2019.\\nСамообучение (Self-Supervision), Александр Дьяконов, Июнь 2020.\\nSelf-Supervised Learning | ICLR, Yann LeCun, May 2020.\\nSelf-Supervised Learning | UC Berkeley, CS294-158 Deep Unsupervised Learning, Aravind Srinivas, Spring 2020.\\nUnsupervised Representation Learning | DeepMind x UCL.\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф6.5. Нейросети для облаков точекСледующий параграф7.2. Дистилляция знанийЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_52.html', 'title': 'Сети бесконечной ширины'}, page_content=\"Сети бесконечной шириныЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/613.1.Введение в теорию глубокого обучения13.2.Обобщающая способность – классическая теория13.3.PAC-байесовские оценки риска13.4.Сети бесконечной шириныПрименение NTK-анализаNTK и Ядровые методыСходимость эмпирического ядраСтандартная параметризация и эволюция ядра13.5.Ландшафт функции потерь13.6.Implicit bias14.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Сети бесконечной ширины13.4. Сети бесконечной шириныАвторыГоликов ЕвгенийВо введении обсуждалось, что истинный риск нейронной сети выходит на асимптоту при стремлении ширины сети (то есть числа нейронов в слое) к бесконечности. Это намекает нам на то, что существует предельная модель, «бесконечно широкая сеть». В этом параграфе мы обсудим подходы к анализу её поведения.\\nДинамика обучения нейронной сети описывается эволюцией в пространстве весов – например, правилом обновления весов в градиентном спуске. Но в каком виде можно записать эволюцию бесконечно широкой сети, в которой весов бесконечно много? Есть два способа это сделать.\\nПервый способ – ввести меру в пространстве весов\\nВ качестве примера рассмотрим нейронную сеть с одним скрытым слоем, скалярным выходом и скалярным входом:\\nf(x)=1n∑i=1naiϕ(wix).f(x) = \\\\frac{1}{n} \\\\sum_{i=1}^n a_i \\\\phi(w_i x).\\nf(x)=n1\\u200bi=1∑n\\u200bai\\u200bϕ(wi\\u200bx).Это выражение можно представить в виде f(x)=∫R2aϕ(wx)\\u2009dμn(a,w)f(x) = \\\\int_{\\\\mathbb{R}^2} a \\\\phi(w x) \\\\, d\\\\mu_n(a,w)f(x)=∫R2\\u200baϕ(wx)dμn\\u200b(a,w), где мера μn\\\\mu_nμn\\u200b на R2\\\\mathbb{R}^2R2 сосредоточена в весах, ассоциированных с каждым из нейронов скрытого слоя:Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nμn(a,w)=1n∑i=1nδai(a)δwi(w).\\\\mu_n(a,w) = \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\delta_{a_i}(a) \\\\delta_{w_i}(w).\\nμn\\u200b(a,w)=n1\\u200bi=1∑n\\u200bδai\\u200b\\u200b(a)δwi\\u200b\\u200b(w).Здесь δx\\\\delta_xδx\\u200b – мера, сосредоточенная в xxx.\\nПри стремлении ширины nnn к бесконечности μn\\\\mu_nμn\\u200b может иметь предел. Так, если все веса насемплированы независимо из стандартного нормального распределения N(0,1)\\\\mathcal{N}(0,1)N(0,1), предельная мера принимает вид двумерного стандартного нормального распределения N(0,I2×2)\\\\mathcal{N}(0,I_{2\\\\times 2})N(0,I2×2\\u200b), а предсказание предельной сети можно записать в виде f(x)=Ea,w∼N(0,1)aϕ(wx)f(x) = \\\\mathbb{E}_{a,w \\\\sim \\\\mathcal{N}(0,1)} a \\\\phi(w x)f(x)=Ea,w∼N(0,1)\\u200baϕ(wx).\\nЗаметим, что множитель 1/n1/n1/n в определении модели выше принципиально важен для того, чтобы предельная мера и представление предельной сети в виде интеграла по мере были определены. Такая параметризация носит название mean-field parameterization.\\nДинамику эволюции весов также можно представить в виде эволюции меры. В самом деле, в случае конечной ширины градиентный спуск говорит нам о том, как за один шаг оптимизации меняются веса, ассоциированные с каждым из нейронов, или, что то же самое, как меняется мера μn\\\\mu_nμn\\u200b. Заменив в этом выражении меру μn\\\\mu_nμn\\u200b на предельную, можно получить эволюцию предельной меры.\\nК сожалению, представление эволюции предельной сети в виде эволюции меры не даёт сказать много о свойствах предельной модели. Так, известно, что предельная модель всегда сходится в глобальный минимум на обучающей выборке, см статью On the global convergence of gradient descent for over-parameterized models using optimal transport, но мало что известно о её обобщающей способности.\\nБолее того, лишь сети с одним скрытым слоем допускают простую формулировку в форме эволюции меры в пределе бесконечной ширины, см. статьи Mean field analysis of neural networks: A central limit theorem, On the global convergence of gradient descent for over-parameterized models using optimal transport и Trainability and accuracy of neural networks: An interacting particle system approach. Для сетей с большим числом слоёв подобная формулировка также возможна, см. статьи A mean-field limit for certain deep neural networks и A rigorous framework for the mean field limit of multilayer neural networks, но анализ усложняется.\\nТак, сходимость в глобальный минимум для сети с двумя скрытыми слоями была доказана лишь совсем недавно в работе Global convergence of three-layer neural networks in the mean field regime; для более глубоких сетей подобные результаты пока неизвестны.\\nВторой способ – вместо эволюции весов рассматривать эволюцию предсказаний модели в каждой точке xxx\\nДля простоты рассмотрим задачу минимизации квадратичной функции потерь на наборе данных (x⃗,y⃗)(\\\\vec x, \\\\vec y)(x,y\\u200b) размера mmm:\\n12∑j=1m(yj−f(xj;θ))2→min\\u2061θ.\\\\frac{1}{2} \\\\sum_{j=1}^m (y_j - f(x_j; \\\\theta))^2 \\\\to \\\\min_\\\\theta.\\n21\\u200bj=1∑m\\u200b(yj\\u200b−f(xj\\u200b;θ))2→θmin\\u200b.Будем оптимизировать эту функцию потерь градиентным спуском с шагом η\\\\etaη:\\nθk+1−θk=−η∇θ(12∑j=1m(yj−f(xj;θk))2)=\\\\theta_{k+1} - \\\\theta_k\\n= -\\\\eta \\\\nabla_\\\\theta\\\\left(\\\\frac{1}{2} \\\\sum_{j=1}^m (y_j - f(x_j; \\\\theta_k))^2\\\\right)\\n=\\nθk+1\\u200b−θk\\u200b=−η∇θ\\u200b(21\\u200bj=1∑m\\u200b(yj\\u200b−f(xj\\u200b;θk\\u200b))2)==η∑j=1m(yj−f(xj;θt))∇θf(xj;θk).=\\\\eta \\\\sum_{j=1}^m (y_j - f(x_j; \\\\theta_t)) \\\\nabla_\\\\theta f(x_j; \\\\theta_k).\\n=ηj=1∑m\\u200b(yj\\u200b−f(xj\\u200b;θt\\u200b))∇θ\\u200bf(xj\\u200b;θk\\u200b).Ниже нам будет удобнее рассматривать градиентный спуск с непрерывным временем вместо дискретного:\\nθ˙t=∑j=1m(yj−f(xj;θt))∇θf(xj;θt).\\\\dot\\\\theta_t\\n= \\\\sum_{j=1}^m (y_j - f(x_j; \\\\theta_t)) \\\\nabla_\\\\theta f(x_j; \\\\theta_t).\\nθ˙t\\u200b=j=1∑m\\u200b(yj\\u200b−f(xj\\u200b;θt\\u200b))∇θ\\u200bf(xj\\u200b;θt\\u200b).Переход к непрерывному времени соответствует устремлению к нулю шага η\\\\etaη, если при этом число шагов растёт как k=[t/η]k = [t / \\\\eta]k=[t/η], где округление применяется в любую сторону.\\nОбозначим через ft(x)f_t(x)ft\\u200b(x) предсказание в точке xxx модели в момент времени ttt. Оно зависит от времени следующим образом:\\nf˙t(x)=θ˙tT∇ft(x)=∑j=1m(yj−ft(xj))∇θTft(xj)∇θft(x).\\\\dot f_t(x)\\n= \\\\dot\\\\theta_t^T \\\\nabla f_t(x)\\n= \\\\sum_{j=1}^m (y_j - f_t(x_j)) \\\\nabla_\\\\theta^T f_t(x_j) \\\\nabla_\\\\theta f_t(x).\\nf˙\\u200bt\\u200b(x)=θ˙tT\\u200b∇ft\\u200b(x)=j=1∑m\\u200b(yj\\u200b−ft\\u200b(xj\\u200b))∇θT\\u200bft\\u200b(xj\\u200b)∇θ\\u200bft\\u200b(x).Введём обозначение:\\nΘ^t(x,x′)=∇θTft(x)∇θft(x′).\\\\color{#348FEA}{\\\\hat\\\\Theta_t(x,x')\\n= \\\\nabla_\\\\theta^T f_t(x) \\\\nabla_\\\\theta f_t(x').}\\nΘ^t\\u200b(x,x′)=∇θT\\u200bft\\u200b(x)∇θ\\u200bft\\u200b(x′).С помощью него уравнение выше можно записать более коротко:\\nf˙t(x)=Θ^t(x,x⃗)(y⃗−ft(x⃗)).(1)\\\\dot f_t(x)\\n= \\\\hat\\\\Theta_t(x,\\\\vec x) (\\\\vec y - f_t(\\\\vec x)).\\n\\\\quad(1)\\nf˙\\u200bt\\u200b(x)=Θ^t\\u200b(x,x)(y\\u200b−ft\\u200b(x)).(1)Здесь и дальше мы будем считать, что Θ^t(x,x⃗)\\\\hat\\\\Theta_t(x,\\\\vec x)Θ^t\\u200b(x,x) имеет размерность 1×m1 \\\\times m1×m.\\nФункция Θ^t(x,x′)\\\\hat\\\\Theta_t(x,x')Θ^t\\u200b(x,x′) называется эмпирическим нейрокасательным ядром (Neural Tangent Kernel, NTK); подробнее о ядрах мы поговорим ниже в параграфе про ядровые методы.\\nЗаметим, что в уравнении (1)(1)(1) вся информация о весах содержится в ядре, которое является отображением из X×X\\\\mathbb{X} \\\\times \\\\mathbb{X}X×X в R\\\\mathbb{R}R. Как мы увидим ниже, при определённых условиях, при стремлении ширины сети к бесконечности ядро имеет предел и он не зависит от ttt.\\nОбозначив этот предел через Θ\\\\ThetaΘ, мы приходим к следующему виду эволюции предсказаний бесконечно широкой сети:\\nf˙t(x)=Θ(x,x⃗)(y⃗−ft(x⃗)).(2)\\\\dot f_t(x)\\n= \\\\Theta(x,\\\\vec x) (\\\\vec y - f_t(\\\\vec x)).\\\\quad(2)\\nf˙\\u200bt\\u200b(x)=Θ(x,x)(y\\u200b−ft\\u200b(x)).(2)Здесь и далее будем называть Θ\\\\ThetaΘ (не эмпирическим) нейрокасательным ядром или NTK. Такой термин был введён в оригинальной работе Neural tangent kernel: Convergence and generalization in neural networks.\\nВ этом случае динамика предсказаний интегрируется следующим образом. На обучающей выборке\\nf˙t(x⃗)=Θ(x⃗,x⃗)(y⃗−ft(x⃗)),\\\\dot f_t(\\\\vec x)\\n= \\\\Theta(\\\\vec x,\\\\vec x) (\\\\vec y - f_t(\\\\vec x)),\\nf˙\\u200bt\\u200b(x)=Θ(x,x)(y\\u200b−ft\\u200b(x)),что даёт\\nft(x⃗)=f0(x⃗)−(I−e−Θ(x⃗,x⃗)t)(f0(x⃗)−y⃗).f_t(\\\\vec x)\\n= f_0(\\\\vec x) - \\\\left(I - e^{-\\\\Theta(\\\\vec x, \\\\vec x) t}\\\\right) (f_0(\\\\vec x) - \\\\vec y).\\nft\\u200b(x)=f0\\u200b(x)−(I−e−Θ(x,x)t)(f0\\u200b(x)−y\\u200b).Подставляя решение в (2)(2)(2), получаем\\nf˙t(x)=Θ(x,x⃗)e−Θ(x⃗,x⃗)t(y⃗−f0(x⃗)),\\\\dot f_t(x)\\n= \\\\Theta(x,\\\\vec x) e^{-\\\\Theta(\\\\vec x, \\\\vec x) t} (\\\\vec y - f_0(\\\\vec x)),\\nf˙\\u200bt\\u200b(x)=Θ(x,x)e−Θ(x,x)t(y\\u200b−f0\\u200b(x)),и, наконец,\\nft(x)=f0(x)−Θ(x,x⃗)Θ−1(x⃗,x⃗)(I−e−Θ(x⃗,x⃗)t)(f0(x⃗)−y⃗).(3)f_t(x) \\n= f_0(x) - \\\\Theta(x, \\\\vec x) \\\\Theta^{-1}(\\\\vec x, \\\\vec x) \\\\left(I - e^{-\\\\Theta(\\\\vec x, \\\\vec x) t}\\\\right) (f_0(\\\\vec x) - \\\\vec y).\\\\quad(3)\\nft\\u200b(x)=f0\\u200b(x)−Θ(x,x)Θ−1(x,x)(I−e−Θ(x,x)t)(f0\\u200b(x)−y\\u200b).(3)Прежде, чем доказывать сходимость ядра, мы обсудим, как может применяться предельное ядро и представление эволюции предсказаний в форме (1).\\nПрименение NTK-анализа\\nNTK как математический аппарат\\nНам удалось проинтегрировать динамику предсказаний в явном виде. Что это даёт?\\nВо-первых, мы получаем достаточное условие на сходимость в глобальный минимум на обучающей выборке. Таким условием является положительная определённость матрицы Грама ядра: Θ(x⃗,x⃗)≥λ0\\\\Theta(\\\\vec x,\\\\vec x) \\\\geq \\\\lambda_0Θ(x,x)≥λ0\\u200b для некоторого λ0>0\\\\lambda_0 > 0λ0\\u200b>0.\\nВ самом деле, в этом случае,\\nddt(12∥y⃗−ft(x⃗)∥22)=−(y⃗−ft(x⃗))TΘ(x⃗,x⃗)(y⃗−ft(x⃗))≤−λ0∥y⃗−ft(x⃗)∥22,\\\\frac{d}{dt}\\\\left(\\\\frac{1}{2} \\\\| \\\\vec y - f_t(\\\\vec x) \\\\|_2^2\\\\right)\\n= -(\\\\vec y - f_t(\\\\vec x))^T \\\\Theta(\\\\vec x, \\\\vec x) (\\\\vec y - f_t(\\\\vec x))\\n\\\\leq -\\\\lambda_0 \\\\| \\\\vec y - f_t(\\\\vec x) \\\\|_2^2,\\ndtd\\u200b(21\\u200b∥y\\u200b−ft\\u200b(x)∥22\\u200b)=−(y\\u200b−ft\\u200b(x))TΘ(x,x)(y\\u200b−ft\\u200b(x))≤−λ0\\u200b∥y\\u200b−ft\\u200b(x)∥22\\u200b,что даёт\\n∥y⃗−ft(x⃗)∥22≤e−λ0t∥y⃗−f0(x⃗)∥22→0при\\xa0t→∞.\\\\| \\\\vec y - f_t(\\\\vec x) \\\\|_2^2\\n\\\\leq e^{-\\\\lambda_0 t} \\\\| \\\\vec y - f_0(\\\\vec x) \\\\|_2^2\\n\\\\to 0 \\\\quad \\\\text{при $t \\\\to \\\\infty$}.\\n∥y\\u200b−ft\\u200b(x)∥22\\u200b≤e−λ0\\u200bt∥y\\u200b−f0\\u200b(x)∥22\\u200b→0при\\xa0t→∞.Во-вторых, раз явное решение известно, можно написать оценку на обобщающую способность.\\nОба этих результата опираются на то, что ядро постоянно. Как мы покажем ниже, постоянство нейрокасательного ядра нейронной сети можно гарантировать лишь в пределе бесконечной ширины. Тем не менее, если сеть конечна, но достаточно широка, можно показать, что её ядро достаточно близко к предельному, и оценки сохраняют силу.\\nНапример, для обоснования сходимости в глобальный минимум достаточно показать, что наименьшее собственное значение эмпирического ядра с высокой вероятностью остаётся отделённым от нуля в течение обучения: Θ^t≥λ0/2\\\\hat\\\\Theta_t \\\\geq \\\\lambda_0/2Θ^t\\u200b≥λ0\\u200b/2 ∀t≥0\\\\forall t \\\\geq 0∀t≥0 с вероятностью ≥1−δ\\\\geq 1-\\\\delta≥1−δ для n≥n∗(δ)n \\\\geq n^*(\\\\delta)n≥n∗(δ). В самом деле, из этого следует, что\\nddt(12∥y⃗−ft(x⃗)∥22)=−(y⃗−ft(x⃗))TΘ^t(x⃗,x⃗)(y⃗−ft(x⃗))≤−λ02∥y⃗−ft(x⃗)∥22,\\\\frac{d}{dt}\\\\left(\\\\frac{1}{2} \\\\| \\\\vec y - f_t(\\\\vec x) \\\\|_2^2\\\\right)\\n= -(\\\\vec y - f_t(\\\\vec x))^T \\\\hat\\\\Theta_t(\\\\vec x, \\\\vec x) (\\\\vec y - f_t(\\\\vec x))\\n\\\\leq -\\\\frac{\\\\lambda_0}2 \\\\| \\\\vec y - f_t(\\\\vec x) \\\\|_2^2,\\ndtd\\u200b(21\\u200b∥y\\u200b−ft\\u200b(x)∥22\\u200b)=−(y\\u200b−ft\\u200b(x))TΘ^t\\u200b(x,x)(y\\u200b−ft\\u200b(x))≤−2λ0\\u200b\\u200b∥y\\u200b−ft\\u200b(x)∥22\\u200b,а значит,\\n∥y⃗−ft(x⃗)∥22≤e−λ0t/2∥y⃗−f0(x⃗)∥22→0при\\xa0t→∞.\\\\| \\\\vec y - f_t(\\\\vec x) \\\\|_2^2\\n\\\\leq e^{-\\\\lambda_0 t / 2} \\\\| \\\\vec y - f_0(\\\\vec x) \\\\|_2^2\\n\\\\to 0 \\\\quad \\\\text{при $t \\\\to \\\\infty$}.\\n∥y\\u200b−ft\\u200b(x)∥22\\u200b≤e−λ0\\u200bt/2∥y\\u200b−f0\\u200b(x)∥22\\u200b→0при\\xa0t→∞.Формальное доказательство вы можете найти в работе Gradient Descent Provably Optimizes Over-parameterized Neural Networks, а также в конспекте лекций автора этого параграфа.\\nВот ещё несколько результатов, полученных в этом направлении:\\n\\nулучшенные оценки на минимальную ширину в работе Quadratic suffices for over-parametrization via matrix chernoff bound;\\nоценки для случая глубоких сетей в работе Gradient descent finds global minima of deep neural networks;\\nоценки на обобщающую способность, полученные через близость ядра к предельному, в работе Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks.\\n\\nОпределение патологий обучения\\nКак мы увидим позже, NTK реальных, стандартно параметризованных, имеющих конечную ширину сетей может меняться за время обучения существенным образом: см. эмпирическую работу Deep learning versus kernel learning и теоретический анализ для сетей с одним скрытым слоем Dynamically Stable Infinite-Width Limits of Neural Classifiers.\\nТем не менее, ядро в инициализации может выявить определённые патологии соответствующей нейронной сети. Рассмотрим один из примеров применения.\\nВ некоторых состоящих из однородных блоков архитектурах (скажем, ResNet) можно увеличивать (и даже устремлять к бесконечности) число слоёв или блоков, и логично задаться вопросом о том, как при этом будет вести себя процесс обучения.\\nНеобходимым условием обучаемости является хороший первый шаг обучения. Если он исчезающе мал, то сеть не обучится ни на первом, ни на каком-либо другом шаге. Если он слишком велик, то обучение разойдётся на первом же шаге. Как мы увидим ниже, индикатором проблем является плохая обусловленность NTK в инициализации. Например, его собственные значения могут с ростом глубины стремиться к нулю или, наоборот, к бесконечности. В первом случае какие-то из компонент выборки никогда не выучатся, во втором обучение невозможно ни при каком конечном темпе обучения.\\nЧтобы в этом убедиться, рассмотрим разложение матрицы Грама ядра по собственным векторами:\\nΘ(x⃗,x⃗)=∑j=1mλjv⃗jv⃗jT,\\\\Theta(\\\\vec x, \\\\vec x) = \\\\sum_{j=1}^m \\\\lambda_j \\\\vec v_j \\\\vec v_j^T,\\nΘ(x,x)=j=1∑m\\u200bλj\\u200bvj\\u200bvjT\\u200b,где λ1≥…≥λm≥0\\\\lambda_1 \\\\geq \\\\ldots \\\\geq \\\\lambda_m \\\\geq 0λ1\\u200b≥…≥λm\\u200b≥0, а векторы v⃗1,…,v⃗m\\\\vec v_1,\\\\ldots,\\\\vec v_mv1\\u200b,…,vm\\u200b образуют ортонормированный базис. Разложим предсказание сети по этому базису: ft(x⃗)=∑j=1mut,jv⃗jf_t(\\\\vec x) = \\\\sum_{j=1}^m u_{t,j} \\\\vec v_jft\\u200b(x)=∑j=1m\\u200but,j\\u200bvj\\u200b. Так как базис ортонормированный, каждая из компонент эволюционирует независимо от других. В самом деле, для дискретного градиентного спуска с шагом η\\\\etaη имеем\\nuk+1,j=uk,j+ηλj(v⃗jTy⃗−ut,j).u_{k+1,j}\\n= u_{k,j} + \\\\eta \\\\lambda_j (\\\\vec v_j^T \\\\vec y - u_{t,j}).\\nuk+1,j\\u200b=uk,j\\u200b+ηλj\\u200b(vjT\\u200by\\u200b−ut,j\\u200b).Таким образом, если λj=0\\\\lambda_j=0λj\\u200b=0, то ut,ju_{t,j}ut,j\\u200b никогда не сойдётся к v⃗kTy⃗\\\\vec v_k^T \\\\vec yvkT\\u200by\\u200b.\\nКроме того, для того, чтобы процесс сходился, шаг η\\\\etaη должен убывать обратно пропорционально наибольшему собственному числу λ1\\\\lambda_1λ1\\u200b. Если последнее стремится к бесконечности, то η\\\\etaη стремится к нулю, а значит, ηλj\\\\eta \\\\lambda_jηλj\\u200b будем мало для всех jjj, для которых λj\\\\lambda_jλj\\u200b конечен; соответствующие компоненты также никогда не сойдутся.\\nПодробности см. в работе Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping, а также в более ранних работах Exponential expressivity in deep neural networks through transient chaos, Deep information propagation, Resurrecting the sigmoid in deep learning through dynamical isometry, Dynamical isometry and a mean field theory of cnns, в которых использовалась похожая идея, но не использовалось понятие NTK явно. См. также главу про инициализацию в конспекте лекций.\\nNTK и Ядровые методы\\nПредельное NTK нейронной сети можно использовать в любом ядровом методе, например, в SVM. Обсудим это поподробнее и заодно разберёмся, почему NTK вообще называют ядром.\\nРассмотрим задачу линейной регрессии:\\nθ^λ=argminθ∈Rd∑j=1mL(yj,θTxj)+λ∥θ∥22(4)\\\\hat\\\\theta_\\\\lambda\\n= \\\\text{argmin}_{\\\\theta \\\\in \\\\mathbb{R}^d} \\\\sum_{j=1}^m \\\\mathcal{L}(y_j, \\\\theta^T x_j) + \\\\lambda \\\\| \\\\theta \\\\|_2^2\\\\quad(4)\\nθ^λ\\u200b=argminθ∈Rd\\u200bj=1∑m\\u200bL(yj\\u200b,θTxj\\u200b)+λ∥θ∥22\\u200b(4)Эту же задачу можно эквивалентно переписать следующим образом:\\nf^λ=argminf∈H∑j=1mL(yj,f(xj))+λ∥f∥H2,(5)\\\\hat f_\\\\lambda\\n= \\\\text{argmin}_{f \\\\in \\\\mathcal{H}} \\\\sum_{j=1}^m \\\\mathcal{L}(y_j, f(x_j)) + \\\\lambda \\\\| f \\\\|_\\\\mathcal{H}^2,\\\\quad(5)\\nf^\\u200bλ\\u200b=argminf∈H\\u200bj=1∑m\\u200bL(yj\\u200b,f(xj\\u200b))+λ∥f∥H2\\u200b,(5)где H\\\\mathcal{H}H – пространство линейных отображений fθ(x)=θTxf_{\\\\theta}(x) = \\\\theta^T xfθ\\u200b(x)=θTx с некоторой нормой ∥f∥H\\\\| f \\\\|_\\\\mathcal{H}∥f∥H\\u200b на нём.\\nСделаем линейное пространство H\\\\mathcal{H}H евклидовым, введя на нём следующее скалярное произведение. Для f(x)=θTxf(x) = \\\\theta^T xf(x)=θTx и f~(x)=θ~Tx\\\\tilde f(x) = \\\\tilde\\\\theta^T xf~\\u200b(x)=θ~Tx определим\\n⟨f,f~⟩=θTθ~.\\\\langle f, \\\\tilde f \\\\rangle = \\\\theta^T \\\\tilde\\\\theta.\\n⟨f,f~\\u200b⟩=θTθ~.Это скалярное произведение порождает норму ∥f∥H=∥θ∥2\\\\| f \\\\|_\\\\mathcal{H} = \\\\| \\\\theta \\\\|_2∥f∥H\\u200b=∥θ∥2\\u200b, что и делает формулировку (5) эквивалентной формулировке (4).\\nПространство линейных моделей слишком узко, однако ничто не мешает нам рассмотреть задачу вида (5), в которой H\\\\mathcal{H}H будет произвольным нормированным пространством функций. Наиболее хорошо изучен случай, когда функции из H\\\\mathcal{H}H являются линейными моделями в некотором (возможно, бесконечномерном) гильбертовом пространстве признаков: f(x)=⟨Φ(x),θ⟩f(x) = \\\\langle \\\\Phi(x), \\\\theta \\\\ranglef(x)=⟨Φ(x),θ⟩, где Φ\\\\PhiΦ отображает xxx в это пространство. Если последнее всё же конечномерно, то мы можем использовать матричную запись f(x)=θTΦ(x)f(x) = \\\\theta^T \\\\Phi(x)f(x)=θTΦ(x); элементы θ\\\\thetaθ в этой записи обычно называют первичными переменными (primal variables).\\nПространство функций H\\\\mathcal{H}H также оказывается гильбертовым: соответствующее скалярное произведение имеет вид\\n⟨fθ,fθ′⟩H=⟨θ,θ′⟩\\\\langle f_{\\\\theta}, f_{\\\\theta'} \\\\rangle_\\\\mathcal{H} = \\\\langle \\\\theta, \\\\theta' \\\\rangle\\n⟨fθ\\u200b,fθ′\\u200b⟩H\\u200b=⟨θ,θ′⟩Таким образом, ∥fθ∥H2=⟨fθ,fθ⟩H=⟨θ,θ⟩\\\\| f_{\\\\theta} \\\\|_\\\\mathcal{H}^2 = \\\\langle f_{\\\\theta}, f_{\\\\theta} \\\\rangle_\\\\mathcal{H} = \\\\langle \\\\theta, \\\\theta \\\\rangle∥fθ\\u200b∥H2\\u200b=⟨fθ\\u200b,fθ\\u200b⟩H\\u200b=⟨θ,θ⟩, если fθ(x)=⟨Φ(x),θ⟩f_{\\\\theta}(x) = \\\\langle \\\\Phi(x), \\\\theta \\\\ranglefθ\\u200b(x)=⟨Φ(x),θ⟩.\\nЛюбому отображению Φ\\\\PhiΦ можно сопоставить симметричную положительно-определённую функцию K(x,x′)=⟨ΦT(x),Φ(x′)⟩K(x,x') = \\\\langle\\\\Phi^T(x), \\\\Phi(x')\\\\rangleK(x,x′)=⟨ΦT(x),Φ(x′)⟩; функции такого вида называются ядрами.\\nВ силу фундаментальной теоремы о представителе любое решение задачи (4) принимает вид\\nf(x)=∑j=1mαjK(x,xj)=K(x,x⃗)α⃗.f(x) = \\\\sum_{j=1}^m \\\\alpha_j K(x,x_j) = K(x, \\\\vec x) \\\\vec\\\\alpha.\\nf(x)=j=1∑m\\u200bαj\\u200bK(x,xj\\u200b)=K(x,x)α.В отличие от θ\\\\thetaθ, вектор α\\\\alphaα всегда конечномерен: его размерность равна размеру обучающей выборки. Элементы α\\\\alphaα называют двойственными (dual) переменными.\\nУпомянутый результат позволяет перейти от минимизации fff в бесконечномерном пространстве функций (или, что то же самое, минимизации θ\\\\thetaθ в бесконечномерном пространстве признаков), к минимизации в конечномерном пространстве двойственных переменных:\\nα⃗=argminα⃗∈Rm∑j=1mL(yj,K(xj,x⃗)α⃗)+λα⃗TK(x⃗,x⃗)α⃗.(6)\\\\vec\\\\alpha\\n= \\\\text{argmin}_{\\\\vec\\\\alpha \\\\in \\\\mathbb{R}^m} \\\\sum_{j=1}^m \\\\mathcal{L}\\\\left(y_j, K(x_j, \\\\vec x) \\\\vec\\\\alpha\\\\right) + \\\\lambda \\\\vec\\\\alpha^T K(\\\\vec x, \\\\vec x) \\\\vec\\\\alpha.\\\\quad(6)\\nα=argminα∈Rm\\u200bj=1∑m\\u200bL(yj\\u200b,K(xj\\u200b,x)α)+λαTK(x,x)α.(6)Если в качестве функции потерь взять квадратичную L(y,z)=12(y−z)2\\\\mathcal{L}(y,z) = \\\\frac{1}{2} (y-z)^2L(y,z)=21\\u200b(y−z)2, то получим ядровую регрессию; если же взять hinge loss L(y,z)=[1−yz]+\\\\mathcal{L}(y,z) = [1- yz]_+L(y,z)=[1−yz]+\\u200b, то SVM.\\nЗаметим, что двойственная задача полностью сформулирована в терминах ядра KKK: отображение в потенциально бесконечное пространство признаков Φ\\\\PhiΦ более нигде не возникает. Поэтому мы можем использовать в качестве KKK любую симметричную положительно определённую функцию двух переменных, не думая о том, для какого пространства признаков оно будет ядром (есть теорема, что такие функции всегда являются ядрами). Это может быть очень полезно. Так, если для эмпирического NTK в инициализации Θ^0(x,x′)\\\\hat\\\\Theta_0(x,x')Θ^0\\u200b(x,x′) имеем Φ(x)=∇θf(x;θ0)\\\\Phi(x) = \\\\nabla_\\\\theta f(x; \\\\theta_0)Φ(x)=∇θ\\u200bf(x;θ0\\u200b), но совершенно неочевидно, какое отображение Φ\\\\PhiΦ соответствует предельному NTK: Θ(x,x′)=lim\\u2061n→∞Θ^0(x,x′)\\\\Theta(x,x') = \\\\lim_{n \\\\to \\\\infty} \\\\hat\\\\Theta_0(x,x')Θ(x,x′)=limn→∞\\u200bΘ^0\\u200b(x,x′).\\nТаким образом, мы можем использовать Θ\\\\ThetaΘ в качестве ядра KKK в двойственной задаче (6) наряду с линейным K(x,x′)=xTx′K(x,x') = x^T x'K(x,x′)=xTx′ или гауссовским ядром K(x,x′)=e−12σ2∥x−y∥22K(x,x') = e^{-\\\\frac{1}{2\\\\sigma^2} \\\\|x-y\\\\|_2^2}K(x,x′)=e−2σ21\\u200b∥x−y∥22\\u200b. Такой подход привлекателен тем, что обучение ядровых методов более устойчиво и имеет меньше гиперпараметров. При этом можно надеяться, что результат обучения ядрового метода с NTK в качестве ядра будет близок к результату обучения соответствующей нейронной сети.\\nОсновная проблема ядровых методов в том, что они требуют вычисления матрицы Грама ядра на обучающем наборе данных K(x⃗,x⃗)K(\\\\vec x, \\\\vec x)K(x,x). Её размер m×mm \\\\times mm×m (где mmm – размер выборки), так что применение ядровых методов на больших данных сильно усложняется. Более того, наивное вычисление динамики ftf_tft\\u200b из формулы (3) требует обращения матрицы Грама, которое занимает O(m3)O(m^3)O(m3) времени.\\nТем не менее, определённые оптимизации существуют. Так например, в работе Kernel methods through the roof предлагается способ приближённого вычисления (ftf_tft\\u200b) за O(m3/2log\\u2061m)O(m^{3/2} \\\\log m)O(m3/2logm) памяти и времени. Другие подходы см. в работах Fast Finite Width Neural Tangent Kernel и Neural tangents: Fast and easy infinite neural networks in python.\\nТак или иначе, на малых наборах данных выражение (3) можно вычислить точно, см. результаты в работе Harnessing the power of infinitely wide deep nets on small-data tasks. Существуют также примеры задач, в которых матрицу Грама ядра достаточно посчитать только для малых mmm, см., например, Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks.\\nЕщё одна проблема использования NTK в ядровых методах состоит в том, что явный подсчёт предельного NTK доступен только для сетей, состоящих из слоёв из определённого класса. В этот класс входят полносвязные и свёрточные слои, average pooling, ряд нелинейностей с одним аргументом (включая, например, ReLU и erf), layer norm, но не входят max pooling и batch norm, часто используемые в реальных архитектурах. Явный подсчёт предельного NTK для «хороших» сетей реализован в библиотеке NeuralTangents; часть явных формул для подсчёта можно найти в статье On exact computation with an infinitely wide neural net.\\nТем не менее, даже в тех случаях, когда посчитать предельное NTK не представляется возможным, в качестве ядра для ядрового метода можно использовать эмпирическое NTK в инициализации\\nΘ^0(x,x′)=∇θTf(x;θ0)∇θf(x′;θ0)\\\\hat\\\\Theta_0(x,x') = \\\\nabla^T_\\\\theta f(x;\\\\theta_0) \\\\nabla_\\\\theta f(x';\\\\theta_0)\\nΘ^0\\u200b(x,x′)=∇θT\\u200bf(x;θ0\\u200b)∇θ\\u200bf(x′;θ0\\u200b)Такое ядро можно рассматривать как шумную и смещённую оценку предельного; для уменьшения шума можно использовать Монте-Карло оценку матожидания. Некоторые оптимизации подсчёта эмпирического ядра см. в работе Neural tangents: Fast and easy infinite neural networks in python.\\nNTK не единственное ядро, которое можно сопоставить нейронной сети. Так, NNGP-ядро K(x,x′)=Ef(x)f(x′)K(x,x') = \\\\mathbb{E} f(x) f(x')K(x,x′)=Ef(x)f(x′) – это ядро гауссовского процесса, реализуемого сетью в пределе бесконечной ширины. Подробнее можно почитать в работах Deep Neural Networks as Gaussian Processes, Wide neural networks of any depth evolve as linear models under gradient descent, Random neural networks in the infinite width limit as Gaussian processes или в конспекте лекций. Можно показать, что оно соответствует NTK-ядру для сети, в которой учится лишь выходной слой.\\nТак как, в отличие от NTK, для подсчёта NNGP-ядра не требуется обратный проход (backward pass), последнее более вычислительно эффективно; Towards nngp-guided neural architecture search – пример работы, в которой предпочтение отдаётся NNGP-ядру именно по этой причине.\\nСходимость эмпирического ядра\\nВы этом параграфе мы покажем, что при определённой параметризации эмпирическое NTK не зависит ни от времени, ни от инициализации. Мы начнём с иллюстративного примера, прежде чем формулировать строгую теорему.\\nРассмотрим сеть с одним скрытым слоем, скалярным выходом и гауссовской инициализацией весов; вход для простоты тоже положим скалярным:\\nf(x;a1:n,w1:n)=∑i=1naiϕ(wix),a1:n∼N(0,n−1I),w1:n∼N(0,I).f(x; a_{1:n}, w_{1:n})\\n= \\\\sum_{i=1}^n a_i \\\\phi(w_i x),\\n\\\\quad\\na_{1:n} \\\\sim \\\\mathcal{N}(0, n^{-1} I),\\n\\\\quad\\nw_{1:n} \\\\sim \\\\mathcal{N}(0, I).\\nf(x;a1:n\\u200b,w1:n\\u200b)=i=1∑n\\u200bai\\u200bϕ(wi\\u200bx),a1:n\\u200b∼N(0,n−1I),w1:n\\u200b∼N(0,I).Здесь nnn – ширина скрытого слоя.\\nСледуя одной из стандартных схем инициализации из статьи Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, дисперсия каждого слоя выбирается обратно пропорционально числу входных нейронов (подробнее см. в параграфе про тонкости обучения нейросетей).\\nНазовём описанную выше параметризацию стандартной.\\nДля сходимости ядра нам придётся несколько её видоизменить:\\nf(x;a1:n,w1:n)=1n∑i=1naiϕ(wix),a1:n∼N(0,I),w1:n∼N(0,I).f(x; a_{1:n}, w_{1:n})\\n= \\\\frac{1}{\\\\sqrt{n}} \\\\sum_{i=1}^n a_i \\\\phi(w_i x),\\n\\\\quad\\na_{1:n} \\\\sim \\\\mathcal{N}(0, I),\\n\\\\quad\\nw_{1:n} \\\\sim \\\\mathcal{N}(0, I).\\nf(x;a1:n\\u200b,w1:n\\u200b)=n\\u200b1\\u200bi=1∑n\\u200bai\\u200bϕ(wi\\u200bx),a1:n\\u200b∼N(0,I),w1:n\\u200b∼N(0,I).Назовём новую параметризацию NTK-параметризацией.\\nОтметим, что распределение выходов нейронов в инициализации остаётся неизменным при переходе от стандартной к NTK-параметризации. Что меняется – это динамика градиентного спуска:\\na˙k=1n∑j=1mϕ(wkxj)(yj−ft(xj)),\\\\dot a_k \\n= \\\\frac{1}{\\\\sqrt{n}} \\\\sum_{j=1}^m \\\\phi(w_k x_j)(y_j - f_t(x_j)),\\na˙k\\u200b=n\\u200b1\\u200bj=1∑m\\u200bϕ(wk\\u200bxj\\u200b)(yj\\u200b−ft\\u200b(xj\\u200b)),w˙k=1n∑j=1makϕ′(wkxj)xj(yj−ft(xj)).\\\\dot w_k\\n= \\\\frac{1}{\\\\sqrt{n}} \\\\sum_{j=1}^m a_k \\\\phi'(w_k x_j) x_j(y_j - f_t(x_j)).\\nw˙k\\u200b=n\\u200b1\\u200bj=1∑m\\u200bak\\u200bϕ′(wk\\u200bxj\\u200b)xj\\u200b(yj\\u200b−ft\\u200b(xj\\u200b)).При t=0t=0t=0 приращения весов для такой параметризации имеют порядок O(n−1/2)O(n^{-1/2})O(n−1/2), в то время как сами веса имеют порядок O(1)O(1)O(1) при t=0t=0t=0. Поэтому ak(t)→ak(0)a_k(t) \\\\to a_k(0)ak\\u200b(t)→ak\\u200b(0) и wk(t)→wk(0)w_k(t) \\\\to w_k(0)wk\\u200b(t)→wk\\u200b(0) при n→∞n \\\\to \\\\inftyn→∞ для любого данного k∈Nk \\\\in \\\\mathbb{N}k∈N и t∈R+t \\\\in \\\\mathbb{R}_+t∈R+\\u200b. Другими словами, с ростом размера скрытого слоя градиент будет стремиться к нулю, и каждый из весов в пределе останется в начальной точке.\\nСравним с градиентным спуском в стандартной параметризации:\\na˙k=∑j=1mϕ(wkxj)(yj−ft(xj)),\\\\dot a_k \\n= \\\\sum_{j=1}^m \\\\phi(w_k x_j)(y_j - f_t(x_j)),\\na˙k\\u200b=j=1∑m\\u200bϕ(wk\\u200bxj\\u200b)(yj\\u200b−ft\\u200b(xj\\u200b)),w˙k=∑j=1makϕ′(wkxj)xj(yj−ft(xj))\\\\dot w_k \\n= \\\\sum_{j=1}^m a_k \\\\phi'(w_k x_j) x_j(y_j - f_t(x_j))\\nw˙k\\u200b=j=1∑m\\u200bak\\u200bϕ′(wk\\u200bxj\\u200b)xj\\u200b(yj\\u200b−ft\\u200b(xj\\u200b))В этом случае веса выходного слоя имеют порядок O(n−1/2)O(n^{-1/2})O(n−1/2) при t=0t=0t=0, но получают приращения порядка O(1)O(1)O(1) в этот момент времени, в то время как веса входного слоя имеют порядок O(1)O(1)O(1) при t=0t=0t=0, но получают в этот момент времени приращения порядка O(n−1/2)O(n^{-1/2})O(n−1/2).\\nВ новой параметризации эмпирическое NTK выглядит следующим образом:\\nΘ^t(x,x′)=∑i=1n(∂aif(x)∂aif(x′)+∂wif(x)∂wif(x′))=\\\\hat\\\\Theta_t(x,x')\\n= \\\\sum_{i=1}^n \\\\left(\\\\partial_{a_i} f(x) \\\\partial_{a_i} f(x') + \\\\partial_{w_i} f(x) \\\\partial_{w_i} f(x')\\\\right)\\n=\\nΘ^t\\u200b(x,x′)=i=1∑n\\u200b(∂ai\\u200b\\u200bf(x)∂ai\\u200b\\u200bf(x′)+∂wi\\u200b\\u200bf(x)∂wi\\u200b\\u200bf(x′))==1n∑i=1n(ϕ(wi(t)x)ϕ(wi(t)x′)+ai2(t)ϕ′(wi(t)x)ϕ′(wi(t)x′)xx′).= \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\left(\\\\phi(w_i(t) x) \\\\phi(w_i(t) x') + a_i^2(t) \\\\phi'(w_i(t) x) \\\\phi'(w_i(t) x') x x'\\\\right).\\n=n1\\u200bi=1∑n\\u200b(ϕ(wi\\u200b(t)x)ϕ(wi\\u200b(t)x′)+ai2\\u200b(t)ϕ′(wi\\u200b(t)x)ϕ′(wi\\u200b(t)x′)xx′).Так как ak(t)→ak(0)a_k(t) \\\\to a_k(0)ak\\u200b(t)→ak\\u200b(0) и wk(t)→wk(0)w_k(t) \\\\to w_k(0)wk\\u200b(t)→wk\\u200b(0) при n→∞n \\\\to \\\\inftyn→∞ для любых заданных k∈Nk \\\\in \\\\mathbb{N}k∈N и t∈R+t \\\\in \\\\mathbb{R}_+t∈R+\\u200b, выражение выше асимптотически эквивалентно\\nΘ^0(x,x′)=1n∑i=1n(ϕ(wi(0)x)ϕ(wi(0)x′)+ai2(0)ϕ′(wi(0)x)ϕ′(wi(0)x′)xx′),\\\\hat\\\\Theta_0(x,x')\\n= \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\left(\\\\phi(w_i(0) x) \\\\phi(w_i(0) x') + a_i^2(0) \\\\phi'(w_i(0) x) \\\\phi'(w_i(0) x') x x'\\\\right),\\nΘ^0\\u200b(x,x′)=n1\\u200bi=1∑n\\u200b(ϕ(wi\\u200b(0)x)ϕ(wi\\u200b(0)x′)+ai2\\u200b(0)ϕ′(wi\\u200b(0)x)ϕ′(wi\\u200b(0)x′)xx′),а значит, сходится к\\nΘ(x,x′)=Ea,w∼N(0,1)(ϕ(wx)ϕ(wx′)+a2ϕ′(wx)ϕ′(wx′)xx′)\\\\Theta(x,x')\\n= \\\\mathbb{E}_{a,w \\\\sim \\\\mathcal{N}(0,1)} \\\\left(\\\\phi(w x) \\\\phi(w x') + a^2 \\\\phi'(w x) \\\\phi'(w x') x x'\\\\right)\\nΘ(x,x′)=Ea,w∼N(0,1)\\u200b(ϕ(wx)ϕ(wx′)+a2ϕ′(wx)ϕ′(wx′)xx′)при n→∞n \\\\to \\\\inftyn→∞ в силу закона больших чисел.\\nПредельное ядро Θ(x,x′)\\\\Theta(x,x')Θ(x,x′) не зависит ни от времени ttt, ни от инициализации. Мы будем называть это ядро нейрокасательным или просто NTK (его не стоит путать с эмпирическим NTK Θ^t\\\\hat\\\\Theta_tΘ^t\\u200b).\\nЕщё раз подчеркнём, что это работает для NTK-параметризации, но не для стандартной. Для стандартной параметризации эмпирическое NTK в инициализации расходится с шириной:\\nΘ^0(x,x′)=∑i=1n(ϕ(wi(0)x)ϕ(wi(0)x′)+ai2(0)ϕ′(wi(0)x)ϕ′(wi(0)x′)xx′)∼\\\\hat\\\\Theta_0(x,x')\\n= \\\\sum_{i=1}^n \\\\left(\\\\phi(w_i(0) x) \\\\phi(w_i(0) x') + a_i^2(0) \\\\phi'(w_i(0) x) \\\\phi'(w_i(0) x') x x'\\\\right)\\n\\\\sim\\nΘ^0\\u200b(x,x′)=i=1∑n\\u200b(ϕ(wi\\u200b(0)x)ϕ(wi\\u200b(0)x′)+ai2\\u200b(0)ϕ′(wi\\u200b(0)x)ϕ′(wi\\u200b(0)x′)xx′)∼∼n⋅Ew∼N(0,1)ϕ(wx)ϕ(wx′).\\\\sim n \\\\cdot \\\\mathbb{E}_{w \\\\sim \\\\mathcal{N}(0,1)} \\\\phi(w x) \\\\phi(w x').\\n∼n⋅Ew∼N(0,1)\\u200bϕ(wx)ϕ(wx′).Подробнее мы поговорим об этом в одном из следующих параграфов.\\nДля NTK-параметризации сходимость эмпирического ядра выполняется не только для сетей с одним скрытым слоем. Так, рассмотрим полносвязную сеть с LLL слоями:\\nf(x)=hL(x),hl(x)=1nl−1Wlxl−1(x),xl−1(x)=ϕ(hl−1(x)),x0(x)=x.f(x) = h_L(x),\\n\\\\quad\\nh_l(x) = \\\\frac{1}{\\\\sqrt{n_{l-1}}} W_l x_{l-1}(x),\\n\\\\quad\\nx_{l-1}(x) = \\\\phi(h_{l-1}(x)),\\n\\\\quad\\nx_0(x) = x.\\nf(x)=hL\\u200b(x),hl\\u200b(x)=nl−1\\u200b\\u200b1\\u200bWl\\u200bxl−1\\u200b(x),xl−1\\u200b(x)=ϕ(hl−1\\u200b(x)),x0\\u200b(x)=x.Здесь W1∈Rn1×n0W_1 \\\\in \\\\mathbb{R}^{n_1 \\\\times n_0}W1\\u200b∈Rn1\\u200b×n0\\u200b, WL∈R1×nL−1W_L \\\\in \\\\mathbb{R}^{1 \\\\times n_{L-1}}WL\\u200b∈R1×nL−1\\u200b и Wl∈Rnl×nl−1W_l \\\\in \\\\mathbb{R}^{n_l \\\\times n_{l-1}}Wl\\u200b∈Rnl\\u200b×nl−1\\u200b для всех остальных lll.\\nПоложим, что веса инициализируются из стандартного нормального распределения. Поставим задачу оптимизации дифференцируемой функции потерь L\\\\mathcal{L}L:\\nθ˙t=−∇θ(∑j=1mL(yj,f(xj;θt)))=∑j=1m∂L(yj,z)∂z∣z=f(xj;θt)∇θf(xj;θt),\\\\dot\\\\theta_t\\n= -\\\\nabla_\\\\theta\\\\left(\\\\sum_{j=1}^m \\\\mathcal{L}(y_j, f(x_j; \\\\theta_t))\\\\right)\\n= \\\\sum_{j=1}^m \\\\left.\\\\frac{\\\\partial \\\\mathcal{L}(y_j, z)}{\\\\partial z}\\\\right|_{z=f(x_j; \\\\theta_t)} \\\\nabla_\\\\theta f(x_j; \\\\theta_t),\\nθ˙t\\u200b=−∇θ\\u200b(j=1∑m\\u200bL(yj\\u200b,f(xj\\u200b;θt\\u200b)))=j=1∑m\\u200b∂z∂L(yj\\u200b,z)\\u200b\\u200bz=f(xj\\u200b;θt\\u200b)\\u200b∇θ\\u200bf(xj\\u200b;θt\\u200b),где θ\\\\thetaθ – объединение всех весов W1:LW_{1:L}W1:L\\u200b сети.\\nТеорема ниже доказана в оригинальной работе по NTK:\\nТеорема. В предположениях выше, если ϕ\\\\phiϕ из C2C^2C2 и липшицева и L\\\\mathcal{L}L из C1C^1C1 и липшицева, то Θ^t(x,x′)\\\\hat\\\\Theta_t(x,x')Θ^t\\u200b(x,x′) сходится к Θ(x,x′)\\\\Theta(x,x')Θ(x,x′) по вероятности при n1:L−1→∞n_{1:L-1} \\\\to \\\\inftyn1:L−1\\u200b→∞ последовательно ∀x,x′∈X\\\\forall x,x' \\\\in \\\\mathbb{X}∀x,x′∈X ∀t≥0\\\\forall t \\\\geq 0∀t≥0.\\nОказывается, что эта теорема верна не только для полносвязных сетей с гладкими активациями.\\nОпределим тензорную программу как начальный набор переменных определённых типов и последовательность команд. Каждая команда порождает новую переменную, действуя на уже имеющиеся.\\nПеременные бывают трёх типов:\\n\\nA\\\\mathsf{A}A: n×nn \\\\times nn×n матрицы с независимыми элементами из N(0,1)\\\\mathcal{N}(0,1)N(0,1);\\nG\\\\mathsf{G}G: вектора размера nnn с асимптотически независимыми нормальными элементами;\\nH\\\\mathsf{H}H: образы G\\\\mathsf{G}G-переменных относительно поэлементных нелинейностей.\\n\\nДля переменной WWW запись W:AW: \\\\mathsf{A}W:A будет означать, что WWW имеет тип A\\\\mathsf{A}A.\\nКоманды бывают следующие:\\n\\ntrspop: W:A→WT:AW: \\\\mathsf{A} \\\\to W^T: \\\\mathsf{A}W:A→WT:A (перевести переменную типа A\\\\mathsf{A}A со значением WWW в переменную типа A\\\\mathsf{A}A со значением WTW^TWT);\\nmatmul: (W:A,\\xa0x:H)→1nWx:G(W: \\\\mathsf{A}, \\\\ x: \\\\mathsf{H}) \\\\to \\\\frac{1}{\\\\sqrt{n}} W x: \\\\mathsf{G}(W:A,\\xa0x:H)→n\\u200b1\\u200bWx:G;\\nlincomb: ({xi:G,\\u2005\\u200aai∈R}i=1k)→∑i=1kaixi:G(\\\\{x_i: \\\\mathsf{G}, \\\\; a_i \\\\in \\\\mathbb{R}\\\\}_{i=1}^k) \\\\to \\\\sum_{i=1}^k a_i x_i: \\\\mathsf{G}({xi\\u200b:G,ai\\u200b∈R}i=1k\\u200b)→∑i=1k\\u200bai\\u200bxi\\u200b:G;\\nnonlin: ({xi:G}i=1k,\\u2005\\u200aϕ:Rk→R)→ϕ(x1:k):H(\\\\{x_i: \\\\mathsf{G}\\\\}_{i=1}^k, \\\\; \\\\phi: \\\\mathbb{R}^k \\\\to \\\\mathbb{R}) \\\\to \\\\phi(x_{1:k}): \\\\mathsf{H}({xi\\u200b:G}i=1k\\u200b,ϕ:Rk→R)→ϕ(x1:k\\u200b):H (здесь мы несколько выходных векторов xix_ixi\\u200b агрегируем в один с помощью покоординатной, возможно, нелинейной функции).\\n\\nФормализм тензорных программ позволяет представить прямой и обратный проход широкого класса нейронных архитектур, который включает свёрточные сети, рекуррентные сети, сети с residual слоями. Хотя и ни одна из операций выше не может порождать новые A\\\\mathsf{A}A-переменные (веса), любое наперёд заданное число шагов градиентного спуска можно представить в рамках одной тензорной программы (посредством «развёртывания» шагов градиентного спуска).\\nНазовём величину nnn шириной тензорной программы.\\nОсновная «предельная» теорема тензорных программ представлена ниже:\\nMaster theorem (G. Yang, Tensor programs III: Neural matrix laws). Рассмотрим тензорную программу с MMM G\\\\mathsf{G}G-величинами, удовлетворяющую определённым начальным условиям. Пусть все нелинейности ϕ\\\\phiϕ и функция ψ:\\u2009RM→R\\\\psi: \\\\, \\\\mathbb{R}^M \\\\to \\\\mathbb{R}ψ:RM→R полиномиально ограничены. Тогда\\n1n∑α=1nψ(gα1,…,gαM)→EZ∼N(μ,Σ)ψ(Z)\\\\frac{1}{n} \\\\sum_{\\\\alpha=1}^n \\\\psi(g^1_\\\\alpha,\\\\ldots,g^M_\\\\alpha)\\n\\\\to \\\\mathbb{E}_{Z \\\\sim \\\\mathcal{N}(\\\\mu,\\\\Sigma)} \\\\psi(Z)\\nn1\\u200bα=1∑n\\u200bψ(gα1\\u200b,…,gαM\\u200b)→EZ∼N(μ,Σ)\\u200bψ(Z)почти наверное при n→∞n \\\\to \\\\inftyn→∞, где μ\\\\muμ и Σ\\\\SigmaΣ могут быть вычислены по некоторым рекурентным правилам.\\nОказывается, что если тензорная программа выражает прямой и обратной проход в некоторой нейронной сети, то NTK сети в инициализации всегда можно представить в виде 1n∑α=1nψ(gα1,…,gαM)\\\\frac{1}{n} \\\\sum_{\\\\alpha=1}^n \\\\psi(g^1_\\\\alpha,\\\\ldots,g^M_\\\\alpha)n1\\u200b∑α=1n\\u200bψ(gα1\\u200b,…,gαM\\u200b) для некоторой функции ψ\\\\psiψ, см. Tensor programs II: Neural tangent kernel for any architecture.Таким образом, теорема выше доказывает существование и детерминированность предельного ядра в инициализации, а также даёт способ его вычисления. Более того, это верно и для ядра в любой фиксированный момент времени, см. Tensor Programs IIb.\\nВ качестве иллюстрации обратимся вновь к сети с одним скрытым слоем. Рассмотрим тензорную программу, вычисляющую прямой и обратный проходы на входах xxx и x′x'x′. Такая программа порождает следующие G\\\\mathsf{G}G-величины: g1=w(0)xg^1 = w(0) xg1=w(0)x, g2=w(0)x′g^2 = w(0) x'g2=w(0)x′, g3=a(0)xg^3 = a(0) xg3=a(0)x и g4=a(0)x′g^4 = a(0) x'g4=a(0)x′. Напомним, что эмпирическое NTK равно\\nΘ^0(x,x′)=1n∑i=1n(ϕ(wi(0)x)ϕ(wi(0)x′)+ai2(0)ϕ′(wi(0)x)ϕ′(wi(0)x′)xx′).\\\\hat\\\\Theta_0(x,x')\\n= \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\left(\\\\phi(w_i(0) x) \\\\phi(w_i(0) x') + a_i^2(0) \\\\phi'(w_i(0) x) \\\\phi'(w_i(0) x') x x'\\\\right).\\nΘ^0\\u200b(x,x′)=n1\\u200bi=1∑n\\u200b(ϕ(wi\\u200b(0)x)ϕ(wi\\u200b(0)x′)+ai2\\u200b(0)ϕ′(wi\\u200b(0)x)ϕ′(wi\\u200b(0)x′)xx′).Положив\\nψ(gα1,…,gα4)=ϕ(gα1)ϕ(gα2)+ϕ′(gα1)ϕ′(gα2)gα3gα4,\\\\psi(g^1_\\\\alpha,\\\\ldots,g^4_\\\\alpha) = \\\\phi(g^1_\\\\alpha) \\\\phi(g^2_\\\\alpha) + \\\\phi'(g^1_\\\\alpha) \\\\phi'(g^2_\\\\alpha) g^3_\\\\alpha g^4_\\\\alpha,\\nψ(gα1\\u200b,…,gα4\\u200b)=ϕ(gα1\\u200b)ϕ(gα2\\u200b)+ϕ′(gα1\\u200b)ϕ′(gα2\\u200b)gα3\\u200bgα4\\u200b,получим выражение как раз в виде, требуемом Master Theorem.\\nСтандартная параметризация и эволюция ядра\\nКак было отмечено в предыдущем параграфе, эмпирическое NTK двухслойной сети расходится с шириной при стандартной параметризации.\\nΘ^t(x,x′)=∑i=1n(ϕ(wi(t)x)ϕ(wi(t)x′)+ai2(t)ϕ′(wi(t)x)ϕ′(wi(t)x′)xx′).\\\\hat\\\\Theta_t(x,x')\\n= \\\\sum_{i=1}^n \\\\left(\\\\phi(w_i(t) x) \\\\phi(w_i(t) x') + a_i^2(t) \\\\phi'(w_i(t) x) \\\\phi'(w_i(t) x') x x'\\\\right).\\nΘ^t\\u200b(x,x′)=i=1∑n\\u200b(ϕ(wi\\u200b(t)x)ϕ(wi\\u200b(t)x′)+ai2\\u200b(t)ϕ′(wi\\u200b(t)x)ϕ′(wi\\u200b(t)x′)xx′).При t=0t=0t=0, так как wiw_iwi\\u200b независимы и имеют порядок O(1)O(1)O(1), сумма расходится пропорционально nnn.Так как для квадратичной функции потерь f˙t(x)=Θ^t(x,x⃗)(y⃗−ft(x⃗))\\\\dot f_t(x) = \\\\hat\\\\Theta_t(x,\\\\vec x) (\\\\vec y - f_t(\\\\vec x))f˙\\u200bt\\u200b(x)=Θ^t\\u200b(x,x)(y\\u200b−ft\\u200b(x)), предсказание модели в любой точке xxx получает приращение порядка O(n)O(n)O(n) на первом же шаге обучения; для задачи регрессии такая модель теряет смысл.\\nОднако для классификации величина предсказаний не играет роли: для бинарной классификации важен лишь знак, а для многоклассовой – индекс максимального логита. Таким образом, в этом случае, несмотря на расходящееся ядро, предел при бесконечной ширине имеет смысл, см. Dynamically Stable Infinite-Width Limits of Neural Classifiers.\\nРассмотрим нормализованное эмпирическое NTK Θ~t(x,x′)=Θ^t(x,x′)/n\\\\tilde\\\\Theta_t(x,x') = \\\\hat\\\\Theta_t(x,x') / nΘ~t\\u200b(x,x′)=Θ^t\\u200b(x,x′)/n. Его предел в инициализации равен Ew∼N(0,1)ϕ(wx)ϕ(wx′)\\\\mathbb{E}_{w \\\\sim \\\\mathcal{N}(0,1)} \\\\phi(w x) \\\\phi(w x')Ew∼N(0,1)\\u200bϕ(wx)ϕ(wx′). Назовём этот предел нормализованным NTK и обозначим Θ~(x,x′)\\\\tilde\\\\Theta(x,x')Θ~(x,x′).\\nВ отличие от ядра в NTK-параметризации, нормализованное NTK при стандартной параметризации зависит от времени:\\ndΘ~t(x,x′)dt=1n∑i=1n(ϕ(wi(t)x)ϕ′(wi(t)x′)x′+ϕ′(wi(t)x)ϕ(wi(t)x′)x)dwi(t)dt+\\\\frac{d\\\\tilde\\\\Theta_t(x,x')}{dt}\\n= \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\left(\\\\phi(w_i(t) x) \\\\phi'(w_i(t) x') x' + \\\\phi'(w_i(t) x) \\\\phi(w_i(t) x') x\\\\right) \\\\frac{dw_i(t)}{dt} \\n+\\ndtdΘ~t\\u200b(x,x′)\\u200b=n1\\u200bi=1∑n\\u200b(ϕ(wi\\u200b(t)x)ϕ′(wi\\u200b(t)x′)x′+ϕ′(wi\\u200b(t)x)ϕ(wi\\u200b(t)x′)x)dtdwi\\u200b(t)\\u200b+dΘ~t(x,x′)dt+1n∑i=1nai2(t)xx′(ϕ′(wi(t)x)ϕ′′(wi(t)x′)x′+ϕ′′(wi(t)x)ϕ′(wi(t)x′)x)dwi(t)dt+\\\\phantom{\\\\frac{d\\\\tilde\\\\Theta_t(x,x')}{dt}}\\n+ \\\\frac{1}{n} \\\\sum_{i=1}^n a_i^2(t) x x' \\\\left(\\\\phi'(w_i(t) x) \\\\phi''(w_i(t) x') x' + \\\\phi''(w_i(t) x) \\\\phi'(w_i(t) x') x\\\\right) \\\\frac{dw_i(t)}{dt}\\n+\\ndtdΘ~t\\u200b(x,x′)\\u200b+n1\\u200bi=1∑n\\u200bai2\\u200b(t)xx′(ϕ′(wi\\u200b(t)x)ϕ′′(wi\\u200b(t)x′)x′+ϕ′′(wi\\u200b(t)x)ϕ′(wi\\u200b(t)x′)x)dtdwi\\u200b(t)\\u200b+dΘ~t(x,x′)dt+1n∑i=1n2ai(t)ϕ′(wi(t)x)ϕ′(wi(t)x′)xx′dai(t)dt.\\\\phantom{\\\\frac{d\\\\tilde\\\\Theta_t(x,x')}{dt}}\\n+ \\\\frac{1}{n} \\\\sum_{i=1}^n 2 a_i(t) \\\\phi'(w_i(t) x) \\\\phi'(w_i(t) x') x x' \\\\frac{da_i(t)}{dt}.\\ndtdΘ~t\\u200b(x,x′)\\u200b+n1\\u200bi=1∑n\\u200b2ai\\u200b(t)ϕ′(wi\\u200b(t)x)ϕ′(wi\\u200b(t)x′)xx′dtdai\\u200b(t)\\u200b.Напомним, как выглядит градиентный спуск в стандартной параметризации:\\nak(t)dt=∑j=1mϕ(wk(t)xj),wk(t)dt=∑j=1mak(t)ϕ′(wk(t)xj)xj.\\\\frac{a_k(t)}{dt} \\n= \\\\sum_{j=1}^m \\\\phi(w_k(t) x_j),\\n\\\\quad\\n\\\\frac{w_k(t)}{dt} \\n= \\\\sum_{j=1}^m a_k(t) \\\\phi'(w_k(t) x_j) x_j.\\ndtak\\u200b(t)\\u200b=j=1∑m\\u200bϕ(wk\\u200b(t)xj\\u200b),dtwk\\u200b(t)\\u200b=j=1∑m\\u200bak\\u200b(t)ϕ′(wk\\u200b(t)xj\\u200b)xj\\u200b.При t=0t=0t=0, a˙k=O(1)\\\\dot a_k = O(1)a˙k\\u200b=O(1), в то время как w˙k=O(n−1/2)\\\\dot w_k = O(n^{-1/2})w˙k\\u200b=O(n−1/2). Так как ak(0)=O(n−1/2)a_k(0) = O(n^{-1/2})ak\\u200b(0)=O(n−1/2) и wk(0)=O(1)w_k(0) = O(1)wk\\u200b(0)=O(1), для любого t>0t > 0t>0, не зависящего от nnn, ak(t)=O(1)a_k(t) = O(1)ak\\u200b(t)=O(1), a˙k(t)=O(1)\\\\dot a_k(t) = O(1)a˙k\\u200b(t)=O(1), wk(t)=O(1)w_k(t) = O(1)wk\\u200b(t)=O(1) и w˙k(t)=O(1)\\\\dot w_k(t) = O(1)w˙k\\u200b(t)=O(1).\\nНаивная оценка сумм даёт dΘ~t(x,x′)dt=O(1)+O(1)+O(1)=O(1)\\\\frac{d\\\\tilde\\\\Theta_t(x,x')}{dt} = O(1) + O(1) + O(1) = O(1)dtdΘ~t\\u200b(x,x′)\\u200b=O(1)+O(1)+O(1)=O(1) для любого t>0t > 0t>0, не зависящего от nnn. Таким образом, нормализованное ядро зависит от времени даже в пределе бесконечной ширины. Экспериментальный анализ эволюции ядра реальной нейронной сети в стандартной параметризации см. в работе Deep learning versus kernel learning.\\nПреимущество нейронных сетей над ядровыми методами, в том числе с NTK, может быть связано, в частности, с зависимостью предельного ядра от времени. В самом деле, ядро измеряет «похожесть» в некотором пространстве признаков. Для NTK это пространство фиксировано, в то время как нейронная сеть меняет своё ядро по ходу обучения, возможно, делая его более подходящим для задачи.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанЗагрузка...Сообщить об ошибкеПредыдущий параграф13.3. PAC-байесовские оценки рискаСледующий параграф13.5. Ландшафт функции потерьЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_44.html', 'title': 'Модели вида ARIMA'}, page_content=\"Модели вида ARIMAЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/510.1.Кластеризация10.2.Временные ряды10.3.Аналитика временных рядов10.4.Модели вида ARIMAМодель скользящего среднего MA()Модель авторегрессии AR()Модель ARMA()Модель ARIMA()Частичная автокорреляцияОценка коэффициентов в ARIMAМодели SARIMA и ARIMAX10.5.Задача ранжирования11.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Модели вида ARIMA10.4. Модели вида ARIMAАвторыВолков НикитаПрежде чем перейти к рассмотрению модели ARIMA, познакомимся сначала с двумя другими моделями: скользящего среднего и моделью авторегрессии.\\nМодель скользящего среднего MA(qqq)\\nМодель скользящего среднего порядка qqq или просто MA(qqq) предполагает следующую зависимость данных:\\nyt\\xa0=\\xa0μ+εt+θ1εt−1+...+θqεt−q,y_t\\\\ =\\\\ \\\\mu + \\\\varepsilon_t + \\\\theta_1 \\\\varepsilon_{t-1} + ... + \\\\theta_q \\\\varepsilon_{t-q},\\nyt\\u200b\\xa0=\\xa0μ+εt\\u200b+θ1\\u200bεt−1\\u200b+...+θq\\u200bεt−q\\u200b,где yty_tyt\\u200b — стационарный ряд со средним μ\\\\muμ, а εt\\\\varepsilon_tεt\\u200b — гауссовский белый шум, то есть εt∼N(0,σ2)\\\\varepsilon_t \\\\sim \\\\mathcal{N}(0, \\\\sigma^2)εt\\u200b∼N(0,σ2) и независимы.\\nПо сути наш ряд yty_tyt\\u200b выражается через сумму некоторого фиксированного среднего μ\\\\muμ, значения белого шума в текущий момент времени εt\\\\varepsilon_tεt\\u200b и не более qqq предыдущих значений белого шума, домноженных на некоторые коэффициенты, которые являются параметрами модели.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nРассмотрим некоторые свойства модели MA(qqq). Как уже было упомянуто выше, ряд yty_tyt\\u200b будет являтьcя стационарным со средним μ\\\\muμ. Найдем также Dyt\\\\mathsf{D}y_tDyt\\u200b. Воспользовавшись свойством независимости для εt\\\\varepsilon_tεt\\u200b, можем заключить, что\\nDyt=(1+θ12+⋯+θq2)σ2.\\\\mathsf{D}y_t = \\\\left(1 + \\\\theta_1^2 + \\\\dots + \\\\theta_q^2\\\\right)\\\\sigma^2.\\nDyt\\u200b=(1+θ12\\u200b+⋯+θq2\\u200b)σ2.Посчитаем автоковариационную функцию для ряда yty_tyt\\u200b, то есть найдем значение cov(yt,yt+τ)cov(y_t, y_{t + \\\\tau})cov(yt\\u200b,yt+τ\\u200b). Легко понять, что если τ>q\\\\tau > qτ>q, то cov(yt,yt+τ)cov(y_t, y_{t+\\\\tau})cov(yt\\u200b,yt+τ\\u200b) = 0, т.к. εt\\\\varepsilon_tεt\\u200b независимы. Если же τ≤q\\\\tau \\\\leq qτ≤q, то тогда\\ncov(yt,yt+τ)=(θτ+θ1θτ+1+⋯+θq−τθq)σ2.cov(y_t, y_{t+\\\\tau}) = \\\\left(\\\\theta_{\\\\tau} + \\\\theta_1\\\\theta_{\\\\tau + 1} + \\\\dots + \\\\theta_{q-\\\\tau}\\\\theta_{q}\\\\right)\\\\sigma^2.\\ncov(yt\\u200b,yt+τ\\u200b)=(θτ\\u200b+θ1\\u200bθτ+1\\u200b+⋯+θq−τ\\u200bθq\\u200b)σ2.Записав более компактно, можем получить:\\ncov(yt,yt+τ)={σ2∑j=0q−τθjθτ+j,τ≤q;0τ>q;cov(y_t, y_{t+\\\\tau}) = \\\\begin{cases}\\n\\\\sigma^2\\\\sum_{j=0}^{q-\\\\tau}\\\\theta_j\\\\theta_{\\\\tau + j}, & \\\\tau \\\\leq q; \\\\\\\\\\n0 & \\\\tau > q;\\n\\\\end{cases}\\ncov(yt\\u200b,yt+τ\\u200b)={σ2∑j=0q−τ\\u200bθj\\u200bθτ+j\\u200b,0\\u200bτ≤q;τ>q;\\u200bгде θ0=1\\\\theta_0 = 1θ0\\u200b=1. Из посчитанных значений для дисперсии и ковариационной функции, можете попробовать получить выражение и для автокорреляционной функции. Ее особенностью будет как раз равенство нулю на лаге, превосходящим qqq.\\nПосмотрим на визуализацию:\\n\\nМодель авторегрессии AR(ppp)\\nМодель авторегрессии для временного ряда можно записать следующим образом:\\nyt\\xa0=\\xa0α+φ1yt−1+...+φpyt−p+εt,y_t\\\\ =\\\\ \\\\alpha + \\\\varphi_1 y_{t-1} + ... + \\\\varphi_p y_{t-p} + \\\\varepsilon_t,\\nyt\\u200b\\xa0=\\xa0α+φ1\\u200byt−1\\u200b+...+φp\\u200byt−p\\u200b+εt\\u200b,где yty_tyt\\u200b — стационарный ряд, а εt\\\\varepsilon_tεt\\u200b — гауссовский белый шум, то есть εt∼N(0,σ2)\\\\varepsilon_t \\\\sim \\\\mathcal{N}(0, \\\\sigma^2)εt\\u200b∼N(0,σ2) и независимы. Отметим, что, вообще говоря,  для стационарности нужны некоторые условия на коэффициенты φ1,...,φp\\\\varphi_1, ..., \\\\varphi_pφ1\\u200b,...,φp\\u200b.\\nПо сути наш ряд yty_tyt\\u200b выражается через сумму некоторого фиксированного числа α\\\\alphaα, значения белого шума в текущий момент времени εt\\\\varepsilon_tεt\\u200b и не более ppp предыдущих значений этого же ряда, домноженных на некоторые коэффициенты, которые являются параметрами модели.\\nДругими словами, модель AR(ppp) — это модель линейной\\xa0регрессией\\\\textit{линейной регрессией}линейной\\xa0регрессией для которой\\n\\nТаргет: yty_tyt\\u200b — значение ряда в момент времени ttt\\nПризнаки: yt−1,...,yt−py_{t-1}, ..., y_{t-p}yt−1\\u200b,...,yt−p\\u200b — значения ряда в предыдущие моменты времени\\n\\nВведем LLL — оператор сдвига, обладающий следующими свойствами:\\n\\nприменение LLL к ряду дает предыдущее значение этого же ряда: Lyt=yt−1.Ly_t = y_{t-1}.Lyt\\u200b=yt−1\\u200b.\\nприменение LLL к белому шуму дает предыдущее значение шума: Lεt=εt−1.L\\\\varepsilon_t = \\\\varepsilon_{t-1}.Lεt\\u200b=εt−1\\u200b.\\nприменение LLL к константе — это константа: Lc=c.Lc = c.Lc=c.\\n\\nОператор LLL иногда называют также лаговым оператором. Можно рассматривать функции от оператора сдвига, например, кратное применение оператора LLL: L2yt=L(Lyt)=L(yt−1)=yt−2L^2 y_t = L(L y_t) = L(y_{t-1}) = y_{t-2}L2yt\\u200b=L(Lyt\\u200b)=L(yt−1\\u200b)=yt−2\\u200b или L−1yt=yt+1L^{-1} y_t= y_{t+1}L−1yt\\u200b=yt+1\\u200b. Для записей некоторых моделей временных рядов будет удобно использовать лаговый многочлен:\\nφ(L)=∑i=1pφiLi\\\\varphi(L) = \\\\sum_{i=1}^p \\\\varphi_i L^{i} \\nφ(L)=i=1∑p\\u200bφi\\u200bLiОбратным к оператору φ(L)\\\\varphi(L)φ(L) называют оператор φ−1(L)\\\\varphi^{-1}(L)φ−1(L) такой, что:\\nφ(L)φ−1(L)yt=φ−1(L)φ(L)yt=yt\\\\varphi(L)\\\\varphi^{-1}(L)y_t = \\\\varphi^{-1}(L)\\\\varphi(L)y_t=y_t\\nφ(L)φ−1(L)yt\\u200b=φ−1(L)φ(L)yt\\u200b=yt\\u200bТак, например, для ∣φ∣<1\\\\lvert \\\\varphi \\\\rvert < 1∣φ∣<1 можно заключить, что:\\n11−φL=(1−φL)−1=∑i=1∞φiLi\\\\frac{1}{1 - \\\\varphi L} = \\\\left(1-\\\\varphi L \\\\right)^{-1} = \\\\sum_{i=1}^{\\\\infty}\\\\varphi^{i}L^{i}\\n1−φL1\\u200b=(1−φL)−1=i=1∑∞\\u200bφiLiРассмотрим модель AR(ppp):\\nyt\\xa0=\\xa0α+φ1yt−1+...+φpyt−p+εty_t\\\\ =\\\\ \\\\alpha + \\\\varphi_1 y_{t-1} + ... + \\\\varphi_p y_{t-p} + \\\\varepsilon_t\\nyt\\u200b\\xa0=\\xa0α+φ1\\u200byt−1\\u200b+...+φp\\u200byt−p\\u200b+εt\\u200bС помощью оператора сдвига ее можно представить в следующем виде:\\na(L)yt\\xa0=\\xa0α+εt,a(L) y_t\\\\ =\\\\ \\\\alpha + \\\\varepsilon_t,\\na(L)yt\\u200b\\xa0=\\xa0α+εt\\u200b,где a(z)=1−φ1z−...−φpzpa(z) = 1 - \\\\varphi_1 z - ... - \\\\varphi_p z^pa(z)=1−φ1\\u200bz−...−φp\\u200bzp — характеристический полином.\\nСформулируем пару важных утверждений:\\n\\nЛюбой стационарный (в широком смысле) процесс представим в виде MA(∞)MA(\\\\infty)MA(∞), то есть в виде модели скользящего среднего с неограниченным количеством слагаемых (конечное или бесконечное число). Этот результат так же известен как теорема Волда о декомпозиции временного ряда.\\nМодель AR(p)AR(p)AR(p) задает стационарный временной ряд ⟺\\\\Longleftrightarrow⟺ все комплексные корни a(z)=0a(z)=0a(z)=0 лежат вне единичного круга.\\n\\nПриведем пояснение второго утверждения. В самом деле, пусть z1,...,zpz_1, ..., z_pz1\\u200b,...,zp\\u200b — все его комплексные корни (их ровно ppp с учетом кратности), тогда справедливо представление:\\na(z)=(z−z1)...(z−zp)=z1...zp(1−zz1)...(1−zzp)a(z) = (z-z_1)...(z-z_p) = z_1...z_p \\\\left(1 - \\\\frac{z}{z_1}\\\\right) ... \\\\left(1 - \\\\frac{z}{z_p}\\\\right)\\na(z)=(z−z1\\u200b)...(z−zp\\u200b)=z1\\u200b...zp\\u200b(1−z1\\u200bz\\u200b)...(1−zp\\u200bz\\u200b)Тогда при представлении временного ряда в виде\\nyt\\xa0=\\xa0α+εta(L)y_t\\\\ =\\\\ \\\\frac{\\\\alpha + \\\\varepsilon_t}{a(L)}\\nyt\\u200b\\xa0=\\xa0a(L)α+εt\\u200b\\u200bи дальнейшего его разложения на простые дроби возникнут слагаемые вида\\nεt1−Lzj.\\\\frac{\\\\varepsilon_t}{1 - \\\\frac{L}{z_j}}.\\n1−zj\\u200bL\\u200bεt\\u200b\\u200b.Если при этом zjz_jzj\\u200b лежит внутри единичного круга или на его границе, то соответствующий ряд будет расходящимся. На самом деле, случай zj=1z_j=1zj\\u200b=1 мы в дальнейшем учтем.\\nВ качестве примера рассмотрим подробнее модель AR(1)AR(1)AR(1).\\nЗависимость имеет вид yt=α+φyt−1+εty_t = \\\\alpha + \\\\varphi y_{t - 1} + \\\\varepsilon_tyt\\u200b=α+φyt−1\\u200b+εt\\u200b, где εt∼N(0,σ2)\\\\varepsilon_t \\\\sim \\\\mathcal{N}(0, \\\\sigma^2)εt\\u200b∼N(0,σ2). Для данного ряда можно выписать следующие свойства:\\n\\nУравнение 1−φz=01 - \\\\varphi z = 01−φz=0, имеет корень λ=1/φ\\\\lambda = 1/\\\\varphiλ=1/φ.\\nТем самым, AR(1)AR(1)AR(1) стационарен ⟺\\\\Longleftrightarrow⟺ ∣φ∣<1\\\\lvert\\\\varphi\\\\rvert < 1∣φ∣<1. Кроме того, чем меньше φ\\\\varphiφ, тем предыдущее значение ряда вносит меньший вклад в текущее значение.\\nЕсли ряд стационарен, то:\\n\\nEyt=α1−φ\\\\mathsf{E} y_t = \\\\frac{\\\\alpha}{1-\\\\varphi}Eyt\\u200b=1−φα\\u200b\\nDyt=σ21−φ2\\\\mathsf{D} y_t = \\\\dfrac{\\\\sigma^2}{1-\\\\varphi^2}Dyt\\u200b=1−φ2σ2\\u200b\\ncov(yt,yt−h)=φh⋅σ21−φ2cov (y_t, y_{t - h}) = \\\\varphi^h \\\\cdot \\\\dfrac{\\\\sigma^2}{1-\\\\varphi^2}cov(yt\\u200b,yt−h\\u200b)=φh⋅1−φ2σ2\\u200b.\\n\\n\\n\\nРазберем первое равенство, остальные получаются аналогично. Возьмем математическое ожидание в уравнении ряда\\nEyt=α+φEyt−1+Eεt\\\\mathsf{E} y_t = \\\\alpha + \\\\varphi \\\\mathsf{E} y_{t - 1} +\\\\mathsf{E} \\\\varepsilon_t\\nEyt\\u200b=α+φEyt−1\\u200b+Eεt\\u200bПоскольку ряд стационарен, то его математическое ожидание не меняется во времени, а для белого шума математическое ожидание равно нулю. Тем самым мы получаем уравнение на m=Eytm = \\\\mathsf{E} y_tm=Eyt\\u200b, откуда следует доказываемая формула.\\nТаким образом, в зависимости от значения φ\\\\varphiφ мы можем получить следующие результаты:\\n\\nЕсли ∣φ∣<1\\\\lvert\\\\varphi\\\\rvert < 1∣φ∣<1, то yt=μ+∑j=0∞φjεt−jy_t = \\\\mu + \\\\sum\\\\limits_{j = 0}^{\\\\infty}\\\\varphi^j\\\\varepsilon_{t - j}yt\\u200b=μ+j=0∑∞\\u200bφjεt−j\\u200b — представление ряда в виде MA(∞\\\\infty∞).\\nЕсли ∣φ∣=1\\\\lvert\\\\varphi\\\\rvert = 1∣φ∣=1, то AR(1)AR(1)AR(1) — это случайное блуждание.\\nЕсли ∣φ∣>1\\\\lvert\\\\varphi\\\\rvert > 1∣φ∣>1, то AR(1)AR(1)AR(1) — экспоненциально растущий процесс.\\n\\nПосмотрим на визуализацию.\\n\\n\\n\\nВ первом случае мы имеем модель yt=−0.5yt−1+εty_t = - 0.5 y_{t - 1} + \\\\varepsilon_tyt\\u200b=−0.5yt−1\\u200b+εt\\u200b, отрицательный коэффициент является следствием больших колебаний ряда.\\n\\n\\nВо втором случае модель yt=0.9yt−1+εty_t = 0.9 y_{t - 1} + \\\\varepsilon_tyt\\u200b=0.9yt−1\\u200b+εt\\u200b, большой положительный коэффициент делает ряд менее шумным.\\n\\n\\nВ третьем случае показано несколько рядов вида случайного блуждания yt=yt−1+εty_t = y_{t - 1} + \\\\varepsilon_tyt\\u200b=yt−1\\u200b+εt\\u200b, что соответствует случаю φ=1\\\\varphi=1φ=1.\\n\\n\\nВ четвертом случае показан экспоненциальный процесс yt=1.1yt−1+εty_t = 1.1 y_{t - 1} + \\\\varepsilon_tyt\\u200b=1.1yt−1\\u200b+εt\\u200b, на графике шум уже не заметен из-за масштаба.\\n\\n\\nНа немного вернемся к модели MA(qqq). Чуть выше мы выяснили, что при некоторых условиях на коэффиценты φ\\\\varphiφ временной ряд модели AR(ppp) будет стационарным, а значит имеет представление в виде MA(∞\\\\infty∞). На самом деле, модель скользящего среднего порядка qqq тоже можно представить с помощью оператора LLL следующим образом:\\nyt\\xa0=\\xa0μ+εt+θ1εt−1+...+θqεt−q\\xa0=\\xa0μ+b(L)εty_t\\\\ =\\\\ \\\\mu + \\\\varepsilon_t + \\\\theta_1 \\\\varepsilon_{t-1} + ... + \\\\theta_q \\\\varepsilon_{t-q}\\\\ =\\\\  \\n\\\\mu + b(L)\\\\varepsilon_tyt\\u200b\\xa0=\\xa0μ+εt\\u200b+θ1\\u200bεt−1\\u200b+...+θq\\u200bεt−q\\u200b\\xa0=\\xa0μ+b(L)εt\\u200bгде b(z)=1+θ1z+...+θqzqb(z) = 1 + \\\\theta_1 z + ... + \\\\theta_q z^qb(z)=1+θ1\\u200bz+...+θq\\u200bzq — характеристический многочлен. Для простоты изложения пусть μ=0\\\\mu = 0μ=0. Важным при такой записи оказывается понятие обратимости, то есть представления в виде\\nεt=b−1(L)yt,\\\\varepsilon_t = b^{-1}(L)y_t,\\nεt\\u200b=b−1(L)yt\\u200b,которое означает, что ряд можно представить в виде бесконечной авторегрессионной модели.\\nЗдесь, как и в рассуждениях выше, можно заключить, что временной ряд yty_tyt\\u200b обратим, если все комплексные корни b(z)=0b(z) = 0b(z)=0 лежат вне единичного круга.\\nМодель ARMA(p,qp, qp,q)\\nМодель ARMA(p,qp, qp,q) по сути является суммой моделей AR(p)AR(p)AR(p) и MA(q)MA(q)MA(q), иначе говоря, модель есть сумма нескольких предыдущих значений ряда и нескольких предыдущих значений белого шума с некоторым коэффициентами.\\nyt\\xa0=\\xa0α+φ1yt−1+...+φpyt−p+εt+θ1εt−1+...+θqεt−qy_t\\\\ =\\\\ \\\\alpha + \\\\varphi_1 y_{t-1} + ... + \\\\varphi_p y_{t-p}\\n+ \\\\varepsilon_t + \\\\theta_1 \\\\varepsilon_{t-1} + ... + \\\\theta_q \\\\varepsilon_{t-q}yt\\u200b\\xa0=\\xa0α+φ1\\u200byt−1\\u200b+...+φp\\u200byt−p\\u200b+εt\\u200b+θ1\\u200bεt−1\\u200b+...+θq\\u200bεt−q\\u200bЭквивалентную запись ряда в терминах оператора сдвига можно получить, рассмотрев два многочлена\\na(L)yt=α+b(L)εta(L) y_t = \\\\alpha + b(L) \\\\varepsilon_t\\na(L)yt\\u200b=α+b(L)εt\\u200bили\\nyt=μ+b(L)a(L)εt,y_t = \\\\mu + \\\\frac{b(L)}{a(L)} \\\\varepsilon_t,\\nyt\\u200b=μ+a(L)b(L)\\u200bεt\\u200b,где a(z)=1−φ1z−...−φpzp,a(z) = 1 - \\\\varphi_1 z - ... - \\\\varphi_p z^p,a(z)=1−φ1\\u200bz−...−φp\\u200bzp, и b(z)=1+θ1z+...+θqzqb(z) = 1 + \\\\theta_1 z + ... + \\\\theta_q z^qb(z)=1+θ1\\u200bz+...+θq\\u200bzq.\\nЗаметим, что во втором представлении константа α\\\\alphaα заменена на μ=Eyt\\\\mu = \\\\mathsf{E} y_tμ=Eyt\\u200b. На самом деле, стационарность такого ряда будет определяться только его AR(ppp) компонентой, то есть значениями коэффициентов φ\\\\varphiφ, так ряд в модели MA(qqq) всегда является стационарным.\\nМодель ARIMA(p,d,qp, d, qp,d,q)\\nМодель ARIMA(p,d,qp, d, qp,d,q) — это расширение моделей типа ARMA на нестационарные временные ряды, которые однако могут стать стационарным после применениея процедуры дифференцирования ряда. Модель ARIMA(p,d,qp, d, qp,d,q) для ряда yty_tyt\\u200b определяется как модель ARMA(p,qp, qp,q) для ряда разностей порядка ddd ряда yty_tyt\\u200b.\\n\\nРазность порядка 1: yt−yt−1=(1−L)yty_t - y_{t-1} = (1 - L) y_tyt\\u200b−yt−1\\u200b=(1−L)yt\\u200b.\\nРазность порядка 2: (1−L)2yt=(1−L)(yt−yt−1)=(yt−yt−1)−(yt−1−yt−2)=yt−2yt−1+yt−2(1 - L)^2 y_t = (1 - L) (y_t - y_{t-1}) = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) = y_t - 2y_{t-1} + y_{t-2}(1−L)2yt\\u200b=(1−L)(yt\\u200b−yt−1\\u200b)=(yt\\u200b−yt−1\\u200b)−(yt−1\\u200b−yt−2\\u200b)=yt\\u200b−2yt−1\\u200b+yt−2\\u200b.\\n\\nПолучаем формулу модели ARIMA:\\na(L)(1−L)dyt=α+b(L)εta(L) (1 - L)^d y_t = \\\\alpha + b(L) \\\\varepsilon_t\\na(L)(1−L)dyt\\u200b=α+b(L)εt\\u200bили\\n(1−L)dyt=μ+b(L)a(L)εt.(1 - L)^d y_t = \\\\mu + \\\\frac{b(L)}{a(L)} \\\\varepsilon_t.\\n(1−L)dyt\\u200b=μ+a(L)b(L)\\u200bεt\\u200b.То есть многочлен a~(z)=a(z)(1−z)d\\\\widetilde{a}(z) = a(z) (1-z)^da(z)=a(z)(1−z)d имеет ddd единичных корней.\\nТем самым такая модель позволяет учесть нестационарности, в частности, тренд.\\nВ качестве примера рассмотрим процесс случайного блуждания:\\nyt=yt−1+εt,y_t = y_{t-1} + \\\\varepsilon_t, \\nyt\\u200b=yt−1\\u200b+εt\\u200b,где εt\\\\varepsilon_tεt\\u200b — белый шум. Как уже упомяналось ранее, такой ряд не является стационарным. Однако, если мы применим операцию дифференцирования, то можем перейти к новому, уже стационарном ряду yt′=yt−yt−1y'_t = y_t - y_{t-1}yt′\\u200b=yt\\u200b−yt−1\\u200b, который можно записать в виде:\\nyt′=εty'_t = \\\\varepsilon_t\\nyt′\\u200b=εt\\u200bЧастичная автокорреляция\\nДля модели скользящего среднего порядка qqq мы выяснили, что значения автокорреляционной функции для такого ряда оказывается равной нулю после лага qqq. Эта особенность позволяет использовать автокорреляционную функцию для определения порядка модели скользящего среднего. Возникает разумный вопрос, как оценить порядок ppp для модели AR(ppp)? Здесь оказывается полезным понятие частичной (частной) автокорреляционной функции.\\nЧастичная автокорреляция (PACF) — корреляция ряда с собой после снятия линеной зависимости от промежуточных значений ряда. Иначе говоря, мы хотим как-то учесть опосредованного влияние промежуточных значений ряда и оценить непосредственное влияние yt−τy_{t - \\\\tau}yt−τ\\u200b на yty_tyt\\u200b. Чуть более формально частичную автокорреляцию можно записать следующим образом:\\nγτ={corr(yt+1,yt),τ=1;corr(yt+τ−yt+τh−1,yt−ytτ−1),τ⩾2,\\\\gamma_{\\\\tau} = \\\\begin{cases} corr(y_{t+1}, y_t), & \\\\tau=1; \\n\\t\\\\\\\\ corr\\\\left(y_{t+\\\\tau} - y_{t+\\\\tau}^{h-1}, y_t - y_t^{\\\\tau-1}\\\\right), & \\\\tau\\\\geqslant2, \\\\end{cases}\\nγτ\\u200b={corr(yt+1\\u200b,yt\\u200b),corr(yt+τ\\u200b−yt+τh−1\\u200b,yt\\u200b−ytτ−1\\u200b),\\u200bτ=1;τ⩾2,\\u200bгде ytτ−1y_t^{\\\\tau-1}ytτ−1\\u200b — линейная регрессия на yt−1,yt−2,...,yt−(τ−1)y_{t-1}, y_{t-2}, ..., y_{t - (\\\\tau-1)}yt−1\\u200b,yt−2\\u200b,...,yt−(τ−1)\\u200b:\\n\\nytτ−1=φ1yt−1+φ2yt−2+...+φτ−1yt−(τ−1)y_t^{\\\\tau-1} = \\\\varphi_1 y_{t-1} + \\\\varphi_2 y_{t-2} + ... + \\\\varphi_{\\\\tau-1} y_{t - (\\\\tau-1)}ytτ−1\\u200b=φ1\\u200byt−1\\u200b+φ2\\u200byt−2\\u200b+...+φτ−1\\u200byt−(τ−1)\\u200b\\nyt+ττ−1=φ1yt+τ−1+φ2yt+τ−2+...+φτ−1yt+1y_{t+\\\\tau}^{\\\\tau-1} = \\\\varphi_1 y_{t+\\\\tau-1} + \\\\varphi_2 y_{t+\\\\tau-2} + ... + \\\\varphi_{\\\\tau-1} y_{t+1}yt+ττ−1\\u200b=φ1\\u200byt+τ−1\\u200b+φ2\\u200byt+τ−2\\u200b+...+φτ−1\\u200byt+1\\u200b\\n\\nПример для τ=2\\\\tau=2τ=2:\\nγ2=corr(yt+2−φ1yt+1,yt−φ1yt−1)\\\\gamma_{2} = corr\\\\left(y_{t+2} - \\\\varphi_1 y_{t+1}, y_t - \\\\varphi_1 y_{t-1}\\\\right)\\nγ2\\u200b=corr(yt+2\\u200b−φ1\\u200byt+1\\u200b,yt\\u200b−φ1\\u200byt−1\\u200b)где φ1\\\\varphi_1φ1\\u200b — МНК-оценка в модели yt=φyt−1y_t = \\\\varphi y_{t-1}yt\\u200b=φyt−1\\u200b.\\nМожно показать, что значение частиной автокорреляции для модели авторегресии AR(ppp) будет ненулевой для лагов τ≤p\\\\tau \\\\leq pτ≤p и равняться нулю для лагов τ>p\\\\tau > pτ>p. Имеет место быть полная аналогия с автокорреляционной функцией и моделью MA(qqq). Таким образом, исследование поведения автокорреляционной и частичной автокорреляционной функции может быть использовано для определения порядка qqq модели скользящего среднего и порядка ppp модели авторегрессии соответсвтенно.\\nОценка коэффициентов в ARIMA\\nПусть гиперпараметры p,d,qp, d, qp,d,q фиксированы.&tab;В предположении, что εt\\\\varepsilon_tεt\\u200b — гауссовский белый шум, в нашей модели мы можем выписать функцию правдоподобия Ly(θ,φ,α)=pθ,φ,α(y1,...,yT),L_y(\\\\theta, \\\\varphi, \\\\alpha) = p_{\\\\theta, \\\\varphi, \\\\alpha}(y_1, ..., y_T),Ly\\u200b(θ,φ,α)=pθ,φ,α\\u200b(y1\\u200b,...,yT\\u200b), где pθ,φ,α(a1,...,aT)p_{\\\\theta, \\\\varphi, \\\\alpha}(a_1, ..., a_T)pθ,φ,α\\u200b(a1\\u200b,...,aT\\u200b) — соместная плотность. Из-за того, что εt\\\\varepsilon_tεt\\u200b имеют нормальное распределение, она будет иметь разумный вид. Соответственно, в качестве оценок параметров берется оценка максимального правдоподобия.\\nДля поиска начальных приближение для параметров ppp и qqq воспользуемся автокорреляционной и частичной автокорреляционной функцией.\\n\\nНачальное приближение ppp: последний значимый пик у PACF.\\nНачальное приближение qqq: последний значимый пик у ACF.\\n\\nДалее обычно используется поиск по сетке вокруг подобранных значений, минимизируя информационный критерий:\\n\\nAIC=−2ℓ∗+2(p+q+1)AIC = -2\\\\ell^* + 2(p+q+1)AIC=−2ℓ∗+2(p+q+1) — критерий Акаике;\\nAICc=−2ℓ∗+2(p+q+1)(p+q+2)T−p−q−2AIC_c = -2\\\\ell^* + \\\\frac{2(p+q+1)(p+q+2)}{T-p-q-2}AICc\\u200b=−2ℓ∗+T−p−q−22(p+q+1)(p+q+2)\\u200b — критерий Акаике (короткие ряды);\\nBIC=−2ℓ∗+(log\\u2061T−2)(p+q+1)BIC = -2\\\\ell^* + (\\\\log T - 2)(p+q+1)BIC=−2ℓ∗+(logT−2)(p+q+1) — Байесовский информационный критерий или критерий Шварца,\\n\\nгде ℓ∗=ln\\u2061Ly(θ^,φ^,α^)\\\\ell^* = \\\\ln L_y\\\\left(\\\\widehat{\\\\theta}, \\\\widehat{\\\\varphi}, \\\\widehat{\\\\alpha}\\\\right)ℓ∗=lnLy\\u200b(θ,φ\\u200b,α) — логарифм функции правдоподобия, TTT — длина временного ряда.\\nПриведем некоторый план при применению модели ARIMA для прогнозирования временных рядов.\\n\\n\\nАнализ выбросов: замена нерелевантых выбросов на NA или усреднение по соседним элементам.\\n\\n\\nСтабилизация дисперсии (преобразования).\\n\\n\\nДифференцирование, если ряд не стационарен.\\n\\n\\nВыбор пилотных ppp и qqq по PACF и ACF.\\n\\n\\nВокруг этих параметров подбираем оптим. модель по AICAICAIC/AICcAIC_cAICc\\u200b.\\n\\n\\nПошаговое построение прогноза:\\n— для t⩽Tt \\\\leqslant Tt⩽T: εt⟹ε^t=yt−y^t\\\\varepsilon_t \\\\Longrightarrow \\\\widehat{\\\\varepsilon}_t = y_t - \\\\widehat{y}_tεt\\u200b⟹εt\\u200b=yt\\u200b−y\\u200bt\\u200b;\\n— для t>Tt > Tt>T: εt⟹0\\\\varepsilon_t \\\\Longrightarrow 0εt\\u200b⟹0;\\n— для t>Tt > Tt>T: yt⟹y^ty_t \\\\Longrightarrow \\\\widehat{y}_tyt\\u200b⟹y\\u200bt\\u200b.\\n\\n\\nПостроение предсказательного интервала:\\n— если остатки модели нормальны и гомоскедастичны (дисперсия постоянна), то строится теоретический предсказательный интервал\\nσ^2(h)=σ^2(1+∑i=1h−1ψ^i2)\\\\widehat \\\\sigma^2(h) = \\\\widehat \\\\sigma^2 \\\\left(1 + \\\\sum\\\\limits_{i = 1}^{h - 1} \\\\widehat{\\\\psi}_i^2\\\\right)\\nσ2(h)=σ2(1+i=1∑h−1\\u200bψ\\u200bi2\\u200b)где hhh — горизонт прогнозирования, σ^2\\\\widehat\\\\sigma^2σ2 — оценка на дисперсию шума εt\\\\varepsilon_tεt\\u200b, ψ^i\\\\widehat{\\\\psi}_iψ\\u200bi\\u200b — коэф. для ряда при его представлении в виде бесконечного процесса скользящего среднего. И σ^2\\\\widehat\\\\sigma^2σ2, и ψ^i\\\\widehat{\\\\psi}_iψ\\u200bi\\u200b могут быть выражены через оценки на параметры φ\\\\varphiφ и θ\\\\thetaθ.\\n— иначе интервалы строятся с помощью бутстрепа.\\n\\n\\nМодели SARIMA и ARIMAX\\nРассмотрим некоторые расширение модели ARIMA. Обобщение модели ARIMA на ряды с наличием сезонной составляющей назвается SARIMA. Пусть sss — известная сезонность ряда. Добавим в модель ARIMA(p,d,qp, d, qp,d,q) компоненты, отвечающие за значения в предыдущие сезоны. Тогда модель SARIMA (p,d,q)×(P,D,Q)s(p, d, q)\\\\times (P, D, Q)_s(p,d,q)×(P,D,Q)s\\u200b может быть записана следующим образом:\\n(1−L)d(1−Ls)Dyt=μ+b(L)B(Ls)a(L)A(Ls)εt,(1 - L)^d (1 - L^s)^D y_t = \\\\mu + \\\\frac{b(L) B(L^s)}{a(L) A(L^s)} \\\\varepsilon_t,\\n(1−L)d(1−Ls)Dyt\\u200b=μ+a(L)A(Ls)b(L)B(Ls)\\u200bεt\\u200b,где\\na(z)=1−φ1z−...−φpzp,a(z) = 1 - \\\\varphi_1 z - ... - \\\\varphi_p z^p,\\na(z)=1−φ1\\u200bz−...−φp\\u200bzp,b(z)=1+θ1z+...+θqzq,b(z) = 1 + \\\\theta_1 z + ... + \\\\theta_q z^q,\\nb(z)=1+θ1\\u200bz+...+θq\\u200bzq,A(z)=1−φ1sz−⋯−φPzP,A(z) = 1 - \\\\varphi_1^s z - \\\\dots - \\\\varphi_P z^P,\\nA(z)=1−φ1s\\u200bz−⋯−φP\\u200bzP,B(z)=1+θ1sz+⋯+θQszQ.B(z) = 1 + \\\\theta^s_1 z + \\\\dots + \\\\theta_Q^s z^Q.\\nB(z)=1+θ1s\\u200bz+⋯+θQs\\u200bzQ.Параметр сезонного дифференцирования DDD, а также параметры P,QP, QP,Q подбираются из тех же соображений, что и для p,d,qp, d, qp,d,q, но только с поправкой, что делается это с учетом сезонности sss. ARIMAX — обобщение модели ARIMA, которая учитывает некоторые экзогенные факторы. Пусть xt∈Rnx_t \\\\in \\\\mathbb{R}^nxt\\u200b∈Rn — ряд регрессоров, известный до начала прогноза.\\nПростой вариант:\\n(1−L)dyt=μ+∑i=1nβia(L)xti+b(L)a(L)εt(1 - L)^d y_t = \\\\mu + \\\\sum_{i=1}^n \\\\frac{\\\\beta_i}{a(L)} x_t^i + \\\\frac{b(L)}{a(L)} \\\\varepsilon_t\\n(1−L)dyt\\u200b=μ+i=1∑n\\u200ba(L)βi\\u200b\\u200bxti\\u200b+a(L)b(L)\\u200bεt\\u200bОбщий случай:\\n(1−L)dyt=μ+∑i=1nui(L)vi(L)xti+b(L)a(L)εt(1 - L)^d y_t = \\\\mu + \\\\sum_{i=1}^n \\\\frac{u_i(L)}{v_i(L)} x_t^i + \\\\frac{b(L)}{a(L)} \\\\varepsilon_t\\n(1−L)dyt\\u200b=μ+i=1∑n\\u200bvi\\u200b(L)ui\\u200b(L)\\u200bxti\\u200b+a(L)b(L)\\u200bεt\\u200bПример: xt=I{в\\xa0момент\\xa0времени\\xa0t\\xa0праздник}x_t = I \\\\{\\\\text{в момент времени t праздник}\\\\}xt\\u200b=I{в\\xa0момент\\xa0времени\\xa0t\\xa0праздник}\\nВышеуказанные модели можно объединить и получить SARIMAX (p,d,q)times(P,D,Q)s(p, d, q)\\\\\\\\times (P, D, Q)_s(p,d,q)times(P,D,Q)s\\u200b:\\n(1−L)d(1−Ls)Dyt=μ+∑i=1nui(L)vi(L)xti+b(L)B(Ls)a(L)A(Ls)εt(1 - L)^d (1 - L^s)^D y_t = \\\\mu + \\\\sum_{i=1}^n \\\\frac{u_i(L)}{v_i(L)} x_t^i + \\\\frac{b(L) B(L^s)}{a(L) A(L^s)} \\\\varepsilon_t\\n(1−L)d(1−Ls)Dyt\\u200b=μ+i=1∑n\\u200bvi\\u200b(L)ui\\u200b(L)\\u200bxti\\u200b+a(L)A(Ls)b(L)B(Ls)\\u200bεt\\u200bПроведем аналогию с линейной регрессией. Это линейная по признакам модель, в которой\\n\\nОтклик: yty_tyt\\u200b — значение ряда в моменты времени ttt\\nПризнаки:\\n\\nyt−1,...,yt−py_{t-1}, ..., y_{t-p}yt−1\\u200b,...,yt−p\\u200b — значения ряда в предыдущие моменты времени\\nЗначение ряда за предыдущие сезоны\\nЗначения признаков в предыдущие моменты времени\\nЗначения признаков в предыдущие сезоны\\n\\n\\nОшибка: сумма шума за предыдущие моменты времени и предыдущие сезоны.\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф10.3. Аналитика временных рядовСледующий параграф10.5. Задача ранжированияЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_13.html', 'title': 'Вероятностный подход в ML'}, page_content='Вероятностный подход в MLЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/74.1.Вероятностный подход в MLСлучайность как источник несовершенства моделиУсловное распределение на таргет, непрерывный случайБолее сложные вероятностные моделиОценка максимального правдоподобия = оптимизация функции потерьПредсказание в вероятностных моделяхУсловное распределение на таргет, дискретный случай4.2.Экспоненциальный класс распределений и принцип максимальной энтропии4.3.Обобщённые линейные модели4.4.Как оценивать вероятности4.5.Генеративный подход к классификации4.6.Байесовский подход к оцениванию4.7.Модели с латентными переменными5.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Вероятностный подход в ML4.1. Вероятностный подход в MLАвторыФедотов СтаниславКак описать привычные модели на\\xa0языке статистики. Оптимизация функции потерь vs\\xa0оценка максимального правоподобияВ этом разделе мы посмотрим на те же самые модели машинного обучения, но с другой стороны: будем интерпретировать их как вероятностные.\\nВ первом параграфе мы расскажем, как обращаться с вероятностными моделями, и покажем, что привычный вам подбор параметров модели с помощью минимизации функции потерь соответствует подбору параметров методом максимального правдоподобия. Это даст возможность транслировать в мир ML известные результаты о свойствах оценок максимального правдоподобия, но в то же время и обнажит их недостатки. Благодаря этому мы сможем по-новому взглянуть на логистическую регрессию и с новым пониманием сформулировать её обобщение — generalized linear model (GLM).\\nПо ходу дела мы обнаружим, что большинство классификаторов, хоть и делают вид, что предсказывают корректные вероятности, на самом деле вводят в заблуждение.\\nВ третьем параграфе мы поговорим о том, как проверить отклонение предсказанных значений от истинных вероятностей и как поправить ситуацию.\\nДалее мы обсудим генеративный подход к классификации и разберём несколько примеров генеративных моделей, после чего перейдём к байесовскому подходу оценивания параметров, который, хоть зачастую и трудно осуществим вычислительно, однако обладает большей теоретической стройностью. Он позволяет оценивать распределение параметров и предсказаний (например, уверенность в нашей оценке), а кроме того — даёт нам возможность измерить качество модели, не прибегая к проверке на тестовой выборке.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nЕсли вы готовы — давайте приступим!\\nСлучайность как источник несовершенства модели\\nПрактически любая наша модель — несовершенна. Но объяснять это несовершенство можно по-разному.\\nПредставим, что мы решаем задачу регрессии y≃⟨x,w⟩y\\\\simeq \\\\langle x, w\\\\rangley≃⟨x,w⟩: например, пытаемся по университетским оценкам выпускника предсказать его годовую зарплату. Ясно, что точная зависимость у нас не получится как минимум потому, что мы многого не знаем о выпускнике: куда он пошёл работать, насколько он усерден, как у него с soft skills и так далее. Как же нам быть?\\nПервый вариант — просто признать, что мы не получим идеальную модель, но постараться выучить оптимальную, насколько это возможно. То есть приблизить таргет предсказаниями наилучшим образом с точки зрения какой-то меры близости, которую мы подберём из экспертных соображений.\\nТак мы получаем простой инженерный подход к машинному обучению: есть формула, в которой присутствуют некоторые параметры (www), есть формализация того, что такое «приблизить» (функция потерь) — и мы бодро решаем задачу оптимизации по параметрам.\\nВторой вариант — свалить вину за неточности наших предсказаний на случайность. В самом деле: если мы что-то не можем измерить, то для нас это всё равно что случайный фактор. В постановке задачи мы заменяем приближённое равенство y≃⟨x,w⟩y\\\\simeq \\\\langle x, w\\\\rangley≃⟨x,w⟩ на точное\\ny=(⟨x,w⟩,\\xa0искажённое\\xa0шумом\\xa0ε)y=(\\\\langle x, w\\\\rangle, \\\\text { искажённое шумом } \\\\varepsilon)\\ny=(⟨x,w⟩,\\xa0искажённое\\xa0шумом\\xa0ε)Например, это может быть аддитивный шум (чаще всего так и делают):\\ny=⟨x,w⟩+εy = \\\\langle x, w\\\\rangle + \\\\varepsilon\\ny=⟨x,w⟩+εгде ε\\\\varepsilonε — некоторая случайная величина, которая представляет этот самый случайный шум. Тогда получается, что для каждого конкретного объекта xix_ixi\\u200b соответствующий ему истинный таргет — это сумма ⟨xi,w⟩\\\\langle x_i, w\\\\rangle⟨xi\\u200b,w⟩ и конкретной реализации шума ε\\\\varepsilonε.\\nПри построении такой модели мы можем выбирать различные распределения шума, кодируя тем самым, какой может быть ошибка. Чаще всего выбирают гауссовский шум: ε∼N(0,σ2)\\\\varepsilon\\\\sim\\\\mathcal{N}(0,\\\\sigma^2)ε∼N(0,σ2) с некоторой фиксированной дисперсией σ2\\\\sigma^2σ2 — но могут быть и другие варианты.\\nПроиллюстрируем, как ведут себя данные, подчиняющиеся закону y=ax+b+εy = ax + b + \\\\varepsilony=ax+b+ε, ε∼N(0,σ2)\\\\varepsilon\\\\sim\\\\mathcal{N}(0, \\\\sigma^2)ε∼N(0,σ2):\\n\\nВопрос на подумать. Зачем человеку может прийти в голову предположить, что в модели линейной регрессии y∼Xw+εy\\\\sim Xw + \\\\varepsilony∼Xw+ε шум ε\\\\varepsilonε имеет распределение Лапласа? А распределение Коши? Чем свойства таких моделей будут отличаться от свойств модели с нормальным шумом?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Давайте посмотрим, как выглядят плотности этих трёх распределений:\\n\\nРаспределение Лапласа имеет «более тяжёлые хвосты», чем нормальное: это значит, что плотность медленнее падает с удалением от среднего. Таким образом, этому распределению могут подчиняться данные, в которых имеются выбросы. Если не гнаться за строгостью, можно сказать, что модель с нормальным шумом будет пытаться объяснить выбросы, меняя под них www, тогда как лапласовский шум потерпит их, не подгоняя www.\\nУ распределения Коши хвосты «ещё более тяжёлые», что, в теории, даёт возможность модели с таким шумом описывать даже ещё более шумные данные.\\nПроиллюстрируем датасеты, сгенерированные из моделей с каждым из типов шума: нормальным, лапласовским и Коши.\\n\\nКак вы могли заметить, в каждом из подходов после того, как мы зафиксировали признаки (то есть координаты xix_ixi\\u200b), остаётся своя степень свободы: в инженерном это выбор функции потерь, а в вероятностном — выбор распределения шума.\\nДальше в этом параграфе мы увидим, что на самом деле эти два подхода глубинным образом связаны между собой, причём выбор функции потерь — это в некотором смысле то же самое, что выбор распределения шума.\\nУсловное распределение на таргет, непрерывный случай\\nДопустим, что мы исследуем вероятностную модель таргета с аддитивным шумом\\ny=fw(x)+ε,y = f_w(x) + \\\\varepsilon,\\ny=fw\\u200b(x)+ε,где fwf_wfw\\u200b — некоторая функция, не обязательно линейная с (неизвестными пока) параметрами www, а ε\\\\varepsilonε — случайный шум с плотностью распределения ε∼pε(t)\\\\varepsilon\\\\sim p_{\\\\varepsilon}(t)ε∼pε\\u200b(t). Для каждого конкретного объекта xix_ixi\\u200b значение fw(xi)f_w(x_i)fw\\u200b(xi\\u200b) — это просто константа, но для yiy_iyi\\u200b оно превращается в случайную величину, зависящую от xix_ixi\\u200b (и ещё от www, на самом деле).\\nТаким образом, можно говорить об условном распределении\\npy(y∣x,w)p_y(y \\\\vert x, w)\\npy\\u200b(y∣x,w)Для каждого конкретного xix_ixi\\u200b и www распределение соответствующего yiy_iyi\\u200b — это просто pε(y−fw(xi))p_{\\\\varepsilon}(y - f_{w}(x_i))pε\\u200b(y−fw\\u200b(xi\\u200b)), ведь y−fw(X)=εy - f_w(X) = \\\\varepsilony−fw\\u200b(X)=ε.\\nПример. Рассмотрим вероятностную модель y=⟨x,w⟩+εy = \\\\langle x, w\\\\rangle + \\\\varepsilony=⟨x,w⟩+ε, где ε∼N(0,σ2)\\\\varepsilon\\\\sim\\\\mathcal{N}(0, \\\\sigma^2)ε∼N(0,σ2). Тогда для фиксированного xix_ixi\\u200b имеем yi=⟨xi,w⟩+εy_i = \\\\langle x_i, w\\\\rangle + \\\\varepsilonyi\\u200b=⟨xi\\u200b,w⟩+ε. Поскольку ⟨xi,w⟩\\\\langle x_i, w\\\\rangle⟨xi\\u200b,w⟩ — константа, мы получаем\\nyi∼N(⟨xi,w⟩,σ2).y_i\\\\sim\\\\mathcal{N}(\\\\langle x_i, w\\\\rangle, \\\\sigma^2).\\nyi\\u200b∼N(⟨xi\\u200b,w⟩,σ2).Это можно записать и так:\\np(yi∣xi,w)∼N(yi∣⟨xi,w⟩,σ2),p(y_i\\\\vert x_i, w)\\\\sim\\\\mathcal{N}(y_i\\\\vert\\\\langle x_i, w\\\\rangle, \\\\sigma^2),\\np(yi\\u200b∣xi\\u200b,w)∼N(yi\\u200b∣⟨xi\\u200b,w⟩,σ2),где выражение справа — это значение функции плотности нормального распределения с параметрами ⟨xi,w⟩,σ2\\\\langle x_i, w\\\\rangle, \\\\sigma^2⟨xi\\u200b,w⟩,σ2 в точке yiy_iyi\\u200b. В частности, ⟨xi,w⟩=E(yi∣xi)\\\\langle x_i, w\\\\rangle = \\\\mathbb{E}(y_i\\\\vert x_i)⟨xi\\u200b,w⟩=E(yi\\u200b∣xi\\u200b).\\nБолее сложные вероятностные модели\\nНа самом деле, мы можем для нашей задачи придумывать любую вероятностную модель py(y∣x,w)p_y(y \\\\vert x, w)py\\u200b(y∣x,w), не обязательно вида y=fw(X)+εy = f_w(X) + \\\\varepsilony=fw\\u200b(X)+ε.\\nПредставьте, что мы хотим предсказывать точку в плоскости штанг, в которую попадает мячом бьющий по воротам футболист. Можно предположить, что она имеет нормальное распределение со средним (цель удара), которое определяется ситуацией на поле и состянием игрока, и некоторой дисперсией (то есть скалярной ковариационной матрицей), которая тоже зависит от состояния игрока и ещё разных сложных факторов, которые мы объявим случайными.\\nСостояние игрока — это сложное понятие, но, вероятно, мы можем выразить его, зная пульс, давление и другие физические показатели. В свою очередь, ситуацию на поле можно описать, как функцию от позиций и движений других игроков, судьи и зрителей — но всего не перечислишь, поэтому нам снова придётся привлекать случайность. Таким образом, мы получаем то, что называется графической моделью:\\n\\nЗдесь стрелки означают статистические зависимости, а отсутствие стрелок — допущение о статистической независимости. Конечно же, это лишь допущение, принятое нами для ограничения сложности модели: ведь пульс человека и давление взаимосвязаны, равно как и поведение различных игроков на поле. Но мы уже обсуждали, что каждая модель, в том числе и вероятностная, является лишь приблизительным отражением бесконечно сложного мира. Впрочем, если у нас много вычислительных ресурсов, то никто не мешает нам попробовать учесть и все пропущенные сейчас зависимости.\\nРасписав всё по определению условной вероятности, мы получаем следующую вероятностную модель:\\n\\nв которой, конечно же, мы должны все вероятности расписать через какие-то понятные и логически обоснованные распределения — но пока воздержимся от этого.\\nОценка максимального правдоподобия = оптимизация функции потерь\\nМы хотим подобрать такие значения параметров www, для которых модель py(y∣x,w)p_y(y \\\\vert x, w)py\\u200b(y∣x,w) была бы наиболее адекватна обучающим данным. Суть метода максимального правдоподобия (maximum likelihood estimation) состоит в том, чтобы найти такое www, для которого вероятность (а в данном, непрерывном, случае плотность вероятности) появления выборки y={y1,…,yN}y = \\\\{y_1, \\\\ldots, y_N\\\\}y={y1\\u200b,…,yN\\u200b} была бы максимальной, то есть\\nw^MLE=argmax\\u2061wp(y∣X,w)\\\\widehat{w}_{MLE} = \\\\underset{w}{\\\\operatorname{argmax}}p(y \\\\vert X, w)\\nwMLE\\u200b=wargmax\\u200bp(y∣X,w)Величина p(y∣X,w)p(y \\\\vert X, w)p(y∣X,w) называется функцией правдоподобия (likelihood). Если мы считаем, что все объекты независимы, то функция правдоподобия распадается в произведение:\\np(y∣X,w)=p(y1∣x1,w)⋅…⋅p(yi∣xi,w)p(y \\\\vert X, w) = p(y_1 \\\\vert x_1, w) \\\\cdot\\\\ldots\\\\cdot p(y_i \\\\vert x_i, w)\\np(y∣X,w)=p(y1\\u200b∣x1\\u200b,w)⋅…⋅p(yi\\u200b∣xi\\u200b,w)Теперь, поскольку перемножать сложно, а складывать легко (и ещё поскольку мы надеемся, что раз наши объекты всё-таки наблюдаются в природе, их правдоподобие отлично от нуля), мы переходим к логарифму функции правдоподобия:\\nl(y∣X,w)=log\\u2061p(y1∣x1,w)+…+log\\u2061p(yi∣xi,w)l(y \\\\vert X,w) = \\\\log{p(y_1 \\\\vert x_1, w)} + \\\\ldots + \\\\log{p(y_i \\\\vert x_i, w)}\\nl(y∣X,w)=logp(y1\\u200b∣x1\\u200b,w)+…+logp(yi\\u200b∣xi\\u200b,w)эту функцию мы так или иначе максимизируем по www, находя оценку максимального правдоподобия w^\\\\hat{w}w^.\\nКак мы уже обсуждали выше, p(yi∣xi,w)=pε(y−fw(xi))p(y_i \\\\vert x_i, w) = p_{\\\\varepsilon}(y - f_{w}(x_i))p(yi\\u200b∣xi\\u200b,w)=pε\\u200b(y−fw\\u200b(xi\\u200b)), то есть\\nl(y∣X,w)=∑i=1Nlog\\u2061pε(yi−fw(xi))l(y \\\\vert X,w) = \\\\sum\\\\limits_{i=1}^N\\\\log{p_{\\\\varepsilon}(y_i - f_w(x_i))}\\nl(y∣X,w)=i=1∑N\\u200blogpε\\u200b(yi\\u200b−fw\\u200b(xi\\u200b))Максимизация функции правдоподобия соответствует минимизации\\n∑i=1N[−log\\u2061pε(yi−fw(xi))]\\\\sum\\\\limits_{i=1}^N\\\\left[-\\\\log{p_{\\\\varepsilon}(y_i - f_w(x_i))}\\\\right]\\ni=1∑N\\u200b[−logpε\\u200b(yi\\u200b−fw\\u200b(xi\\u200b))]а это выражение можно интерпретировать, как функцию потерь. Вот и оказывается, что подбор параметров вероятностей модели с помощью метода максимального правдоподобия — это то же самое, что «инженерная» оптимизация функции потерь. Давайте посмотрим, как это выглядит в нескольких простых случаях.\\nПример. Давайте предположим, что наш таргет связан с данными вот так:\\nyi=⟨xi,w⟩+εy_i = \\\\langle x_i, w \\\\rangle + \\\\varepsilon\\nyi\\u200b=⟨xi\\u200b,w⟩+εгде ε∼N(0,σ2)\\\\varepsilon\\\\sim\\\\mathcal{N}(0, \\\\sigma^2)ε∼N(0,σ2), то есть\\np(ε)=12πσ2exp\\u2061(−ε22σ2)p(\\\\varepsilon) = \\\\frac{1}{\\\\sqrt{2 \\\\pi \\\\sigma^2}} \\\\exp\\\\left(-\\\\frac{\\\\varepsilon^2}{2\\\\sigma^2}\\\\right)\\np(ε)=2πσ2\\u200b1\\u200bexp(−2σ2ε2\\u200b)Случайная величина yiy_iyi\\u200b получается из шума ε\\\\varepsilonε сдвигом на постоянный вектор ⟨xi,w⟩\\\\langle x_i, w \\\\rangle⟨xi\\u200b,w⟩, так что она тоже распределена нормально с той же дисперсией σ2\\\\sigma^2σ2 и со средним ⟨xi,w⟩\\\\langle x_i, w \\\\rangle⟨xi\\u200b,w⟩\\np(yi∣⟨xi,w⟩)=12πσ2exp\\u2061(−(yi−⟨xi,w⟩)22σ2)p(y_i\\\\vert \\\\langle x_i, w \\\\rangle) = \\\\frac{1}{\\\\sqrt{2 \\\\pi \\\\sigma^2}} \\\\exp\\\\left(-\\\\frac{(y_i - \\\\langle x_i, w \\\\rangle)^2}{2\\\\sigma^2}\\\\right)\\np(yi\\u200b∣⟨xi\\u200b,w⟩)=2πσ2\\u200b1\\u200bexp(−2σ2(yi\\u200b−⟨xi\\u200b,w⟩)2\\u200b)Правдоподобие выборки имеет вид\\np(y∣X,w)=∏i=1Np(yi∣xi,w)=∏i=1N12πσ2exp\\u2061(−(yi−⟨w,xi⟩)22σ2)p(y\\\\vert X, w) = \\\\prod_{i=1}^N p(y_i \\\\vert x_i, w) = \\\\prod_{i=1}^N \\\\frac{1}{\\\\sqrt{2 \\\\pi \\\\sigma^2}} \\\\exp\\\\left(-\\\\frac{(y_i-\\\\langle w,x_i\\\\rangle)^2}{2\\\\sigma^2}\\\\right)\\np(y∣X,w)=i=1∏N\\u200bp(yi\\u200b∣xi\\u200b,w)=i=1∏N\\u200b2πσ2\\u200b1\\u200bexp(−2σ2(yi\\u200b−⟨w,xi\\u200b⟩)2\\u200b)Логарифм правдоподобия можно переписать в виде\\nl(y∣X,w)=∑i=1N(−log\\u2061(2πσ2)−(yi−⟨w,xi⟩)22σ2)l(y \\\\vert X,w) = \\\\sum_{i=1}^N \\\\left(-\\\\log({\\\\sqrt{2 \\\\pi \\\\sigma^2}}) -\\\\frac{(y_i-\\\\langle w,x_i\\\\rangle)^2}{2\\\\sigma^2}\\\\right)\\nl(y∣X,w)=i=1∑N\\u200b(−log(2πσ2\\u200b)−2σ2(yi\\u200b−⟨w,xi\\u200b⟩)2\\u200b)Постоянными слагаемыми можно пренебречь, и тогда оказывается, что максимизация этой величины равносильна минимизации\\n∑i=1N(yi−⟨w,xi⟩)2 \\\\sum_{i=1}^N (y_i-\\\\langle w,x_i\\\\rangle)^2\\ni=1∑N\\u200b(yi\\u200b−⟨w,xi\\u200b⟩)2Мы получили обычную квадратичную функцию потерь. Итак, обучать вероятностную модель линейной регрессии с нормальным шумом — это то же самое, что учить «инженерную» модель с функцией потерь MSE.\\nВопрос на подумать. Какая вероятностная модель соответствует обучению линейной регрессии с функцией потерь MAE\\n∑i=1N∣yi−⟨w,xi⟩∣?\\\\sum_{i=1}^N \\\\vert y_i-\\\\langle w,x_i\\\\rangle\\\\vert?\\ni=1∑N\\u200b∣yi\\u200b−⟨w,xi\\u200b⟩∣?Ответ (не открывайте сразу; сначала подумайте сами!)Минимизация функции потерь MAE соответствует максимизации\\n∑i=1N[−∣yi−⟨w,xi⟩∣]\\\\sum_{i=1}^N\\\\left[-\\\\vert y_i-\\\\langle w,x_i\\\\rangle\\\\vert\\\\right]\\ni=1∑N\\u200b[−∣yi\\u200b−⟨w,xi\\u200b⟩∣]Мы хотим найти такое распределение, для которого эта штука является с точностью до константы логарифмом функции правдоподобия. Что ж, возьмём экспоненту:\\nexp[−∑i=1N∣yi−⟨w,xi⟩∣]=∏i=1Nexp(−∣yi−⟨w,xi⟩∣)\\\\text{exp}\\\\left[-\\\\sum_{i=1}^N\\\\vert y_i-\\\\langle w,x_i\\\\rangle\\\\vert\\\\right] =\\n\\\\prod_{i=1}^N\\\\text{exp}\\\\left(-\\\\vert y_i-\\\\langle w,x_i\\\\rangle\\\\vert\\\\right)exp[−i=1∑N\\u200b∣yi\\u200b−⟨w,xi\\u200b⟩∣]=i=1∏N\\u200bexp(−∣yi\\u200b−⟨w,xi\\u200b⟩∣)Если теперь это умножить на (12)N\\\\left(\\\\frac12\\\\right)^{N}(21\\u200b)N, то мы получим функцию правдоподобия для распределения Лапласа:\\n∏i=1N12exp(−∣yi−⟨w,xi⟩∣)=∏i=1NLaplace(yi−⟨w,xi⟩)\\\\prod_{i=1}^N\\\\frac12\\\\text{exp}\\\\left(-\\\\vert y_i-\\\\langle w,x_i\\\\rangle\\\\vert\\\\right) = \\\\prod_{i=1}^NLaplace\\\\left(y_i-\\\\langle w,x_i\\\\rangle\\\\right)\\ni=1∏N\\u200b21\\u200bexp(−∣yi\\u200b−⟨w,xi\\u200b⟩∣)=i=1∏N\\u200bLaplace(yi\\u200b−⟨w,xi\\u200b⟩)Итак, учить «инженерную» модель с функцией потерь MAE — это то же самое, что обучать вероятностную модель линейной регрессии с лапласовским шумом.\\nПредсказание в вероятностных моделях\\nТеперь представим, что параметры подобраны, и подумаем о том, как же теперь делать предсказания.\\nРассмотрим модель линейной регрессии\\ny=⟨x,w⟩+ε,ε∼N(0,σ2)y = \\\\langle x, w\\\\rangle + \\\\varepsilon,\\\\quad\\\\varepsilon\\\\sim\\\\mathcal{N}(0,\\\\sigma^2)\\ny=⟨x,w⟩+ε,ε∼N(0,σ2)Если www известен, то для нового объекта x0x_0x0\\u200b соответствующий таргет имеет вид\\ny0=⟨x0,w⟩+ε∼N(⟨x0,w⟩,σ2)y_0 = \\\\langle x_0, w\\\\rangle + \\\\varepsilon\\\\sim\\\\mathcal{N}(\\\\langle x_0, w\\\\rangle, \\\\sigma^2)\\ny0\\u200b=⟨x0\\u200b,w⟩+ε∼N(⟨x0\\u200b,w⟩,σ2)Таким образом, y0y_0y0\\u200b дан нам не точно, а в виде распределения (и логично: ведь мы оговорились выше, что ответы у нас искажены погрешностью, проинтерпретированной, как нормальный шум). Но что делать, если требуют назвать конкретное число? Кажется логичным выдать условное матожидание E(y0∣x0)=⟨x0,w⟩\\\\mathbb{E}(y_0\\\\vert x_0) = \\\\langle x_0, w\\\\rangleE(y0\\u200b∣x0\\u200b)=⟨x0\\u200b,w⟩, тем более что оно совпадает с условной медианой и условной модой этого распределения.\\nЕсли же медиана, мода и математическое ожидание различаются, то можно выбрать что-то из них с учётом особенностей задачи. Но на практике в схеме y∼f(x)+εy\\\\sim f(x) + \\\\varepsilony∼f(x)+ε чаще всего рассматривают именно симметричные распределения с нулевым матожиданием, потому что для них f(x)f(x)f(x) совпадает с условным матожиданием E(y∣x)\\\\mathbb{E}(y\\\\vert x)E(y∣x) и является логичным точечным предсказанием.\\nПриведём пример. Допустим шум ε\\\\varepsilonε был бы из экспоненциального распределения. Тогда f(x)f(x)f(x) была бы условным минимумом распределения. В принципе, можно придумать задачу, для которой такая постановка (предсказание минимума) была бы логичной. Но это всё же довольно экзотическая ситуация. Приводим для сравнения модели с нормальным, лапласовским и экспоненциальным шумом:\\n\\nУсловное распределение на таргет, дискретный случай\\nДопустим, мы имеем дело с задачей классификации с KKK классами. Как мы можем её решать? Самый наивный вариант — научиться по каждому объекту xix_ixi\\u200b предсказывать некоторое число для каждого класса, и у кого число больше — тот класс и выбираем! Наверное, так можно сделать, если мы придумаем хорошую функцию потерь. Но сразу в голову приходит мысль: почему бы не начать предсказывать не просто число, а вероятность?\\nТаким образом, задача классификации сводится к предсказанию\\nP(yi=k∣xi)P(y_i = k \\\\vert x_i)\\nP(yi\\u200b=k∣xi\\u200b)и как будто бы выбору класса с наибольшей вероятностью. Впрочем, как мы увидим дальше, всё не всегда работает так просто.\\nОдну такую модель — правда, только для бинарной классификации — вы уже знаете. Это логистическая регрессия:\\nP(yi=1∣xi,w)=11+e−⟨xi,w⟩,P(yi=0∣xi,w)=e−(xi,w)1+e−⟨xi,w⟩=11+e⟨xi,w⟩P(y_i = 1 \\\\vert x_i,w) = \\\\frac{1}{1+e^{-\\\\langle x_i, w\\\\rangle}},\\\\quad P(y_i = 0 \\\\vert x_i,w) = \\\\frac{e^{-(x_i, w)}}{1+e^{-\\\\langle x_i, w\\\\rangle}} = \\\\frac{1}{1+e^{\\\\langle x_i, w\\\\rangle}}\\nP(yi\\u200b=1∣xi\\u200b,w)=1+e−⟨xi\\u200b,w⟩1\\u200b,P(yi\\u200b=0∣xi\\u200b,w)=1+e−⟨xi\\u200b,w⟩e−(xi\\u200b,w)\\u200b=1+e⟨xi\\u200b,w⟩1\\u200bкоторую также можно записать в виде\\nyi∣xi∼Bern(11+e−⟨xi,w⟩)y_i \\\\vert x_i \\\\sim \\\\color{red}{Bern}\\\\left(\\\\frac{1}{1+e^{-\\\\langle x_i, w\\\\rangle}}\\\\right)\\nyi\\u200b∣xi\\u200b∼Bern(1+e−⟨xi\\u200b,w⟩1\\u200b)где Bern(p)\\\\color{red}{Bern}(p)Bern(p) — распределение Бернулли с параметром ppp.\\nНахождение вероятностей классов можно разделить на два этапа:\\n\\xa0Находим\\xa0xi→\\xa0логиты\\xa0(−⟨xi,w⟩,⟨xi,w⟩)→σ(σ(−⟨xi,w⟩),σ(⟨xi,w⟩))\\\\begin{aligned}\\n&\\\\text { Находим }\\\\\\\\\\n&x_i \\\\xrightarrow{\\\\text { логиты }}\\\\left(-\\\\left\\\\langle x_i, w\\\\right\\\\rangle,\\\\left\\\\langle x_i, w\\\\right\\\\rangle\\\\right) \\\\xrightarrow{\\\\sigma}\\\\left(\\\\sigma\\\\left(-\\\\left\\\\langle x_i, w\\\\right\\\\rangle\\\\right), \\\\sigma\\\\left(\\\\left\\\\langle x_i, w\\\\right\\\\rangle\\\\right)\\\\right)\\n\\\\end{aligned}\\n\\u200b\\xa0Находим\\xa0xi\\u200b\\xa0логиты\\xa0\\u200b(−⟨xi\\u200b,w⟩,⟨xi\\u200b,w⟩)σ\\u200b(σ(−⟨xi\\u200b,w⟩),σ(⟨xi\\u200b,w⟩))\\u200bгде, напомним, σ\\\\sigmaσ — это сигмоида:\\nσ(t)=11+e−t\\\\sigma(t) = \\\\frac{1}{1+e^{-t}}\\nσ(t)=1+e−t1\\u200bСигмоида тут не просто так. Она обладает теми счастливыми свойствами, что\\n\\n\\nмонотонно возрастает;\\n\\n\\nотображает всю числовую прямую на интервал (0,1)(0,1)(0,1);\\n\\n\\nσ(−x)=1−σ(x)\\\\sigma(-x) = 1 - \\\\sigma(x)σ(−x)=1−σ(x).\\n\\n\\nВот такой вид имеет её график:\\n\\nИными словами, с помощью сигмоиды можно делать «вероятности» из чего угодно, то есть более или менее для любого отображения fwf_wfw\\u200b (из признакового пространства в R\\\\mathbb{R}R) с параметрами www построить модель бинарной классификации:\\nP(yi=0∣xi,w)=σ(fw(−xi)),P(yi=1∣xi,w)=σ(fw(xi)).P(y_i = 0 \\\\vert x_i, w) = \\\\sigma(f_w(-x_i)),\\\\quad P(y_i = 1 \\\\vert x_i, w) = \\\\sigma(f_w(x_i)).\\nP(yi\\u200b=0∣xi\\u200b,w)=σ(fw\\u200b(−xi\\u200b)),P(yi\\u200b=1∣xi\\u200b,w)=σ(fw\\u200b(xi\\u200b)).Как и в случае логистической регрессии, такая модель равносильна утверждению о том, что\\nfw(xi)=log\\u2061p(y=1∣xi,w)p(y=0∣xi,w).f_w(x_i) = \\\\log{\\\\frac{p(y = 1 \\\\vert x_i,w)}{p(y = 0 \\\\vert x_i, w)}}.\\nfw\\u200b(xi\\u200b)=logp(y=0∣xi\\u200b,w)p(y=1∣xi\\u200b,w)\\u200b.Похожим способом можно строить и модели для многоклассовой классификации. В этом нам поможет обобщение сигмоиды, которое называется softmax:\\nsoftmax(t1,…,tK)=(et1∑k=1Ketk,…,etK∑k=1Ketk)softmax(t_1,\\\\ldots,t_K) = \\\\left(\\\\frac{e^{t_1}}{\\\\sum_{k=1}^Ke^{t_k}},\\\\ldots,\\\\frac{e^{t_K}}{\\\\sum_{k=1}^Ke^{t_k}}\\\\right)\\nsoftmax(t1\\u200b,…,tK\\u200b)=(∑k=1K\\u200betk\\u200bet1\\u200b\\u200b,…,∑k=1K\\u200betk\\u200betK\\u200b\\u200b)А именно, для любого отображения fwf_wfw\\u200b из пространства признаков в RK\\\\mathbb{R}^KRK мы можем взять модель\\n(P(yi=k∣xi,w))k=1K=softmax(fw(xi))\\\\left(P(y_i = k \\\\vert x_i, w)\\\\right)^K_{k=1} = softmax(f_w(x_i))\\n(P(yi\\u200b=k∣xi\\u200b,w))k=1K\\u200b=softmax(fw\\u200b(xi\\u200b))Если все наши признаки — вещественные числа, а fw(xi)=xiWf_w(x_i) = x_iWfw\\u200b(xi\\u200b)=xi\\u200bW — просто линейное отображение, то мы получаем однослойную нейронную сеть\\n(P(yi=k∣xi,w))k=1K=softmax(xiW)\\\\left(P(y_i = k \\\\vert x_i, w)\\\\right)^K_{k=1} = softmax(x_iW)\\n(P(yi\\u200b=k∣xi\\u200b,w))k=1K\\u200b=softmax(xi\\u200bW)\\nПредостережение. Всё то, что мы описали выше, вполне работает на практике (собственно, классификационные нейросети зачастую так и устроены), но корректным не является.\\nВ самом деле, мы говорим, что строим оценки вероятностей P(yi=k∣xi,w)P(y_i = k \\\\vert x_i, w)P(yi\\u200b=k∣xi\\u200b,w), но для подбора параметров используем не эмпирические вероятности, а только лишь значения argmax\\u2061k\\xa0P(yi=k∣xi,w)\\\\underset{k}{\\\\operatorname{argmax}} \\\\ P(y_i = k \\\\vert x_i, w)kargmax\\u200b\\xa0P(yi\\u200b=k∣xi\\u200b,w), то есть метки предсказываемых классов. Таким образом, при обучении мы не будем различать следующие две ситуации:\\n\\nЭто говорит нам о некоторой неполноценности такого подхода.\\nЗаметим ещё вот что. В случае бинарной классификации выбор предсказываемого класса как argmax\\u2061kP(yi=k∣xi,w)\\\\underset{k}{\\\\operatorname{argmax}} P(y_i=k \\\\vert x_i,w)kargmax\\u200bP(yi\\u200b=k∣xi\\u200b,w) равносилен выбору того класса, для которого P(yi=k∣xi,w)>12P(y_i=k \\\\vert x_i,w) > \\\\frac{1}{2}P(yi\\u200b=k∣xi\\u200b,w)>21\\u200b. Но если наши оценки вероятностей неадекватны, то этот вариант проваливается, и мы встаём перед проблемой выбора порога: каким должно быть значение t^\\\\widehat{t}t, чтобы мы могли приписать класс 1 тем объектам xix_ixi\\u200b, для которых σ(fw(xi))>t^\\\\sigma(f_w(x_i)) > \\\\widehat{t}σ(fw\\u200b(xi\\u200b))>t?\\nВ одном из следующих параграфов мы обсудим, как всё-таки правильно предсказывать вероятности.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф3.3. Подбор гиперпараметровКак эффективно подбирать значения гиперпараметров модели и\\xa0не\\xa0переобучиться при этомСледующий параграф4.2. Экспоненциальный класс распределений и принцип максимальной энтропииСамые главные семейства распределений в\\xa0жизни любого data scientist’аЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_64.html', 'title': 'Матричная факторизация'}, page_content='Матричная факторизацияЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/716.1.Матричное дифференцирование16.2.Матричная факторизацияИтак, я разложил матрицу в произведения — и что же?Сингулярное разложениеИспользование SVD: латентные признакиНа что не способно сингулярное разложениеПрактические кейсыИспользование SVD: разделённые представления и рекомендательная система для бедныхВероятностное обличье модели латентных факторовАнализ независимых компонент (ICA)Неотрицательное матричное разложение (NMF)16.3.Вероятностные распределения16.4.Многомерные распределения16.5.Независимость и условные распределения вероятностей16.6.Параметрические оценки16.7.Энтропия и семейство экспоненциальных распределенийГлавная/Хендбуки/Учебник по машинному обучению/Матричная факторизация16.2. Матричная факторизацияАвторыФедотов СтаниславЕсли наш датасет таков, что все признаки целочисленные и вещественные, нет пропущенных значений и других приятных сюрпризов, то матрица объекты-признаки — это просто матрица, к которой можно пробовать применять инструменты из линейной алгебры, а среди таковых весьма полезными оказываются матричные разложения, то есть различные способы представить матрицу в виде произведения двух или более матриц, обычно специального вида. Такие разновидности, как LU-разложение, QR-разложение, разложение Холецкого вы несомненно встретите, если откроете код любой библиотеки численной линейной алгебры, но суждено ли матричным разложениям играть роль только лишь винтиков и шестерёнок, запрятанных внутри инструментов машинного обучения, или какие-то из них и сами по себе могут помочь вам анализировать данные? На этот вопрос мы попробуем ответить в данном разделе. Но прежде, чем переходить к конкретным методам, мы разберёмся, к каким моделям данных можно прийти, разложив матрицу в произведение.\\nИтак, я разложил матрицу в произведения — и что же?\\nПредположим, что нашу матрицу объекты-признаки XXX мы представили в виде произведения (или, более общно, приблизили в каком-либо смысле таким произведением):\\nX\\u2061N×D∼B\\u2061N×R⋅C\\u2061R×D\\\\underset{N\\\\times D}{\\\\operatorname{X}} \\\\sim \\\\underset{N\\\\times R}{\\\\operatorname{B}} \\\\cdot \\\\underset{R\\\\times D}{\\\\operatorname{C}}\\nN×DX\\u200b∼N×RB\\u200b⋅R×DC\\u200bгде внизу указаны размеры матриц (то есть в нашем датасете NNN объектов и DDD признаков). Что это может означать?\\nСмесь признаков\\nМы считаем, что каждый из DDD признаков нашего исходного датасета — это смесь (то есть линейная комбинация) RRR скрытых (латентных) признаков:Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\n\\nПо сути это одна из самых простых моделей с латентными переменными, в которой исходные признаки выражаются через латентные линейным образом. Если R<DR < DR<D, то мы получаем приближённое описание нашего датасета с помощью меньшего количества признаков. На уровне объектов каждый объект xix_ixi\\u200b (DDD-мерная строка) приобретает латентное представление ziz_izi\\u200b (RRR-мерная строка), с которой он связан соотношением xi=ziCx_i = z_i\\\\color{green}{C}xi\\u200b=zi\\u200bC. Мы можем представлять, что наши объекты xix_ixi\\u200b представляют из себя не DDD-мерное облако, а лежат на некоторой RRR-мерной плоскости; переходя к RRR-мерным представлениям ziz_izi\\u200b, мы обнажаем эту структуру.\\nТочность аппроксимации можно измерять по-разному; наиболее популярной (в силу вычислительной простоты) является норма Фробениуса ∣A∣∗fro2=∑∗Aij2=tr\\u2061(ATA)| A |*{fro}^2  = \\\\sum\\\\limits* A_{i j}^2 = \\\\operatorname{tr}(A^TA)∣A∣∗fro2=∑∗Aij2\\u200b=tr(ATA)  — соответствующую модель называют  анализом главных компонент, или PCA (Principal Component Analysis).\\nПонижение размерности признакового пространства\\nМы можем захотеть описать наш датасет меньшим чем DDD количеством признаков (а может быть, и вообще каким-то весьма маленьким). У нас может быть несколько причин для этого, например:\\n\\n\\nПризнаков очень много, и мы боимся, что обучение на них будет занимать очень много времени или что в процессе обучения нам потребуется слишком много оперативной памяти;\\n\\n\\nМы считаем, что в данных есть шум или что часть признаков связаны соотношением приближённой линейной зависимости — иными словами, мы уверены, что значительную часть информации можно закодировать меньшим числом признаков\\n\\n\\nМы уже обсуждали, что это можно получить, построив приближённое разложение:\\nX\\u2061N×D∼B\\u2061N×T⋅C\\u2061T×D\\\\underset{N\\\\times D}{\\\\operatorname{X}} \\\\sim \\\\underset{N\\\\times T}{\\\\operatorname{B}} \\\\cdot \\\\underset{T\\\\times D}{\\\\operatorname{C}}\\nN×DX\\u200b∼N×TB\\u200b⋅T×DC\\u200bМатематика помогает. Матрица имеет ранг TTT тогда и только тогда, когда она представляется в виде\\nB\\u2061NtimesS⋅C\\u2061S×D\\\\underset{Ntimes S}{\\\\operatorname{B}}\\\\cdot\\\\underset{S\\\\times D}{\\\\operatorname{C}}NtimesSB\\u200b⋅S×DC\\u200b для S=TS = TS=T\\nи не представляется в таком виде для меньших SSS.\\nДоказывать это мы не будем, но подметим, что  приблизить датасет линейной смесью TTT признаков — это то же самое, что приблизить матрицу XXX матрицей X^\\\\hat{X}X^ ранга TTT .\\nКачество приближения. Нам, конечно же, хочется, чтобы приближение было наилучшим — скажем, в том смысле, чтобы разность X−BCX - BCX−BC была минимальной в каком-либо смысле. Можно предложить много разных метрик; остановимся на двух:\\n\\n\\n Норма Фробениуса.\\nПредставим, что матрица A=(X−BC)A = (X - BC)A=(X−BC) — это просто вектор из N×DN\\\\times DN×D чисел, который зачем-то записали в виде прямоугольной таблицы. Тогда его норму можно записать в виде\\n∥A∥fro=∑i,jaij2=tr(ATA)\\\\|A\\\\|_{fro} = \\\\sqrt{\\\\sum\\\\limits_{i,j}a_{ij}^2} = \\\\sqrt{\\\\mathrm{tr}\\\\left(A^TA\\\\right)}\\n∥A∥fro\\u200b=i,j∑\\u200baij2\\u200b\\u200b=tr(ATA)\\u200bЭту норму (а точнее, её квадрат) легко оптимизировать.\\n\\n\\nОператорная  l2l_2l2\\u200b-норма.\\nВычислять её тяжко, а уж оптимизировать вообще непонятно как, зато звучит круто. Идея в том, что отображения можно сравнивать в зависимости от того, как оно действует на векторы: чем больше оно умеет удлинять векторы — тем оно «больше»:\\n∥A∥2=sup\\u2061{∣Av∣∣v∣∣v∈RD}\\\\|A\\\\|_2 = \\\\sup \\\\left\\\\{ \\\\frac{\\\\vert Av \\\\vert}{\\\\vert v \\\\vert} \\\\mid v \\\\in\\\\mathbb{R}^D \\\\right\\\\}\\n∥A∥2\\u200b=sup{∣v∣∣Av∣\\u200b∣v∈RD}Поскольку ∣A(λv)∣∣λv∣=∣Av∣∣v∣\\\\frac{\\\\vert A(\\\\lambda v)\\\\vert}{\\\\vert \\\\lambda v \\\\vert} = \\\\frac{\\\\vert Av \\\\vert}{ \\\\vert v \\\\vert}∣λv∣∣A(λv)∣\\u200b=∣v∣∣Av∣\\u200b, достаточно брать супремум только по векторам единичной длины, то есть по единичной сфере. Так как это компакт, непрерывная функция v↦∣Av∣v\\\\mapsto \\\\vert Av \\\\vertv↦∣Av∣ достигает на нём своего максимального значения, то есть мы можем переписать\\n∥A∥2=sup\\u2061{∣Av∣∣v∈RD,\\u2009∣v∣=1}\\\\|A\\\\|_2 = \\\\sup \\\\left\\\\{ \\\\vert Av \\\\vert \\\\mid v\\\\in\\\\mathbb{R}^D,\\\\,\\\\vert v \\\\vert = 1 \\\\right\\\\}\\n∥A∥2\\u200b=sup{∣Av∣∣v∈RD,∣v∣=1}\\n\\nСмесь объектов\\nМы считаем, что каждый из NNN объектов нашего исходного датасета — смесь (то есть линейная комбинация) RRR скрытых объектов:\\n\\nТакая интерпретация может быть полезна, например, в ситуации, когда объекты — это записи с каждого из нескольких микрофонов в помещении, признаки — фреймы, а скрытые объекты — это голоса отдельных людей.\\nТакже данную модель можно интерпретировать как что-то вроде поиска типичных объектов.\\nОтдельные представления для объектов и признаков\\nЭту интерпретацию лучше всего пояснить на примере. Пусть объекты нашего датасета соответствуют пользователям интернет-магазина, а признаки — товарам, причём в клетке с индексом (i,j)(i,j)(i,j) записана единица, если пользователь интересовался товаром, и ноль — если нет (или, в более общей ситуации, рейтинги, которые пользователи ставят товарам).\\n\\nПри перемножении матриц BBB и CCC на (i,j)(i, j)(i,j)-м месте произведении стоит скалярное произведение iii-й строки BBB и jjj-го столбца CCC. Таким образом, степень релевантности товара пользователю моделируется скалярным произведением (напрашивается сравнение с косинусным расстоянием) вектора, представляющего iii-го пользователя, и вектора, представляющего jjj-й товар.\\nXij=bi1c1j+…+bikckj==(i-ый\\xa0пользователь,j-ый\\xa0товар)\\\\begin{array}{l}\\nX_{\\\\color{red}{i} \\\\color{green}{j}}=\\\\color{red}{b_{i 1}} \\\\color{green}{c_{1 j}} + \\\\ldots + \\\\color{red}{b_{i k}} \\\\color{green}{c_{k j}}= \\\\\\\\\\n= ( \\\\color{red}{\\\\text{i-ый пользователь}}, \\\\color{green}{ \\\\text{j-ый товар} })\\n\\\\end{array}\\nXij\\u200b=bi1\\u200bc1j\\u200b+…+bik\\u200bckj\\u200b==(i-ый\\xa0пользователь,j-ый\\xa0товар)\\u200bЗаметим ещё, что RRR координат вектора, ответчающего пользователю, равно как и RRR координат вектора, отвечающего товару, можно рассматривать как RRR латентных признаков, которые в идеальном мире являются интерпретируемыми и характеризуют «сродство» пользователя и товара с некоторым аспектом бытия:\\n\\nМатрицы в разложении: физический смысл\\nНо матрицы в разложении обычно не абы какие — так какие из разновидностей могут быть полезны?\\nВо всех известных вам матричных разложениях к отдельным сомножителям предъявляются определённые требования: симметричность, треугольность, ортогональность — некоторые из них (скажем, симметричность) не имеют физического смысла ни в одной из указанных выше интерпретаций. Но одно оказывается полезным.\\nКовариация и дисперсия признаков\\nДля начала — и это важно —  предположим, что матрица XXX центрирована по столбцам, то есть среднее в каждом из столбцов (= признаков) равно нулю (если это не так, то вычтем из каждого столбца его среднее).\\nТеперь матрица ковариации признаков может быть с точностью до константы оценена как XTXX^TXXTX:\\n\\nИ мы видим: iii-й и jjj-й столбцы матрицы XXX ортогональны тогда и только тогда, когда соответствующие признаки не коррелированы.\\nПри этом (i,i)(i,i)(i,i)-й диагональный элемент матрицы XTXX^TXXTX — это дисперсия iii-го признака.\\nВывод: матрица, ортонормированная по столбцам, отвечает датасету, в котором признаки не коррелированы и имеют единичную дисперсию\\nСингулярное разложение\\nС помощью сингулярного разложения можно перейти от DDD исходных признаков к потенциально небольшому количеству «самых важных» , по-быстрому визуализовать данные или построить простенькую рекомендательную систему. Конечно, глубинные автоэнкодеры, TSNE или DSSM справятся с этим гораздо лучше, но если данных относительно немного или если хочется что-нибудь быстро попробовать «на коленке», старое доброе сингулярное разложение всегда подставит плечо.\\nМатематическое определение\\nСингулярным разложением матрицы XXX называется разложение\\nX=UΣVT,X = \\\\color{orange}{U} \\\\color{green}{\\\\Sigma}\\\\color{magenta}{V^T},\\nX=UΣVT,где U\\\\color{orange}{U}U и V\\\\color{magenta}{V}V — матрицы, ортонормированные по столбцам, а Σ=diag(σ1,σ2,…)\\\\color{green}{\\\\Sigma} = \\\\mathrm{diag}(\\\\color{green}{\\\\sigma_1},\\\\color{green}{\\\\sigma_2},\\\\ldots)Σ=diag(σ1\\u200b,σ2\\u200b,…) — диагональная матрица, у которой σ1⩾σ2⩾…⩾σR>σR+1=0\\\\color{green}{\\\\sigma_1} \\\\geqslant \\\\color{green}{\\\\sigma_2} \\\\geqslant \\\\ldots \\\\geqslant \\\\color{green}{\\\\sigma_R}> \\\\color{green}{\\\\sigma_{R+1}}=0σ1\\u200b⩾σ2\\u200b⩾…⩾σR\\u200b>σR+1\\u200b=0.\\nЧисла σi\\\\color{green}{\\\\sigma_i}σi\\u200b называются сингулярными\\xa0числами\\\\color{green}{\\\\text{сингулярными числами}}сингулярными\\xa0числами, а столбцы U\\\\color{orange}{U}U и V\\\\color{magenta}{V}V — левыми\\\\color{orange}{\\\\text{левыми}}левыми  и правыми\\\\color{magenta}{\\\\text{правыми}}правыми сингулярными векторами соответственно (их алгебраический смысл станет ясен чуть ниже).\\nСингулярное разложение можно записать в полном или в усечённом виде:\\n\\nПара предостережений по поводу ортогональности по столбцам:\\nUUU ортогональна по столбцам\\nUTU=EU^TU=EUTU=E (элементы UTUU^TUUTU — скалярные произведения столбцов UUU)\\nно UUT≠EUU^T \\\\neq EUUT\\ue020=E (не обязательно равно; элементы UUTUU^TUUT  — скалярные произведения строк UUU)\\nЯсно, что хранить полное разложение нет смысла: ведь бесполезные, умножающиеся на нули, блоки будут лишь занимать память.\\nПо-английски сингулярное разложение называется SVD (singular value decomposition), и мы будем активно использовать эту аббревиатуру.\\nЕсли вы не любите математику, можете пропустить. С точки зрения математики сингулярное разложение говорит следующее. Пусть XXX — матрица линейного отображения φ:RD⟶RN\\\\varphi:\\\\mathbb{R}^D\\\\longrightarrow\\\\mathbb{R}^Nφ:RD⟶RN.\\nТогда найдётся ортонормированый базис v1,…,vD\\\\color{magenta}{v_1},\\\\dots,\\\\color{magenta}{v_D}v1\\u200b,…,vD\\u200b в пространстве RD\\\\mathbb{R}^DRD и ортонормированый базис u1,…,uN\\\\color{orange}{u_1},\\\\dots,\\\\color{orange}{u_N}u1\\u200b,…,uN\\u200b в пространстве RN\\\\mathbb{R}^NRN, в которых действие оператора записывается следующим образом:\\nφ(v1)=σ1u1,⋮φ(vR)=σruR,φ(vR+1)=0,⋮φ(vD)=0\\\\begin{align*}\\n\\\\varphi(\\\\color{magenta}{v_1}) &= \\\\color{green}{\\\\sigma_1}\\\\color{orange}{u_1},\\\\\\\\\\n\\\\vdots\\\\\\\\\\n\\\\varphi(\\\\color{magenta}{v_R}) &= \\\\color{green}{\\\\sigma_r}\\\\color{orange}{u_R},\\\\\\\\\\n\\\\varphi(\\\\color{magenta}{v_{R+1}}) &= 0,\\\\\\\\\\n\\\\vdots\\\\\\\\\\n\\\\varphi(\\\\color{magenta}{v_D}) &= 0\\n\\\\end{align*}φ(v1\\u200b)⋮φ(vR\\u200b)φ(vR+1\\u200b)⋮φ(vD\\u200b)\\u200b=σ1\\u200bu1\\u200b,=σr\\u200buR\\u200b,=0,=0\\u200b(знатоки функционального анализа могут узнать в этом частный случай теоремы Гильберта-Шмидта).\\nСингулярное разложение и операторная l2-норма.\\nМожно показать, что ∥∥A∥∥2\\\\|\\\\|A\\\\|\\\\|_2∥∥A∥∥2\\u200b, эта самая операторная l2-норма матрицы, равна σ12\\\\sigma_1^2σ12\\u200b — квадрату наибольшего сингулярного числа.\\nСингулярное разложение и норма Фробениуса. Можно показать, что\\n∣∣A∣∣fro=∑iσi2||A||_{fro} = \\\\sqrt{\\\\sum_i\\\\sigma_i^2}\\n∣∣A∣∣fro\\u200b=i∑\\u200bσi2\\u200b\\u200bКонтрольный вопрос.Единственно ли сингулярное разложение матрицы? Давайте сразу считать, что речь об усечённом разложении.\\nЕсть очень простой источник неоднозначности: если φ(vi)=σ1ui\\\\varphi(\\\\color{magenta}{v_i}) = \\\\color{green}{\\\\sigma_1}\\\\color{orange}{u_i}φ(vi\\u200b)=σ1\\u200bui\\u200b, то и φ(−vi)=σ1(−ui)\\\\varphi(\\\\color{magenta}{-v_i}) = \\\\color{green}{\\\\sigma_1}\\\\color{orange}{(-u_i)}φ(−vi\\u200b)=σ1\\u200b(−ui\\u200b); при этом умножение вектора на (−1)(-1)(−1) не попортит ортонормированности базиса. Иными словами, мы можем одновременно поменять знаки iii-х столбцов матриц U\\\\color{orange}{U}U и V\\\\color{magenta}{V}V (без транспонирования!) без ущерба для разложения.\\nЕсть и более тонкие, хотя и весьма частные, ситуации. Можете ли вы, например, указать несколько различных сингулярных разложений матрицы EEE? Да-да, для неё сингулярное разложение максимально неоднозначно. Можете ли вы теперь придумать не скалярную матрицу, у которой были бы различные SVD, отличающиеся не только знаками столбцов матриц U\\\\color{orange}{U}U и V\\\\color{magenta}{V}V?\\nТеоретико-вероятностная интерпретация SVD\\nЕсли X=UΣVTX =\\\\color{orange}{U}\\\\color{green}{\\\\Sigma} \\\\color{magenta}{V^T}X=UΣVT (рассмотрим сейчас не усечённое, а полное разложение, в котором матрицы U\\\\color{orange}{U}U и V\\\\color{magenta}{V}V квадратные ортогональные), то\\nXTX=VΣTUT⋅U⏟=EΣVT=VΣTΣVTX^TX = \\\\color{magenta}{V}\\\\color{green}{\\\\Sigma^T}\\\\underbrace{\\\\color{orange}{U^T}\\\\cdot \\\\color{orange}{U}}_{=E}\\\\color{green}{\\\\Sigma} \\\\color{magenta}{V^T} =\\n\\\\color{magenta}{V}\\\\color{green}{\\\\Sigma}^T\\\\color{green}{\\\\Sigma} \\\\color{magenta}{V^T}XTX=VΣT=EUT⋅U\\u200b\\u200bΣVT=VΣTΣVTОтметим, что в рассматриваемой ситуации Σ\\\\color{green}{\\\\Sigma}Σ не обязательно квадратная, и поэтому нельзя написать, что ΣTΣ=Σ2\\\\color{green}{\\\\Sigma}^T\\\\color{green}{\\\\Sigma}=\\\\color{green}{\\\\Sigma}^2ΣTΣ=Σ2; тем не менее, ΣTΣ\\\\color{green}{\\\\Sigma}^T\\\\color{green}{\\\\Sigma}ΣTΣ — это квадратная матрица с числами σ12,σ22,…\\\\color{green}{\\\\sigma_1}^2,\\\\color{green}{\\\\sigma_2}^2,\\\\ldotsσ1\\u200b2,σ2\\u200b2,… на диагонали.\\nКонтрольный вопросТочно так же мы можем вычислить XXT=UTΣΣTUXX^T = \\\\color{orange}{U}^T\\\\color{green}{\\\\Sigma}\\\\color{green}{\\\\Sigma}^T\\\\color{orange}{U}XXT=UTΣΣTU, где опять-таки ΣΣT\\\\color{green}{\\\\Sigma}\\\\color{green}{\\\\Sigma}^TΣΣT — квадратная матрица с числами σ12,σ22,…\\\\color{green}{\\\\sigma_1}^2,\\\\color{green}{\\\\sigma_2}^2,\\\\ldotsσ1\\u200b2,σ2\\u200b2,… на диагонали. И всё бы хорошо, но если Σ\\\\color{green}{\\\\Sigma}Σ не квадратная, матрицы ΣTΣ\\\\color{green}{\\\\Sigma}^T\\\\color{green}{\\\\Sigma}ΣTΣ и ΣΣT\\\\color{green}{\\\\Sigma}\\\\color{green}{\\\\Sigma}^TΣΣT имеют разные размеры. Так в чём же подвох?\\nКак бы то ни было, в (ортогональном!) базисе из (ортогональных!) столбцов V\\\\color{magenta}{V}V матрица XTXX^TXXTX приводится к диагональному виду с числами σ12,σ22,…\\\\color{green}{\\\\sigma_1}^2,\\\\color{green}{\\\\sigma_2}^2,\\\\ldotsσ1\\u200b2,σ2\\u200b2,… на диагонали.\\nТеперь представим, что наши объекты x1,x2,…,xNx_1,x_2,\\\\ldots,x_Nx1\\u200b,x2\\u200b,…,xN\\u200b выбраны из DDD-мерного нормального распределения\\np(xi)=1(2π)D/2∣C∣1/2e−12(xi−μ)C−1(xi−μ)Tp(x_i) = \\\\frac{1}{(2\\\\pi)^{D/2}|C|^{1/2}}e^{-\\\\frac12(x_i - \\\\mu)C^{-1}(x_i - \\\\mu)^T}\\np(xi\\u200b)=(2π)D/2∣C∣1/21\\u200be−21\\u200b(xi\\u200b−μ)C−1(xi\\u200b−μ)Tгде μ\\\\muμ — вектор средних, а CCC — матрица ковариации. Это, в частности, значит, что облако точек представляет из себя нечто вроде эллипсоида в DDD-мерном пространстве с центром μ\\\\muμ.\\nПредположим, что μ=0\\\\mu = 0μ=0 (все признаки центрированы); тогда оценкой матрицы ковариации признаков является матрица 1nXTX\\\\frac1n X^TXn1\\u200bXTX. Допустим, что эта оценка точная, тогда разложение XTX=VΣTΣVTX^TX = \\\\color{magenta}{V}\\\\color{green}{\\\\Sigma}^T\\\\color{green}{\\\\Sigma} \\\\color{magenta}{V}^TXTX=VΣTΣVT даёт нам аналогичное разложение C=V(1nΣTΣ)VTC = \\\\color{magenta}{V}(\\\\frac1n\\\\color{green}{\\\\Sigma}^T\\\\color{green}{\\\\Sigma}) \\\\color{magenta}{V}^TC=V(n1\\u200bΣTΣ)VT. Теперь замена координат x=zVTx = z\\\\color{magenta}{V}^Tx=zVT (с матрицей замены V\\\\color{magenta}{V}V — то есть переход происходит в базис из столбцов матрицы V\\\\color{magenta}{V}V) даёт нам\\np(xi)=const⋅exp\\u2061(−12⋅xi⋅C−1⋅xiT)p(x_i) = \\\\mathrm{const}\\\\cdot \\\\exp\\\\left(-\\\\frac12\\\\cdot\\\\color{red}{x_i}\\\\cdot \\\\color{blue}{C^{-1}}\\\\cdot \\\\color{grey}{x_i^T}\\\\right)\\np(xi\\u200b)=const⋅exp(−21\\u200b⋅xi\\u200b⋅C−1⋅xiT\\u200b)Обратите внимание, что xix_ixi\\u200b и xiTx_i^TxiT\\u200b стоят в формуле на непривычных местах, как будто их перепутали, но нет — просто xix_ixi\\u200b у нас является строкой, а не столбцом.\\np(z)=const⋅exp\\u2061(−12⋅ziVT⋅V⏟=E(nΣ−1Σ−T)VT⋅V⏟=EziT)=p(z) = \\\\mathrm{const}\\\\cdot \\\\exp\\\\left(-\\\\frac12\\\\cdot\\\\color{red}{z_i}\\\\underbrace{\\\\color{red}{V^T}\\\\cdot \\\\color{blue}{V}}_{=E}\\\\color{blue}{(n\\\\Sigma^{-1}\\\\Sigma^{-T})}\\\\underbrace{\\\\color{blue}{V^T}\\\\cdot \\\\color{cyan}{V}}_{=E}\\\\color{grey}{z_i^T}\\\\right)=\\np(z)=const⋅exp(−21\\u200b⋅zi\\u200b=EVT⋅V\\u200b\\u200b(nΣ−1Σ−T)=EVT⋅V\\u200b\\u200bziT\\u200b)==const⋅exp\\u2061(−n2ziΣ−1Σ−TziT)=const⋅exp\\u2061(−n2(1σ12zi12+1σ22zi22+…))== \\\\mathrm{const}\\\\cdot \\\\exp\\\\left(-\\\\frac{n}2z_i\\\\Sigma^{-1}\\\\Sigma^{-T}z_i^T\\\\right)=\\\\mathrm{const}\\\\cdot \\\\exp\\\\left(-\\\\frac{n}2\\\\left(\\\\frac1{\\\\sigma_1^2}z_{i1}^2 + \\\\frac1{\\\\sigma_2^2}z_{i2}^2 + \\\\ldots\\\\right)\\\\right)=\\n=const⋅exp(−2n\\u200bzi\\u200bΣ−1Σ−TziT\\u200b)=const⋅exp(−2n\\u200b(σ12\\u200b1\\u200bzi12\\u200b+σ22\\u200b1\\u200bzi22\\u200b+…))==p(xi1′)⋅…⋅p(xiD′)=p(x\\'_{i1})\\\\cdot\\\\ldots\\\\cdot p(x\\'_{iD})\\n=p(xi1′\\u200b)⋅…⋅p(xiD′\\u200b)Итак, если наши данные взяты из многомерного нормального распределения, после перехода к базису из столбцов V\\\\color{magenta}{V}V новые координаты становятся независимыми; вместе с тем это соответствует переходу к главным осям ковариационной матрицы — и геометрически столбцы V\\\\color{magenta}{V}V соответствуют главным осям эллипсоида-облака точек.\\n\\nИспользование SVD: латентные признаки\\nЗапишем\\nX\\u2061N×D=UΣ\\u2061N×R⋅VT\\u2061R×D\\\\underset{N\\\\times D}{\\\\operatorname{X}} = \\\\underset{N\\\\times R}{\\\\operatorname{U\\\\Sigma}} \\\\cdot \\\\underset{R\\\\times D}{\\\\operatorname{V^T}}\\nN×DX\\u200b=N×RUΣ\\u200b⋅R×DVT\\u200bи вспомним самую первую интерпретацию матричного разложения.\\n\\nСтолбцы UΣU\\\\SigmaUΣ ортогональны (так как они пропорциональны столбцам ортогональной по столбцам матрицы UUU) — то есть  латентные признаки не коррелированы. При этом, поскольку длина каждого из столбцов UUU равна 1, длины столбцов UΣU\\\\SigmaUΣ порпорциональны σi\\\\sigma_iσi\\u200b — а, значит,  латентные признаки упорядочены по невозрастанию дисперсии (ведь с точностью до константы оценка дисперсии признака — это квадрат длины вектора его значений).\\nЗаметим, что перед применением SVD признаки лучше центрировать, иначе первая компонента будет указывать в сторону центра масс облака точек (зачем нам это?), а остальные вынуждены будут ей быть ортогональны:\\n\\nПонижение размерности признакового пространства\\nМы уже обсуждали, что это можно получить, построив приближение ранга TTT или, что то же самое, приближённое разложение\\nX\\u2061N×D∼B\\u2061N×T⋅C\\u2061T×D\\\\underset{N\\\\times D}{\\\\operatorname{X}} \\\\sim \\\\underset{N\\\\times T}{\\\\operatorname{B}} \\\\cdot \\\\underset{T\\\\times D}{\\\\operatorname{C}}\\nN×DX\\u200b∼N×TB\\u200b⋅T×DC\\u200bдля некоторого и желательно небольшого TTT. И тут SVD приходится более чем кстати.\\nТеорема Эккарта-Янга\\nНаилучшее по норме Фробениуса приближение ранга TTT — это\\n\\nТаким образом, если вы хотите получить TTT «самых важных» признаков, то вы можете использовать SVD. Но что это за признаки? Что именно означают эти слова «самые важные»? Давайте обратимся к геометрии, которая, как мы помним, тесно связана с теорией вероятностей:\\n\\n\\nЕсли применить SVD к датасету, изображённому на последней картинке, и взять два первых латентных признака, то эллипсоид превратится в эллипс; меньшая из полуосей, похожая на шум, будет забыта, останется две бOльших. Видим: самое важное для SVD — это самое масштабное.\\nА правда ли у нас получится хорошее приближение с помощью TTT новых признаков? Посчитаем норму разности. Везде ниже U\\\\color{orange}{U}U и V\\\\color{magenta}{V}V — квадратные ортогональные матрицы; в частности Σ\\\\color{green}{\\\\Sigma}Σ не обязательно квадратная матрица размера N×DN\\\\times DN×D.\\n\\n∣∣Δ∣∣fro2=tr((UΣ~VT)TUΣ~VT)=tr(VΣ~TUTU⏟=EΣ~VT)=||\\\\Delta||_{fro}^2 = \\\\mathrm{tr}\\\\left((\\\\color{orange}{U}\\\\color{green}{\\\\widetilde{\\\\Sigma}}\\\\color{magenta}{V}^T)^T\\\\color{orange}{U}\\\\color{green}{\\\\widetilde{\\\\Sigma}}\\\\color{magenta}{V}^T\\\\right) = \\n\\\\mathrm{tr}\\\\left(\\\\color{magenta}{V}\\\\color{green}{\\\\widetilde{\\\\Sigma}}^T\\\\underbrace{\\\\color{orange}{U}^T\\\\color{orange}{U}}_{=E}\\\\color{green}{\\\\widetilde{\\\\Sigma}}\\\\color{magenta}{V}^T\\\\right) =∣∣Δ∣∣fro2\\u200b=tr((UΣVT)TUΣVT)=tr(VΣT=EUTU\\u200b\\u200bΣVT)==tr(VΣ~TΣ~VT)=tr(Σ~TΣ~VTV⏟=E)=∣∣Σ~∣∣fro2=σT+12+…+σR2=\\\\mathrm{tr}\\\\left(\\\\color{magenta}{V}\\\\color{green}{\\\\widetilde{\\\\Sigma}}^T\\\\color{green}{\\\\widetilde{\\\\Sigma}}\\\\color{magenta}{V}^T\\\\right) = \\\\mathrm{tr}\\\\left(\\\\color{green}{\\\\widetilde{\\\\Sigma}}^T\\\\color{green}{\\\\widetilde{\\\\Sigma}}\\\\underbrace{\\\\color{magenta}{V}^T\\\\color{magenta}{V}}_{=E}\\\\right) = ||\\\\color{green}{\\\\widetilde{\\\\Sigma}}||^2_{fro} = \\\\color{green}{\\\\sigma_{T+1}}^2 + \\\\ldots + \\\\color{green}{\\\\sigma_{R}}^2\\n=tr(VΣTΣVT)=tr(ΣTΣ=EVTV\\u200b\\u200b)=∣∣Σ∣∣fro2\\u200b=σT+1\\u200b2+…+σR\\u200b2Аналогичным образом\\n∣∣Δ∣∣2=σT+1||\\\\Delta||_2 = \\\\color{green}{\\\\sigma_{T+1}}\\n∣∣Δ∣∣2\\u200b=σT+1\\u200bпотому что умножение на ортогональную матрицу не меняет операторную l2l_2l2\\u200b-норму. Таким образом, если сингулярные значения убывают достаточно медленно (например, линейно), то мы вряд ли сможем приблизить исходную матрицу матрицей маленького ранга с очень хорошей точностью.\\nКак избавиться от иллюзий.\\nСгенерируйте матрицу 100×100100\\\\times100100×100 с помощью np.random.rand или np.random.randn. Для какого TTT вы сможете найти матрицу ранга TTT, приближающую исходную с относительной точностью 10−610^{-6}10−6?\\nК счастью, в реальных датасетах сингулярные значения убывают достаточно быстро или же нам хватает довольно грубого приближения.\\nПереход из исходного признакового пространства в новое и обратно\\nДопустим, мы построили приближённое разложение ранга TTT:\\nX∼U^Σ^V^T=U⋅(σ1⋱σT0⋱)⏟=:Σ′⋅VT X\\\\sim\\\\color{orange}{\\\\widehat{U}}\\\\color{green}{\\\\widehat{\\\\Sigma}}\\\\color{magenta}{\\\\widehat{V}}^T =\\n\\\\color{orange}{U}\\\\cdot\\\\underbrace{\\\\color{green}{\\\\begin{pmatrix}\\n\\\\sigma_1 &\\\\\\\\\\n & \\\\ddots & \\\\\\\\\\n & & \\\\sigma_T &\\\\\\\\\\n& & & 0 & \\\\\\\\\\n& & & & \\\\ddots\\\\end{pmatrix}}}_{=:\\\\color{green}{\\\\Sigma\\'}}\\\\cdot\\\\color{magenta}{V}^TX∼UΣVT=U⋅=:Σ′\\u200bσ1\\u200b\\u200b⋱\\u200bσT\\u200b\\u200b0\\u200b⋱\\u200b\\u200b\\u200b\\u200b⋅VTМатрица U^Σ^\\\\color{orange}{\\\\widehat{U}}\\\\color{green}{\\\\widehat{\\\\Sigma}}UΣ — это первые TTT столбцов матрицы UΣ′\\\\color{orange}{U}\\\\color{green}{\\\\Sigma\\'}UΣ′, и они же первые TTT столбцов матрицы UΣ=X⋅V\\\\color{orange}{U}\\\\color{green}{\\\\Sigma} = X\\\\cdot\\\\color{magenta}{V}UΣ=X⋅V. Таким образом, для перевода объекта xix_ixi\\u200b в новое признаковое пространство нужно произвести xi↦xi⋅Vx_i\\\\mapsto x_i\\\\cdot\\\\color{magenta}{V}xi\\u200b↦xi\\u200b⋅V и взять первые TTT столбцов или, что то же самое, xi↦xi⋅V[:,:T]x_i\\\\mapsto x_i\\\\cdot\\\\color{magenta}{V[:,:T]}xi\\u200b↦xi\\u200b⋅V[:,:T].\\nТеперь пусть задана вектор-строка ziz_izi\\u200b длины TTT — латентное представление, соответствующего некоторому объекту, то есть одна из строк матрицы U^Σ^\\\\color{orange}{\\\\widehat{U}}\\\\color{green}{\\\\widehat{\\\\Sigma}}UΣ. Тогда точно восстановить исходный xix_ixi\\u200b мы не сможем: ведь равенство X∼U^Σ^V^TX\\\\sim\\\\color{orange}{\\\\widehat{U}}\\\\color{green}{\\\\widehat{\\\\Sigma}}\\\\color{magenta}{\\\\widehat{V}}^TX∼UΣVT не точное, но для приближённого восстановления xix_ixi\\u200b мы должны произвести zi↦zi⋅V^T=zi⋅V[:,:T]Tz_i\\\\mapsto z_i\\\\cdot \\\\color{magenta}{\\\\widehat{V}}^T = z_i\\\\cdot\\\\color{magenta}{V[:,:T]}^Tzi\\u200b↦zi\\u200b⋅VT=zi\\u200b⋅V[:,:T]T. \\nНа что не способно сингулярное разложение\\nСингулярное разложение умеет находить дающие самый существенный вклад в дисперсию линейные комбинации признаков, притом некоррелированные; в случае нормально распределённых данных эти направления оказываются главными осями эллипсоида, которым является облако данных. К сожалению, эта суперспособность SVD столь же охотно превращается в слабость, ведь:\\n\\nДанные не всегда распределены нормально, они могут обладать сложной геометрией, но SVD будет упрямо искать эллипсоид.\\nСамое важное не всегда самое масштабное. Забыть привести признаки к одному масштабу — хороший способ выстрелить себе в ногу при работе с сингулярным разложением.\\nНовые признаки не обязаны быть хорошо интерпретируемыми. Линейная комбинация возраста, стажа работы и зарплаты — это не то, что хотелось бы показывать банковскому регулятору.\\nВыбросы почти наверняка усложнят вам жизнь, хотя, возможно, SVD поможет вам их увидеть.\\n\\nПрактические кейсы\\nMNIST и путешествие по латентному пространству\\nВозьмём большой датасет MNIST, состоящий из чёрно-белых изображений рукописных цифр размера 28×2828\\\\times2828×28 пикселей (его можно загрузить, к примеру, отсюда), вытянем каждое из изображений в вектор, получив тем самым матрицу размера 60000×(28⋅28)60000\\\\times(28\\\\cdot28)60000×(28⋅28), и применим к этой матрице SVD. Теперь возьмём первые два латентных признака (то есть первые два столбца матрицы UΣU\\\\SigmaUΣ) — получается, что каждая рукописная цифра у нас теперь кодируется вектором из двух чисел. Нарисуем на плоскости точки, соответствующие этим векторам (скажем, по 100 из каждого класса, чтобы хоть что-нибудь было понятно):\\n\\nЧто же мы видим? Единицы и нули оказались особенными, то есть уже первые два латентных признака хорошо их различают, правда, с середине какая-то каша. А почему? Да потому, что мы забыли центрировать данные. Давайте перед применением SVD вычтем из каждого признака (то есть из каждого пикселя) его среднее по всем картинкам, а потом нарисуем всё заново:\\n\\nТеперь стало получше: например, семёрки, девятки и четвёрки сгуппировались вместе с другой стороны от восьмёрок и троек (собственно говоря, это отражает тот факт, что рукописные написания семёрок, девяток и четвёрок могут быть похожи друг на друга, так и человек не сразу отличит — а вот с тройкой их спутать намного труднее).\\nЗаметим ещё вот что. В (28⋅28)(28\\\\cdot28)(28⋅28)-мерном пространстве наборов пикселей совсем не каждая точка соответствует какой-то рукописной цифре — то, что может приходить из реального мира, лежит на некоторой хитрой поверхности в этом пространстве (если выражаться корректнее, то на подмногообразии). Если же мы попробуем нарисовать «изображения», лежащие на отрезке, соединяющем два изображения цифр, то получим нечто не слишком интересное:\\n\\nОдно изображение просто наложилось и затем сменило другое — скучно! Но если мы сделаем то же самое в двумерном пространстве, образованном первыми двумя латентными признаками SVD, то мы будем получать, может быть, не совсем реалистичные изображения цифр, но что-то явно из мира рукописных символов:\\n\\nКонтрольный вопросЕсли x1x_1x1\\u200b — вектор-строка длины 28⋅2828\\\\cdot2828⋅28, отвечающая первой картинке, x2x_2x2\\u200b — второй, а UUU, Σ\\\\SigmaΣ, VTV^TVT — матрицы из сингулярного разложения, то как получить картинку, соответствующую середине отрезка, соединяющего в пространстве двух первых латентных признаков SVD точки x1′x_1\\'x1′\\u200b и x2′x_2\\'x2′\\u200b, отвечающие этим картинкам?\\nХимический состав рек\\nПосмотрим на небольшой кусок вот этого датасета, который доступен для скачивания нигде (ха-ха), и попробуем что-нибудь понять про химических состав рек европейского союза, а заодно соберём шишки, которые могут попасться при визуализации с помощью SVD.\\nКонечно, сразу хочется нарисовать все объекты датасета в виде точек на плоскости. Мы знаем, что в этом может помочь SVD — попробуем же! Центрируем признаки — и рисуем:\\n\\nОй, что-то пошло не так. Но почему же?! Наверное, надо хотя бы посмотреть на данные...\\n\\nОбъекты имеют вид «GBPKER0059», «GB20227», «LVV0120100» и так далее — это коды станций, измеряющих состав воды;\\nПризнаки имеют вид «1985 BOD5», «1985 Chlorophyll a», «1985 Orthophosphates» и так далее — тут указан год измерения и показатель;\\nПосмотрев статистики, убеждаемся, что все показатели неотрицательны (то есть уж точно распределены не нормально — но может, и так сработает); при этом почти все элементы нашей матрицы находятся в пределах 1000, но три значения космически огромны, причём в одном столбце «2008 Total oxidised nitrogen» (а строки соответствуют каким-то греческим станциям, с которыми вообще всё странно), и ещё одно тоже очень большое («2005 Total organic carbon (TOC)») — вот они-то и дали нам четыре точки на графике, отличных от начала координат.\\nКстати говоря, если космически большие значения, по-видимому, являются результатам поломки, то по поводу четвёртого, не столь злостного, выброса есть подозрение, что это реальные значения. Посмотрев в данные, мы видим, что показатель был измерен на станции Zidlochovice, на реке Srvatka ниже Брно — а, как говорит нам википедия: As a result of water pollution by communal sewage, the reservoir suffered from an extensive amount of cyanobacteria for a long time. Так или иначе, все четыре станции мы уберём, чтобы они не портили нам SVD.\\nОдин из признаков «2002 Kjeldahl Nitrogen» принимает только нулевые значения. Уберём его, чтобы не мешался.\\n\\nПочистив выбросы в исходных данных, опять центрируем и рисуем:\\n\\nУже лучше. Попробуем понять, что за вещества внесли вклад в первые два латентных признака. Как это сделать? Латентные признаки — это столбцы матрицы UΣ=XVU\\\\Sigma = XVUΣ=XV; линейная алгебра говорит нам, что iii-й столбец произведения XVXVXV — это линейная комбинация столбцов XXX с коэффициентами из iii-го столбца VVV. Находим номера самых больших по модулю координат VVV — и оказывается, что первые два латентных признака складываются почти сплошь из насыщения воды кислородом, только за разные годы (первый за более старые, второй за чуть более свежие):\\nFirst latent feature\\n\\n\\n\\n\\nПризнак\\n\\n\\nэлемент VVV\\n\\n\\n\\n\\n2001 Oxygen saturation\\n\\n\\n0.27\\n\\n\\n\\n\\n2002 Oxygen saturation\\n\\n\\n0.27\\n\\n\\n\\n\\n2000 Oxygen saturation\\n\\n\\n0.26\\n\\n\\n\\n\\n2004 Oxygen saturation\\n\\n\\n0.26\\n\\n\\n\\n\\n⋮\\\\vdots⋮\\n\\n\\n⋮\\\\vdots⋮\\n\\n\\n\\n\\nSecond latent feature\\n\\n\\n\\n\\nПризнак\\n\\n\\nэлемент VVV\\n\\n\\n\\n\\n2001 Oxygen saturation\\n\\n\\n-0.62\\n\\n\\n\\n\\n2002 Oxygen saturation\\n\\n\\n-0.49\\n\\n\\n\\n\\n2000 Oxygen saturation\\n\\n\\n-0.45\\n\\n\\n\\n\\n2004 Oxygen saturation\\n\\n\\n-0.26\\n\\n\\n\\n\\n⋮\\\\vdots⋮\\n\\n\\n⋮\\\\vdots⋮\\n\\n\\n\\n\\nНеужели насыщение кислородом действительно так важно? Нет, просто мы не отмасштабировали признаки. Оказывается, что насыщение кислородом имеет на порядок больший масштаб, чем многое другое, и потому забивает все остальные признаки. Тем не менее, мы можем попробовать сделать вывод и из имеющейся картинки. По оси \"у\" что-то не очень интересное, а по оси \"х\" видим большой кластер (напомним, это меньшие значения насыщения воды кислородом в начале 2000-х), содержащий, если проверить, примерно три четверти всех точек, и ещё некоторой размазанный шлейф.\\nИтак, на многих станциях насыщение воды кислородом в начале 2000-х было примерно в одинаковой степени мало — проверив глазами, обнаруживаем, что там просто нули. Поскольку вряд ли это так на самом деле, видимо, стоит сделать вывод, что в первой половине 2000-х насыщение кислородом измерялось из рук вон плохо.\\nТеперь вдобавок к центрированию поделим каждый признак на его стандартное отклонение и снова нарисуем:\\n\\nОпять видим тесный кластер. При этом первый латентный признак складывается в основном из «Nitrate» , «pH»  и «Dissolved oxygen» за разные годы, все с положительными коэффициентами, а второй — из «Total ammonium», «Total phosphorus» и «Kjeldahl Nitrogen» за разные годы, причём с отрицательными коэффициентами. В частности, справа у нас точки с высоким содержанием нитратов и высокой кислотностью. Среди этих точек:\\n\\nРека Тейм, про которую Википедия пишет: The Tame was once one of Britain\\'s dirtiest rivers.\\nРека Кёрёш, про которую тоже можно найти вот такую информацию: For some time the municipal government of Kanjiža (to which the mouth of the river belongs) protests about the extreme pollution of the Kereš\\'s water, as it represents the single largest polluter of the Tisa river\\nТемза (станция немного выше Лондона).\\n\\nЧто ещё можно было бы сделать? Например, мы можем посмотреть распределения признаков и увидеть, что многие из них далеки от нормальных и в целом выиграли бы от логарифмирования — тогда, возможно, итоговая картинка стала бы красноречивей.\\nИспользование SVD: разделённые представления и рекомендательная система для бедных\\nМы уже обсуждали, что, вообще говоря, любое матричное разложение можно с той или иной степенью успеха использовать для построения рекомендательной системы. Основанные на этом модели называются моделями латентных факторов (Latent factor models). В 2006 году SVD-подобный алгоритм даже помог Саймону Фанку (Simon Funk; под этим псевдонимом скрывался Brandyn Webb) занять высокое место на соревновании Netflix Prize.\\nПодход на чистом SVD\\nВернёмся к примеру из пункта 1.3. Пусть вновь объекты нашего датасета соответствуют пользователям интернет-магазина, а признаки — товарам, причём в клетке с индексом (i,j)(i,j)(i,j) записаны рейтинги ρij\\\\rho_{ij}ρij\\u200b, которые пользователи ставят товарам. На основе этих данных мы хотим порекомендовать некоторому nnn-му пользователю kkk очередных товаров. Если бы нам были известны ρnj\\\\rho_{nj}ρnj\\u200b для всех индексов товаров jjj, задача не стоила бы выеденного яйца: мы бы просто взяли kkk товаров с максимальными значениями рейтингов. Более того, мы могли бы с помощью матричного разложения построить модель и надеяться, что координаты латентных представлений пользователей и товаров окажутся интерпретируемыми (нет).\\n\\nА именно, если бы мы знали все ρnj\\\\rho_{nj}ρnj\\u200b, построить отдельные представления для пользователей и для товаров некоторой (подбираемой; это гиперпараметр модели) длины TTT мы могли бы с помощью SVD и приближения из теоремы Эккарта-Янга:\\nX∼X^U^Σ^V^T=U^Σ^12⋅Σ^12V^TX\\\\sim\\\\widehat{X}\\\\widehat{U}\\\\widehat{\\\\Sigma}\\\\widehat{V}^T = \\\\color{red}{\\\\widehat{U}\\\\widehat{\\\\Sigma}^{\\\\frac12}}\\\\cdot\\n\\\\color{green}{\\\\widehat{\\\\Sigma}^{\\\\frac12}\\\\widehat{V}^T}X∼XUΣVT=UΣ21\\u200b⋅Σ21\\u200bVTНо на деле матрица (ρij)i,j\\\\left(\\\\rho_{ij}\\\\right)_{i,j}(ρij\\u200b)i,j\\u200b обычно разреженная: в ней лишь сравнительно немного известных рейтингов, а в остальных ячейках стоят пропуски. Что же делать?\\nНаивный вариант — заменить все пропуски нулями (то есть положить, что если пользователь не ставил рейтинг товару, то он ему вдребезги не интересен, что не всегда правдоподобно) или средними по строке/столбцу, после чего сделать SVD и радоваться жизни. В этой ситуации наша приближённая модель предсказывает рейтинг, выставленный iii-м пользователем jjj-му товару, как скалярное произведение представлений пользователя и товара — то есть iii-й строки матрицы U^Σ^12\\\\color{red}{\\\\widehat{U}\\\\widehat{\\\\Sigma}^{\\\\frac12}}UΣ21\\u200b и jjj-го столбца матрицы Σ^12V^T\\\\color{green}{\\\\widehat{\\\\Sigma}^{\\\\frac12}\\\\widehat{V}^T}Σ21\\u200bVT\\nТеперь  чтобы порекомендовать n-му пользователю k очередных товаров, мы просто берём n-ю строку матрицы X^\\\\widehat{X}X и находим номера её наибольших элементов.\\nК сожалению, у этого метода есть как минимум две проблемы:\\n\\nПропусков обычно очень много; если их все заменить какими попало значениями, оценка будет очень шумной;\\nПри таком подходе нет простого способа обновить рекомендации при добавлении новых данных — SVD придётся переучивать заново.\\n\\nРазвиваем идею: как побороть разреженность\\nК счастью, есть и другой путь. Давайте подумаем: чего вообще мы требуем от матриц B:=U^Σ^12\\\\color{red}{B} := \\\\color{red}{\\\\widehat{U}\\\\widehat{\\\\Sigma}^{\\\\frac12}}B:=UΣ21\\u200b и CT:=Σ^12V^T\\\\color{green}{C}^T := \\\\color{green}{\\\\widehat{\\\\Sigma}^{\\\\frac12}\\\\widehat{V}^T}CT:=Σ21\\u200bVT? По сути нам нужны две вещи:\\n\\nB⋅CT∼X\\\\color{red}{B}\\\\cdot\\\\color{green}{C}^T \\\\sim XB⋅CT∼X;\\nОбе матрицы ортогональны по столбцам.\\n\\nПоследнее можно опустить. Ясной пользы для рекомендательной системы от этого нет; да, это давало бы нам некоррелированность латентных признаков, но мы уже видели, что интерпретируемости это не влечёт. Первое же условие удобно сформулировать в терминах векторов латентных представлений пользователей (обозначим их bi\\\\color{red}{b_i}bi\\u200b; это строки B\\\\color{red}{B}B) и товаров (обозначим их ci\\\\color{green}{c_i}ci\\u200b — это строки C\\\\color{green}{C}C). А именно, нам нужно, чтобы скалярное произведение (bi,cj)(\\\\color{red}{b_i},\\\\color{green}{c_j})(bi\\u200b,cj\\u200b) было как можно ближе к ρij\\\\rho_{ij}ρij\\u200b для всех пар (i,j)(i, j)(i,j), для которых ρij\\\\rho_{ij}ρij\\u200b нам известно.\\nВот именно! Мы можем просто не обращать внимания на неизвестные значения, оптимизируя только по тем клеткам XXX, для которых нам что-то известно:\\n∑i,j:,ρij≠NA(ρij−(bi,cj))2⟶min\\u2061bi,ci\\\\sum_{i,j:,\\\\rho_{ij}\\\\neq\\\\mathrm{NA}}(\\\\rho_{ij}-(b_i,c_j))^2\\\\longrightarrow\\\\min_{b_i,c_i}\\ni,j:,ρij\\u200b\\ue020=NA∑\\u200b(ρij\\u200b−(bi\\u200b,cj\\u200b))2⟶bi\\u200b,ci\\u200bmin\\u200bНо как решить эту оптимизационную задачу? Разумеется, с помощью стохастического градиентного спуска. В базовом варианте мы случайным образом перебираем пары (i,j)(i, j)(i,j), для которых ρij\\\\rho_{ij}ρij\\u200b нам известно, и обновляем координаты векторов bi\\\\color{red}{b_i}bi\\u200b и cj\\\\color{green}{c_j}cj\\u200b следующим образом:\\nbit:=bit+ηεijcjtcjt:=cjt+ηεijbit,t=1,…,T,где\\xa0εij=ρij−(bi,cj)b_{it}:=b_{it}+\\\\eta\\\\varepsilon_{ij}c_{jt}\\\\\\\\c_{jt}:=c_{jt}+\\\\eta\\\\varepsilon_{ij}b_{it}, t=1,\\\\ldots,T,\\\\quad\\\\text{где }\\\\varepsilon_{ij}=\\\\rho_{ij}-(b_i,c_j)\\nbit\\u200b:=bit\\u200b+ηεij\\u200bcjt\\u200bcjt\\u200b:=cjt\\u200b+ηεij\\u200bbit\\u200b,t=1,…,T,где\\xa0εij\\u200b=ρij\\u200b−(bi\\u200b,cj\\u200b)где η\\\\etaη — гиперпараметр, отвечающий за темп обучения.\\nПриятное свойство такого подхода: в нём легко добавлять новые товары/пользователей (дообучаем их векторы, заморозив остальные), а также новые оценки ρij\\\\rho_{ij}ρij\\u200b (добавляем в оптимизируемый функционал и проводим дооптимизацию).\\nОтметим, что в ходе оптимизации мы попеременно осуществляем градиентный спуск, обновляя то B\\\\color{red}{B}B, то C\\\\color{green}{C}C. Эту идею можно развить следующим образом. Заметим, что при фиксированной матрице CCC задача минимизации по BBB выражения\\n∑i,j:,ρij≠NA(ρij−(bi,cj))2⟶min\\u2061bi,ci\\\\sum_{i,j:,\\\\rho_{ij}\\\\neq\\\\mathrm{NA}}(\\\\rho_{ij}-(b_{i},c_{j}))^{2}\\\\longrightarrow\\\\min_{b_{i},c_{i}}\\ni,j:,ρij\\u200b\\ue020=NA∑\\u200b(ρij\\u200b−(bi\\u200b,cj\\u200b))2⟶bi\\u200b,ci\\u200bmin\\u200bпревращается по сути в обычный метод наименьших квадратов, для которого можно даже выписать «точное» решение (а вы можете это сделать?). Точно так же и при фиксированном BBB легко находится минимум по CCC. Чередуя эти два шага, мы будем сходиться к решению быстрее и надёжнее, чем с помощью SGD. Данный алгоритм носит название Alternating Least Squares (ALS).\\nРазвиваем идею: как ещё усовершенствовать модель\\nМожно ввести много дополнительных эвристик и предположений, которые уведут нас совсем далеко от старого доброго SVD. Например:\\n\\nРейтинг не всегда является продуктом чистого взаимодействия пользователя с товаром. Бывают товары, которые сами по себе ужасно популярны (скажем, человек купит туалетную бумагу даже если не очень интересуется товарами для дома) или так ужасны, что даже интересующийся данной «латентной категорией» покупатель не станет их высоко оценивать. Это можно промоделировать, добавив к скалярному произведению члены, зависящие только от пользователя и только от товара соответственно:\\n\\nρij∼b‾i+c‾j+(bi,cj)\\\\rho_{ij}\\\\sim \\\\color{red}{\\\\overline{b}_i} +\\\\color{green}{\\\\overline{c}_j} + (\\\\color{red}{b_i},\\\\color{green}{c_j})\\nρij\\u200b∼bi\\u200b+cj\\u200b+(bi\\u200b,cj\\u200b)Тогда наша задача оптимизации примет вид:\\n∑i,j:ρij≠NA(ρij−b‾i−c‾j−(bi,cj))2⟶min\\u2061bi,ci,b‾i,c‾j\\\\sum_{i,j:\\\\rho_{ij}\\\\neq\\\\mathrm{NA}}(\\\\rho_{ij}-\\\\overline{b}_i-\\\\overline{c}_j-(b_i,c_j))^2\\\\longrightarrow\\\\min_{b_i,c_i,\\\\overline{b}_i,\\\\overline{c}_j}\\ni,j:ρij\\u200b\\ue020=NA∑\\u200b(ρij\\u200b−bi\\u200b−cj\\u200b−(bi\\u200b,cj\\u200b))2⟶bi\\u200b,ci\\u200b,bi\\u200b,cj\\u200bmin\\u200b\\nМожно добавлять регуляризационные члены. Например:\\n\\n∑i,j:,ρij≠NA(ρij−(bi,cj))2+λ∑i∣∣bi∣∣2+μ∑j∣∣cj∣∣2⟶min\\u2061bi,ci\\\\sum_{i,j:,\\\\rho_{ij}\\\\neq\\\\mathrm{NA}}(\\\\rho_{ij}-(b_i,c_j))^2+\\\\lambda\\\\sum_i||b_i||^2+\\\\mu\\\\sum_j||c_j||^2\\\\longrightarrow\\\\min_{b_i,c_i}\\ni,j:,ρij\\u200b\\ue020=NA∑\\u200b(ρij\\u200b−(bi\\u200b,cj\\u200b))2+λi∑\\u200b∣∣bi\\u200b∣∣2+μj∑\\u200b∣∣cj\\u200b∣∣2⟶bi\\u200b,ci\\u200bmin\\u200b\\nМы можем не игнорировать неизвестные нам элементы матрицы XXX, а присвоить им нулевые значения и ставить более низкие веса соответствующим слагаемым функции потерь:\\n\\n∑i,jw(ρij)(ρij−(bi,cj))2⟶min\\u2061bi,ci\\\\color{blue}{\\\\sum_{i,j}w(\\\\rho_{ij})(\\\\rho_{ij} - (\\\\color{red}{b_i},\\\\color{green}{c_j}))^2\\\\longrightarrow \\\\min\\\\limits_{\\\\color{red}{b_i},\\\\color{green}{c_i}}}\\ni,j∑\\u200bw(ρij\\u200b)(ρij\\u200b−(bi\\u200b,cj\\u200b))2⟶bi\\u200b,ci\\u200bmin\\u200bгде w(ρij)w(\\\\rho_{ij})w(ρij\\u200b) маленькое, если ρij=NA\\\\rho_{ij}=\\\\mathrm{NA}ρij\\u200b=NA, и большое в противном случае. Это имеет смысл, например, если отсутствие данных в самом деле может быть логично интерпретировать, как отсутствие интереса.\\n\\nМожно ввести требования неотрицательности: bit⩾0\\\\color{red}{b_{it}}\\\\geqslant 0bit\\u200b⩾0, cjs⩾0\\\\color{green}{c_{js}}\\\\geqslant0cjs\\u200b⩾0. Подробнее об этом в параграфе про неотрицательное матричное разложение.\\nИли даже всё это вместе 😄\\n\\nКонтрольный вопросПредположим, что каждый рейтинг ρij\\\\rho_{ij}ρij\\u200b имеет также временную метку tijt_{ij}tij\\u200b. Можете ли вы придумать, как их использовать?\\nВероятностное обличье модели латентных факторов\\nВы могли заметить, что задача\\n∑i,j:,ρij≠NA(ρij−(bi,cj))2⟶min\\u2061bi,ci\\\\sum_{i,j:,\\\\rho_{ij}\\\\neq\\\\mathrm{NA}}(\\\\rho_{ij}-(b_i,c_j))^2\\\\longrightarrow\\\\min_{b_i,c_i}\\ni,j:,ρij\\u200b\\ue020=NA∑\\u200b(ρij\\u200b−(bi\\u200b,cj\\u200b))2⟶bi\\u200b,ci\\u200bmin\\u200bподозрительно напоминает задачу наименьших квадратов, и неспроста. В базовой формулировке мы предполагаем, что\\nρij=(bi,cj)+ξij,где\\xa0ξij∼N(0,σ2)\\xa0-\\xa0нормальный\\xa0шум\\\\rho_{ij}=(b_i,c_j)+\\\\xi_{ij},\\\\quad\\\\text{где }\\\\xi_{ij}\\\\sim\\\\mathcal{N}(0,\\\\sigma^2)\\\\text{ - нормальный шум}\\nρij\\u200b=(bi\\u200b,cj\\u200b)+ξij\\u200b,где\\xa0ξij\\u200b∼N(0,σ2)\\xa0-\\xa0нормальный\\xa0шумИными словами,\\nρij∼N((bi,cj),σ2)\\\\color{blue}{\\\\rho_{ij} \\\\sim\\\\mathcal{N}\\\\left((\\\\color{red}{b_i},\\\\color{green}{c_j}), \\\\sigma^2\\\\right)}\\nρij\\u200b∼N((bi\\u200b,cj\\u200b),σ2)По крайней мере, те из них, которые нам известны. Нахождение bi\\\\color{red}{b_i}bi\\u200b и cj\\\\color{green}{c_j}cj\\u200b методом максимального правдоподобия как раз и приводит к описанной выше оптимизационной задаче.\\nКак обычно, мы можем добавить априорную информацию о распределении латентных векторов bi\\\\color{red}{b_i}bi\\u200b и cj\\\\color{green}{c_j}cj\\u200b. Например, такую:\\nbi∼N(0,σbE),cj∼N(0,σcE)\\\\color{red}{b_i}\\\\sim\\\\mathcal{N}(0, \\\\sigma_bE),\\\\qquad\\\\color{green}{c_j}\\\\sim\\\\mathcal{N}(0, \\\\sigma_cE)\\nbi\\u200b∼N(0,σb\\u200bE),cj\\u200b∼N(0,σc\\u200bE)Расписывая логарифм правдоподобия\\np(ρ;b,c)=p(ρ∣b,c)p(b)p(c)p(\\\\rho;\\\\color{red}{b},\\\\color{green}{c}) = p(\\\\rho|\\\\color{red}{b},\\\\color{green}{c})p(\\\\color{red}{b})p(\\\\color{green}{c})\\np(ρ;b,c)=p(ρ∣b,c)p(b)p(c)и убирая константные члены, которые содержат только сигмы, приводим задачу максимизации логарифма правдоподобия к виду\\n−12σ2∑i,j:,ρij≠NA(ρij−(bi,cj))2−12σ12∑i∣∣bi∣∣2−12σ22∑j∣∣cj∣∣2⟶max\\u2061bi,ci-\\\\frac{1}{2\\\\sigma^{2}}\\\\sum_{i,j:,\\\\rho_{ij}\\\\neq\\\\mathrm{NA}}(\\\\rho_{ij}-(b_{i},c_{j}))^{2}-\\\\frac{1}{2\\\\sigma_{1}^{2}}\\\\sum_{i}||b_{i}||^{2}-\\\\frac{1}{2\\\\sigma_{2}^{2}}\\\\sum_{j}||c_{j}||^{2}\\\\longrightarrow\\\\max_{b_{i},c_{i}}\\n−2σ21\\u200bi,j:,ρij\\u200b\\ue020=NA∑\\u200b(ρij\\u200b−(bi\\u200b,cj\\u200b))2−2σ12\\u200b1\\u200bi∑\\u200b∣∣bi\\u200b∣∣2−2σ22\\u200b1\\u200bj∑\\u200b∣∣cj\\u200b∣∣2⟶bi\\u200b,ci\\u200bmax\\u200bвполне объясняющему, почему в предыдущем пункте у нас могла появляться L2-регуляризация.\\nАнализ независимых компонент (ICA)\\nICA изначально был придуман для задачи разделения сигналов («blind source separation»). Рассмотрим пример из sklearn\\n\\nИзначально были три сигнала (красный, рыжий и синий на второй сверху картинке), их смешали, получив три линейных комбинации (на верхней картинке). Теперь попробуем их разделить. Первая мысль, которая нам приходит в голову: воспользуемся SVD (проинтерпретировав моменты времени как объекты, а сигналы из смеси как признаки — то есть взяв матрицу 2000×32000\\\\times32000×3)! Но на нижней картинке мы видим результат, который не радует, но не радует ожидаемо, и вот почему:\\n\\nВ первый латентный признак SVD старается собрать максимально возможную дисперсию — мы видим, что красный график на нижней картинке действительно ловит самые значительные колебания сигналов из смеси; при этом в третий (рыжий) сигнал уже попадает более или менее случайный шум.\\nЕсли посмотреть на значения исходных сигналов, то они распределены не нормально (распределения значений синего и красного имеют две моды, а у рыжего близко к равномерному), а мы помним, что SVD плохо приспособлено к работе с не гауссовскими данными.\\n\\nАнализ независимых компонент (ICA) состоит в аппроксимации xi∼ziVTx_i\\\\sim z_iV^Txi\\u200b∼zi\\u200bVT наблюдаемых признаков линейной смесью латентных, которые являются независимыми как случайные величины.\\nЗамечание. Оригинальная формулировка несколько другая: изначально ICA — это аппроксимация наблюдаемых сигналов линейной смесью некоторого числа независимых сигналов, то есть речь шла о смеси объектов. Описываемые далее методы можно точно также использовать и для разделения смеси объектов, конечно.\\nВажно, что в данном случае предъявляется требование независимости, а не просто некоррелированности — более сильное, впрочем, труднодостижимое и столь же трудно проверяемое.\\nКак построить ICA? Путешествие в мир удивительных эвристик\\nМы будем излагать алгоритм FastICA по статье его создателей, она же реализована в библиотеке sklearn; в статье вас ждёт гораздо больше подробностей и тонкостей реализации.\\nАлгоритм базируется на следующем эвристическом соображении: линейная комбинация нескольких независимых негауссовских величин в большей степени гауссовская, чем сами эти величины  — довольно смелый вывод из   Центральной предельной теоремы. Таким образом, мы будем искать линейную комбинацию исходных признаков, которая была бы в наименьшей степени гауссовской — это и будет первая из независимых компонент. Но как померить близость к нормальности?\\nПусть zzz — некоторая (одномерная) случайная величина с плотностью p(z)p(z)p(z). Рассмотрим её энтропию\\nH(z)=−∫p(t)log\\u2061p(t)dtH(z) = -\\\\int p(t)\\\\log{p(t)}dt\\nH(z)=−∫p(t)logp(t)dtИмеет место теорема: гауссовская случайная величина имеет максимальную энтропию среди всех случайных величин с заданной дисперсией. Рассмотрим теперь\\nJ(z)=H(zgauss)−H(z)J(z) = H(z_{gauss}) - H(z)\\nJ(z)=H(zgauss\\u200b)−H(z)где zgaussz_{gauss}zgauss\\u200b — гауссовская случайная величина с той же дисперсией, что и у zzz. Величина J(z)J(z)J(z) всегда неотрицательна и равна нулю в том случае, если zzz гауссовская. Решая задачу\\nJ(Xw)⟶max\\u2061wJ(Xw)\\\\longrightarrow\\\\max\\\\limits_{w}\\nJ(Xw)⟶wmax\\u200bмы могли бы найти самую негауссовскую линейную комбинацию наших признаков. Проблема в том, что J(z)J(z)J(z) трудно посчитать. Авторы статьи предлагают использовать приближение\\nJ(z)∼(EG(z)−EG(w))2,J(z)\\\\sim\\\\left(\\\\mathbb{E}G(z) - \\\\mathbb{E}G(w)\\\\right)^2,\\nJ(z)∼(EG(z)−EG(w))2,где w∼N(0,1)w\\\\sim\\\\mathcal{N}(0,1)w∼N(0,1), а GGG неквадратичная функция (в статье предлагаются конкретные варианты). Последующие независимые компоненты можно искать в ортогональном подпространстве (всё-таки они должны быть и некоррелированными).\\nПодготовка данных для ICA\\nПеред тем, как строить разложение нужно центрировать данные (вычесть из признаков их средние) и убедиться, что ковариационная матрица признаков является единичной.\\nКонтрольный вопрос: как добиться последнего с помощью линейной замены?При линейной замене x=Cx′x = Cx\\'x=Cx′ матрица ковариации меняется, как Σ′=CTΣC\\\\Sigma\\' = C^T\\\\Sigma CΣ′=CTΣC. Осталось вспомнить, что, поскольку матрица ковариации симметричная и положительно определённая, существует линейная замена, для которой CTΣC=EC^T\\\\Sigma C = ECTΣC=E. Например, в качестве CCC можно взять (симметричный положительно определённый) квадратный корень из Σ−1\\\\Sigma^{-1}Σ−1.\\nНеотрицательное матричное разложение (NMF)\\nМотивация: тематическое моделирование\\nДопустим, что у нас есть датасет, в котором объекты — тексты, признаки — токены (например, слова), а на (i,j)(i,j)(i,j)-м месте написана частота встречаемости jjj-го токена в iii-м тексте (то есть nijnj\\\\frac{n_{ij}}{n_j}nj\\u200bnij\\u200b\\u200b, где nijn_{ij}nij\\u200b — сколько раз iii-й токен встретился в jjj-м документе, а njn_jnj\\u200b — общее число токенов в этом документе).\\n\\nПриблизим нашу матрицу произведением\\nX\\u2061N×D∼B\\u2061N×R⋅C\\u2061R×D\\\\underset{N\\\\times D}{\\\\operatorname{X}} \\\\sim \\\\underset{N\\\\times R}{\\\\operatorname{B}} \\\\cdot \\\\underset{R\\\\times D}{\\\\operatorname{C}}\\nN×DX\\u200b∼N×RB\\u200b⋅R×DC\\u200bОдна из возможных интерпретаций такова. Есть DDD тем:\\n\\nЗа этим стоит вполне ясная вероятностная модель:\\np(word∣document)∼∑themep(word∣theme)⋅p(theme∣document)p(word\\\\mid document)\\\\sim\\\\sum_{theme}p(word\\\\mid theme)\\\\cdot p(theme\\\\mid document)\\np(word∣document)∼theme∑\\u200bp(word∣theme)⋅p(theme∣document)Вопрос в том, как получить такое разложение. Конечно, чисто технически можно использовать SVD. Но тогда элементы матриц разложения вряд ли будут иметь вероятностный смысл: они же даже не обязаны быть неотрицательными. С другой стороны, если потребовать, чтобы все элементы B\\\\color{red}{B}B и C\\\\color{green}{C}C были неотрицательными, ситуация исправится.\\nОпределение NMF\\nНеотрицательное матричное разложение неотрицательной матрицы XXX — это произведение BCBCBC матриц с неотрицательным элементами, наилучшим образом приближающее XXX по норме Фробениуса\\n∣∣X−BC∣∣fro2⟶min\\u2061B,Cbij,ckl⩾0∀i,j,k,l||X - BC||^2_{fro}\\\\longrightarrow\\\\min\\\\limits_{\\\\begin{smallmatrix}B, C\\\\\\\\b_{ij}, c_{kl}\\\\geqslant0\\\\,\\\\forall i,j,k,l\\\\end{smallmatrix}}\\n∣∣X−BC∣∣fro2\\u200b⟶B,Cbij\\u200b,ckl\\u200b⩾0∀i,j,k,l\\u200bmin\\u200bAlternating Least Squares (ALS)\\nALS — один из популярных методов для решения факторизационных задач. Несмотря на то, что оптимизационная задача в целом не является выпуклой, по отдельности задача поиска каждого из сомножителей является выпуклой и может решаться с помощью привычных нам методов. Таким образом, мы можем чередовать поиск BBB при фиксированном CCC и поиск CCC при фиксированном BBB, итеративно сходясь к итоговому решению:\\n\\nЗаметим, что из-за насильного обнуления элементов будут получаться разреженные матрицы.\\nРазумеется, можно рассматривать и более сложные функционалы, прибавляя к ∥X−BC∥2\\\\Vert X - BC\\\\Vert^2∥X−BC∥2 различные регуляризационные члены, скажем, поощряющие большую разреженность матриц BBB и CCC.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф16.1. Матричное дифференцированиеКак дифференцировать матрицы и\\xa0дифференцировать по\\xa0матрицам: всё, что вам не\\xa0рассказали про дифференцирование на\\xa0матанализеСледующий параграф16.3. Вероятностные распределенияЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_33.html', 'title': 'Генеративно-состязательные сети (GAN)'}, page_content=\"Генеративно-состязательные сети (GAN)Яндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/68.1.Введение в генеративное моделирование8.2.Variational Autoencoder (VAE)8.3.Генеративно-состязательные сети (GAN)ВведениеОсновы обучения GAN-овМетрики качестваБазовые моделиСовременные моделиПрименения генеративных состязательных нейросетей8.4.Нормализующие потоки8.5.Диффузионные модели8.6.Языковые модели9.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Генеративно-состязательные сети (GAN)8.3. Генеративно-состязательные сети (GAN)АвторыЕгор ЗахаровДенис ВолхонскийВведение\\nГенеративно-состязательные сети (Generative Adversarial Networks, GAN) – это большой класс генеративных моделей, общая черта которых заключается в том, что они обучаются одновременно с другой сетью, которая старается отличить сгенерированные объекты от настоящих. В этом параграфе мы рассмотрим основы основ GAN-ов, интуитивное объясним принципы их работы, а также детально погрузимся в многочисленные приёмы и модификации оригинального подхода, которые применяются в наиболее успешных моделях. Мы также приведём примеры нескольких типов практических задач, в которых применяются генеративно-состязательные сети.\\n\\nГенеративно-состязательные сети — это неявная генеративная модель. То есть она не восстанавливает плотность данных в явном виде, но умеет сэмплировать из распределения данных. Самый простой и эффективный дизайн генеративных моделей, которые умеют только сэмплировать, но не умеют оценивать плотность, – это отображение одних случайных величин в другие.\\nПодобного вида модель после обучения работает следующим образом: пусть xxx – случайная величина, обозначающая сэмпл из распределения нужных нам данных (например, картинок с нарисованными цифрами), а zzz – сэмпл из какого-то распределения, который нам легко получить (например, каждая его компонента берётся из стандартного нормального). Тогда, если у нас есть обученная функция GGG, которая переводит сэмплы из p(z)p(z)p(z) в сэмплы из p(x)p(x)p(x), то процесс генерации происходит в два этапа: сначала мы случайным образом получаем вектор z∼p(z)z \\\\sim p(z)z∼p(z), а затем отображаем его в x^=G(z)\\\\hat{x} = G(z)x^=G(z):Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nz∼p(z),x^=G(z):x^∼p(x)z \\\\sim p(z),\\\\quad \\\\hat{x} = G(z):\\\\quad \\\\hat{x} \\\\sim p(x)\\nz∼p(z),x^=G(z):x^∼p(x)Ключевым вопросом в таких моделях является соотношение размерностей zzz и xxx. Есть генеративные модели, где dim(z)≈dim(x)\\\\text{dim}(z) \\\\approx \\\\text{dim}(x)dim(z)≈dim(x). Примером таких подходов являются, например, нормализующие потоки. В случае генеративных состязательных сетей (как и другого класса популярных генеративных моделей, вариационных автоэнкодеров), dim(z)≪dim(x)\\\\text{dim}(z) \\\\ll \\\\text{dim}(x)dim(z)≪dim(x). Поэтому работу этих моделей можно рассматривать как поиск многообразия размерности dim(z)\\\\text{dim}(z)dim(z) среди всех случайных примеров из домена, на котором определяется p(x)p(x)p(x). Например, в случае генерации цифр это соответствует поиску в домене [0,1]H×W[0, 1]^{H \\\\times W}[0,1]H×W, где HHH – это ширина картинки, а WWW – её высота, подмножества, в котором каждый элемент изображает какую-либо цифру. Таким образом, задача обучения генеративных состязательных сетей может рассматриваться как задача компрессии данных в низкоразмерное представление.\\nОсновы обучения GAN-ов\\n\\nКлассическая аналогия того, как учатся GANы — это фальшивомонетчик и полицейский. Задача фальшивомонетчика — научиться создавать купюры, которые полицейский не сможет отличить от реальных. Задача полицейского — научиться отличать купюры фальшивомонетчика от настоящих.\\nЧтобы понять, как обучаются GANы, надо представить себе следующий мысленный эксперимент. Допустим, фальшивомонетчик и полицейский — друзья, которые решили поучиться друг у друга. Фальшивомонетчик создаёт несколько фальшивых купюр и показывает полицейскому. Полицейский говорит фальшивомонетчику, какие из его купюр, по его мнению, поддельные, а какие — настоящие. Фальшивомонетчик запоминает отзыв полицейского и в следующий раз улучшит свои купюры на основе отзыва от полицейского. Сам полицейский при этом тоже учится: он запоминает, что купюры, которые он видел — поддельные.\\nВ нашем мысленном эксперименте представим, что фальшивомонетчик взаимодействует с полицейским много раз. Что получается в результате? С каждым разом купюры фальшивомонетчика всё труднее отличить от настоящих. И с каждым разом умение выявлять поддельные купюры у полицейского выше.\\nВажный вопрос для понимания работы GANов: в какой момент мы можем утверждать, что фальшивомонетчик хорошо подделывает купюры?\\nОтвет:\\nКогда фальшивомонетчик сможет обманывать сильного полицейского. В начале нашего эксперимента полицейский плохо отличает подделку от оригинала. Поэтому обмануть его можно купюрами плохого качества. Нам же интересно получить фальшивомонетчика, который будет выдавать купюры, неотличимые от оригинала даже профессионалом.\\nРассмотрим задачу обучения более формально. Пусть у нас есть генератор GθG_\\\\thetaGθ\\u200b (фальшивомонетчик) с параметрами θ\\\\thetaθ, и дискриминатор DϕD_\\\\phiDϕ\\u200b (полицейский) с параметрами ϕ\\\\phiϕ. Генератор отображает векторы z∼N(0,I)z \\\\sim \\\\mathcal{N}(0, I)z∼N(0,I) в x^∼q(x)\\\\hat{x} \\\\sim q(x)x^∼q(x), распределение которых приближает реальное распределение данных p(x)p(x)p(x). Дискриминатор каждому реальному сэмплу xxx и фейковому x^\\\\hat{x}x^ ставит в соответствие вероятность D(x)D(x)D(x), которая оценивает степень принадлежности xxx к реальным данным, т.е. он решает задачу бинарной классификации. Самый простой способ это сделать – при помощи минимизации бинарной кросс-энтропии:\\nmin\\u2061ϕ\\xa0Ex∼p(x)−log\\u2061Dϕ(x)+Ex^∼q(x)−log\\u2061[1−Dϕ(x^)].    \\\\min_\\\\phi\\\\ \\\\mathbb{E}_{x \\\\sim p(x)} - \\\\log D_\\\\phi(x) + \\\\mathbb{E}_{\\\\hat{x} \\\\sim q(x)} - \\\\log \\\\big[ 1 - D_\\\\phi(\\\\hat{x}) \\\\big].\\nϕmin\\u200b\\xa0Ex∼p(x)\\u200b−logDϕ\\u200b(x)+Ex^∼q(x)\\u200b−log[1−Dϕ\\u200b(x^)].Учитывая обозначение x^=Gθ(z)\\\\hat{x} = G_\\\\theta(z)x^=Gθ\\u200b(z), и то, что мы пытаемся максимизировать вероятность принадлежности к реальным данным, как её оценивает дискриминатор, задачу, которую решает генератор, можно расписать следующим образом (используя свойство выпуклости логарифма):\\nθ∗=\\xa0arg\\u2061max\\u2061θ\\xa0Ez∼p(z)Dθ(Gθ(z))=\\xa0arg\\u2061min\\u2061θ\\xa0Ez∼p(z)−Dθ(Gθ(z))=\\xa0arg\\u2061min\\u2061θ\\xa0Ez∼p(z)[1−Dθ(Gθ(z))]=\\xa0arg\\u2061min\\u2061θ\\xa0Ez∼p(z)log\\u2061[1−Dθ(Gθ(z))]=\\xa0arg\\u2061max\\u2061θ\\xa0Ez∼p(z)−log\\u2061[1−Dθ(Gθ(z))].\\\\begin{aligned}\\n    \\\\theta^* =\\\\ & \\\\arg \\\\max_\\\\theta\\\\ \\\\mathbb{E}_{z\\\\sim p(z)} D_\\\\theta \\\\big( G_\\\\theta(z) \\\\big) \\\\\\\\\\n             =\\\\ & \\\\arg \\\\min_\\\\theta\\\\ \\\\mathbb{E}_{z\\\\sim p(z)} - D_\\\\theta \\\\big( G_\\\\theta(z) \\\\big) \\\\\\\\\\n             =\\\\ & \\\\arg \\\\min_\\\\theta\\\\ \\\\mathbb{E}_{z\\\\sim p(z)} \\\\big[ 1 - D_\\\\theta \\\\big( G_\\\\theta(z) \\\\big) \\\\big] \\\\\\\\\\n             =\\\\ & \\\\arg \\\\min_\\\\theta\\\\ \\\\mathbb{E}_{z\\\\sim p(z)} \\\\log \\\\big[ 1 - D_\\\\theta \\\\big( G_\\\\theta(z) \\\\big) \\\\big] \\\\\\\\\\n             =\\\\ & \\\\arg \\\\max_\\\\theta\\\\ \\\\mathbb{E}_{z\\\\sim p(z)} -\\\\log \\\\big[ 1 - D_\\\\theta \\\\big( G_\\\\theta(z) \\\\big) \\\\big].\\n\\\\end{aligned}\\nθ∗=\\xa0=\\xa0=\\xa0=\\xa0=\\xa0\\u200bargθmax\\u200b\\xa0Ez∼p(z)\\u200bDθ\\u200b(Gθ\\u200b(z))argθmin\\u200b\\xa0Ez∼p(z)\\u200b−Dθ\\u200b(Gθ\\u200b(z))argθmin\\u200b\\xa0Ez∼p(z)\\u200b[1−Dθ\\u200b(Gθ\\u200b(z))]argθmin\\u200b\\xa0Ez∼p(z)\\u200blog[1−Dθ\\u200b(Gθ\\u200b(z))]argθmax\\u200b\\xa0Ez∼p(z)\\u200b−log[1−Dθ\\u200b(Gθ\\u200b(z))].\\u200bЭто равенство позволяет записать задачи, которые решают генератор и дискриминатор, вместе. (Мы также избавимся от лишних минусов, сделав так, чтобы дискриминатор решал задачу максимизации.)\\nmin\\u2061θmax\\u2061ϕ\\xa0Ex∼p(x)log\\u2061Dϕ(x)+Ez∼p(z)log\\u2061[1−Dϕ(Gθ(z))].   \\\\min_\\\\theta \\\\max_\\\\phi\\\\ \\\\mathbb{E}_{x \\\\sim p(x)} \\\\log D_\\\\phi(x) + \\\\mathbb{E}_{z \\\\sim p(z)} \\\\log \\\\big[ 1 - D_\\\\phi\\\\big(G_\\\\theta(z)\\\\big) \\\\big].\\nθmin\\u200bϕmax\\u200b\\xa0Ex∼p(x)\\u200blogDϕ\\u200b(x)+Ez∼p(z)\\u200blog[1−Dϕ\\u200b(Gθ\\u200b(z))].Получается, что на самом деле генератор и дискриминатор пытаются оптимизировать одну функцию: генератор её минимизирует, а дискриминатор максимизирует. Обозначим эту функцию (минус бинарную кросс-энтропию) как L\\xa0θ,ϕ\\\\mathcal{L}_{\\\\ \\\\theta, \\\\phi}L\\xa0θ,ϕ\\u200b. Тогда эту задачу оптимизации можно записать в сокращённом виде:\\nmin\\u2061θmax\\u2061ϕ\\xa0L\\xa0θ,ϕ.    \\\\min_\\\\theta \\\\max_\\\\phi\\\\ \\\\mathcal{L}_{\\\\ \\\\theta, \\\\phi}.\\nθmin\\u200bϕmax\\u200b\\xa0L\\xa0θ,ϕ\\u200b.По параметрам дискриминатора минимум бинарной кросс-энтропии (или минимум L\\xa0θ,ϕ\\\\mathcal{L}_{\\\\ \\\\theta, \\\\phi}L\\xa0θ,ϕ\\u200b по ϕ\\\\phiϕ) достигается на следующей функции – оптимальном дискриминаторе для фиксированного генератора:\\nDϕ∗(x)=p(x)p(x)+q(x).    D_{\\\\phi^*}(x) = \\\\frac{p(x)}{p(x) + q(x)}.\\nDϕ∗\\u200b(x)=p(x)+q(x)p(x)\\u200b.Её оптимальность нетрудно проверить, используя выпуклость логарифма. Учитывая это, и формулу для L\\\\mathcal{L}L, интуицию работы метода обучения GANов со стороны генератора можно сформулировать следующим образом:\\n\\nМы замеряем, насколько реалистичными являются сгенерированные сэмплы x^1,…,x^2\\\\\\\\{\\\\hat{x}_1, \\\\dots, \\\\hat{x}_2\\\\\\\\}x^1\\u200b,…,x^2\\u200b, используя для этого оптимальный дискриминатор D_ϕ∗(x)D\\\\_{\\\\phi^*}(x)D_ϕ∗(x).\\nМы хотим увеличить отклик дискриминатора на каждом сэмпле, т.е. пытаемся модифицировать каждый предсказанный элемент x^i\\\\hat{x}_ix^i\\u200b так, чтобы на нём стало выше значение D_ϕ∗(x^i)D\\\\_{\\\\phi^*}(\\\\hat{x}_i)D_ϕ∗(x^i\\u200b).\\n\\nЕщё более простую интуицию для этой задачу можно сформулировать следующим образом. Как нужно модифицировать плотность q(x)q(x)q(x), чтобы она стала ближе к p(x)p(x)p(x), если к плотности распределения мы имеем доступ только через сэмплы из него? Визуализацию желаемых градиентов по случайным сэмплам для задачи сопоставления двух гауссиан можно видеть на графике ниже, где f(x)=D_ϕ∗(x)f(x) = D\\\\_{\\\\phi^*}(x)f(x)=D_ϕ∗(x).\\n\\nНаправленные вниз стрелки показывают, насколько нужно уменьшить координаты точек из распределения q(x)q(x)q(x), чтобы получилось нечто максимально похожее на p(x)p(x)p(x). То есть на самом деле точки будут сдвигаться на то же самое расстояние влево.\\nФормализуем эту интуицию, и заодно поймём, почему вообще такой метод должен работать. Подставив выражение для оптимального дискриминатора в L\\\\mathcal{L}L, мы можем избавиться от внутренней максимизации в исходной задаче и оставить только внешнюю минимизацию по параметрам генератора. Тем самым, мы получим в явном виде функцию потерь, которую минимизирует генератор (обозначим её за Dθ\\\\mathcal{D}_\\\\thetaDθ\\u200b). Для неё мы распишем математическое ожидание через интеграл и упростим дроби:\\nDθ=\\xa0\\u2009\\u2063max\\u2061ϕLθ,ϕ=\\xa0Ex∼p(x)log\\u2061Dϕ∗(x)+Ex∼q(x)log\\u2061[1−Dϕ∗(x)]=\\xa0Ex∼p(x)log\\u2061p(x)p(x)+q(x)+Ex∼q(x)log\\u2061[1−p(x)p(x)+q(x)]=\\xa0∫p(x)log\\u2061p(x)p(x)+q(x)dx+∫q(x)log\\u2061q(x)p(x)+q(x)dx\\\\begin{aligned}\\n    \\\\mathcal{D}_\\\\theta \\n    =\\\\ & \\\\! \\\\max_\\\\phi \\\\mathcal{L}_{\\\\theta, \\\\phi} \\\\\\\\\\n    =\\\\ & \\\\mathbb{E}_{x \\\\sim p(x)} \\\\log D_{\\\\phi^*}(x) + \\\\mathbb{E}_{x \\\\sim q(x)} \\\\log \\\\big[ 1 - D_{\\\\phi^*}(x) \\\\big] \\\\\\\\\\n    =\\\\ & \\\\mathbb{E}_{x \\\\sim p(x)} \\\\log \\\\frac{p(x)}{p(x) + q(x)} + \\\\mathbb{E}_{x \\\\sim q(x)} \\\\log \\\\bigg[ 1 - \\\\frac{p(x)}{p(x) + q(x)} \\\\bigg] \\\\\\\\\\n    =\\\\ & \\\\int p(x) \\\\log \\\\frac{p(x)}{p(x) + q(x)} dx + \\\\int q(x) \\\\log \\\\frac{q(x)}{p(x) + q(x)} dx \\\\\\\\\\n\\\\end{aligned}\\nDθ\\u200b=\\xa0=\\xa0=\\xa0=\\xa0\\u200bϕmax\\u200bLθ,ϕ\\u200bEx∼p(x)\\u200blogDϕ∗\\u200b(x)+Ex∼q(x)\\u200blog[1−Dϕ∗\\u200b(x)]Ex∼p(x)\\u200blogp(x)+q(x)p(x)\\u200b+Ex∼q(x)\\u200blog[1−p(x)+q(x)p(x)\\u200b]∫p(x)logp(x)+q(x)p(x)\\u200bdx+∫q(x)logp(x)+q(x)q(x)\\u200bdx\\u200bУпростим выражение для D(x)\\\\mathcal{D}(x)D(x) ещё раз, прибавив и отняв константу log\\u20614\\\\log4log4, а также учитывая, что ∫p(x)\\xa0dx=1\\\\int p(x)\\\\ dx = 1∫p(x)\\xa0dx=1 и ∫q(x)\\xa0dx=1\\\\int q(x)\\\\ dx = 1∫q(x)\\xa0dx=1:\\nDθ=\\xa0−log\\u20614+∫p(x)log\\u20612p(x)p(x)+q(x)dx+∫q(x)log\\u20612q(x)p(x)+q(x)dx=\\xa0−log\\u20614+KL(p\\u2005\\u200a∥\\u2005\\u200ap+q2)+KL(q\\u2005\\u200a∥\\u2005\\u200ap+q2)=\\xa0−log\\u20614+2⋅JSD(p\\u2005\\u200a∥\\u2005\\u200aq).\\\\begin{aligned}\\n    \\\\mathcal{D}_\\\\theta\\n    =\\\\ & - \\\\log 4 + \\\\int p(x) \\\\log \\\\frac{2p(x)}{p(x) + q(x)} dx + \\\\int q(x) \\\\log \\\\frac{2q(x)}{p(x) + q(x)} dx \\\\\\\\\\n    =\\\\ & - \\\\log 4 + \\\\text{KL} \\\\bigg( p \\\\;\\\\Big\\\\|\\\\; \\\\frac{p + q}{2} \\\\bigg) + \\\\text{KL} \\\\bigg( q \\\\;\\\\Big\\\\|\\\\; \\\\frac{p + q}{2} \\\\bigg) \\\\\\\\\\n    =\\\\ & - \\\\log 4 + 2 \\\\cdot JSD(p\\\\;\\\\|\\\\; q).\\n\\\\end{aligned}\\nDθ\\u200b=\\xa0=\\xa0=\\xa0\\u200b−log4+∫p(x)logp(x)+q(x)2p(x)\\u200bdx+∫q(x)logp(x)+q(x)2q(x)\\u200bdx−log4+KL(p\\u200b2p+q\\u200b)+KL(q\\u200b2p+q\\u200b)−log4+2⋅JSD(p∥q).\\u200bЗдесь KL(P\\u2005\\u200a∥\\u2005\\u200aQ)\\\\text{KL}(P \\\\;\\\\|\\\\; Q)KL(P∥Q) означает, как обычно, KL-дивергенцию\\nKL(P\\u2005\\u200a∥\\u2005\\u200aQ)=∫P(x)log\\u2061P(x)Q(x)dx,\\\\begin{equation}\\n    \\\\text{KL}(P \\\\;\\\\|\\\\; Q) = \\\\int P(x) \\\\log \\\\frac{P(x)}{Q(x)} dx,\\n\\\\end{equation}\\nKL(P∥Q)=∫P(x)logQ(x)P(x)\\u200bdx,\\u200b\\u200bкоторая показывает, насколько два распределения отличаются друг от друга. Через JSD(p\\u2005\\u200a∥\\u2005\\u200aq)JSD(p \\\\;\\\\|\\\\; q)JSD(p∥q) обозначает ещё один вид дивергенции (её называют дивергенцией Йенсена-Шеннона). Получается, что при оптимальном дискриминаторе генератор, решая внешнюю задачу оптимизации, уменьшает расстояние между распределениями реальных и фейковых данных, действительно приближая их друг к другу!\\nИсходя из этого, и в предположении достаточной capacity генератора и дискриминатора (т.е. предполагая, что их параметризация позволяет достичь оптимума), мы можем сформулировать первый, наивный алгоритм обучения генеративно-состязательных сетей.\\n\\n\\nРешить внутреннюю задачу максимизации по ϕ\\\\phiϕ, повторяя шаги ниже до сходимости по параметрам дискриминатора ϕ\\\\phiϕ к оптимальному значению ϕ∗\\\\phi^*ϕ∗:\\n— Составить мини-батч сэмплов шума z1,…,zn\\\\\\\\{z_1, \\\\dots, z_n\\\\\\\\}z1\\u200b,…,zn\\u200b из p(z)p(z)p(z).\\n— Составить мини-батч сэмплов данных x1,…,xn\\\\\\\\{x_1, \\\\dots, x_n\\\\\\\\}x1\\u200b,…,xn\\u200b из p(x)p(x)p(x).\\n— Обновить дикриминатор, сделав шаг вверх по его градиенту:\\n∇ϕ1n∑i=1n[log\\u2061Dϕ(xi)+log\\u2061(1−Dϕ(Gθ(zi)))]    \\\\nabla_\\\\phi \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\Big[ \\\\log D_\\\\phi(x_i) + \\\\log \\\\Big( 1 - D_\\\\phi\\\\big(G_\\\\theta(z_i)\\\\big) \\\\Big) \\\\Big] \\\\\\\\\\n∇ϕ\\u200bn1\\u200bi=1∑n\\u200b[logDϕ\\u200b(xi\\u200b)+log(1−Dϕ\\u200b(Gθ\\u200b(zi\\u200b)))]\\n\\nСделать шаг SGD для внешней задачи минимизации по θ\\\\thetaθ:\\n— Составить мини-батч сэмплов шума {z1,…,zn}\\\\{z_1, \\\\dots, z_n\\\\}{z1\\u200b,…,zn\\u200b} из p(z)p(z)p(z).\\n— Обновить генератор, сделав шаг вниз по его градиенту:\\n∇θ1n∑i=1nlog\\u2061(1−Dϕ∗(Gθ(zi)))=1n∑i=1n−∇θf(Gθ(zi))1−f(Gθ(zi)),    \\\\nabla_\\\\theta \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\log \\\\Big( 1 - D_{\\\\phi^*}\\\\big(G_\\\\theta(z_i)\\\\big) \\\\Big) = \\\\frac{1}{n} \\\\sum_{i=1}^n - \\\\frac{\\\\nabla_\\\\theta f\\\\big(G_\\\\theta(z_i)\\\\big) }{1 - f\\\\big(G_\\\\theta(z_i)\\\\big)},\\n∇θ\\u200bn1\\u200bi=1∑n\\u200blog(1−Dϕ∗\\u200b(Gθ\\u200b(zi\\u200b)))=n1\\u200bi=1∑n\\u200b−1−f(Gθ\\u200b(zi\\u200b))∇θ\\u200bf(Gθ\\u200b(zi\\u200b))\\u200b,где через fff мы для краткости обозначили Dϕ∗(x)D_{\\\\phi^*}(x)Dϕ∗\\u200b(x).\\n\\n\\nКакие у этого наивного подхода могут быть недостатки? Во-первых, он очень медленный, потому что необходимо обучать дискриминатор до сходимости, чтобы сделать всего один шаг по градиенту генератора. Но вторая проблема намного серьёзнее: функция потерь генератора может насыщаться и выдавать близкие к нулю градиенты. Проиллюстрируем это на примере обучения простой модели, которая будет сэмплировать из одномерной гауссианы с заданными параметрами.\\n\\nРаспределение p(x)p(x)p(x) в этом случае известно, а распределение q(x)q(x)q(x) мы можем получить с помощью методов оценки плотности по сэмплам. Визуализируем эти плотности, а также градиент по сэмплам из генератора (a). Видно, что в случае, когда пики распределений плохо пересекаются друг с другом, градиент будет равен нулю на большинстве сэмплов, которые выдаёт генератор, т.е. они никак не будут использоваться для обучения. Чтобы понять причину происходящего, давайте посмотрим на градиент функции потерь генератора. На точках, далёких от основной «массы» p(x)p(x)p(x), дискриминатор выдаёт что-то близкое к нулю, то есть знаменатель градиента практически не будет ни на что влиять, а в числителе тоже будет практически ноль: ведь если мы немного поменяем параметры генератора, то «плохие» точки по-прежнему будут далеки от p(x)p(x)p(x), так что изменение лосса будет пренебрежимо малым, и градиент тоже.\\nЭто приводит к тому, что обучение происходит недостаточно эффективно: мы тратим время на вычисление сэмплов, которые не делают никакой вклад в обновление параметров генератора. Но более существенная проблема возникает в вырожденном случае: если изначально два распределения практически не пересекаются своими плотностями (b): в этом случае процесс обучения практически не идёт. Часто ли встречается такая вырожденная ситуация на практике? Довольно часто! Достаточно представить себе ситуацию, когда мы хотим генерировать реалистичные изображения лиц, а генератор в начале обучения вместо этого выдаёт случайный шум.  Из-за наличия такой проблемы описанная выше функция потерь генератора называется «сатурирующей».\\nВ оригинальном подходе по обучению генеративно-состязательных сетей было предложено два решения этой проблемы. Во-первых, мы можем обучать дискриминатор на каждой итерации не до сходимости, а с небольшим фиксированным числом шагов NNN (на практике чаще всего используется N≤2N \\\\le 2N≤2). Это позволяет существенно улучшить исходную ситуацию с переобучением дискриминатора. Также мы могли бы улучшить функцию потерь для генератора, сделав так, чтобы она сглаживала выходы дискриминатора около нуля. В качестве такой функции изначально был предложен логарифм. Нетрудно видеть, что оптимум улучшенной функции потерь («несатурирующий лосс») совпадает с исходной, что позволяет сохранить все описанные выше теоретические гарантии:\\nθ∗=\\xa0arg\\u2061min\\u2061θL\\xa0θ,ϕ=\\xa0arg\\u2061min\\u2061θEz∼p(z)log\\u2061[1−Dϕ(Gθ(z))]=\\xa0arg\\u2061min\\u2061θEz∼p(z)−log\\u2061Dϕ(Gθ(z)).\\\\begin{aligned}\\n    \\\\theta^* =\\\\ & \\\\arg \\\\min_\\\\theta \\\\mathcal{L}_{\\\\ \\\\theta,\\\\phi} \\\\\\\\\\n    =\\\\ & \\\\arg \\\\min_\\\\theta \\\\mathbb{E}_{z \\\\sim p(z)} \\\\log \\\\big[ 1 - D_\\\\phi \\\\big( G_\\\\theta(z) \\\\big) \\\\big] \\\\\\\\\\n    =\\\\ & \\\\arg \\\\min_\\\\theta \\\\mathbb{E}_{z \\\\sim p(z)} - \\\\log D_\\\\phi \\\\big( G_\\\\theta(z) \\\\big).\\n\\\\end{aligned}\\nθ∗=\\xa0=\\xa0=\\xa0\\u200bargθmin\\u200bL\\xa0θ,ϕ\\u200bargθmin\\u200bEz∼p(z)\\u200blog[1−Dϕ\\u200b(Gθ\\u200b(z))]argθmin\\u200bEz∼p(z)\\u200b−logDϕ\\u200b(Gθ\\u200b(z)).\\u200bТочка минимума у новой функции потерь та же, что у исходной, а градиенты оказываются ненулевыми на всех сгенерированных сэмплах.\\n\\nПомимо этого, на практике вместо обычного метода стохастического градиентного спуска используются его модификации, которые учитывают и первые, и вторые моменты градиентов например, Adam. Вообще, GAN-ы – довольно капризные модели, и настоятельно рекомендуется использовать готовые реализации с GitHub, оставляя большую часть гиперпараметров без изменений. Наиболее критичными среди них являются learning rate и расписание (то есть количество обновлений дискриминатора на одно обновление генератора).\\nМетрики качества\\nПосле успешного обучения генератора хотелось бы также понять, насколько хорошо он работает. Для этого рассмотрим на примере задачи генерации изображений типовые ошибки, которые может совершать GAN. Наиболее частая проблема – плохое качество или наличие артефактов – вызвана ограничениями, связанными с capacity генератора и несовершенством самих методов обучения. Здесь всё просто: наша генеративная модель плохо работает, и мы это видим на сгенерированных сэмплах. Более скрытым видом ошибок является так называемый mode collapse: обученный генератор выдаёт реалистично выглядящие картинки, но они не покрывают всё разнообразие распределения p(x)p(x)p(x). Например, если наша модель учится генерировать изображения с животными, то она может проигнорировать более редкие виды, а научиться генерировать только наиболее часто встречающиеся. Более экстремальная форма подобного поведения – это когда модель вообще выдаёт вариацию одной картинки. Иногда в литературе общее качество результатов работы нейросети, по аналогии с задачей классификации, измеряется точностью метода (precision), а отсутствие mode collapse измеряется полнотой (recall).\\nСамый простой и действенный способ измерить как precision, так и recall – сгенерировать данные и посмотреть на них, дав экспертную оценку уровня их реализма. Не стоит им пренебрегать! Формализовать этот подход в метрику можно в виде эксперимента, который в литературе называется user study. Например, мы можем сделать опрос экспертов, которым будем показывать два примера, настоящий и сгенерированный, и попросить их угадать, где фейк. Тогда процент неправильных ответов будет являться метрикой качества для нашего метода. Такой опрос в основном показывает степень реализма полученных результатов: есть ли в них какие-то заметные артефакты, соответствуют ли они реальным примерам по своей структуре, и так далее. Отчасти он также замеряет разнообразие примеров: то, насколько они хорошо покрывают носитель распределения p(x)p(x)p(x). Если обученная модель генерирует очень похожие друг на друга примеры (то есть имеет место существенный mode collapse), то эксперт через несколько примеров научится определять ненастоящие. С другой стороны, если сэмплы в целом разнообразные, но всё равно не полностью покрывают основу целевого распределения, то user study не позволит обнаружить эту проблему.\\nFrechet Inception Distance\\nЕсть метрики, с помощью которых можно автоматически проводить тестирование, похожее на user study. Для изображений наиболее используемой является Frechet Inception Distance (FID). Чтобы её посчитать, нам в идеале понадобится нейросеть, предобученная на датасете, который мы генерируем, но на практике во всех случаях используется модель Inception v3, предобученная на датасете ImageNet (отсюда слово Inception в названии метрики).\\nДля того, чтобы понять идею этой метрики, рассмотрим следующий пример: если выходом нейросети является класс (число), то его вероятность можно смоделировать мультиномиальным распределением. Гипотетически, чтобы сравнить два распределения картинок ppp и qqq, нам достаточно измерить расстояние между двумя мультиномиальными распределениями, построенными на выходах предобученного классификатора после прогона датасетов реальных и сгенерированных данных. Если в распределении qqq примеров из каких-то классов будет меньше или больше, чем в ppp, то такая метрика будет отличная от нуля.\\nПонятно, что это слишком грубое приближение расстояния между двумя распределениями, т.к. оно практически никак не учитывает реализм получаемых картинок. Поэтому вместо выходов нейросети в FID было предложено использовать признаки с её глубоких слоёв. Они кодируют высокоуровневую семантику изображений, потому что по этим признакам модель предсказывает вероятность принадлежности картинки к тому или иному классу. При этом в них остаётся довольно много информации об исходном изображении и свойств локальных признаков (текстур), которые могут помочь распознать артефакты. Метрика FID работает таким образом, что сравнивает два распределения высокоуровневых признаков для реальных и сгенерированных картинок, используя в качестве их приближения многомерные гауссианы (каждая размерность соответствует одному каналу). Для измерения расстояния между этими двумя распределениями используется метрика Вассерштейна:\\nFID=∥μ−μ^∥2+Tr(Σ+Σ^−2(ΣΣ^)1/2),\\\\begin{equation}\\n    \\\\text{FID} = \\\\| \\\\mu - \\\\hat\\\\mu \\\\|^2 + \\\\text{Tr}\\\\big( \\\\Sigma + \\\\hat\\\\Sigma - 2(\\\\Sigma\\\\hat\\\\Sigma)^{1/2} \\\\big),\\n\\\\end{equation}\\nFID=∥μ−μ^\\u200b∥2+Tr(Σ+Σ^−2(ΣΣ^)1/2),\\u200b\\u200bгде μ∈RC\\\\mu \\\\in \\\\mathbb{R}^Cμ∈RC и Σ∈RC×C\\\\Sigma \\\\in \\\\mathbb{R}^{C \\\\times C}Σ∈RC×C – это среднее и матрица ковариаций глубоких признаков Fi∈RC×H×Wi=1N\\\\\\\\{F_i \\\\in \\\\mathbb{R}^{C\\\\times H\\\\times W}\\\\\\\\}_{i=1}^NFi\\u200b∈RC×H×Wi=1N\\u200b, которые считаются по выборке из NNN реальных картинок. При этом как средние, так и матрицы ковариаций считаются по объединению всех признаков со всех картинок без учёта пространственной размерности, т.е. по второй размерности матрицы F∈RC×NHWF \\\\in \\\\mathbb{R}^{C \\\\times NHW}F∈RC×NHW. То же самое делается для сгенерированных картинок, для них средние и ковариации обозначены как μ^\\\\hat\\\\muμ^\\u200b и Σ^\\\\hat\\\\SigmaΣ^. Минимум этой метрики равен нулю, и достигается в случае, когда статистики, посчитанные по двум распределениям, совпадают. На практике эта метрика используется как для измерения реализма изображений, так и для детектирования mode collapse.\\nИнтерполяции в скрытом пространстве\\nЕщё один способ измерения качества, который мы рассмотрим, напрямую связан с тем, что генеративно-состязательные модели эффективно занимаются кодированием потенциально высокоразмерных данных в низкоразмерное представление. Но как для нейросети с большим числом параметров проверить, занимается ли она реальным кодированием или простым запоминанием выборки?\\nРассмотрим следующий пример. Пусть наша генеративная модель к случайным векторам zzz применяет их функцию распределения и отображает векторы в равномерно распределённые на отрезке [0,1][0, 1][0,1] числа uuu. Упорядочим наш датасет. В качестве случайного сэмпла пусть наша модель выдаёт ту картинку, чей индекс, поделённый на размер датасета, ближе всего к uuu. Другими словами, наша генеративная модель будет выдавать случайные картинки из датасета вместо генерации новых картинок. Методы оценки качества, которые мы описали выше, пропустят эту проблему: ведь «сгенерированные» картинки будут в точности совпадать с настоящими.  Поэтому для полной проверки качества работы генеративной модели важно понимать, действительно ли она производит сжатие выборки в низкоразмерное представление или просто запоминает обучающие примеры.\\nОдним из тестов на подобное поведение является интерполяция между сгенерированными примерами. Возьмём два случайных вектора z1z_1z1\\u200b и z2z_2z2\\u200b из p(z)p(z)p(z). Рассмотрим все векторы, которые лежат между ними z=αz1+(1−α)z2,\\xa0α∈[0,1]z = \\\\alpha z_1 + (1 - \\\\alpha) z_2,\\\\ \\\\alpha \\\\in [0, 1]z=αz1\\u200b+(1−α)z2\\u200b,\\xa0α∈[0,1]. К каждому такому вектору zzz применим наш генератор и получим x^\\\\hat{x}x^ для промежуточных векторов и x^1,x^2\\\\hat{x}_1, \\\\hat{x}_2x^1\\u200b,x^2\\u200b для z1z_1z1\\u200b и z2z_2z2\\u200b. Для правильно обученного GANа мы должны увидеть следующую картинку: при изменении коэффициента α\\\\alphaα изображение x^\\\\hat{x}x^ должно плавно меняться и перетекать из x^1\\\\hat{x}_1x^1\\u200b в x^2\\\\hat{x}_2x^2\\u200b. При этом каждая промежуточная картинка должна быть так же реалистичным сэмплом.\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nКачество такой интерполяции сложно измерить численно, но если мы видим, что промежуточные результаты меняются случайно без какой-либо связи с семантикой интерполируемых примеров, то это говорит о плохом качестве генератора. Стоит упомянуть, что для сэмплов из нормального распределения, которое обычно имеют векторы zzz, намного лучше работает интерполяция по сфере (Slerp), потому что в многомерном пространстве векторы zzz практически всегда будут лежать в объёме вокруг сферы диаметра d\\\\sqrt{d}d\\u200b, где ddd – размерность вектора zzz.\\nИнтерполяция в скрытом пространстве с недавних пор стала использоваться для генерации анимаций и видео. Ведь анимация — это последовательность кадров, плавно переходящих друг в друга. И если у нас есть обученный GAN для генерации картинок, то нам нужно лишь найти путь в скрытом пространстве таким образом, чтобы набор сгенерированных картинок складывался в анимацию. Более того, в скрытом пространстве можно находить различные интерпретируемые пути. Например, путь, при движении по которому размывается задний фон или меняется причёска. Почитать подробнее про это можно тут.\\n\\n\\n\\nссылка на источник картинки\\n\\n\\nБлижайшие соседи\\nЕщё одним способом проверить, не запомнил ли генератор датасет, является поиск ближайших соседей по датасету. Для этого следует сгенерировать несколько изображений. Для каждого изображения нужно найти несколько ближайших соседей из датасета. В качестве признаков для картинок можно взять признаки с последних слоёв сети Inception. На соседей стоит посмотреть глазами. Если мы увидим, что ближайшие соседи из датасета визуально совпадают со сгенерированными сэмплами, то это значит, что генератор запомнил сэмплы из датасета.\\nБазовые модели\\nЧтобы лучше понимать современные модели, давайте сначала рассмотрим более базовые модели. Хотя они редко используются напрямую, многие идеи из них легли в основу современных моделей.\\nDCGAN\\nНаиболее простая версия генеративной модели для изображений — это DCGAN (Deep Convolutional GAN, 2015 год). Её до сих пор можно иногда встретить как в литературе, так и на практике.\\nВ основе DCGAN лежит простая идея: нейросети, основанные на свёртках, отлично подходят для распознавания изображений, а значит вполне могут подойти и для их генерации. Единственное отличие, которое требуется – это постепенно увеличивать внутри нейросети пространственный размер признаков, а не уменьшать. Для этого в современных нейросетях делается операция nearest upsampling, очень похожая на max pooling. В nearest upsampling пространственное разрешение карты признаков увеличивается за счёт того, что каждый вектор повторяется KKK раз по горизонтали и по вертикали. К примеру, после увеличения таким образом карты признаков, состоящей из одной единицы, мы получим квадрат размера K×KK \\\\times KK×K из единиц. На практике увеличение размерности происходит по аналогии с размерами пулинга в свёрточных дискриминативных сетях и почти всегда равно K=2K=2K=2.\\n\\n\\n\\nМодель DCGAN. Ссылка на источник картинки.\\n\\n\\nТаким образом, генератор в случае DCGAN является последовательностью свёрток, слоёв батч нормализации, нелинейностей и слоёв upsampling, а дискриминатор – обычной классификационной нейросетью. При этом первым слоем в генераторе является линейный слой, который отображает вектор шума zzz в карту признаков с начальным разрешением (как правило, размера 4×44 \\\\times 44×4).\\n\\n\\n\\nРезультат работы DCGAN. Ссылка на источник картинки.\\n\\n\\nХотя результаты работы DCGAN довольно смазанные, эта модель показала большие перспективы генеративных нейросетей для изображений.\\n\\n\\n\\nТак выглядит интерполяция в скрытом пространстве для модели DCGAN. Ссылка на источник картинки.\\n\\n\\nУсловная генерация\\nДопустим, что в нашем датасете есть изображения, относящиеся к разным классам, и мы хотели бы уметь генерировать изображение заданного класса. В этом случае речь идёт об условной генерации. В качестве условия может выступать не только метка класса, но и объект любой природы. Например, когда вы можете захотеть сгенерировать изображение по текстовому описанию.\\nДалее будем обозначать условие как yyy. Наша задача — построить генератор, который бы моделировал p(x∣y)p(x \\\\mid y)p(x∣y).\\nConditional GAN\\nСамый основной метод условной генерации — конкатенация условия с вектором шума, который генератор принимает на вход. В статье Conditional GAN 2014 года, где предложили этот метод, рекомендовалось подавать условие не только в генератор, но и в дискриминатор.\\n\\n\\n\\nМодель Conditional GAN. Ссылка на источник картинки.\\n\\n\\nЕсли мы генерируем векторные данные, то вектор на вход дискриминатора подаётся конкатенированным с yyy. При этом если если yyy — это метка класса, то стоит её закодировать с помощью one-hot encoding. Если же мы работаем с изображениями, то нам из вектора условия следует сделать изображение. Например, если картинки из датасета имеют размер H×WH \\\\times WH×W, то следует размножить вектор yyy, создав из него тензор размера H×W×dyH \\\\times W \\\\times d_yH×W×dy\\u200b, где dyd_ydy\\u200b — размерность вектора yyy. Далее полученное «изображение» конкатенируется с входным изображением.\\nСовременные модели\\nТеперь на примере наиболее успешных моделей мы расскажем об улучшениях, которые во многом отходят от оригинального подхода к обучению GAN-ов и при этом значительно улучшают практические результаты, а значит расширяют практическую применимость.\\nВ этом разделе мы рассмотрим state-of-the-art систему генерации изображений StyleGAN, методы её обращения (т.е., поиска векторов шума, соответствующих произвольной картинке), а также методы манипуляции семантикой изображений. После этого мы рассмотрим несколько примеров условных генеративных моделей, которые вместо шума принимают на вход изображения. Такие модели используются как для задачи повышения разрешения (super resolution), так и стилизации (например, превращение пейзажей в картины Моне). Мы сфокусируемся на изображениях, так как в этой области сконцентрирован как основной прогресс, так и наиболее впечатляющие применения генеративных моделей.\\nStyleGAN\\nСамой известной генеративно-состязательной моделью, работающей с изображениями, по праву считается StyleGAN, который до сих пор активно развивается и имеет большое количество расширений (например, существуют разнообразные методы его обращения).\\nProgressive Growing\\nАрхитектура StyleGAN переняла progressive growing из модели Progressive Growing of GANs. Суть данной техники заключается в том, чтобы не сразу генерировать изображение высокого разрешения, а постепенно. Давайте рассмотрим это подробнее.\\n\\n\\n\\nИллюстрация работы генератора и дискриминатора в модели Progressive Growing GANs. Ссылка на источник картинки.\\n\\n\\nМы хотим получить генератор, который генерирует изображения размера 1024x1024. Обучить такой генератор очень сложно. Поэтому мы начинаем с разрешения 4x4. У генератора мы оставляем только первый блок слоёв, который позволяет из шума получить изображение размера 4x4. У дискриминатора мы оставим, наоборот, только последний, который принимает на вход изображение размером 4x4. Такой GAN мы обучаем на изображениях из датасета (предварительно уменьшив их в размере).\\nСпустя сколько-то итераций мы понимаем, что сеть уже умеет генерировать маленькие изображения. В этот момент мы добавляем к генератору один блок, чтобы на выходе у неё получалось изображение размера 8x8. Так же мы добавляем один блок в начало дискриминатора, чтобы он на вход принимал изображения размера 8x8. Теперь генератор и дискриминатор состоят из двух блоков, которые мы и обучаем.\\nТакой процесс мы повторяем несколько раз, пока в итоге не дойдём до нужного нам разрешения 1024x1024. Эта схема в итоге показала себя действенным способом генерации реалистичных изображений высокого разрешения.\\nПодача шума в нейросеть\\nКлючевой частью StyleGAN является используемый в нём способ подачи шума zzz в нейросеть, и именно из-за него метод и получил своё название. Для того чтобы понять, что конкретно в нём особенного, давайте подробнее посмотрим на архитектуру сети (рисунок из предыдущего раздела, модель StyleGAN справа).\\n\\n\\n\\nСлева: традиционный генератор. Справа: генератор модели StyleGAN. Ссылка на источник картинки.\\n\\n\\nВо-первых, вместо того, чтобы подавать вектор шума zzz только в самом начале генератора, нейросеть обуславливают на него много раз на разных разрешениях признаков. Исторически, впервые похожим методом решалась задача переноса стиля одной картинки на другую, отсюда и название: a style-based generator.\\nВ качестве метода обуславливания используются так называемые адаптивные слои. Это модификация обычных слоёв нейросетей, в которых часть параметров предсказывается другой нейросетью. Вообще говоря, адаптивным можно сделать любой вид нормализации, включая батч нормализацию, но наиболее известным примером такого слоя является адаптивная инстанс нормализация (adaptive instance normalization), и именно она использовалась в первой версии StyleGAN.\\nВспомним, как именно работает неадаптивная версия этого слоя. Пусть у нас есть батч Fin∈RB×C×H×WF^\\\\text{in} \\\\in \\\\mathbb{R}^{B \\\\times C \\\\times H \\\\times W}Fin∈RB×C×H×W, элементы которого будем обозначать как fbchwin∈Rf_{bchw}^\\\\text{in} \\\\in \\\\mathbb{R}fbchwin\\u200b∈R. Здесь BBB обозначает размер мини-батча, CCC – количество признаков, а HHH и WWW – высоту и ширину. Тогда внутри слоя инстанс нормализации выполняется следующая операция:\\nFout=Fin−μσ⋅γ+β.μ,σ∈RB×C,γ,β∈RC.    F^\\\\text{out} = \\\\frac{F^\\\\text{in} - \\\\mu}{\\\\sigma} \\\\cdot \\\\gamma + \\\\beta.\\\\quad \\\\mu, \\\\sigma \\\\in \\\\mathbb{R}^{B \\\\times C},\\\\quad \\\\gamma, \\\\beta \\\\in \\\\mathbb{R}^C.\\nFout=σFin−μ\\u200b⋅γ+β.μ,σ∈RB×C,γ,β∈RC.Здесь μ\\\\muμ и σ\\\\sigmaσ обозначают матрицы средних и стандартных отклонений, которые считаются отдельно для каждого элемента мини-батча и для каждого признака:\\nμbc=1HW∑h=1H∑w=1Wfbchw,σbc=1HW∑h=1H∑w=1W(fbchw−μbc)2.    \\\\mu_{bc} = \\\\frac{1}{HW} \\\\sum_{h=1}^H \\\\sum_{w=1}^W f_{bchw},\\\\quad \\\\sigma_{bc} = \\\\sqrt{\\\\frac{1}{HW} \\\\sum_{h=1}^H \\\\sum_{w=1}^W (f_{bchw} - \\\\mu_{bc})^2}.\\nμbc\\u200b=HW1\\u200bh=1∑H\\u200bw=1∑W\\u200bfbchw\\u200b,σbc\\u200b=HW1\\u200bh=1∑H\\u200bw=1∑W\\u200b(fbchw\\u200b−μbc\\u200b)2\\u200b.При этом γ\\\\gammaγ и β\\\\betaβ являются параметрами слоя, которые настраиваются в процессе обучения. Особенностью этого слоя является то, что, в отличие от батч нормализации, он применяется одинаковым образом как при обучении, так и во время инференса. То есть вместо того, что приближать средние и стандартные отклонения по батчу при помощи скользящих средних, как это делается в батч нормализации, мы честно каждый раз считаем эти статистики для каждой новой картинки отдельно от всех остальных. Это делает инстанс нормализацию очень популярной в области обработки и генерации изображений, где зачастую бывает невозможным обучение с большим размером мини-батча, а значит и использование батч нормализации.\\nАдаптивной инстанс нормализацией (AdaIN) называется слой, где γ\\\\gammaγ и β\\\\betaβ являются не обучаемыми параметрами, а нейросетями, которые предсказывают эти векторы из какого-то общего для всех слоёв адаптивной инстанс нормализации входа (обозначим его через www):\\nFout=Fin−μσ⋅γ(w)+β(w).    F^\\\\text{out} = \\\\frac{F^\\\\text{in} - \\\\mu}{\\\\sigma} \\\\cdot \\\\gamma(w) + \\\\beta(w).\\nFout=σFin−μ\\u200b⋅γ(w)+β(w).Это означает, что вместо оптимизации по векторам γ\\\\gammaγ и β\\\\betaβ будет происходить оптимизация по параметрам этих двух нейросетей. Также это означает, что у адаптивной инстанс нормализации добавляется ещё один вход помимо набора признаков FinF^\\\\text{in}Fin, который определяет её поведение: некоторый вектор www, который также называют вектором стиля.\\nКак правило, в качестве γ(w)\\\\gamma(w)γ(w) и β(w)\\\\beta(w)β(w) используется нейросеть с одним линейным слоем или неглубокий персептрон.\\nНетрудно видеть, что если в качестве вектора www подавать сгенерированный шум zzz, то это будет хорошим способом многократного обуславливания нашего генератора на вектор шума. Это позволило бы глубоким слоям нейросети выучивать лишь часть той информации о выходном изображении, которая содержится в векторе www, например, глобальные признаки картинки. А информация о локальных признаках выходного изображения (текстурах) может появляться уже ближе к последним слоям на более высоком разрешении промежуточных признаков. Таким образом, у генератора нет необходимости хранить во всех своих картах признаков всю информацию о сгенерированной картинке, как это происходит в случае DCGAN: он может декодировать её напрямую из вектора шума по мере необходимости, что существенно облегчает обучение таких моделей и улучшает качество результатов.\\nАвторы StyleGAN пошли даже дальше: в качестве дополнительной регуляризации они специально заставляли нейросеть использовать информацию из вектора шума частями. А именно, во время обучения все слои адаптивной нормализации случайным образом делятся на две последовательно идущие группы: первая группа обуславливается при помощи одной части вектора шума z1z_1z1\\u200b, а вторая – при помощи другой части z2z_2z2\\u200b. На практике это приводит к следующему эффекту: нейросеть учиться декодировать часть признаков изображения, используя вектор z1z_1z1\\u200b, а часть – используя z2z_2z2\\u200b. Это позволяет после обучения напрямую манипулировать выходами нейросети, смешивая разные векторы стилей zzz.\\nОбучение нового латентного пространства\\nВторое ключевое открытие авторов StyleGAN связано с задачей поиска семантически значимого редактирования векторов из выученного низкоразмерного многообразия p(z)p(z)p(z). Зачем это нужно на практике мы уже упоминали ранее: на этом низкоразмером многообразии значительно проще семантически редактировать изображения, чем на уровне пикселей. Например, для задачи генерации лиц на многообразии zzz за изменение возраста или гендера может отвечать простой аддитивный сдвиг вектора zzz на Δz\\\\Delta zΔz. Если же мы попытаемся приблизить такую операцию в пространстве пикселей, то для этого уже понадобится большая нейросеть с сотнями тысяч или даже миллионами параметров.\\nПри этом, как правило, мы хотим использовать наиболее простые операции редактирования. В идеале, мы бы хотели ограничить класс преобразований редактирования (а) сдвигами на какой-то вектор и (б) линейной (или сферически-линейной) интерполяцией двух векторов. С одной стороны, кажется, что так задача редактирования векторов существенно усложняется: этот класс преобразований даже менее выразителен, чем линейные операции. Но, с другой стороны, для таких простых преобразований легче гарантировать, что они не выведут нас за пределы многообразия, в котором у распределения p(z)p(z)p(z) большая «масса», т.е. того множества векторов zzz, которые генератор чаще всего видел во время обучения. Для нормального распределения, как было сказано ранее, это многообразие можно приблизить сферой радиуса d\\\\sqrt{d}d\\u200b.\\nНо будет ли легко найти хорошо работающие преобразования на многообразии случайных векторов zzz, взятых из нормального распределения? Авторы StyleGAN обнаружили, что если сначала пропустить векторы zzz через многослойный персептрон fff, и подавать на вход свёрточного генератора его выходы w=f(z)w = f(z)w=f(z), то редактировать латентные векторы на выученном многообразии w∈Ww \\\\in \\\\mathcal{W}w∈W станет намного проще. Это объясняется тем, что функция fff имеет возможность выучить достаточно сложное распределение для переменной www, которое упростило бы задачу генерации картинки для свёрточной части генератора. И на практике оказывается, что такое выученное представление W\\\\mathcal{W}W улучшает не только качество генерируемых картинок, но и качество результатов для семантического редактирования векторов.\\n\\n\\n\\nПримеры генерации StyleGAN. Ссылка на источник картинки.\\n\\n\\nTruncation trick\\nПоследняя важная деталь, которая тем не менее очень сильно помогла авторам StyleGAN получить настолько хорошие результаты – это так называемый truncation trick. Он был впервые предложен в более ранних работах и продолжает оказывает огромное влияние на качество результатов. Его суть состоит в том, чтобы после обучения сэмплировать те примеры из латентного пространства, которые чаще всего видел генератор во время обучения. Например, если мы во время обучения брали вектор zzz из нормального распределения, то при использовании truncation trick после обучения мы бы его сэмплировали из нормального распределения с обрезанными хвостами. Тем самым, интуитивно, мы убираем из сгенерированной выборки те примеры входных векторов, которые генератор реже видел во время обучения. Однако, нетрудно заметить что такая процедура приводит к потере разнообразия в выходных картинках. Например, если мы обрежем нормальное распределение вплоть до его среднего значения, то тогда нейросеть сможет выдавать лишь один пример. При всём при этом потеря разнообразия выходов – не такая большая проблема, т.к. обученный генератор всё ещё может часто ошибаться и выдавать маргинальные примеры. Фильтрация таких плохих примеров по какому-то выставленному порогу – в этом и есть суть применения truncation trick.\\nВ случае StyleGAN, авторам хотелось бы применять этот трюк непосредственно на распределении в выученном латентном пространстве W\\\\mathcal{W}W. Для этого они применяют простой трюк: сначала считают центр масс W\\\\mathcal{W}W, усредняя векторы www для большой выборки сэмплов zzz:\\nwˉ=Ez∼p(z)[f(z)],    \\\\bar{w} = \\\\mathbb{E}_{z \\\\sim p(z)} [ f(z) ],\\nwˉ=Ez∼p(z)\\u200b[f(z)],а затем сдвигают каждый сгенерированный вектор www по направлению к этому центру:\\nw′=wˉ+ψ(w−wˉ),    w' = \\\\bar{w} + \\\\psi (w - \\\\bar{w}),\\nw′=wˉ+ψ(w−wˉ),где ψ<1\\\\psi < 1ψ<1 – это параметр, который задаёт trade-off между качеством результатов и их разнообразием.\\nStyleGAN-2\\nХотя работа StyleGAN показала довольно хорошие результаты, авторы статьи про StyleGAN-2 Analyzing and Improving the Image Quality of StyleGAN заметили, что в некоторых случаях она может выдавать некачественные изображения. В частности, StyleGAN в некоторых случая может выдавать изображения с артефактами.\\n\\n\\n\\nПримеры артефактов StyleGAN. Ссылка на источник картинки.\\n\\n\\nОсновной причиной этих артефактов оказалась адаптивная нормализация. Изначально, адаптивная нормализация состояла из двух частей: нормализация (на рисунке обозначена как Norm) и модуляция (на рисунке обозначена как Mod). В нормализации мы вычитали среднее и делили на стандартное отклонение. В модуляции мы умножали на новое выученное стандартное отклонение и прибавляли новое выученное среднее.\\nАвторы StyleGAN2 предложили несколько модификаций для этапа нормализации. Каждое изменение в статье добавляли последовательно, следя за изменением общего качества генерации.\\n\\n\\nКак из нормализации, так и из модуляции убрали вычитание/прибавление среднего. Нормализация и модуляция теперь выполняются независимо друг от друга и были перемещены в начало/конец стилевых блоков (см рисунок ниже, (c) Revised architecture).\\n\\n\\nНормализацию из предыдущего пункта заменили на демодуляцию весов. По сути это та же нормализация, только теперь нормализуются веса свёрток, а не входные данные (см рисунок ниже, (d) Weight demodulation. Обратите внимание на w1,w2,w3w_1, w_2, w_3w1\\u200b,w2\\u200b,w3\\u200b).\\n\\n\\n\\n\\n\\nИзменения, которые добавили в StyleGAN. Ссылка на источник картинки.\\n\\n\\nВ StyleGAN используется техника progressive growing (см раздел про StyleGAN). Из-за этого StyleGAN появляются артефакты, возникающие при исследовании латентного пространства с помощью интерполяций. Некоторые объекты лиц (глаза, зубы), которые должны вращаться при вращении головы, оставались на месте. Чтобы побороть эти артефакты, вместо progressive growing в StyleGAN2 стали использовать residual connections.\\n\\n\\n\\n\\nАртефакты из-за прогрессивной генерации в StyleGAN. Ссылка на источник картинки.\\n\\n\\nЭти изменения позволили улучшить качество генерируемых изображений и избавиться от артефактов StyleGAN. Вот, например, некоторые примеры сгенерированных изображений модели StyleGAN2:\\n\\n\\n\\nПримеры изображений, сгенерированных с помощью StyleGAN2. Ссылка на источник картинки.\\n\\n\\nStyleGAN-ADA\\nСледующий шаг в развитии архитектуры StyleGAN — это статья StyleGAN-ADA. ADA расшифровывается как Adaptive Discriminator Augmentation. В данной статье авторы предложили механизм аугментации данных, который позволяет стабилизировать обучение и избежать переобучения дискриминатора.\\n\\n\\n\\nНа левом рисунке (b) изображено, куда добавляется аугментация (синие блоки). На правом рисунке (c) изображена степень аугментации в зависимости от контролирующего её параметра p. Ссылка на источник картинки.\\n\\n\\nВсего в статье использовали 18 разных аугментаций. В статье также предложили некоторую эвристику того, как понимать, насколько переобучился дискриминатор. Эвристика нужна для того, чтобы адаптивно контролировать параметр аугментации ppp в зависимости от степени переобучения.\\nОсновная идея алгоритма контроля ppp в процессе обучения следующая. Изначально этот параметр равен нулю. Его значение изменяется на фиксированную величину каждые четыре мини-батча (авторы пишут, что частота обновлений не влияет на результат). Если наблюдается, что дискриминатор слишком переобучился, то параметр ppp увеличивается. И наоборот, при низкой степени переобучении дискриминатора значение ppp уменьшается.\\nАугментация, как показали авторы, действительно помогает стабилизировать обучение при маленьком количестве данных. Однако, большой набор реальных данных всегда будет выигрывать у аугментации.\\nStyleGAN-T\\nБольшинство современных моделей, которые показали впечатляющие результаты для генерации изображений, работают по схеме text to image. То есть текст является входом для нейросети, изображение — выходом. Обычно текст на входе называют prompt. По такой схеме работают модели Stable Diffusion, DALLE 2, Midjourney. Все эти модели являются диффузионными. Однако, пока что списывать GANы со счетов не стоит. Хотя качество у GANов не такое высокое, как у диффузионных моделей, а обучать их сложнее, у них есть неоспоримое преимущество — быстрая генерация изображений.\\nНа момент написания этого параграфа самая свежая статья про генерацию изображений с помощью GANов — StyleGAN-T. Её авторы решили на основе StyleGAN сделать модель для генерации изображений из текста.\\nАрхитектура модели StyleGAN-T очень похожа на архитектуру модели StyleGAN (за основу авторы взяли StyleGAN-XL — версию StyleGAN для больших обучающих выборок). В качестве кодировщика текста была использована предобученная модель CLIP.\\n\\n\\n\\nАрхитектура модели StyleGAN-T. Ссылка на источник картинки.\\n\\n\\nНа что стоит обратить внимание в данной архитектуре:\\n\\nТекст, закодированный CLIP text encoder, подаётся на вход как генератору, так и дискриминатору. Дискриминатор в данном случае классифицирует не отдельное изображение, а пару текст/изображение.\\nСгенерированное изображение пропускается через фиксированный кодировщик изображений, также взятый из модели CLIP (CLIP image encoder на рисунке архитектуры). Полученное представление изображения должно быть близко с представлением текста, полученным с помощью CLIP text encoder. Это достигается за счёт добавление CLIP guidance loss в общую функцию потерь. Для разрешения выше 64x64 авторы берут случайные кропы размера 64x64 на изображении, чтобы посчитать CLIP guidance loss.\\n\\nОсновное новшество модели StyleGAN-T — это лучшая GAN-модель для генерации изображений из текста. До этого большинство хорошо работающих моделей позволяли генерировать изображения для заданного класса или вообще без условий. Связать текст с изображением — гораздо более сложная задача.\\nРезультаты генерации\\nПоскольку на вход модель принимает не только шум, но и закодированный текст, она позволяет делать интерполяции по пространству текста. Примеры сгенерированных изображений и интерполяций по текстовому пространству вы можете видеть на рисунке ниже.\\n\\n\\n\\nРезультаты генерации моделью StyleGAN-T. Ссылка на источник картинки.\\n\\n\\nКачество изображений StyleGAN-T отстаёт от диффузионных моделей, таких как Stable Diffusion или DALLE 2, о чём пишут сами авторы. Однако данная модель сильно выигрывает по скорости: на одной и той же видеокарте Stable Diffusion генерирует изображение за 3.7 секунды, в то время как StyleGAN-T за 0.02 секунды.\\nDisclaimer\\nВыше были перечислены лишь основные особенности данного класса генеративных моделей, на которые стоит обратить внимание. Эти соображения нашли применение в других задачах помимо генерации картинок из шума. На самом деле, список трюков и нюансов, необходимых для успешного обучения такой модели, намного обширнее. На практике для генеративных моделей настоятельно рекомендуется отталкиваться от готовых кодовых баз, внося минимальные и контролируемые изменения в процесс обучения. Особенно чувствительны генеративные модели бывают к архитектуре генератора и дискриминатора, к параметрам оптимизации (learning rate, количество обновлений весов дискриминатора на одно обновление генератора, и т.д.), а также к значениям весов лоссов (например, к весу R1 регуляризации, которую мы тут не обсуждали).\\nПрименения генеративных состязательных нейросетей\\nДо этого мы рассмотрели основные особенности генеративных состязательных нейросетей, а также их применение в задаче генерации изображений. В этом разделе мы рассмотрим, какие ещё задачи можно решать с помощью таких моделей.\\nОтметим, что задачи, которые мы рассмотрим ниже, можно решать и другими способами без ГАНов. Зачастую диффузионные модели (MidJourney, Stable Diffusion) показывают лучшие результаты в этих задачах. Тем не менее в данном же разделе мы рассмотрим именно методы на основе генеративных состязательных нейросетей.\\nInpainting\\nПредставьте, что вы хотите удалить с фотографии людей на заднем плане. Встаёт вопрос, чем их заменить? Для этого существует задача инпеинтинга (inpainting). Она заключается в том, чтобы восстановить часть изображения, которая была выделена маской. Если выделить людей или объекты на фотографии маской, то нейросеть для инпеинтинга будет способна зарисовать эти участки чем-то подходящим для конкретной фотографии.\\n\\n\\n\\nПример работы модели инпеинтинга. Ссылка на источник картинки.\\n\\n\\nОбычно генератор модели GANs для инпеинтинга представляют собой image-to-image модели. То есть изображение подаётся как на вход, так и на выход. То, что происходит внутри генератора, зависит от архитектуры модели. Как правило, используются U-Net-подобные архитектуры с какими-то дополнениями.  Так, например, в одной из последних работ по инпеинтигу на основе GANs Resolution-robust Large Mask Inpainting with Fourier Convolutions используются Fast Fourier Convolutions.\\nЧтобы обучить модель инпеинтинга, нужно подготовить данные в формате пар <изображение с маской, изображение без маски>. Сделать это не сложно. Достаточно на существующем наборе изображений случайным образом выделить участки для удаления, после чего обучать нейросеть их восстанавливать.\\nOutpainting\\nЗадачу inpainting можно так же превратить в задачу outpainting, то есть дорисовки изображения по краям. Для этого нужно в качестве маски подать пиксели, которые находятся за рамками изображения. При этом само исходное изображение можно уменьшить, если того требуют размерности нейросети.\\n\\n\\n\\nПример outpainting, сделанный моделью In&Out. Ссылка на источник картинки.\\n\\n\\nЗадача outpainting может быть полезна, когда хочется расширить изображение, например, чтобы увеличить его разрешение.\\nРедактирование изображений\\nДо этого мы рассматривали, как можно редактировать латентное пространство обученной состязательной модели, чтобы это отражалось на сгенерированных изображениях. В 2023 году вышла работа Drag Your GAN, которая основана на этом принципе, и позволяет редактировать изображения перетаскиванием одной точки в другую.\\n\\nПример работы Drag Your GAN.cсылка на источник изображения\\nМетод Drag Your GAN основан на модели StyleGAN2. Ему на вход подаётся набор изначальных точек и набор конечных точек. Внутри метода поочерёдно выполняются следующие два шага:\\n\\nОбновление латентного пространства и обновления изображения с помощью оптимизации;\\nОбновление координат точек (трекинг точек).\\n\\nИзначально метод работает только со сгенерированным изображениями. Однако, нет проблем в том, чтобы добавить кодировщик, который бы переводил реальные изображения в латентное пространство модели. В таком случае можно будет редактировать и реальные изображения.\\nДемо Drag Your GAN доступно по ссылке.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф8.2. Variational Autoencoder (VAE)Следующий параграф8.4. Нормализующие потокиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_25.html', 'title': 'Нейросети для работы с последовательностями'}, page_content=\"Нейросети для работы с последовательностямиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/56.1.Свёрточные нейросети6.2.Нейросети для работы с последовательностямиWord EmbeddingsРекуррентные нейронные сетиSeq2seqМеханизм внимания (attention)Self-attentionОсобенности работы с текстами6.3.Трансформеры6.4.Графовые нейронные сети6.5.Нейросети для облаков точек7.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Нейросети для работы с последовательностями6.2. Нейросети для работы с последовательностямиАвторыЯнина АнастасияВ этом разделе вы познакомитесь с нейросетями для работы с данными, имеющими вид последовательностей некоторых токенов. Это может быть музыка или видео, временные ряды или траектория движения робота, последовательности аминокислот в белке или много чего ещё, но одним из самых богатых источников таких данных является Natural Language Processing (NLP).\\nКак следует из названия, Natural Language Processing (обработка естественного языка) — это область data science, посвященная анализу текстов, написанных на естественных (человеческих) языках. С задачами обработки текста мы встречаемся каждый день, например, когда просим Siri или Алису включить любимую песню или добавить напоминание в календарь, когда используем автодополнение при вводе поискового запроса или проверяем орфографию и пунктуацию с помощью специальных программ.\\nВот ещё несколько примеров задач, относящихся к обработке естественного языка:\\n\\nклассификация документов (по темам, рубрикам, жанрам и так далее);\\nопределение спама;\\nопределение частей речи;\\nисправление орфографических ошибок и опечаток;\\nпоиск ключевых слов, синонимов / антонимов в тексте;\\nраспознавание именованных сущностей (имен, названий географических объектов, дат, номеров телефонов, адресов);\\nопределение эмоциональной окраски текста (sentiment analysis);\\nпоиск релевантных документов по запросу, а также их ранжирование;\\nзадача суммаризации (автоматическое составление краткого пересказа текста);\\nавтоматический перевод с одного языка на другой (машинный перевод);\\nдиалоговые системы и чат-боты;\\nвопросно-ответные системы (выбор ответа из нескольких предложенных вариантов или вопросы с открытым ответом);\\nкроме того, к NLP также относят задачу распознавания речи (Automated Speech Recognition, ASR).\\n\\nДля работы с такими данными есть несколько возможных режимов:\\n\\nMany-to-one. На вход подается последовательность объектов, на выходе один объект. Пример 2: классификация текстов или видео. Пример 2: тематическая классификация. По предложению нефиксированной длины генерируем вектор вероятностей упоминания заранее фиксированных тем во входном предложении. Размерность выходного вектора постоянна и равна количеству тем.\\nOne-to-many. На вход подается один объект, на выходе последовательность объектов. Пример: генерация заголовка к изображению (image captioning).\\nMany-to-many. На входе и выходе последовательности нефиксированной длины. Примеры: машинный перевод, суммаризация текста, генерация заголовка к статье.\\nСинхронизированный вариант many-to-many. На входе и выходе последовательности одинаковой длины, токены входной явно сопоставлены соответствующим токенам выходной. Пример: генерация покадровых субтитров к видео, PoS-tagging (part of speech tagging, для каждого слова в предложении предсказываем, что это за часть речи).\\n\\nВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nМы начнём с архитектур, в которых размер выхода предсказуемым образом зависит от размера входа: many-to-one и синхронизованном варианте many-to-many — но в итоге доберёмся и до остальных.\\nWord Embeddings\\nПеред тем, как рассказать об архитектурах, которые часто используются для работы с текстами, надо разобраться, каким образом можно кодировать текстовые данные: ведь нужно их превратить во что-то векторное, прежде чем подавать на вход нейросети. К векторизации текстов есть два базовых подхода:\\n\\nвекторизовать текст целиком, превращая его в один вектор;\\nвекторизовать отдельные структурные единицы, превращая текст в последовательность векторов.\\n\\nПервые, статистические подходы к векторизации следовали первому подходу и рассматривали текст как неупорядоченный набор («мешок») токенов (обычно токенами являются слова). Тем самым, тексты «Я не люблю ML» и «Я люблю не ML» получали одинаковые векторизации, то есть по ходу терялась существенная информация. Поэтому мы лишь коротко упомянем о них.\\nНемного о статистических подходахСамый очевидный вариант так и называется — Bag-of-Words («мешок слов»). Текст предлагается представить в виде вектора частот встречаемости каждого токена, кроме элементов заранее заданного списка «стоп-слов», в которые обычно включают самые вездесущие токены: личные местоимения, артикли и так далее.\\nЧуть более усложненной версией является TF-IDF (Term Frequency-Inverted Document Frequency). Этот подход использует не только информацию из текста, но и пытается соотнести её с контекстом — остальными текстами из имеющейся у нас коллекции DDD. Представление текста ddd состоит из произведений TF(t,d)⋅IDF(t,D)TF(t, d)\\\\cdot IDF(t, D)TF(t,d)⋅IDF(t,D) по всем токенам ttt. Разберёмся отдельно с каждым из сомножителей:\\n\\nTF(t,d)=nt∑knkTF(t,d) = \\\\dfrac{n_t}{\\\\sum_k n_k}TF(t,d)=∑k\\u200bnk\\u200bnt\\u200b\\u200b, где ntn_tnt\\u200b — число вхождений токена ttt в документ, а в знаменателе стоит общее число слов в данном документе ddd. Это частота вхождения токена в документ.\\nIDF(t,D)=log\\u2061∣D∣∣{di∈D∣t∈di}∣IDF(t,D) = \\\\log \\\\dfrac{\\\\vert D \\\\vert}{\\\\vert\\\\{d_i \\\\in D \\\\vert t \\\\in d_i\\\\}\\\\vert}IDF(t,D)=log∣{di\\u200b∈D∣t∈di\\u200b}∣∣D∣\\u200b, где {di∈D∣t∈di}\\\\{d_i \\\\in D \\\\vert t \\\\in d_i\\\\}{di\\u200b∈D∣t∈di\\u200b} — число документов в текстовой коллекции DDD, в которых встречается слово ttt. Этот множитель штрафует компоненты, отвечающие слишком распространённым токенам, и повышает вес специфических для отдельных текстов (и, вероятно, информативных) слов.\\n\\nОбратимся теперь к другому подходу и подумаем, как сопоставить векторы (эмбеддинги) словам.\\nДопустим, что у нас одно и то же слово будет представлено одним и тем же вектором во всех текстах и в любых позициях. Как заключить в векторе его смысл, содержающуюся в нём информацию? Ответ предвосхищает одну из основных идей обучения представлений: нужно использовать контекст. Если, читая книгу на иностранном языке, вы встречаете незнакомое слово, вы нередко можете угадать его значение по контексту, что оно значит. Можно сказать, что смысл слова — это те слова, которые встречаются с ним рядом.\\nОдним из воплощений такого подхода является Word2vec. Впервые он был предложен Т.Миколовым в 2013 году в статье Efficient Estimation of Word Representations in Vector Space.\\nДля обучения авторы предложили две стратегии: Skip-gram и CBOW (Сontinuous bag-of-words):\\n\\nВ архитектуре CBOW модель учится предсказывать данное (центральное) слово по контексту (например, по двум словам перед данным и двум словам после него).\\nВ архитектуре Skip-gram модель учится по слову предсказывать контекст (например, каждого из двух соседей слева и справа);\\n\\n\\nАвторы предложили для каждого слова www обучать два эмбеддинга: vu\\\\color{#FFC100}{v_u}vu\\u200b и vw\\\\color{#97C804}{v_{w}}vw\\u200b, первое из которых используется, когда www является центральным, а второе — когда оно рассматривается, как часть контекста. В модели CBOW при фиксированном контексте context\\\\color{#97C804}{\\\\text{context}}context вычисляются логиты\\nlogitsu=⟨∑w∈\\xa0context\\xa0vw,vu⟩logits_u = \\\\langle\\\\sum_{w\\\\in\\\\color{#97C804}{\\\\text{ context }}}\\\\color{#97C804}{v_{w}},\\\\color{#FFC100}{v_u}\\\\rangle\\nlogitsu\\u200b=⟨w∈\\xa0context\\xa0∑\\u200bvw\\u200b,vu\\u200b⟩после чего «вероятности» всевозможных слов uuu быть центральным словом для контекста context\\\\color{#97C804}{\\\\text{context}}context вычисляются как softmax(logits)\\\\text{softmax}(logits)softmax(logits). Модель учится с помощью SGD на кросс-энтропию полученного распределения с истинным рапределением центральных слов.\\n\\nВ модели Skip-gram по данному центральному слову uuu для каждой позиции контекста context\\\\color{#97C804}{\\\\text{context}}context независимо предсказывается распределение вероятностей. В качестве функции потерь выступает сумма кросс-энтропий распределений слов контекста с их истинными распределениями.\\n\\nРазмерность эмбеддинга в каждой из архитектур — это гиперпараметр и подбирается эмпирически. В оригинальной статье предлагается взять размерность эмбеддинга 300. Полученные представления центральных слов могут дальше использоваться в качестве эмбеддингов слов, которые сохраняют семантическую связь слов друг с другом.\\nМы не будем здесь останавливаться подробно на деталях работы Word2vec и его современных модификациях и предложим читателю обратиться к соответствующей лекции в учебнике Лены Войта по NLP. А мы лишь продемонстрируем, что он работает.\\nПримеры. Возьмём несколько слов и посмотрим, как выглядят топ-10 слов, ближайших к ним в пространстве эмбеддингов (обученных на одном из датасетов Quora Questions с помощью word2vec):\\n\\nquantum: electrodynamics, computation, relativity, theory, equations, theoretical, particle, mathematical, mechanics, physics;\\npersonality: personalities, traits, character, persona, temperament, demeanor, narcissistic, trait, antisocial, charisma;\\ntriangle: triangles, equilateral, isosceles, rectangle, circle, choke (догадаетесь, почему?), quadrilateral, hypotenuse, bordered, polygon;\\nart: arts, museum, paintings, painting, gallery, sculpture, photography, contemporary, exhibition, artist.\\n\\nВопрос на подумать. В реальных текстах наверняка будут опечатки, странные слова и другие подобные неприятности. Word2vec же учится для фиксированного словаря. Что делать, если на этапе применения вам попадается неизвестное слово? Да и вообще, хорошо ли учить вложения для редких слов или слов с нетривиальными опечатками, которые, может быть, только раз встретятся в тексте?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Начнём с последнего вопроса: наверное, не очень хорошо. Словарь может получиться слишком большим и займёт всю оперативную память мира. Нередко для достаточно редких слов вовсе не учат специального эмбеддинга, вместо этого для всех них обучая представление одного единственного токена UNK (unknown). В таком случае и всем незнакомым словам, встреченным на этапе применения, также можно сопоставить этот же эмбеддинг.\\nВ реальных сервисах, имеющих дело с текстами (например, в автоматических переводчиках) зачастую вовсе не имеют дела со словами, предпочитая дополнительно разбивать их на subword units. Самым популярным на данный момент решением является BPE (Byte pair encoding). Верхнеуровневая идея состоит в том, что мы фиксируем размер словаря (обычно не очень большой, несколько тысяч или десятков тысяч единиц), добавляем в него все символы, после чего повторяем, пока словарь не заполнится:\\n\\nнаходим самую часто встречающуюся вместе пару токенов;\\nдобавляем их конкатенацию в словарь.\\n\\nБолее подробно о BPE вы можете прочитать в учебнике Лены Войта.\\nВопрос на подумать. В некоторых случаях всё же полезно уметь строить эмбеддинги не отдельных слов, а текстов (например, для поиска похожих документов). Можете ли вы, вдохновившись идеей word2vec, придумать более тонкий способ сделать это, чем BoW или TF-IDF?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Самый простой способ это сделать — сложить или усреднить эмбеддинги отдельных слов, но так мы теряем их порядок, и вместе с ним значительную часть смысла (очень похоже на bag-of-words, не так ли?). Другой подход, развивающий идеи word2vec, был предложен в другой статье Т. Миколова и носит название doc2vec. Он также имеет CBOW-подобную и Skip-gram-подобную версии. Остановимся на первой. Если в оригинальном CBOW мы предсказывали центральное слово по контексту, то теперь добавляется ещё дополнительный вектор vdocument\\\\color{#5002A7}{v_{document}}vdocument\\u200b, уникальный для каждого документа и кодирующий присутствующую в нём смысловую специфику:\\n\\nРекуррентные нейронные сети\\nИтак, мы представили текст в виде последовательности векторов, соответствующих словам или их кусочкам. Как с ней работать? Один из вариантов мы уже рассматривали: можно посмотреть на последовательность из kkk векторов размерности ddd как на «изображение» k×1k\\\\times 1k×1 с ddd «каналами», после чего использовать уже знакомые нам свёрточные нейросети, только с одномерными свёртками вместо двумерных.\\nВ каких-то случаях это действительно будет работать, но всё же есть несколько сомнительных моментов:\\n\\nХотя изображения тоже могут быть разного размера, всё же в датасете редко попадаются рядом картинки 1920×10801920\\\\times 10801920×1080 и 3×33\\\\times 33×3, а среди, скажем, отзывов на ресторан могут попадаться как труды, сопоставимые по размеру с «Войной и миром», так и безликие «Да, вроде норм». И если обработать слишком длинное предложение нам поможет (с потерей информации, конечно) global pooling, слишком короткое может что-нибудь поломать, особенно если мы забываем про паддинг.\\nСлегка философское соображение. Изображение однородно, в нём нет предпочтительных направлений, тогда как текст пишется и читается последовательно. Нам может показаться, что это стоит использовать: при обработке очередного токена обращаться к предыдущим, как к его контексту.\\n\\nВ последнем соображении уже непосредственно видна идея рекуррентных нейронных сетей (recurrent networks, RNN):\\n\\nДавайте разберёмся, что тут происходит. Чтобы хранить информацию о предыдущих токенах, мы вводим понятие внутренней памяти или скрытого состояния (hidden state, векторы hn\\\\color{#5002A7}{h_n}hn\\u200b). В простейшем случае оно выражается одним вектором фиксированной размерности. На каждом (дискретном) шаге в сеть подаются данные (например, эмбеддинг токена), при этом происходит обновление скрытого состояния.\\nПример:\\nhn=tanh(hn−1W1+xnW2)\\\\color{#5002A7}{h_n} = \\\\text{tanh}(\\\\color{#5002A7}{h_{n-1}}\\\\color{#292183}{W_1} +  \\\\color{#97C804}{x_n}\\\\color{#292183}{W_2})\\nhn\\u200b=tanh(hn−1\\u200bW1\\u200b+xn\\u200bW2\\u200b)после чего по скрытому состоянию предсказывается выходной сигнал, к примеру, следующим образом:\\nyn=hnW3\\\\color{#FFC100}{y_n} =  \\\\color{#5002A7}{h_n}\\\\color{#292183}{W_3}\\nyn\\u200b=hn\\u200bW3\\u200bОбратите внимание, что веса Wi\\\\color{#292183}{W_i}Wi\\u200b одинаковы на всех итерациях, то есть вы можете представлять себе, что очередные xn\\\\color{#97C804}{x_n}xn\\u200b и hn−1\\\\color{#5002A7}{h_{n-1}}hn−1\\u200b подаются на вход одного и того же слоя, зацикленного на себе.\\nРекуррентную сеть можно обучать на ошибку, равную суммарному отклонению по всем выходных сигналам yn\\\\color{#FFC100}{y_n}yn\\u200b нашей сети.\\nВопрос на подумать. Как инициализировать веса Wi\\\\color{#292183}{W_i}Wi\\u200b мы, наверное, понимаем (про это можно почитать в параграфе про тонкости обучения нейросетей). А как инициализировать начальное скрытое состояние h0\\\\color{#5002A7}{h_0}h0\\u200b? Можно ли инициализировать его нулём?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Важно разобраться, что в данном случае значит «инициализировать». Сравнение с инициализацией Wi\\\\color{#292183}{W_i}Wi\\u200b не работает, поскольку h0\\\\color{#5002A7}{h_0}h0\\u200b пока не объявлялся обучаемым параметром (хотя его, конечно, можно и обучать, если очень хочется). Можно всегда полагать h0\\\\color{#5002A7}{h_0}h0\\u200b нулём: как во время обучения, так и во время применения. И так действительно можно делать: в конце концов, скрытое состояние хранит информацию о предыдущих элементов последовательности, а до первого шага никакой информации нет.\\nНетрудно представить себе и нейросеть с несколькими рекуррентными слоями: первый слой RNN будет принимать на вход исходную последовательность, вторая RNN — выходы первой сети, третья — выходы второй и т.д. Такие сети называют глубокими рекуррентными сетями.\\nВот пример схему глубокой рекуррентной сети:\\n\\nВы, наверное, заметили, что описанная выше архитектура RNN решает синхронизованную версию задачи many-to-many. Её, впрочем, легко переделать для решения задачи many-to-one: достаточно убрать все выходы, кроме последнего:\\n\\nBidirectional RNN\\nСтандартная RNN учитывает только предыдущий контекст. Но ведь слово в предложении связано не только с предыдущими, но и с последующими словами. В таких случаях имеет смысл использовать двунаправленную рекуррентную сеть (bidirectional RNN, BRNN).\\nКак следует из названия, в bidirectional RNN есть две рекуррентных подсети: прямая (forward, токены в нее подаются от первого к последнему) и обратная (backward, токены подаются в обраттном порядке).\\nВот пример такой архитектуры:\\n\\nКонечно, формула для yn\\\\color{#FFC100}{y_n}yn\\u200b может быть и другой. Например, выходы обеих рекуррентных сетей могут агрегироваться путем усреднения, или суммирования, или любым другим способом.\\nОбратите внимание, что двунаправленная рекуррентная сеть работает с входом фиксированного размера, и по-прежнему не может решать не синхронизованный вариант задачи many-to-many. Backward RNN должна точно знать, где заканчивается входная последовательность, чтобы начать её обрабатывать с конца. Зато такая архитектура может помочь в решении задачи определения именованных сущностей или частей речи, использоваться в качестве энкодера в машинном переводе и так далее.\\nВзрыв и затухание градиента в RNN\\nПри всех неоспоримых плюсах описанной выше глубокой рекуррентной архитектуры, на практике обычно используется её модифицированный вариант, который позволяет бороться с проблемой затухания или зашкаливания (взрыва) градиентов. Давайте разберёмся подробнее, почему она возникает.\\nРассмотрим функцию потерь Ln=L(yn,y^n)\\\\mathcal{L}_n = L(y_n, \\\\hat{y}_n)Ln\\u200b=L(yn\\u200b,y^\\u200bn\\u200b), измеряющую отклонение предсказанного nnn-го выхода от истинного (напомним, что архитектура many-to-many обучается на ∑n=1NLn\\\\sum_{n=1}^N \\\\mathcal{L}_n∑n=1N\\u200bLn\\u200b, а архитектура many-to-one — на LN\\\\mathcal{L}_NLN\\u200b). Выход yny_nyn\\u200b зависит от скрытого состояния hnh_nhn\\u200b, а то, в свою очередь, от всех hi,i<nh_i, i < nhi\\u200b,i<n. Обновление градиента при переходе через преобразование hi=tanh(hi−1W1+xiW2)h_i = \\\\text{tanh}(h_{i-1}W_1 + x_iW_2)hi\\u200b=tanh(hi−1\\u200bW1\\u200b+xi\\u200bW2\\u200b) имеет, как мы хорошо знаем, вид\\n∇hi−1L=(∇hi−1L)W1T⊙tanh′(hi−1W1+xiW2)=\\\\nabla_{h_{i-1}}L = \\\\left(\\\\nabla_{h_{i-1}}L\\\\right)W_1^T\\\\odot \\\\text{tanh}'(h_{i-1}W_1 + x_iW_2) = \\n∇hi−1\\u200b\\u200bL=(∇hi−1\\u200b\\u200bL)W1T\\u200b⊙tanh′(hi−1\\u200bW1\\u200b+xi\\u200bW2\\u200b)=То есть в ходе вычисления ∇W1Ln\\\\nabla_{W_1}\\\\mathcal{L}_n∇W1\\u200b\\u200bLn\\u200b мы (n−1)(n-1)(n−1) раз будем умножать на W1TW_1^TW1T\\u200b. Если у W1TW_1^TW1T\\u200b есть собственные значения, по модулю большие 111, и нам не посчастливится попадать в их окрестность, градиент будет стремиться к бесконечности («взрываться»).\\nТакие градиенты делают обучение нестабильным, а в крайнем случае значения весов могут стать настолько большими, что произойдет численное переполнение, и значения весов перестанут обновляться. Если же у W1TW_1^TW1T\\u200b есть маленькие собственные значения, градиент может затухать. В любом случае, эти проблемы делают получение информации от далеких по времени состояний затруднительным.\\nТеоретические выкладки о том, почему RNN без модификаций не могут достаточно хорошо учитывать долговременные зависимости, появились ещё в 90х. Их можно прочесть в статье Y.Bengio, 1994 или диссертации  Josef Hochreiter, 1991.\\nНо как бороться с этой проблемой?\\nПростым инженерным решением является gradient clipping. Эта техника устанавливает максимально возможное значение градиента и заменяет все значения выше выбранного порога на это значение. При обратном распространении ошибки пробрасывается «ограниченный» градиент:\\n∥∇L∥clip={∥∇L∥,\\xa0if\\xa0∥∇L∥<ττ,\\xa0otherwise\\\\|\\\\nabla \\\\mathcal{L}\\\\|_{clip} =\\n\\\\begin{cases}\\n\\t\\\\|\\\\nabla\\\\mathcal{L}\\\\|,\\\\ \\\\text{if}\\\\ \\\\|\\\\nabla \\\\mathcal{L}\\\\| < \\\\tau \\\\\\\\\\n       \\t\\\\tau,\\\\ \\\\text{otherwise}\\n\\\\end{cases}∥∇L∥clip\\u200b={∥∇L∥,\\xa0if\\xa0∥∇L∥<ττ,\\xa0otherwise\\u200bгде τ\\\\tauτ — гиперпараметр, подбираемый порог.\\nНо сам по себе gradient clipping это довольно грубый инструмент. Поэтому были придуманы сложные модификации рекуррентных сетей, позволяющие им выучивать длинные зависимости.\\nLSTM\\nВдохновение при написании этого параграфа черпалось из статьи в блоге исследователя Кристофера Олаха, из него же взяты иллюстрации.\\nСеть с долговременной и кратковременной памятью (Long short term memory, LSTM) частично решает проблему исчезновения или зашкаливания градиентов в процессе обучения рекуррентных сетей методом обратного распространения ошибки. Эта архитектура была предложена Hochreiter & Schmidhuber в 1997 году. LSTM построена таким образом, чтобы учитывать долговременные зависимости.\\nРассмотрим подробнее архитектуру LSTM.\\nВсе рекуррентные сети можно представить в виде цепочки из повторяющихся блоков. В RNN таким блоком обычно является один линейный слой с гиперболическим тангенсом в качестве функции активации. В LSTM повторяющийся блок имеет более сложную структуру, состоящую не из одного, а из четырех слоев. Кроме скрытого состояния hnh_nhn\\u200b, в LSTM появляется понятие состояния блока (cell state, cnc_ncn\\u200b).\\nCell state cnc_ncn\\u200b будет играть роль внутренней, закрытой информации LSTM-блока, тогда как скрытое состояние hnh_nhn\\u200b теперь становится передаваемым наружу (не только в следующий блок, но и на следующий слой или выход всей сети) значением. LSTM может добавлять или удалять определенную информацию из cell state с помощью специальных механизмов, которые называются gates (ворота или вентили в русскоязычной литературе).\\nРассмотрим этот механизм подробнее.\\nОсновное назначение вентиля — контролировать количество проходящей через него информации. Для этого матрица, проходящая по каналу, который контролирует вентиль, поточечно умножается на выражение вида\\nσ(W1hn−1+W2xn)\\\\sigma(W_1 h_{n-1} + W_2 x_n)\\nσ(W1\\u200bhn−1\\u200b+W2\\u200bxn\\u200b)Сигмоида выдает значение от 000 до 111. Оно означает, какая доля информации сможет пройти через вентиль. Рассмотрим типы гейтов в том порядке, в каком они применяются в LSTM.\\nForget gate (вентиль забывания). Он позволяет на основе предыдущего скрытого состояния ht−1h_{t-1}ht−1\\u200b и нового входа xtx_txt\\u200b определить, какую долю информации из ct−1c_{t-1}ct−1\\u200b (состояния предыдущего блока) стоит пропустить дальше, а какую забыть.\\n\\n\\n\\n    LSTM: вентиль забывания.Источник\\n\\n\\nДоля ftf_tft\\u200b сохраняемой информации из ct−1c_{t-1}ct−1\\u200b вычисляется следующим образом:\\nft=σ(ht−1W1f+xtW2f+bf)f_t = \\\\sigma(h_{t-1} W^f_1 + x_t W^f_2 + b_f)\\nft\\u200b=σ(ht−1\\u200bW1f\\u200b+xt\\u200bW2f\\u200b+bf\\u200b)Дальше ftf_tft\\u200b поэлементно умножается на ct−1c_{t-1}ct−1\\u200b.\\nСледующий шаг — определить, что нового мы внесём в cell state. Для этого у нас есть отличная кандидатура — уже привычное:\\nCt~=tanh(ht−1W1С+xtW2C+bc)\\\\tilde{C_t} = \\\\text{tanh}(h_{t-1}W^С_1 + x_tW^C_2 + b_c)\\nCt\\u200b~\\u200b=tanh(ht−1\\u200bW1С\\u200b+xt\\u200bW2C\\u200b+bc\\u200b)Но мы не уверены, что вся эта информация достаточно релевантна и достойна переноса в cell state, и хотим взять лишь некоторую её долю. Какую именно — поможет узнать наш следующий персонаж.\\nInput gate (вентиль входного состояния). Вычислим\\nit=σ(ht−1W1i+xtW2i+bi)i_t = \\\\sigma(h_{t-1}W^i_1 + x_tW^i_2 + b_i)\\nit\\u200b=σ(ht−1\\u200bW1i\\u200b+xt\\u200bW2i\\u200b+bi\\u200b)\\n\\n\\n    LSTM: вентиль входного состояния. Источник\\n\\n\\nи умножим почленно на ct~\\\\tilde{c_t}ct\\u200b~\\u200b, чтобы получить информацию, которая поступит в cell state от ht−1h_{t-1}ht−1\\u200b и xtx_txt\\u200b. А именно, новое состояние cell state будет равно:\\nct=ft⊙ct−1+it⊙ct~c_t = f_t \\\\odot c_{t-1} + i_t \\\\odot \\\\tilde{c_t}\\nct\\u200b=ft\\u200b⊙ct−1\\u200b+it\\u200b⊙ct\\u200b~\\u200bгде ⊙\\\\odot⊙ — это поэлементное умножение. Первое слагаемое отвечает за «забывание» нерелевантной информации из ct−1c_{t-1}ct−1\\u200b, а второе — за привнесение новой, релевантной.\\n\\n\\n\\n    LSTM: обновление состояния блока. Источник\\n\\n\\nКак мы уже отмечали, роль выходного вектора LSTM-блока будет играть hnh_nhn\\u200b. Он вычисляется по cell state с помощью последнего вентиля.\\nOutput gate (вентиль выходного состояния). Он отвечает на вопрос о том, сколько информации из cell state следует отдавать на выход из LSTM-блока. Доля вычисляется следующим образом:\\not=σ(ht−1W1o+xtW2o+bo)o_t = \\\\sigma(h_{t-1}W^o_1 + x_tW^o_2 + b_o)\\not\\u200b=σ(ht−1\\u200bW1o\\u200b+xt\\u200bW2o\\u200b+bo\\u200b)Теперь пропускаем cell state через гиперболический тангенс, чтобы значения были в диапазоне от −1-1−1 до 111, и умножаем полученный вектор на o_n, чтобы отфильтровать информацию из cell state, которую нужно подать на выход:\\nht=ot⊙tanh(ct)h_t = o_t \\\\odot \\\\text{tanh}(c_t)\\nht\\u200b=ot\\u200b⊙tanh(ct\\u200b)\\n\\n\\n    LSTM: вентиль выходного состояния. Источник\\n\\n\\nОписанная архитектура выглядит несколько сложно. Кроме того, вычисление четырех различных типов гейтов может быть вычислительно невыгодным. Поэтому были разработаны различные вариации LSTM, одна из самых популярных (Gated Recurrent Unit, GRU) освещена ниже.\\nGated Recurrent Unit (GRU)\\nGated Recurrent Unit был предложен в статье Cho et al. в 2014 году. GRU объединяет input gate и forget gate в один update gate, также устраняет разделение внутренней информации блока на hidden и cell state. Вот общий вид GRU-блока:\\n\\n\\n\\n    GRU. Источник\\n\\n\\nВнимательно посмотрев на структуру LSTM, можно заметить, что функции forget gate и input gate похожи. Первый механизм определяет, какие значения ct−1c_{t-1}ct−1\\u200b надо забыть, а второй — какие значения нового вектора ct~\\\\tilde{c_t}ct\\u200b~\\u200b нужно использовать для обновления старого cell state ct−1c_{t-1}ct−1\\u200b. Давайте объединим эти функции воедино: грубо говоря, будем забывать только те значения, которые собираемся обновить. Такую роль в GRU выполняет update gate (ztz_tzt\\u200b):\\nzt=σ(ht−1W1z+xtW2z+bz)z_t = \\\\sigma(h_{t-1}W^z_1 + x_tW^z_2 + b_z)\\nzt\\u200b=σ(ht−1\\u200bW1z\\u200b+xt\\u200bW2z\\u200b+bz\\u200b)Новый тип гейта, который появляется в GRU — reset gate (rtr_trt\\u200b). Он определяет, какую долю информации из ht−1h_{t-1}ht−1\\u200b с прошлого шага надо «сбросить», инициализировать заново.\\nrt=σ(ht−1W1r+xtW2r+br)r_t = \\\\sigma(h_{t-1}W^r_1 + x_tW^r_2 + b_r)\\nrt\\u200b=σ(ht−1\\u200bW1r\\u200b+xt\\u200bW2r\\u200b+br\\u200b)Теперь мы вычисляем потенциальное обновление для скрытого состояния\\nh~n=tanh((rt⊙ht−1)W1h+xtW2h+bh)\\\\tilde{h}_n = tanh((r_t\\\\odot h_{t-1})W^h_1 + x_tW^h_2 + b_{h})\\nh~n\\u200b=tanh((rt\\u200b⊙ht−1\\u200b)W1h\\u200b+xt\\u200bW2h\\u200b+bh\\u200b)и, наконец, решаем, что из старого забыть, а что из нового добавить:\\nht=(1−zt)⊙ht−1+zt⊙h~th_t = (1 - z_t) \\\\odot h_{t-1} + z_t \\\\odot \\\\tilde{h}_t\\nht\\u200b=(1−zt\\u200b)⊙ht−1\\u200b+zt\\u200b⊙h~t\\u200bВ итоге GRU имеет меньше параметров, чем LSTM (в GRU нет output gate) и при прочих равных, быстрее учится. GRU и LSTM показывают сопоставимое качество на многих задачах, включая генерацию музыки, распознавание речи, многие задачи обработки естественного языка.\\nМодификации RNN, которые помогают лучше моделировать долгосрочные зависимости (LSTM, GRU) — важная веха развития нейросетей в NLP. Следующий большой этап в развитии — механизм внимания — мы рассмотрим чуть ниже.\\nSeq2seq\\nВы, должно быть обратили внимание, что мы пока не касались задач, связанных с порождением последовательностей (синхронизованный варианты many-to-many не в счёт).\\nДействительно: имевшиеся у нас пока инструменты не позволяли генерировать последовательности произвольной длины. Но как тогда переводить с одного языка на другой? Ведь мы не знаем, какой должна быть длина перевода фразы, да и однозначного соответствия между словами исходного предложения и его перевода обычно нет.\\nЕстественным решением для задачи sequence-to-sequence (seq2seq) является использование архитектуры энкодер-декодер, состоящей из кодировщика (энкодера) для кодирования информации об исходной последовательности в контекстном векторе (context vector) и декодировщика (декодера) для превращения закодированной энкодером информации в новую последовательность.\\n\\nОчевидным выбором на роль энкодера и декодера являются рекуррентные сети, например, LSTM. Простейшая архитектура будет иметь вид:\\n\\nРассмотрим подробнее энкодер и декодер.\\nЭнкодер читает входное предложение токен за токеном и обрабатывает их с помощью блоков рекуррентной сети. Hidden state последнего блока становится контекстным вектором. Часто энкодер читает предложение в обратном порядке. Это делается для того, чтобы последний токен, который видит энкодер, совпал (или примерно совпал) с первыми токенами, которые будет генерировать декодер. Таким образом, декодеру проще начать процесс воссоздания предложения. Несколько первых правильных токенов сильно упрощают процесс дальнейшей генерации.\\nАрхитектура декодера аналогична энкодеру. При этом каждый блок декодера должен учитывать токены, сгенерированные к текущему моменту, и также информацию о предложении на исходном языке. Вектор скрытого состояния в нулевом блоке декодера (g0g_0g0\\u200b) инициализируется с помощью контекстного вектора.\\nТаким образом, декодер получит сжатое представление исходного предложения. Предложение генерируется следующим образом: в первый блок подаем метку начала последовательности (например, -токен, begin of sentence), на выходе первого блока получаем первый токен новой последовательности, и затем подаем его на вход следующего блока декодера. Повторяем аналогичную процедуру до тех пор, пока не сгенерируется метка конца последовательности (например, , end of sentence) или не будет достигнута максимально возможная длина предложения. Таким образом, декодер работает в режиме языковой модели, генерируя предложение токен за токеном и учитывая предыдущий контекст.\\nРазумеется, энкодер может быть и более сложным. Например, можно использовать многослойную двунаправленную сеть, лишь бы выходом её был один вектор контекста. С декодером сложнее: он должен порождать слова по одному, в одном направлении.\\nДалее мы очень коротко остановимся на нетривиальных моментах обучения и применения такой модели.\\nТонкости применения\\nВ предыдущих разделах мы не останавливались подробно на том, что происходит с выходами yny_nyn\\u200b, но сейчас всё-таки попробуем разобраться. Если мы решаем задачу машинного перевода, то на очередном этапе декодер выдаёт нам условное распределение\\np(yn∣x,y<n)p(y_n\\\\mid x, y_{<n})\\np(yn\\u200b∣x,y<n\\u200b)на словах (или каких-то subword unit, например, BPE), из которого мы будем выбирать самое вероятное слово yny_nyn\\u200b и подавать его на вход следующего блока. Но эта, жадная, стратегия может и подвести. Легко представить себе ситуацию, в которой самое вероятное на данный момент слово приведёт дальше к менее вероятной подпоследовательности:\\n\\nЧтобы справиться с этим, на этапе применения модели используют beam search. В каждый момент времени мы поддерживаем некоторое количество BBB самых вероятных гипотез, на nnn-м шаге пытаясь продолжать все сохранённые, а из продолжений выбирая топ-BBB по метрике\\n∏t=1np(yt∣x,y<t)\\\\prod_{t=1}^np(y_t\\\\mid x, y_{<t})\\nt=1∏n\\u200bp(yt\\u200b∣x,y<t\\u200b)\\nЧисло BBB нет смысла делать большим (это и вычислительно будет тяжко, и может привести к более плохим результатам), можете брать в пределах 101010.\\nТонкости обучения\\nКак уже было сказано выше, на каждом шаге декодер предсказывает распределение вероятностей p(yn∣x,y<n)p(y_n\\\\mid x, y_{<n})p(yn\\u200b∣x,y<n\\u200b). Вся модель учится на сумму по всем nnn кросс-энтропиям этих распределений с истинными yny_nyn\\u200b.\\nОдна из сложностей такого обучения состоит в том, что единожды ошибившись и предсказав неправильный y^n\\\\widehat{y}_ny\\u200bn\\u200b вместо истинного yny_nyn\\u200b, модель скорее всего и следующие токены предскажет неверно, а это сделает всё дальнейшее обучение малополезным: ведь мы будем учить декодер предсказывать правильное продолжение неправильного начала. Одним из способов борьбы с этим является teacher forcing. Суть его в том, что на этапе обучения мы подаём на вход декодера не предсказанный им на предыдущем этапе токен, а истинный:\\n\\nА как же one-to-many?\\nУ нас остался лишь один неразобранный тип задач: one-to-many. К счастью, чтобы с ним справиться, ничего нового не нужно: достаточно уже знакомой модели энкодер-декодер, лишь с корректировкой энкодера.\\nРассмотрим для примера задачу генерации подписей к изображениям (image captioning). Если мы уже умеем как-то превращать картинки в векторы, то эти векторы мы можем напрямую подавать в декодер в качестве векторов контекста:\\n\\nБолее подробно о том, как строить векторизации для изображений, вы узнаете в параграфе про обучение представлений.\\nА если у вас есть все данные мира, то вы можете в качестве энкодера взять свёрточную нейросеть и обучать её вместе с декодером end-to-end:\\n\\nМеханизм внимания (attention)\\nКак человек переводит предложения с одного языка на другой? Обычно переводчик уделяет особое внимание слову, которое записывает в данный момент. Хочется сообщить аналогичную интуицию нейронным сетям. Рассмотрим, как можно реализовать такой механизм на примере машинного перевода.\\nВнимательно посмотрим на seq2seq модель для машинного перевода. Вся информация о предложении на исходном языке заключена в контекстном векторе, но разные слова в предложении могут иметь разную смысловую значимость и следовательно, должны учитываться с разными весами. Кроме того, при генерации разных частей перевода следует обращать внимание на разные части исходного предложения. Например, первое слово переведенной фразы нередко связано с первыми словами в предложении, поданном на вход энкодеру, а порой одно слово перевода передаёт смысл нескольких слов, разбросанных по исходному предложению (вдруг кто-нибудь сталкивался с отделяемыми приставками в немецком?).\\nМеханизм внимания (attention) реализует эту интуицию путем предоставления декодеру информации обо всех токенах исходного предложения на каждом шаге генерации. Рассмотрим классическую модель внимания, предложенную Bahdanau et al. в 2014 году.\\nОбозначим скрытые состояния энкодера (h0,h1,…,hn)(h_0, h_1, …, h_n)(h0\\u200b,h1\\u200b,…,hn\\u200b), а скрытые состояния декодера (s0,s1,…,sm)(s_0, s_1, …, s_m)(s0\\u200b,s1\\u200b,…,sm\\u200b). Важно отметить, что hn=s0h_n = s_0hn\\u200b=s0\\u200b, это контекстный вектор. На каждом шаге декодера будем считать attention scores, умножая sis_isi\\u200b на вектор скрытого состояния каждого блока энкодера  (h0,h1,…,hn)(h_0, h_1, …, h_n)(h0\\u200b,h1\\u200b,…,hn\\u200b). Таким образом, получаем nnn значений, указывающих, насколько каждый из токенов c номерами (0...n)(0...n)(0...n) из исходного предложения важен для генерации токена iii из перевода:\\nei=[⟨si,h0⟩,⟨si,h1⟩,…,⟨si,hn⟩]=e_i = [\\\\langle s_i, h_0\\\\rangle, \\\\langle s_i, h_1\\\\rangle, …, \\\\langle s_i, h_n\\\\rangle] =\\nei\\u200b=[⟨si\\u200b,h0\\u200b⟩,⟨si\\u200b,h1\\u200b⟩,…,⟨si\\u200b,hn\\u200b⟩]==[sih0T,…,sihnT]=[s_ih_0^T,\\\\ldots,s_ih_n^T]\\n=[si\\u200bh0T\\u200b,…,si\\u200bhnT\\u200b](здесь sis_isi\\u200b и hjh_jhj\\u200b, как обычно, являются строками, так что sihjTs_ih_j^Tsi\\u200bhjT\\u200b — скаляр).\\nТеперь превращаем эти значения в attention distribution, применив к ним softmax:\\nαi=softmax(ei)\\\\alpha_i = \\\\text{softmax}(e_i)\\nαi\\u200b=softmax(ei\\u200b)Используем αi\\\\alpha_iαi\\u200b в качестве весов для нахождения окончательного вектора внимания aia_iai\\u200b:\\nai=∑j=0nαjhja_i = \\\\sum_{j=0}^{n}\\\\alpha_jh_j\\nai\\u200b=j=0∑n\\u200bαj\\u200bhj\\u200bТеперь в декодере на шаге i вместо вектора скрытого состояния (h0,h1,...,hn)(h_0, h_1, ..., h_n)(h0\\u200b,h1\\u200b,...,hn\\u200b) будем использовать вектор [si,ai][s_i, a_i][si\\u200b,ai\\u200b] -- конкатенацию скрытого состояния блока и соответствующего attention вектора. Таким образом, на каждом шаге декодер получает информацию о важности всех токенов входного предложения. Данная схема вычисления attention представлена на следующем рисунке.\\n\\n\\n\\n    Вычисление attention в seq2seq модели\\n  \\n\\nСуществует много разных видов механизмов внимания, например:\\n\\nБазовый dot-product, рассмотренный ранее: ei=[sihjT]j=0ne_i = [s_ih_j^T]_{j=0}^nei\\u200b=[si\\u200bhjT\\u200b]j=0n\\u200b\\nМультипликативный: ei=[siWhjT]j=0ne_i = [s_iWh_j^T]_{j=0}^nei\\u200b=[si\\u200bWhjT\\u200b]j=0n\\u200b, где WWW — обучаемая матрица весов.\\nMLP: eij=tanh(hjW1+siW2)ve_{ij} = \\\\text{tanh}(h_jW_1 + s_iW_2) veij\\u200b=tanh(hj\\u200bW1\\u200b+si\\u200bW2\\u200b)v, где W1W_1W1\\u200b, W2W_2W2\\u200b — обучаемые матрицы весов, vvv — обучаемый вектор весов\\n\\nВажной особенностью механизма внимания является то, что его веса несут в себе информацию о связях слов в двух языках, участвующих в переводе. Визуализировав веса механизма внимания, получаем таблицу взаимосвязей между словами:\\n\\n\\n\\n    Пример визуализации весов attention\\n  \\n\\nВесьма логично, что слово dogs теснее всего связано со словом собак, а слову очень соответствуют целых два слова: very и much.\\nSelf-attention\\nВ предыдущем разделе мы обсуждали применение механизма внимания во время работы декодера, но оказывается, что и энкодеру это может быть полезно.\\nМеханизм внутреннего внимания (self-attention) используется, чтобы посмотреть на другие слова во входной последовательности во время кодирования конкретного слова. Изначально этот механизм был представлен в статье Attention is all you need как элемент архитектуры «трансформер» (Transformer).\\nЭффективность трансформера демонстировалась на примере задачи машинного перевода. Сейчас трансформеры и self-attention обрели огромную популярность и используются не только в NLP, но и в других областях (например, в компьютерном зрении: Vision Transformer, Video Transformer, Multimodal Transformer for Video Retrieval и так далее).\\nБолее подробный обзор архитектуры Трансформер оставим курсу Лены Войта по NLP, а пока остановимся на механизме внутреннего внимания. Пусть на вход нейросети пришли два предложения «Мама мыла раму. Она держала в руках тряпку». Местоимение «она» относится к маме или к раме? Для человека это очень простой вопрос, но для модели машинного обучения — нет. Self-attention помогает выучить взаимосвязи между токенами в последовательности, моделируя «смысл» других релевантных слов в последовательности при обработке текущего токена.\\nЧто происходит внутри self-attention-модуля? Для начала, из входного вектора (например, эмбеддинга каждого токена) формируются три вектора: Query (запрос), Key (ключ) и Value (значение). Они получаются с помощью умножения входного вектора на матрицы WQW_QWQ\\u200b, WKW_KWK\\u200b и WVW_VWV\\u200b, веса которых учатся вместе со всеми остальными параметрами модели с помощью обратного распространения ошибки.\\nВыделение этих трех абстракций нужно, чтобы разграничить эмбеддинги, задающие «направление» внимания (query, key) и смысловую часть токена (value). Вектор query задает модальность «начальной точки» механизма внутреннего внимания (от какого токена направлено внимание), вектор key — модальность «конечной точки» (к какому токену направлено внимание). Таким образом, один и тот же токен может выступать как «начальной», так и «конечной» точкой направления внимания: self-attention вычисляется между всеми токенами в выбранном фрагменте текста.\\nПроцесс происходит так: по очереди фиксируется каждый токен (становится query) и просчитывается степень его связанности со всеми оставшимися токенами. Для этого поочередно key-вектора всех токенов скалярно умножаются на query-вектор текущего токена. Полученные числа будут показывать, насколько важны остальные токены при кодировании query токена в конкретной позиции.\\nДальше полученные числа надо нормализовать и пропустить через софтмакс, чтобы получить распределение. Затем подсчитывается взвешенная сумма value векторов,где в качестве весов используются полученные на предыдущем шаге вероятности. Полученный вектор и будет выходом слоя внутреннего внимания для одного токена. Изложенную выше схему вычисления self-attention вектора для одного токена можно представить простой схемой:\\n\\n\\n\\n    Вычисление self-attention для одного токена. Источник\\n\\n\\nНа практике self-attention не вычисляется для каждого токена по отдельности, вместо этого используются матричные вычисления. Например, вместо вычисления query, key и value векторов для каждого токена, настакаем эмбеддинги входных токенов в матрицу XXX и посчитаем матрицы Q=WQ∗XQ = W_Q * XQ=WQ\\u200b∗X, K=WK∗XK = W_K * XK=WK\\u200b∗X и V=WV∗XV = W_V * XV=WV\\u200b∗X.\\nЗатем происходит повторение описанных в предыдущем абзаце шагов, только для матриц. Посчитаем итоговую матрицу ZZZ, подав матрицы Q, K и V в формулу:\\nZ=softmax(QK˙Tnorm\\xa0const)V˙Z=softmax(\\\\frac{Q\\\\dot{K}^T}{\\\\text{norm const}})\\\\dot{V}\\nZ=softmax(norm\\xa0constQK˙T\\u200b)V˙В оригинальной статье Vaswani et al., 2017 в качестве нормализующей константы выбрали число 8 (квадратный корень размерности key-векторов). Нормализация приводила к более стабильным градиентам в процессе обучения.\\nИнтересно, что обычно используют параллельно несколько self-attention блоков. Такая схема называется multi-head self-attention. Вычисление self-attention происходит несколько раз с разными матрицами весов, затем полученные матрицы конкатенируются и умножаются на еще одну матрицу весов WOW_OWO\\u200b (см. схему).\\nЭто позволяет разным self-attention головам фокусироваться на разных взаимосвязях, например, одна голова может отвечать за признаковые описания, другая за действия, третья за отношения «объект-субъект». Разные головы могут вычисляться параллельно, при этом входная матрица эмбеддингов отображается в разные подпространства представлений, что значительно обогащает возможности внутреннего внимания моделировать взаимосвязи между словами. В виде формулы вычисление multihead self-attention можно представить так:\\nMultiHead(Q,K,V)=Concat(head1,head2,...,headn)W˙OMultiHead(Q,K,V) = Concat(head_1, head_2, ..., head_n) \\\\dot W^OMultiHead(Q,K,V)=Concat(head1\\u200b,head2\\u200b,...,headn\\u200b)W˙O,\\nгде headi(Q,K,V)=softmax(QK˙Tnorm\\xa0const)V˙head_i(Q,K,V)=softmax(\\\\frac{Q\\\\dot{K}^T}{\\\\text{norm const}})\\\\dot{V}headi\\u200b(Q,K,V)=softmax(norm\\xa0constQK˙T\\u200b)V˙\\n\\n\\n\\n    Схема вычисления multi-head self-attention.  Источник\\n\\n\\nЕсть много реализаций self-attention (PyTorch, TensorFlow). Также советуем ознакомиться с jupyter-ноутбуком от Гарвардской NLP-группы, в котором представлена реализация архитектуры «трансформер» с подробными объяснениями. Еще один отличный источник, позволяющий подробнее разобраться с self-attention и трансформером, - это статья Jay Alammar под названием «Illustrated Transformer».\\nОсобенности работы с текстами\\nПредобработка текстов\\nПеред тем, как применять описанные выше архитектуры (или даже использовать простые подходы, вроде TF-IDF или word2vec),  нужно разобраться, как делать предобработку текстов.\\nПервым делом надо научиться представлять связный текст в виде последовательности. Для начала имеет смысл разбить текст на предложения, а дальше уже на слова или символьные n-граммы. Этот процесс называется токенизацией. Можно делать токенизацию вручную, например, с помощью регулярных выражений, или воспользоваться готовыми методами из библиотеки NLTK.\\nПредставим, что мы получили упорядоченный список слов, из которых состоит текст. Но это еще не все. Обычно тексты содержат разные грамматические формы одного и того же слова. Привести все словоформы к начальной форме можно с помощью лемматизации.  Лемматизация - это алгоритм приведения слова к его начальной форме с использованием морфологическего анализа и знаний об особенностях конкретного языка.\\nПример работы лемматизатора:\\n«собаки, собака, с собакой, собаками -> собака»\\nДругой способ приведения всех словоформ к одной форме - это стемминг. Стемминг — это более грубый процесс на основе эвристик, который действует без знания контекста, словарей и морфологии. Стеммер не поймет, что слова с чередованием имеют один и тот же корень (только если прописать в явном виде такую эвристику) или что слова «есть», «буду» и «был» - это формы глагола «быть». Стемминг - менее аккуратный процесс по сравнению с лемматизацией, зато гораздо более быстрый.\\nЕще один важный этап предобработки текстов - это удаление стоп-слов. Стоп-словами называют междометия, союзы, предлоги, артикли, в общем все слова, которые будут вносить шум в работу алгоритма машинного обучения. Иногда дополнительно убирают слова общей лексики, оставляя только специфические термины. Универсального списка слов не существует, но для начала можно использовать список стоп-слов из библиотеки NLTK.\\nАугментации для текстов\\nАугментации данных часто используются, чтобы увеличить количество данных в обучающей выборке, а также повысить обобщаемость модели. И если для компьютерного зрения аугментации относительно простые и могут выполняться на лету (масштабирование, обрезка, вращение, добавление шума и т.д.), то для текстов в виду грамматической структуры, синтаксиса и особенностей языка все не так просто.\\nАугментации текста менее «автоматические», в идеале нужно понимать смысл фразы и иметь под рукой отлично работающий механизм перефразирования. Рассмотрим несколько популярных способов аугментации текстовых данных:\\n\\nОбратный перевод. Переводим исходный текст на какой-то язык, и затем переводим его обратно. Это помогает сохранить контекст, но при этом получить синонимичную формулировку.\\nЗамены слова на синонимичное/близкое по смыслу. Для этого можно использовать словари синонимов либо искать близкое слово в пространстве эмбеддингов, минимизируя расстояние между соответствующими векторами. В качестве таких эмбеддингов можно взять привычный word2vec, fasttext или контекстуализированные эмбеддинги на основе претренированных моделей (BERT, ELMO, GPT-2/GPT-3 и так далее).\\nВставка синонима слова в случайное место в предложении.\\nЗамена сокращения на полное наименование и обратно. Для английского языка этот способ более актуален, чем для русского.\\nСлучайная вставка/удаление/замена/перемена местами слов в предложении.\\nСлучайная перестановка местами предложений.\\nСлучайное изменение букв на произвольные/ближайшие на клавиатуре, добавление/исправление орфографических и пунктуационных ошибок, изменение регистра.\\nMixUp для текстов. В задаче классификации смешиваем признаковые описания двух объектов и с такими же весами смешиваем их метки классов, получаем новый объект с признаками xijx_{ij}xij\\u200b и меткой класса yijy_{ij}yij\\u200b:\\n\\nxij=λxi+(1−λ)xjx_{ij} = \\\\lambda x_i + (1 - \\\\lambda) x_j\\nxij\\u200b=λxi\\u200b+(1−λ)xj\\u200byij=λyi+(1−λ)yjy_{ij} = \\\\lambda y_i + (1 - \\\\lambda) y_j\\nyij\\u200b=λyi\\u200b+(1−λ)yj\\u200bДля текстов признаковые описания можно смешивать на уровне слов (выбирать ближайшее слово в пространстве word embeddings) или на уровне предложений. Еще один вариант: сэмплировать слова из двух разных текстов с вероятностями λ\\\\lambdaλ и 1−λ1-\\\\lambda1−λ.\\n9. Аугментации с использованием синтаксического дерева предложения.\\n10. Генерация текста языковыми моделями. Например, генерация текста с помощью упоминавшейся ранее модели GPT-3.\\nПодробнее про некоторые методы аугментации текстов можно почитать в статье Easy Data Augmentation (EDA). Многие из описанных выше и в статье методов реализованы в библиотеке NLPAug, использование которой сильно упрощает задачу аугментации текстовых данных на практике.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф6.1. Свёрточные нейросетиСледующий параграф6.3. ТрансформерыЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_3.html', 'title': 'Первые шаги'}, page_content='Первые шагиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/31.1.Об этой книге1.2.Первые шагиРабочее окружение для ML-специалистаSaaS-платформыПолучение доступа и настройка DataSphereЛабораторная работаПолезные ссылки1.3.Машинное обучение2.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Первые шаги1.2. Первые шагиАвторыМарина КошелеваДаниил ЕфимовДмитрий СошниковДмитрий РыбалкоАртем МакеенокВ этой главе мы поговорим о рабочем окружении ML-специалиста — какие сервисы и библиотеки в него входят, как его развернуть, на что обратить внимание.\\xa0\\nА кроме того, в качестве быстрой практики обучим собственную модель генерировать ответы в стиле Льва Толстого.Рабочее окружение для ML-специалиста\\nГрубо говоря, оно делится на две большие категории:\\n\\nЖелезо и вычислительные ресурсы для обучения моделей;\\nПрограммы и библиотеки для работы с данными.\\n\\nНачнём со второй категории.\\nЧтобы начать работу, нужно установить Python — именно этот язык программирования доминирует в индустрии, благодаря большому количеству библиотек и фреймворкам, предназначенным именно для машинного обучения — TensorFlow или PyTorch.\\nФреймворк — слой абстракции над языком программирования, который облегчает разработку. Например — нам нужно сделать отверстие в доске. Эту задачу можно решить разными способами: закрутить и выкрутить шуруп, забить и вытащить гвоздь и так далее. А можно взять дрель и сверло. Дрель в этом примере и есть фреймворк.\\nPyTorch чаще выбирают для академических исследований — он более гибкий и больше подходит для экспериментов. TensorFlow — для продакшен-решений, поскольку он более подходит для масштабирования моделей.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nДалее нужно установить библиотеки для Python. Продолжая строительную аналогию: библиотека — это насадка для дрели. То есть инструмент для конкретной задачи: можно установить сверло для дерева, для бетона, для металла, а можно коронку или щётку — зависит от задачи.\\nЧаще всего применяют:\\n\\nScikit-learn — библиотека машинного обучения для классических алгоритмов: классификации, регрессии, ансамблей и других. О них мы подробнее поговорим далее в этом хендбуке.\\nPandas — библиотека для предварительной обработки данных, и работы с данными вообще. С её помощью можно загрузить датасет, обработать недостающие значения, закодировать категориальные переменные и многое другое.\\nMatplotlib и Seaborn — библиотеки для создания визуализаций и графиков в Python.\\n\\nПосле этого — выбрать IDE, то есть текстовый редактор для кода: Visual Studio Code, Jupyter, Sublime, PyCharm, и так далее.\\nТеоретически, всё это можно установить на домашний компьютер или ноутбук — именно так и делали ещё 15-20 лет назад. Но вам не хватит вычислительных ресурсов для обучения моделей, в первую очередь — объёма памяти GPU (видеокарты).\\nДаже для файнтюнинга небольших языковых моделей, таких как BERT, необходим графический процессор с минимум 16 Гб видеопамяти. Мало кто может позволить себе дома оборудование для обучения более сложных моделей.\\nСейчас исследователи и студенты чаще берут вычислительные мощности в аренду. Тут есть два способа:\\n\\nарендовать устройство «в облаке» (эта модель называется IaaS),\\nвоспользоваться специальной платформой для ML (эта модель называется SaaS).\\n\\nIaaS-сервис, грубо говоря, — очень мощный удалённый компьютер. Это значит, что прежде чем решать задачу на такой машине, её всё равно необходимо настроить: развернуть IDE, установить Python, фреймворки и библиотеки и многое другое. Это не всегда удобно: иногда хочется, чтобы всё работало «из коробки».\\n«Из коробки», как вы могли догадаться, работают SaaS-сервисы: они предоставляют полностью настроенные среды, готовые к немедленному использованию в решении задач.\\nЭти платформы обычно включают в себя:\\n\\nIDE или другие среды программирования, часто представленные в формате ноутбуков.\\nЗаранее настроенные рабочие окружения, оптимизированные для конкретной системы.\\nВозможности для загрузки и хранения данных и файлов.\\nИнтеграцию с известными сервисами, такими как GitHub.\\n\\nО них мы и поговорим далее. Но если вам ближе путь самурая — то вот несколько IaaS провайдеров. По ссылкам можно узнать, как развернуть окружение для ML в IaaS-сервисе.\\nSaaS-платформы\\nКак мы уже выяснили, главное преимущество SaaS – это простота входа: вы получаете доступ к необходимым ресурсам без забот о их настройке и оптимизации, что позволяет быстро приступить к работе над ML-задачами.\\nК популярным SaaS-платформам относят:\\n\\nGoogle Colab\\nKaggle Notebooks\\nAWS SageMaker\\nAzure ML Studio\\nYandex DataSphere\\n\\nНиже мы собрали в таблицу их возможности, плюсы и минусы.\\n\\nДалее мы расскажем, как решать ML-задачи на примере Yandex DataSphere. Но если вас заинтересовали другие платформы, то в конце параграфа будет список ссылок на руководства по работе с ними.\\nПолучение доступа и настройка DataSphere\\nПрежде чем мы начнём настройку — несколько важных моментов.\\nDataSphere — это платный сервис, но вы можете начать работу бесплатно, с помощью тестового гранта. Также у сервиса есть специальные гранты для учебных программ. Чтобы воспользоваться грантом, нужно попросить своего преподавателя заполнить форму, — это откроет доступ к сервису для всех студентов группы.\\nОтлично, теперь можем приступить к настройке. Для этого:\\n\\nПерейдите на сайт DataSphere\\nНажмите большую синюю кнопку и авторизуйтесь в Яндекс ID\\nСоздайте сообщество и нажмите «Привязать платежный аккаунт»\\nна появившемся красном дисклеймере.\\n\\nВ созданном сообществе вы сможете взаимодействовать со всеми важными сущностями в DataSphere. Теперь можно создать проект на вкладке «Проект».\\n\\nВ созданном проекте вы можете запустить JupyterLab:\\n\\nПосле незначительного ожидания откроется выбор среды исполнения. Там вы можете выбрать любой из примеров ноутбуков с различными снипеттами кода под разные задачи. Создадим новый пустой ноутбук, нажав “DataSphere Kernel”.\\n\\nТеперь, в появившемся новом ноутбуке, если мы запустим любой код в одной из ячеек, вам будет предложено выбрать конфигурацию виртуального рабочего места (более подробно о доступных конфигурациях можно почитать тут).\\nПосле выделения ресурсов, которое тоже займет небольшое количество времени, все последующие выполнения ячеек будут происходить без выбора конфигурации.\\nТеперь, когда у нас всё готово — DataSphere настроена, ресурсы выделены, можем выполнить тестовую лабораторную работу!\\nЛабораторная работа\\nВ ней мы будем обучать генеративную трансформерную модель с помощью библиотеки transformers.\\nСама работа находится в DataSphere — переходите по ссылке, чтобы ознакомиться с заданием. А как закончите —  возвращайтесь, чтобы завершить урок.\\nВот и всё! Если вы читаете эти строки, и у вас всё получилось — вы большой молодец. Если не получилось — ничего страшного, с первого раза мало у кого всё получается. Советуем вступить в сообщество хендбука и попросить помощи или совета.\\nПолезные ссылки\\n\\nРуководство по работе с Google Colab.\\nГайд для новичков по Kaggle Notebooks.\\nРуководство для AWS SageMaker.\\nДокументация по настройке Azure ML Studio\\nСтатья про то, как используется DataSphere в образовании\\nКак DataSphere помогает изучать снежных барсов\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф1.1. Об этой книгеЭта книга написана коллективом добрых людей, состоящим из\\xa0преподавателей и\\xa0выпускников Школы анализа данныхСледующий параграф1.3. Машинное обучениеЧто такое машинное обучение и каким оно бывает. Основные понятия машинного обучения: признаки, таргеты, метрики, переобучениеМожет ли машинное обучение спасать барсов?Да, если создать нейросеть для обработки данных с фотоловушек, которая поможет учёным обработать тысячи фотографий со 170 фотоловушек из Сайлюгемского заповедника.Узнать, как изучают снежных барсовЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_48.html', 'title': 'Bias-variance decomposition'}, page_content='Bias-variance decompositionЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/112.1.Bias-variance decompositionВывод разложения bias-variance для MSEBias-variance trade-off: в каких ситуациях он применимСписок литературы13.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Bias-variance decomposition12.1. Bias-variance decompositionАвторы Елистратова ЕвгенияКлассический взгляд на\\xa0то, почему слишком сложные модели переобучаютсяВ данном параграфе мы изучим инструмент, который позволяет анализировать ошибку алгоритма в зависимости от некоторого набора факторов, влияющих на итоговое качество его работы. Этот инструмент в литературе называется bias-variance decomposition — разложение ошибки на смещение и разброс. В разложении, на самом деле, есть и третья компонента — случайный шум в данных, но ему не посчастливилось оказаться в названии. Данное разложение оказывается полезным в некоторых теоретических исследованиях работы моделей машинного обучения, в частности, при анализе свойств ансамблевых моделей.\\nНекоторые картинки в тексте кликабельны. Это означает, что они были заимствованы из какого-то источника и при клике вы сможете перейти к этому источнику.\\nВывод разложения bias-variance для MSE\\nРассмотрим задачу регрессии с квадратичной функцией потерь. Представим также для простоты, что целевая переменная yyy — одномерная и выражается через переменную xxx как:\\ny=f(x)+ε,    y = f(x) + \\\\varepsilon,\\ny=f(x)+ε,где fff — некоторая детерминированная функция, а ε\\\\varepsilonε — случайный шум со следующими свойствами:Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nEε=0,\\u2009Varε=Eε2=σ2.     \\\\mathbb{E} \\\\varepsilon = 0, \\\\, \\\\mathbb{V}\\\\text{ar} \\\\varepsilon = \\\\mathbb{E} \\\\varepsilon^2 = \\\\sigma^2.\\nEε=0,Varε=Eε2=σ2.В зависимости от природы данных, которые описывает эта зависимость, её представление в виде точной f(x)f(x)f(x) и случайной ε\\\\varepsilonε может быть продиктовано тем, что:\\n\\n\\nданные на самом деле имеют случайный характер;\\n\\n\\nизмерительный прибор не может зафиксировать целевую переменную абсолютно точно;\\n\\n\\nимеющихся признаков недостаточно, чтобы исчерпывающим образом описать объект, пользователя или событие.\\n\\n\\nФункция потерь на одном объекте xxx равна\\nMSE=(y(x)−a(x))2    MSE = (y(x) - a(x))^2\\nMSE=(y(x)−a(x))2Однако знание значения MSE только на одном объекте не может дать нам общего понимания того, насколько хорошо работает наш алгоритм. Какие факторы мы бы хотели учесть при оценке качества алгоритма? Например, то, что выход алгоритма на объекте xxx зависит не только от самого этого объекта, но и от выборки XXX, на которой алгоритм обучался:\\nX=((x1,y1),…,(xℓ,yℓ))    X = ((x_1, y_1), \\\\ldots, (x_\\\\ell, y_\\\\ell))\\nX=((x1\\u200b,y1\\u200b),…,(xℓ\\u200b,yℓ\\u200b))a(x)=a(x,X)    a(x) = a(x, X)\\na(x)=a(x,X)Кроме того, значение yyy на объекте xxx зависит не только от xxx, но и от реализации шума в этой точке:\\ny(x)=y(x,ε)    y(x) = y(x, \\\\varepsilon)\\ny(x)=y(x,ε)Наконец, измерять качество мы бы хотели на тестовых объектах xxx — тех, которые не встречались в обучающей выборке, а тестовых объектов у нас в большинстве случаев более одного. При включении всех вышеперечисленных источников случайности в рассмотрение логичной оценкой качества алгоритма aaa кажется следующая величина:\\nQ(a)=ExEX,ε[y(x,ε)−a(x,X)]2    Q(a) = \\\\mathbb{E}_x \\\\mathbb{E}_{X, \\\\varepsilon} [y(x, \\\\varepsilon) - a(x, X)]^2\\nQ(a)=Ex\\u200bEX,ε\\u200b[y(x,ε)−a(x,X)]2Внутреннее матожидание позволяет оценить качество работы алгоритма в одной тестовой точке xxx в зависимости от всевозможных реализаций XXX и ε\\\\varepsilonε, а внешнее матожидание усредняет это качество по всем тестовым точкам.\\nЗамечание. Запись EX,ε\\\\mathbb{E}_{X, \\\\varepsilon}EX,ε\\u200b в общем случае обозначает взятие матожидания по совместному распределению XXX и ε\\\\varepsilonε. Однако, поскольку XXX и ε\\\\varepsilonε независимы, она равносильна последовательному взятию матожиданий по каждой из переменных: EX,ε=EXEε\\\\mathbb{E}_{X, \\\\varepsilon} = \\\\mathbb{E}_{X} \\\\mathbb{E}_{\\\\varepsilon}EX,ε\\u200b=EX\\u200bEε\\u200b, но последний вариант выглядит несколько более громоздко.\\nПопробуем представить выражение для Q(a)Q(a)Q(a) в более удобном для анализа виде. Начнём с внутреннего матожидания:\\nEX,ε[y(x,ε)−a(x,X)]2=EX,ε[f(x)+ε−a(x,X)]2=    \\\\mathbb{E}_{X, \\\\varepsilon} [y(x, \\\\varepsilon) - a(x, X)]^2 = \\\\mathbb{E}_{X, \\\\varepsilon}[f(x) + \\\\varepsilon - a(x, X)]^2 = \\nEX,ε\\u200b[y(x,ε)−a(x,X)]2=EX,ε\\u200b[f(x)+ε−a(x,X)]2==EX,ε[(f(x)−a(x,X))2⏟не\\xa0зависит\\xa0от\\xa0ε+2ε⋅(f(x)−a(x,X))⏟множители\\xa0независимы+ε2]=    = \\\\mathbb{E}_{X, \\\\varepsilon} [ \\\\underbrace{(f(x) - a(x, X))^2}_{\\\\text{не зависит от $\\\\varepsilon$}} + \\n       \\\\underbrace{2 \\\\varepsilon \\\\cdot (f(x) - a(x, X))}_{\\\\text{множители независимы}} + \\\\varepsilon^2 ] = \\n=EX,ε\\u200b[не\\xa0зависит\\xa0от\\xa0ε(f(x)−a(x,X))2\\u200b\\u200b+множители\\xa0независимы2ε⋅(f(x)−a(x,X))\\u200b\\u200b+ε2]==EX[(f(x)−a(x,X))2]+2Eε[ε]⏟=0⋅EX(f(x)−a(x,X))+Eεε2=    = \\\\mathbb{E}_X \\\\left[\\n        (f(x) - a(x, X))^2\\n    \\\\right] + 2 \\\\underbrace{\\\\mathbb{E}_\\\\varepsilon[\\\\varepsilon]}_{=0} \\\\cdot \\\\mathbb{E}_X (f(x) - a(x, X)) + \\\\mathbb{E}_\\\\varepsilon \\\\varepsilon^2 =\\n=EX\\u200b[(f(x)−a(x,X))2]+2=0Eε\\u200b[ε]\\u200b\\u200b⋅EX\\u200b(f(x)−a(x,X))+Eε\\u200bε2==EX[(f(x)−a(x,X))2]+σ2    = \\\\mathbb{E}_X \\\\left[ (f(x) - a(x, X))^2 \\\\right] + \\\\sigma^2\\n=EX\\u200b[(f(x)−a(x,X))2]+σ2Из общего выражения для Q(a)Q(a)Q(a) выделилась шумовая компонента σ2\\\\sigma^2σ2. Продолжим преобразования:\\nEX[(f(x)−a(x,X))2]=EX[(f(x)−EX[a(x,X)]+EX[a(x,X)]−a(x,X))2]=    \\\\mathbb{E}_X \\\\left[ (f(x) - a(x, X))^2 \\\\right] = \\\\mathbb{E}_X \\\\left[\\n        (f(x) - \\\\mathbb{E}_X[a(x, X)] + \\\\mathbb{E}_X[a(x, X)] - a(x, X))^2 \\n    \\\\right] = \\nEX\\u200b[(f(x)−a(x,X))2]=EX\\u200b[(f(x)−EX\\u200b[a(x,X)]+EX\\u200b[a(x,X)]−a(x,X))2]==EX[(f(x)−EX[a(x,X)])2]⏟не\\xa0зависит\\xa0от\\xa0X+EX[(a(x,X)−EX[a(x,X)])2]⏟=VarX[a(x,X)]+    = \\\\mathbb{E}_X\\\\underbrace{\\\\left[ \\n        (f(x) - \\\\mathbb{E}_X[a(x, X)])^2 \\n    \\\\right]}_{\\\\text{не зависит от $X$}} + \\\\underbrace{\\\\mathbb{E}_X \\\\left[ (a(x, X) - \\\\mathbb{E}_X[a(x, X)])^2 \\\\right]}_{\\\\text{$=\\\\mathbb{V}\\\\text{ar}_X[a(x, X)]$}} + \\n=EX\\u200bне\\xa0зависит\\xa0от\\xa0X[(f(x)−EX\\u200b[a(x,X)])2]\\u200b\\u200b+=VarX\\u200b[a(x,X)]EX\\u200b[(a(x,X)−EX\\u200b[a(x,X)])2]\\u200b\\u200b++2EX[(f(x)−EX[a(x,X)])⏟не\\xa0зависит\\xa0от\\xa0X⋅(EX[a(x,X)]−a(x,X))]=    + 2 \\\\mathbb{E}_X[\\\\underbrace{(f(x) - \\\\mathbb{E}_X[a(x, X)])}_{\\\\text{не зависит от $X$}} \\\\cdot (\\\\mathbb{E}_X[a(x, X)] - a(x, X))] = \\n+2EX\\u200b[не\\xa0зависит\\xa0от\\xa0X(f(x)−EX\\u200b[a(x,X)])\\u200b\\u200b⋅(EX\\u200b[a(x,X)]−a(x,X))]==(f(x)−EX[a(x,X)]⏟biasXa(x,X))2+VarX[a(x,X)]+2(f(x)−EX[a(x,X)])⋅(EX[a(x,X)]−EX[a(x,X)])⏟=0=    = (\\\\underbrace{f(x) - \\\\mathbb{E}_X[a(x, X)]}_{\\\\text{bias}_X a(x, X)})^2 + \\\\mathbb{V}\\\\text{ar}_X[a(x, X)] + 2 (f(x) - \\\\mathbb{E}_X[a(x, X)]) \\\\cdot \\\\underbrace{(\\\\mathbb{E}_X[a(x, X)] - \\\\mathbb{E}_X [a(x, X)])}_{=0} =\\n=(biasX\\u200ba(x,X)f(x)−EX\\u200b[a(x,X)]\\u200b\\u200b)2+VarX\\u200b[a(x,X)]+2(f(x)−EX\\u200b[a(x,X)])⋅=0(EX\\u200b[a(x,X)]−EX\\u200b[a(x,X)])\\u200b\\u200b==biasX2a(x,X)+VarX[a(x,X)]    = \\\\text{bias}_X^2 a(x, X)+ \\\\mathbb{V}\\\\text{ar}_X[a(x, X)]\\n=biasX2\\u200ba(x,X)+VarX\\u200b[a(x,X)]Таким образом, итоговое выражение для Q(a)Q(a)Q(a) примет вид\\nQ(a)=ExEX,ε[y(x,ε)−a(x,X)]2=ExbiasX2a(x,X)+ExVarX[a(x,X)]+σ2,    Q(a) = \\\\mathbb{E}_x \\\\mathbb{E}_{X, \\\\varepsilon} [y(x, \\\\varepsilon) - a(x, X)]^2 = \\\\mathbb{E}_x \\\\text{bias}_X^2 a(x, X) + \\\\mathbb{E}_x \\\\mathbb{V}\\\\text{ar}_X[a(x, X)] + \\\\sigma^2,\\nQ(a)=Ex\\u200bEX,ε\\u200b[y(x,ε)−a(x,X)]2=Ex\\u200bbiasX2\\u200ba(x,X)+Ex\\u200bVarX\\u200b[a(x,X)]+σ2,где\\nbiasXa(x,X)=f(x)−EX[a(x,X)]    \\\\text{bias}_X a(x, X) = f(x) - \\\\mathbb{E}_X[a(x, X)]\\nbiasX\\u200ba(x,X)=f(x)−EX\\u200b[a(x,X)]— смещение предсказания алгоритма в точке xxx, усреднённого по всем возможным обучающим выборкам, относительно истинной зависимости fff;\\nVarX[a(x,X)]=EX[a(x,X)−EX[a(x,X)]]2    \\\\mathbb{V}\\\\text{ar}_X[a(x, X)] = \\\\mathbb{E}_X \\\\left[ a(x, X) - \\\\mathbb{E}_X[a(x, X)] \\\\right]^2\\nVarX\\u200b[a(x,X)]=EX\\u200b[a(x,X)−EX\\u200b[a(x,X)]]2— дисперсия (разброс) предсказаний алгоритма в зависимости от обучающей выборки XXX;\\nσ2=ExEε[y(x,ε)−f(x)]2    \\\\sigma^2 = \\\\mathbb{E}_x \\\\mathbb{E}_\\\\varepsilon[y(x, \\\\varepsilon) - f(x)]^2\\nσ2=Ex\\u200bEε\\u200b[y(x,ε)−f(x)]2— неустранимый шум в данных.\\nСмещение показывает, насколько хорошо с помощью данного алгоритма можно приблизить истинную зависимость fff, а разброс характеризует чувствительность алгоритма к изменениям в обучающей выборке. Например, деревья маленькой глубины будут в большинстве случаев иметь высокое смещение и низкий разброс предсказаний, так как они не могут слишком хорошо запомнить обучающую выборку. А глубокие деревья, наоборот, могут безошибочно выучить обучающую выборку и потому будут иметь высокий разброс в зависимости от выборки, однако их предсказания в среднем будут точнее. На рисунке ниже приведены возможные случаи сочетания смещения и разброса для разных моделей:\\n\\n\\n\\nИсточник\\n\\n\\nСиняя точка соответствует модели, обученной на некоторой обучающей выборке, а всего синих точек столько, сколько было обучающих выборок. Красный круг в центре области представляет ближайшую окрестность целевого значения. Большое смещение соответствует тому, что модели в среднем не попадают в цель, а при большом разбросе модели могут как делать точные предсказания, так и довольно сильно ошибаться.\\nПолученное нами разложение ошибки на три компоненты верно только для квадратичной функции потерь. Для других функций потерь существуют более общие формы этого разложения (Domigos, 2000, James, 2003) с похожими по смыслу компонентами. Это позволяет предполагать, что для большинства основных функций потерь имеется некоторое представление в виде смещения, разброса и шума (хоть и, возможно, не в столь простой аддитивной форме).\\nПример расчёта оценок bias и variance\\nПопробуем вычислить разложение на смещение и разброс на каком-нибудь практическом примере. Наши обучающие и тестовые примеры будут состоять из зашумлённых значений целевой функции f(x)f(x)f(x), где f(x)f(x)f(x) определяется как\\nf(x)=xsin\\u2061x    f(x) = x \\\\sin x\\nf(x)=xsinxВ качестве шума добавляется нормальный шум с нулевым средним и дисперсией σ2\\\\sigma^2σ2, равной во всех дальнейших примерах 9. Такое большое значение шума задано для того, чтобы задача была достаточно сложной для классификатора, который будет на этих данных учиться и тестироваться. Пример семпла из таких данных:\\n\\nПосмотрим на то, как предсказания деревьев зависят от обучающих подмножеств и максимальной глубины дерева. На рисунке ниже изображены предсказания деревьев разной глубины, обученных на трёх независимых подвыборках размера 20 (каждая колонка соответствует одному подмножеству):\\n\\nГлядя на эти рисунки, можно выдвинуть гипотезу о том, что с увеличением глубины дерева смещение алгоритма падает, а разброс в зависимости от выборки растёт. Проверим, так ли это, вычислив компоненты разложения для деревьев со значениями глубины от 1 до 15.\\nДля обучения деревьев насемплируем 1000 случайных подмножеств Xtrain=(xtrain,ytrain)X_{train} = (x_{train}, y_{train})Xtrain\\u200b=(xtrain\\u200b,ytrain\\u200b) размера 500, а для тестирования зафиксируем случайное тестовое подмножество точек xtestx_{test}xtest\\u200b также размера 500. Чтобы вычислить матожидание по ε\\\\varepsilonε, нам нужно несколько экземпляров шума ε\\\\varepsilonε для тестовых лейблов:\\nytest=y(xtest,ε^)=f(xtest)+ε^    y_{test} = y(x_{test}, \\\\hat \\\\varepsilon) = f(x_{test}) + \\\\hat \\\\varepsilon\\nytest\\u200b=y(xtest\\u200b,ε^)=f(xtest\\u200b)+ε^Положим количество семплов случайного шума равным 300. Для фиксированных Xtrain=(xtrain,ytrain)X_{train} = (x_{train}, y_{train})Xtrain\\u200b=(xtrain\\u200b,ytrain\\u200b) и Xtest=(xtest,ytest)X_{test} = (x_{test}, y_{test})Xtest\\u200b=(xtest\\u200b,ytest\\u200b) квадратичная ошибка вычисляется как\\nMSE=(ytest−a(xtest,Xtrain))2    MSE = (y_{test} - a(x_{test}, X_{train}))^2\\nMSE=(ytest\\u200b−a(xtest\\u200b,Xtrain\\u200b))2Взяв среднее от MSEMSEMSE по XtrainX_{train}Xtrain\\u200b, xtestx_{test}xtest\\u200b и ε\\\\varepsilonε, мы получим оценку для Q(a)Q(a)Q(a), а оценки для компонент ошибки мы можем вычислить по ранее выведенным формулам.\\nНа графике ниже изображены компоненты ошибки и она сама в зависимости от глубины дерева:\\n\\nПо графику видно, что гипотеза о падении смещения и росте разброса при увеличении глубины подтверждается для рассматриваемого отрезка возможных значений глубины дерева. Правда, если нарисовать график до глубины 25, можно увидеть, что разброс становится равен дисперсии случайного шума. То есть деревья слишком большой глубины начинают идеально подстраиваться под зашумлённую обучающую выборку и теряют способность к обобщению:\\n\\nКод для подсчёта разложения на смещение и разброс, а также код отрисовки картинок можно найти в данном ноутбуке.\\nBias-variance trade-off: в каких ситуациях он применим\\nВ книжках и различных интернет-ресурсах часто можно увидеть следующую картинку:\\n\\n\\n\\nИсточник\\n\\n\\nОна иллюстрирует утверждение, которое в литературе называется bias-variance trade-off: чем выше сложность обучаемой модели, тем меньше её смещение и тем больше разброс, и поэтому общая ошибка на тестовой выборке имеет вид UUU-образной кривой. С падением смещения модель всё лучше запоминает обучающую выборку, поэтому слишком сложная модель будет иметь нулевую ошибку на тренировочных данных и большую ошибку на тесте. Этот график призван показать, что существует оптимальная сложность модели, при которой соблюдается баланс между переобучением и недообучением и ошибка при этом минимальна.\\nСуществует достаточное количество подтверждений bias-variance trade-off для непараметрических моделей. Например, его можно наблюдать для метода kkk ближайших соседей при росте kkk и для ядерной регрессии при увеличении ширины окна σ\\\\sigmaσ (Geman et al., 1992):\\n\\nЧем больше соседей учитывает kkk-NN, тем менее изменчивым становится его предсказание, и аналогично для ядерной регрессии, из-за чего сложность этих моделей в некотором смысле убывает с ростом kkk и σ\\\\sigmaσ. Поэтому традиционный график bias-variance trade-off здесь симметрично отражён по оси xxx.\\nОднако, как показывают последние исследования, непременное возрастание разброса при убывании смещения не является абсолютно истинным предположением. Например, для нейронных сетей с ростом их сложности может происходить снижение и разброса, и смещения. Одна из наиболее известных статей на эту тему — статья Белкина и др. (Belkin et al., 2019), в которой, в частности, была предложена следующая иллюстрация:\\n\\nСлева — классический bias-variance trade-off: убывающая часть кривой соответствует недообученной модели, а возрастающая — переобученной. А на правой картинке — график, называемый в статье double descent risk curve. На нём изображена эмпирически наблюдаемая авторами зависимость тестовой ошибки нейросетей от мощности множества входящих в них параметров (H\\\\mathcal HH). Этот график разделён на две части пунктирной линией, которую авторы называют interpolation threshold. Эта линия соответствует точке, в которой в нейросети стало достаточно параметров, чтобы без особых усилий почти идеально запомнить всю обучающую выборку. Часть до достижения interpolation threshold соответствует «классическому» режиму обучения моделей: когда у модели недостаточно параметров, чтобы сохранить обобщающую способность при почти полном запоминании обучающей выборки. А часть после достижения interpolation threshold соответствует «современным» возможностям обучения моделей с огромным числом параметров. На этой части графика ошибка монотонно убывает с ростом количества параметров у нейросети. Авторы также наблюдают похожее поведение и для «древесных» моделей: Random Forest и бустинга над решающими деревьями. Для них эффект проявляется при одновременном росте глубины и числа входящих в ансамбль деревьев.\\nВ качестве вывода к этому разделу хочется сформулировать два основных тезиса:\\n\\nBias-variance trade-off нельзя считать непреложной истиной, выполняющейся для всех моделей и обучающих данных.\\nРазложение на смещение и разброс не влечёт немедленного выполнения bias-variance trade-off и остаётся верным и для случая, когда все компоненты ошибки (кроме неустранимого шума) убывают одновременно. Этот факт может оказаться незамеченным из-за того, что в учебных пособиях часто разговор о разложении дополняется иллюстрацией с UUU-образной кривой, благодаря чему в сознании эти два факта могут слиться в один.\\n\\nСписок литературы\\n\\nБлог-пост про bias-variance от\\nЙоргоса Папахристудиса\\nБлог-пост про bias-variance от Скотта Фортмана-Роу\\nСтатьи от Домингоса (2000) и Джеймса (2003) про обобщённые формы bias-variance decomposition\\nБлог-пост от Брейди Нила про необходимость пересмотра традиционного взгляда на bias-variance trade-off\\nСтатья Гемана и др. (1992), в которой была впервые предложена концепция bias-variance trade-off\\nСтатья Белкина и др. (2019), в которой был предложен double-descent curve\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф11.2. КраудсорсингСледующий параграф13.1. Введение в теорию глубокого обученияЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_14.html', 'title': 'Экспоненциальный класс распределений и принцип максимальной энтропии'}, page_content='Экспоненциальный класс распределений и принцип максимальной энтропииЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/74.1.Вероятностный подход в ML4.2.Экспоненциальный класс распределений и принцип максимальной энтропииМотивация: метод моментовЭнтропия и дивергенция Кульбака-ЛейблераПринцип максимальной энтропииЭкспоненциальное семейство распределенийMLE для семейства из экспоненциального классаТеорема Купмана-Питмана-Дармуа4.3.Обобщённые линейные модели4.4.Как оценивать вероятности4.5.Генеративный подход к классификации4.6.Байесовский подход к оцениванию4.7.Модели с латентными переменными5.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Экспоненциальный класс распределений и принцип максимальной энтропии4.2. Экспоненциальный класс распределений и принцип максимальной энтропииАвторыФедотов СтаниславСамые главные семейства распределений в\\xa0жизни любого data scientist’аМотивация: метод моментов\\nМетод моментов — это ещё один способ, наряду с методом максимального правдоподобия, оценки параметров распределения по данным x1,…,xNx_1,\\\\ldots,x_Nx1\\u200b,…,xN\\u200b. Суть его в том, что мы выражаем через параметры распределения теоретические значения моментов μk=Exk\\\\mu_k = \\\\mathbb{E}x^kμk\\u200b=Exk нашей случайной величины, затем считаем их выборочные оценки μ^k=1N∑ixik\\\\widehat{\\\\mu}_k = \\\\frac1N\\\\sum_ix_i^kμ\\u200bk\\u200b=N1\\u200b∑i\\u200bxik\\u200b, приравниваем их все друг к другу и, решая полученную систему, находим оценки параметров.\\nМожно доказать, что полученные оценки являются состоятельными, хотя могут быть смещены.\\nПример 1. Оценим параметры нормального распределения N(μ,σ2)\\\\mathcal{N}(\\\\mu, \\\\sigma^2)N(μ,σ2) с помощью метода моментов.\\nТеоретические моменты равны\\nμ1=μ,μ2=σ2+μ2\\\\mu_1 = \\\\mu,\\\\quad\\\\mu_2 = \\\\sigma^2 + \\\\mu^2\\nμ1\\u200b=μ,μ2\\u200b=σ2+μ2Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.ВступитьЗапишем систему:\\n{μ^=1N∑ixi,σ^2+μ^2=1N∑ixi2\\\\begin{cases}\\n\\\\widehat{\\\\mu} = \\\\frac1N\\\\sum_i x_i,\\\\\\\\\\n\\\\widehat{\\\\sigma}^2 + \\\\widehat{\\\\mu}^2 = \\\\frac1N\\\\sum_ix_i^2\\n\\\\end{cases}\\n{μ\\u200b=N1\\u200b∑i\\u200bxi\\u200b,σ2+μ\\u200b2=N1\\u200b∑i\\u200bxi2\\u200b\\u200bИз неё очевидным образом находим\\nμ^=1N∑ixi\\\\widehat{\\\\mu} = \\\\frac1N\\\\sum_ix_i \\nμ\\u200b=N1\\u200bi∑\\u200bxi\\u200bσ^2=1N∑ixi2−(1N∑ixi)2=\\\\widehat{\\\\sigma}^2 = \\\\frac1N\\\\sum_ix_i^2 - \\\\left(\\\\frac1N\\\\sum_i x_i\\\\right)^2=\\nσ2=N1\\u200bi∑\\u200bxi2\\u200b−(N1\\u200bi∑\\u200bxi\\u200b)2==1N∑i(xi−μ^)2=\\\\frac1N\\\\sum_i\\\\left(x_i - \\\\widehat{\\\\mu}\\\\right)^2\\n=N1\\u200bi∑\\u200b(xi\\u200b−μ\\u200b)2Легко видеть, что полученные оценки совпадают с оценками максимального правдоподобия\\nПример 2. Оценим параметр μ\\\\muμ логнормального распределения\\np(x)=1x2πσ2exp\\u2061(−(log\\u2061x−μ)22σ2)p(x) = \\\\frac1{x\\\\sqrt{2\\\\pi\\\\sigma^2}}\\\\exp\\\\left(-\\\\frac{(\\\\log{x} - \\\\mu)^2}{2\\\\sigma^2}\\\\right)\\np(x)=x2πσ2\\u200b1\\u200bexp(−2σ2(logx−μ)2\\u200b)при известном σ2\\\\sigma^2σ2. Будет ли оценка совпадать с оценкой, полученной с помощью метода максимального правдоподобия?\\nТеоретическое математическое ожидание равно exp\\u2061(μ+σ22)\\\\exp\\\\left(\\\\mu + \\\\frac{\\\\sigma^2}2\\\\right)exp(μ+2σ2\\u200b), откуда мы сразу находим оценку μ^=log\\u2061(1N∑ixi)−σ22\\\\widehat{\\\\mu} = \\\\log\\\\left(\\\\frac1N\\\\sum_i x_i\\\\right) - \\\\frac{\\\\sigma^2}2μ\\u200b=log(N1\\u200b∑i\\u200bxi\\u200b)−2σ2\\u200b.\\nТеперь запишем логарифм правдоподобия:\\nl(X)=−∑ilog\\u2061xi−∑i(log\\u2061xi−μ)22σ2+constl(X) = -\\\\sum_i\\\\log{x_i} - \\\\sum_i\\\\frac{(\\\\log{x_i} - \\\\mu)^2}{2\\\\sigma^2} + const\\nl(X)=−i∑\\u200blogxi\\u200b−i∑\\u200b2σ2(logxi\\u200b−μ)2\\u200b+constДифференцируя по μ\\\\muμ и приравнивая производную к нулю, получаем\\nμ^MLE=1N∑ilog\\u2061xi\\\\widehat{\\\\mu}_{MLE} = \\\\frac1N\\\\sum_i\\\\log{x_i}\\nμ\\u200bMLE\\u200b=N1\\u200bi∑\\u200blogxi\\u200bчто вовсе не совпадает с оценкой выше.\\nНесколько приукрасив ситуацию, можно сделать вывод, что первые два выборочных момента позволяют если не править миром, то уверенно восстанавливать параметры распределений. А теперь давайте представим, что мы посчитали 1N∑ixi\\\\frac1N\\\\sum_ix_iN1\\u200b∑i\\u200bxi\\u200b и 1N∑ixi2\\\\frac1N\\\\sum_ix_i^2N1\\u200b∑i\\u200bxi2\\u200b, а семейство распределений пока не выбрали.\\nКак же совершить этот судьбоносный выбор? Давайте посмотрим на следующие три семейства и подумаем, в каком из них мы бы стали искать распределение, зная его истинные матожидание и дисперсию?\\n\\nПочему-то хочется сказать, что в первом. Почему? Второе не симметрично — но почему мы так думаем? Если мы выберем третье, то добавим дополнительную информацию как минимум о том, что у распределения конечный носитель. А с чего бы? У нас такой инфомации вроде бы нет.\\nОбщая идея такова: мы будем искать распределение, которое удовлетворяет только явно заданным нами ограничениям и не отражает никакого дополнительного знания о нём. Но чтобы эти нестрогие рассуждения превратить в формулы, придётся немного обогатить наш математический аппарат и научиться измерять количество информации.\\nЭнтропия и дивергенция Кульбака-Лейблера\\nИзмерять «знание» можно с помощью энтропии Шэннона. Она определяется как\\nH(P)=−∑xP(x)log\\u2061P(x)\\\\color{#348FEA}{H(P) = -\\\\sum_xP(x)\\\\log{P(x)}}\\nH(P)=−x∑\\u200bP(x)logP(x)для дискретного распределения и\\nH(p)=−∫p(x)log\\u2061p(x)dx\\\\color{#348FEA}{H(p) = -\\\\int p(x)\\\\log{p(x)}dx}\\nH(p)=−∫p(x)logp(x)dxдля непрерывного. В классическом определении логарифм двоичный, хотя, конечно, варианты с разным основанием отличаются лишь умножением на константу.\\nНеформально можно представлять, что энтропия показывает, насколько сложно предсказать значение случайной величины. Чуть более строго — сколько в среднем бит нужно потратить, чтобы передать информацию о её значении.\\nПример 1. Рассмотрим схему Бернулли с вероятностью успеха ppp. Энтропия её результата равна\\n−(1−p)⋅log\\u20612(1−p)−p⋅log\\u20612p-(1 - p)\\\\cdot\\\\log_2(1 - p) - p\\\\cdot\\\\log_2{p}\\n−(1−p)⋅log2\\u200b(1−p)−p⋅log2\\u200bpДавайте посмотрим на график этой функции:\\n\\nМинимальное значение (нулевое) энтропия принимает при p∈{0,1}p\\\\in\\\\{0,1\\\\}p∈{0,1}. В самом деле, для такого эксперимента мы всегда можем наверняка сказать, каков будет его исход; обращаясь к другой интерпретации — чтобы сообщить кому-то о результате эксперимента, достаточно 000 бит (ведь получатель сообщения и так понимает, что вышло).\\nМаксимальное значение принимается в точке 12\\\\frac1221\\u200b, что вполне соответствует тому, что при p=12p=\\\\frac12p=21\\u200b предсказать исход эксперимента сложнее всего.\\nДополнение для ценителей математики.Попробуем для этого простого примера объяснить, почему среднее число бит, необходимых для передачи информации об исходе эксперимента, выражается формулой с логарифмами.\\nТеперь пусть ppp произвольно. Рассмотрим N»1N»1N»1 независимых испытаний x1,…,xNx_1,\\\\ldots, x_Nx1\\u200b,…,xN\\u200b; среди них будет n0≈(1−p)Nn_0\\\\approx (1-p)Nn0\\u200b≈(1−p)N неудачных и n1≈pNn_1\\\\approx pNn1\\u200b≈pN удачных. Посчитаем, сколько бит потребуется, чтобы закодировать последовательность xix_ixi\\u200b для известных n0n_0n0\\u200b и n1n_1n1\\u200b. Общее число таких последовательностей равно CNn1=N!n0!n1!C_N^{n_1} = \\\\frac{N!}{n_0!n_1!}CNn1\\u200b\\u200b=n0\\u200b!n1\\u200b!N!\\u200b, а чтобы закодировать каждую достаточно будет log\\u20612(N!n0!n1!)\\\\log_2\\\\left(\\\\frac{N!}{n_0!n_1!}\\\\right)log2\\u200b(n0\\u200b!n1\\u200b!N!\\u200b) бит — это количество информации, содержащееся во всей последовательности. Таким образом, в среднем, чтобы закодировать результат одного испытания необходимо\\n1Nlog\\u20612(N!n0!n1!)\\\\frac1N\\\\log_2\\\\left(\\\\frac{N!}{n_0!n_1!}\\\\right)\\nN1\\u200blog2\\u200b(n0\\u200b!n1\\u200b!N!\\u200b)бит информации. Перепишем это выражение, использовав формулу Стирлинга log\\u2061N!≈Nlog\\u2061N−N\\\\log{N!}\\\\approx N\\\\log{N} - NlogN!≈NlogN−N:\\n1N(log\\u20612N!−log\\u20612n0!−log\\u20612n1!)≈\\\\frac1N\\\\left(\\\\log_2{N!} - \\\\log_2{n_0!} - \\\\log_2{n_1!}\\\\right) \\\\approx \\nN1\\u200b(log2\\u200bN!−log2\\u200bn0\\u200b!−log2\\u200bn1\\u200b!)≈≈const⋅1N(Nlog\\u20612N−N−n0log\\u20612n0+n0−n1log\\u20612n1+n1)=\\\\approx const\\\\cdot\\\\frac1N\\\\left(N\\\\log_2{N} - N - n_0\\\\log_2{n_0} + n_0 - n_1\\\\log_2{n_1} + n_1\\\\right) =\\n≈const⋅N1\\u200b(Nlog2\\u200bN−N−n0\\u200blog2\\u200bn0\\u200b+n0\\u200b−n1\\u200blog2\\u200bn1\\u200b+n1\\u200b)==const⋅1N(Nlog\\u20612N−(1−p)Nlog\\u20612(1−p)N−pNlog\\u20612pN)==const\\\\cdot\\\\frac1N\\\\left(N\\\\log_2{N} - (1-p)N\\\\log_2{(1-p)N} - pN\\\\log_2{pN}\\\\right) =\\n=const⋅N1\\u200b(Nlog2\\u200bN−(1−p)Nlog2\\u200b(1−p)N−pNlog2\\u200bpN)==const⋅(−(1−p)⋅log\\u20612(1−p)−plog\\u20612p)=const\\\\cdot\\\\left(-(1-p)\\\\cdot\\\\log_2(1-p) - p\\\\log_2{p}\\\\right)\\n=const⋅(−(1−p)⋅log2\\u200b(1−p)−plog2\\u200bp)Вот мы и вывели формулу энтропии!\\nПример 2. Энтропия нормального распределения N(μ,σ2)\\\\mathcal{N}(\\\\mu, \\\\sigma^2)N(μ,σ2) равна 12log\\u2061(2πσ2)+12\\\\frac12\\\\log(2\\\\pi\\\\sigma^2) + \\\\frac1221\\u200blog(2πσ2)+21\\u200b, и чем меньше дисперсия, тем меньше энтропия, что и логично: ведь когда дисперсия мала, значения сосредоточены возле матожидания, и они становятся менее «разнообразными».\\nЭнтропия тесно связана с другим важным понятием из теории информации — дивергенцией Кульбака-Лейблера. Она определяется для p(x)p(x)p(x) q(x)q(x)q(x) как\\nKL(p∣∣q)=∫p(x)log\\u2061p(x)q(x)dx\\\\color{#348FEA}{KL(p\\\\vert\\\\vert q) = \\\\int p(x)\\\\log{\\\\frac{p(x)}{q(x)}}dx}\\nKL(p∣∣q)=∫p(x)logq(x)p(x)\\u200bdxв непрерывном случае и точно так же, но только с суммой вместо интеграла в дискретном.\\nДивергенцию можно представить в виде разности:\\nKL(p∣∣q)=(−∫p(x)log\\u2061q(x)dx)−(−∫p(x)log\\u2061p(x)dx)KL(p\\\\vert\\\\vert q) = (-\\\\int p(x)\\\\log{q(x)}dx) - (-\\\\int p(x)\\\\log{p(x)}dx)\\nKL(p∣∣q)=(−∫p(x)logq(x)dx)−(−∫p(x)logp(x)dx)Вычитаемое — это энтропия, которая, как мы уже поняли, показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины. Уменьшаемое похоже по виду, и можно показать, что оно говорит о том, сколько в среднем бит потребуется на кодирование случайной величины с плотностью ppp алгоритмом, оптимизированным для кодирования случайной величины qqq.\\nИными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя длина кодов для значений ppp, если при настройке алгоритма кодирования вместо ppp использовать qqq. Более подробно вы можете почитать, например, в этом посте.\\nДивергенция Кульбака-Лейблера в некотором роде играет роль расстояния между распределениями. В частности, KL(p∣∣q)⩾0KL(p\\\\vert\\\\vert q)\\\\geqslant0KL(p∣∣q)⩾0, причём дивергенция равна нулю, только если распределения совпадают почти всюду. Но при этом она не является симметричной: вообще говоря, KL(p∣∣q)≠KL(q∣∣p)KL(p\\\\vert\\\\vert q)\\\\ne KL(q\\\\vert\\\\vert p)KL(p∣∣q)\\ue020=KL(q∣∣p).\\nВопрос на подумать. Пусть p(x)p(x)p(x) — распределение, заданное на отрезке [a,b][a, b][a,b]. Выразите энтропию через дивергенцию Кульбака-Лейблера p(x)p(x)p(x) с равномерным на отрезке распределением qU(x)=1b−aI[a,b](x)q_U(x)=\\\\frac1{b-a}\\\\mathbb{I}_{[a,b]}(x)qU\\u200b(x)=b−a1\\u200bI[a,b]\\u200b(x).\\nОтвет (не открывайте сразу; сначала подумайте сами!)Распишем дивергенцию:\\nKL(p∣∣qU)=−(−∫abp(x)log\\u2061p(x)dx)−∫abp(x)log\\u2061q(x)⏟=1b−a\\xa0на\\xa0[a,b]dx=KL(p\\\\vert\\\\vert q_U) = -\\\\left(-\\\\int_a^b p(x)\\\\log{p(x)}dx\\\\right) - \\\\int_a^b p(x)\\\\log{\\\\underbrace{q(x)}_{=\\\\frac1{b-a}\\\\text{ на }[a,b]}}dx=\\nKL(p∣∣qU\\u200b)=−(−∫ab\\u200bp(x)logp(x)dx)−∫ab\\u200bp(x)log=b−a1\\u200b\\xa0на\\xa0[a,b]q(x)\\u200b\\u200bdx==log\\u2061(b−a)−H(p)=\\\\log(b-a) - H(p)\\n=log(b−a)−H(p)Аналогичное соотношение можно выписать и для распределения, заданного на конечном множестве.\\nПринцип максимальной энтропии\\nТеперь наконец мы готовы сформулировать, какие распределения мы хотим искать.\\nПринцип максимальной энтропии. Среди всех распределений на заданном носителе X\\\\mathbb{X}X, удовлетворяющих условиям Eu1(x)=μ1\\\\mathbb{E}u_1(x) = \\\\mu_1Eu1\\u200b(x)=μ1\\u200b, ..., Euk(x)=μk\\\\mathbb{E}u_k(x) = \\\\mu_kEuk\\u200b(x)=μk\\u200b, где uiu_iui\\u200b — некоторые функции, мы хотим иметь дело с тем, которое имеет наибольшую энтропию.\\nВ самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше — тем более «произвольное распределение», по крайней мере, в теории.\\nДавайте рассмотрим несколько примеров, которые помогут ещё лучше понять, почему некоторые распределения так популярны:\\nПример 1. На конечном множестве 1,…,n1,\\\\ldots,n1,…,n наибольшую энтропию имеет равномерное распределение (носитель — конечное множество из nnn элементов, других ограничений нет).\\nДоказательство: Пусть pip_ipi\\u200b, i=1,…,ni=1,\\\\ldots,ni=1,…,n — некоторое распределение, qi=1nq_i=\\\\frac1nqi\\u200b=n1\\u200b — равномерное. Запишем их дивергенцию Кульбака-Лейблера:\\nKL(p∣∣q)=∑ipilog\\u2061pi−∑ipilog\\u2061qi=KL(p\\\\vert\\\\vert q) = \\\\sum_i p_i\\\\log{p_i} - \\\\sum_i p_i\\\\log{q_i} =\\nKL(p∣∣q)=i∑\\u200bpi\\u200blogpi\\u200b−i∑\\u200bpi\\u200blogqi\\u200b==−H(p)+log\\u2061n∑ipi⏟=1= -H(p) + \\\\log{n}\\\\underbrace{\\\\sum_ip_i}_{=1}\\n=−H(p)+logn=1i∑\\u200bpi\\u200b\\u200b\\u200bТак как дивергенция Кульбака-Лейблера всегда неотрицательна, получаем, что H(p)⩽log\\u2061nH(p)\\\\leqslant\\\\log{n}H(p)⩽logn. При этом равенство возможно, только если распределения совпадают.\\nПример 2. Среди распределений, заданных на всей вещественной прямой и имеющих заданные матожидание μ\\\\muμ и дисперсию σ2\\\\sigma^2σ2 наибольшую энтропию имеет нормальное распределение N(μ,σ2)\\\\mathcal{N}(\\\\mu,\\\\sigma^2)N(μ,σ2).\\nДоказательство: Пусть p(x)p(x)p(x) — некоторое распределение, q(x)∼N(μ,σ2)q(x)\\\\sim\\\\mathcal{N}(\\\\mu, \\\\sigma^2)q(x)∼N(μ,σ2). Запишем их дивергенцию Кульбака-Лейблера:\\nKL(p∣∣q)=∫p(x)log\\u2061p(x)dx−∫p(x)log\\u2061q(x)dx=KL(p\\\\vert\\\\vert q) = \\\\int p(x)\\\\log{p(x)}dx - \\\\int p(x)\\\\log{q(x)}dx =\\nKL(p∣∣q)=∫p(x)logp(x)dx−∫p(x)logq(x)dx==−H(p)−∫p(x)(−12log\\u2061(2πσ2)−12σ2(x−μ)2)dx== -H(p) - \\\\int p(x)\\\\left(-\\\\frac12\\\\log(2\\\\pi\\\\sigma^2) - \\\\frac1{2\\\\sigma^2}(x - \\\\mu)^2\\\\right)dx =\\n=−H(p)−∫p(x)(−21\\u200blog(2πσ2)−2σ21\\u200b(x−μ)2)dx==−H(p)+12log\\u2061(2πσ2)⋅∫p(x)dx⏟=1+12σ2∫(x−μ)2p(x)dx⏟=Vp=σ2== - H(p) +\\\\frac12\\\\log(2\\\\pi\\\\sigma^2)\\\\cdot\\\\underbrace{\\\\int p(x)dx}_{=1} + \\\\frac1{2\\\\sigma^2}\\\\underbrace{\\\\int(x - \\\\mu)^2p(x)dx}_{=\\\\mathbb{V}p=\\\\sigma^2} =\\n=−H(p)+21\\u200blog(2πσ2)⋅=1∫p(x)dx\\u200b\\u200b+2σ21\\u200b=Vp=σ2∫(x−μ)2p(x)dx\\u200b\\u200b==−H(p)+12log\\u2061(2πσ2)+12⏟=H(q)= - H(p) + \\\\underbrace{\\\\frac12\\\\log(2\\\\pi\\\\sigma^2) + \\\\frac12}_{=H(q)}\\n=−H(p)+=H(q)21\\u200blog(2πσ2)+21\\u200b\\u200b\\u200bТак как дивергенция Кульбака-Лейблера всегда неотрицательна, получаем, что H(p)⩽H(q)H(p)\\\\leqslant H(q)H(p)⩽H(q). При этом равенство возможно, только если распределения ppp и qqq совпадают почти всюду, а с точки зрения теории вероятностей такие распределения различать не имеет смысла.\\nПример 3. Среди распределений, заданных на множестве положительных вещественных чисел и имеющих заданное матожидание λ\\\\lambdaλ наибольшую энтропию имеет показательное распределение с параметром 1λ\\\\frac1{\\\\lambda}λ1\\u200b (его плотность равна p(x)=1λexp\\u2061(−1λx)I(0;+∞)(x)p(x) = \\\\frac1{\\\\lambda}\\\\exp\\\\left(-\\\\frac1{\\\\lambda}x\\\\right)\\\\mathbb{I}_{(0;+\\\\infty)}(x)p(x)=λ1\\u200bexp(−λ1\\u200bx)I(0;+∞)\\u200b(x)).\\nВсе хорошо знакомые нам распределения, не правда ли? Проблема в том, что они свалились на нас чудесным образом. Возникает вопрос, можно ли их было не угадать, а вывести как-нибудь? И как быть, если даны не эти конкретные, а какие-то другие ограничения?\\nОказывается, что при некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса. Давайте же познакомимся с ними поближе.\\nЭкспоненциальное семейство распределений\\nГоворят, что семейство распределений относится к экспоненциальному классу, если оно может быть представлено в следующем виде:\\np(x∣θ)=1h(θ)g(x)⋅exp\\u2061(θTu(x))\\\\color{#348FEA}{p(x\\\\vert\\\\theta) = \\\\frac1{h(\\\\theta)}g(x)\\\\cdot\\\\exp\\\\left(\\\\theta^Tu(x)\\\\right)}\\np(x∣θ)=h(θ)1\\u200bg(x)⋅exp(θTu(x))где θ\\\\thetaθ — вектор вещественнозначных параметров (различные значения которых дают те или иные распределения из семейства), h,g>0h, g > 0h,g>0, uuu — некоторая вектор-функция, и, разумеется, сумма или интеграл по xxx равняется единице. Последнее, в частности, означает, что\\nh(θ)=∫g(x)exp\\u2061(θTu(x))dxh(\\\\theta) = \\\\int g(x)\\\\exp\\\\left(\\\\theta^Tu(x)\\\\right)dx\\nh(θ)=∫g(x)exp(θTu(x))dx(или сумма в дискретном случае).\\nПример 1. Покажем, что нормальное распределение принадлежит экспоненциальному классу. Для этого мы должны представить привычную нам функцию плотности\\np(x∣μ,σ2)=12πσexp\\u2061(−(x−μ)22σ2)p(x \\\\vert \\\\mu, \\\\sigma^2) = \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}\\\\exp\\\\left(-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}\\\\right)\\np(x∣μ,σ2)=2π\\u200bσ1\\u200bexp(−2σ2(x−μ)2\\u200b)в виде\\np(x∣θ)=g(x)⋅exp\\u2061(∑i(параметр)i⋅(функция\\xa0от\\xa0x)i)что-то,\\xa0не\\xa0зависящее\\xa0от\\xa0xp(x\\\\vert\\\\theta) = \\\\frac{g(x)\\\\cdot\\\\exp\\\\left(\\\\sum_i\\\\text{(параметр)}_i\\\\cdot\\\\text{(функция от x)}_i\\\\right)}{\\\\text{что-то, не зависящее от $x$}}\\np(x∣θ)=что-то,\\xa0не\\xa0зависящее\\xa0от\\xa0xg(x)⋅exp(∑i\\u200b(параметр)i\\u200b⋅(функция\\xa0от\\xa0x)i\\u200b)\\u200bРаспишем\\n12πσexp\\u2061(−(x−μ)22σ2)=12πσexp\\u2061(−12σ2x2+μσ2x+μ22σ2)=\\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}\\\\exp\\\\left(-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}\\\\right) =\\n\\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}\\\\exp\\\\left(-\\\\frac1{2\\\\sigma^2}x^2 + \\\\frac{\\\\mu}{\\\\sigma^2}x + \\\\frac{\\\\mu^2}{2\\\\sigma^2}\\\\right)=2π\\u200bσ1\\u200bexp(−2σ2(x−μ)2\\u200b)=2π\\u200bσ1\\u200bexp(−2σ21\\u200bx2+σ2μ\\u200bx+2σ2μ2\\u200b)=exp\\u2061(−12σ2x2+2μ2σ2x)2πσexp\\u2061(−μ22σ2)=\\\\frac{\\\\exp\\\\left(-\\\\frac1{2\\\\sigma^2}x^2 + \\\\frac{2\\\\mu}{2\\\\sigma^2}x\\\\right)}{\\\\sqrt{2\\\\pi}\\\\sigma\\\\exp\\\\left(-\\\\frac{\\\\mu^2}{2\\\\sigma^2}\\\\right)}=\\n2π\\u200bσexp(−2σ2μ2\\u200b)exp(−2σ21\\u200bx2+2σ22μ\\u200bx)\\u200b=Определим\\nu1(x)=x,u2(x)=x2u_1(x) = x,\\\\qquad u_2(x) = x^2\\nu1\\u200b(x)=x,u2\\u200b(x)=x2θ1=μσ2,θ2=−12σ2\\\\theta_1 = \\\\frac{\\\\mu}{\\\\sigma^2},\\\\quad \\\\theta_2 = -\\\\frac1{2\\\\sigma^2}\\nθ1\\u200b=σ2μ\\u200b,θ2\\u200b=−2σ21\\u200bh(θ)=2πσexp\\u2061(−μ22σ2)h(\\\\theta) = \\\\sqrt{2\\\\pi}\\\\sigma\\\\exp\\\\left(-\\\\frac{\\\\mu^2}{2\\\\sigma^2}\\\\right)\\nh(θ)=2π\\u200bσexp(−2σ2μ2\\u200b)Если теперь всё-таки честно выразить hhh через θ\\\\thetaθ (это мы оставляем в качестве лёгкого упражнения), то получится\\np(x∣μ,σ2)=1h(θ)exp\\u2061(θTu(x))p(x \\\\vert \\\\mu, \\\\sigma^2) = \\\\frac1{h(\\\\theta)}\\\\exp\\\\left(\\\\theta^Tu(x)\\\\right)\\np(x∣μ,σ2)=h(θ)1\\u200bexp(θTu(x))В данном случае функция g(x)g(x)g(x) просто равна единице.\\nПример 2. Покажем, что распределение Бернулли принадлежит экспоненциальному классу. Для этого попробуем преобразовать функцию вероятности (ниже xxx принимает значения 000 или 111):\\nP(x∣p)=px(1−p)1−x=exp\\u2061(xlog\\u2061p+(1−x)log\\u2061(1−p))P(x \\\\vert p) = p^x(1 - p)^{1 - x} = \\\\exp\\\\left(x\\\\log{p} + (1 - x)\\\\log(1 - p)\\\\right)\\nP(x∣p)=px(1−p)1−x=exp(xlogp+(1−x)log(1−p))Теперь мы можем положить u(x)=(x,1−x)u(x) = \\\\left(x, 1 - x\\\\right)u(x)=(x,1−x), θ=(p,1−p)\\\\theta = \\\\left(p, 1 - p\\\\right)θ=(p,1−p), и всё получится. Единственное, что смущает, — это то, что компоненты вектора u(x)u(x)u(x) линейно зависимы. Хотя это не является формальной проблемой, но всё же хочется с этим что-то сделать. Исправить это можно, если переписать\\npx(1−p)1−x=(1−p)exp\\u2061(xlog\\u2061p+(−x)log\\u2061(1−p))=p^x(1 - p)^{1 -x} = (1 - p)\\\\exp\\\\left(x\\\\log{p} + (-x)\\\\log(1 - p)\\\\right) =\\npx(1−p)1−x=(1−p)exp(xlogp+(−x)log(1−p))==(1−p)exp\\u2061(xlog\\u2061p1−p)=(1 - p)\\\\exp\\\\left(x\\\\log{\\\\frac{p}{1 - p}}\\\\right)\\n=(1−p)exp(xlog1−pp\\u200b)и определить уже минимальное представление с u(x)=xu(x) = xu(x)=x, θ=log\\u2061p1−p\\\\theta = \\\\log{\\\\frac{p}{1 - p}}θ=log1−pp\\u200b — мы ведь уже сталкивались с этим выражением, когда изучали логистическу регрессию, не так ли?\\nВопрос на подумать. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках U[a,b]U[a, b]U[a,b]? Казалось бы, да: так как:\\np(x)=1b−aI[a,b](x)exp\\u2061(0)p(x) = \\\\frac{1}{b - a}\\\\mathbb{I}_{[a,b]}(x)\\\\exp(0)\\np(x)=b−a1\\u200bI[a,b]\\u200b(x)exp(0)В чём может быть подвох?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Нет, не принадлежит. Давайте вспомним, как звучало определение экспоненциального семейства. Возможно, вас удивило, что там было написано не «распределение относится», а «семейство распределений относится».\\nЭто важно: ведь семейство определяется именно различными значениями θ\\\\thetaθ, и если нас интересует семейство равномерных распределений на отрезках, определяемое параметрами aaa и bbb, то они не могут быть в функции g(x)g(x)g(x), они должны быть под экспонентой, а экспонента ни от чего не может быть равна индикатору.\\nПри этом странное и не очень полезное семейство с нулём параметров, состоящее из одинокого распределения U[0,1]U[0,1]U[0,1] можно считать относящимся к экспоненциальному классу: ведь для него формула\\np(x)=I[0,1](x)exp\\u2061(0)p(x) = \\\\mathbb{I}_{[0,1]}(x)\\\\exp(0)\\np(x)=I[0,1]\\u200b(x)exp(0)будет работать.\\nКак мы увидели, к экспоненциальным семействам относятся как непрерывные, так и дискретные распределения. Вообще, к ним относится большая часть распределений, которыми Вам на практике может захотеться описать Y∣XY \\\\vert XY∣X.\\nВ том числе:\\n\\nнормальное;\\nраспределение Пуассона;\\nэкспоненциальное;\\nбиномиальное, мультиномиальное (с фиксированным числом испытаний);\\nгеометрическое;\\nχ2\\\\chi^2χ2-распределение;\\nбета-распределение;\\nгамма-распределение;\\nраспределение Дирихле.\\n\\nК экспоненциальным семействам не относятся, к примеру:\\n\\nравномерное распределение на отрезке;\\nttt-распределение Стьюдента;\\nраспределение Коши;\\nсмесь нормальных распределений.\\n\\nMLE для семейства из экспоненциального класса\\nВозможно, вас удивил странный и на первый взгляд не очень естественный вид p(x∣θ)p(x\\\\vert\\\\theta)p(x∣θ). Но всё не просто так: оказывается, что оценка максимального правдоподобия параметров распределений из экспоненциального класса устроена очень интригующе.\\nЗапишем функцию правдоподобия выборки X=(x1,…,xN)X = (x_1,\\\\ldots,x_N)X=(x1\\u200b,…,xN\\u200b):\\np(X∣θ)=h(θ)−N⋅(∏i=1Ng(xi))⋅exp\\u2061(θT[∑i=1Nu(xi)])p(X\\\\vert\\\\theta) = h(\\\\theta)^{-N}\\\\cdot\\\\left(\\\\prod_{i=1}^Ng(x_i)\\\\right)\\\\cdot\\\\exp\\\\left(\\\\theta^T\\\\left[\\\\sum_{i=1}^Nu(x_i)\\\\right]\\\\right)\\np(X∣θ)=h(θ)−N⋅(i=1∏N\\u200bg(xi\\u200b))⋅exp(θT[i=1∑N\\u200bu(xi\\u200b)])Её логарифм равен\\nl(X∣θ)=−Nlog\\u2061h(θ)+∑i=1Nlog\\u2061g(xi)+θT[∑i=1Nu(xi)]l(X\\\\vert\\\\theta) = -N\\\\log{h(\\\\theta)} + \\\\sum_{i=1}^N\\\\log{g(x_i)} + \\\\theta^T\\\\left[\\\\sum_{i=1}^Nu(x_i)\\\\right]\\nl(X∣θ)=−Nlogh(θ)+i=1∑N\\u200blogg(xi\\u200b)+θT[i=1∑N\\u200bu(xi\\u200b)]Дифференцируя по θ\\\\thetaθ, получаем\\n∇θl(X∣θ)=−N∇θlog\\u2061h(θ)+[∑i=1Nu(xi)]\\\\nabla_{\\\\theta}l(X\\\\vert\\\\theta) = -N\\\\nabla_{\\\\theta}\\\\log{h(\\\\theta)} + \\\\left[\\\\sum_{i=1}^Nu(x_i)\\\\right]\\n∇θ\\u200bl(X∣θ)=−N∇θ\\u200blogh(θ)+[i=1∑N\\u200bu(xi\\u200b)]Тут нам потребуется следующая\\nЛемма. ∇θlog\\u2061h(θ)=Eu(x)\\\\nabla_{\\\\theta}\\\\log{h(\\\\theta)} = \\\\mathbb{E}u(x)∇θ\\u200blogh(θ)=Eu(x)\\nДоказательство:\\nКак мы уже отмечали в прошлом пункте:\\nh(θ)=∫g(x)exp\\u2061(θTu(x))dxh(\\\\theta) = \\\\int g(x)\\\\exp\\\\left(\\\\theta^Tu(x)\\\\right)dx\\nh(θ)=∫g(x)exp(θTu(x))dxСледовательно,\\n∇θlog\\u2061h(θ)=∇θ∫g(x)exp\\u2061(θTu(x))dx∫g(x)exp\\u2061(θTu(x))dx=\\\\nabla_{\\\\theta}\\\\log{h(\\\\theta)} = \\\\frac{\\\\nabla_{\\\\theta}\\\\int g(x)\\\\exp\\\\left(\\\\theta^Tu(x)\\\\right)dx}{\\\\int g(x)\\\\exp\\\\left(\\\\theta^Tu(x)\\\\right)dx} =\\n∇θ\\u200blogh(θ)=∫g(x)exp(θTu(x))dx∇θ\\u200b∫g(x)exp(θTu(x))dx\\u200b==∫u(x)g(x)exp\\u2061(θTu(x))dxh(θ)== \\\\frac{\\\\int u(x)g(x)\\\\exp\\\\left(\\\\theta^Tu(x)\\\\right)dx}{h(\\\\theta)} =\\n=h(θ)∫u(x)g(x)exp(θTu(x))dx\\u200b==∫u(x)⋅1h(θ)g(x)exp\\u2061(θTu(x))dx=Eu(x)=\\\\int u(x)\\\\cdot\\\\frac1{h(\\\\theta)}g(x)\\\\exp\\\\left(\\\\theta^Tu(x)\\\\right)dx = \\\\mathbb{E}u(x)\\n=∫u(x)⋅h(θ)1\\u200bg(x)exp(θTu(x))dx=Eu(x)Кстати, можно ещё доказать, что\\n∂∂θi∂θjlog\\u2061h(θ)=Cov(ui(x),uj(x))\\\\frac{\\\\partial}{\\\\partial \\\\theta_i\\\\partial\\\\theta_j}\\\\log{h(\\\\theta)} = \\\\text{Cov}(u_i(x), u_j(x))\\n∂θi\\u200b∂θj\\u200b∂\\u200blogh(θ)=Cov(ui\\u200b(x),uj\\u200b(x))Приравнивая ∇θl(X∣θ)\\\\nabla_{\\\\theta}l(X\\\\vert\\\\theta)∇θ\\u200bl(X∣θ) к нулю и применяя лемму, мы получаем, что\\nEu(x)=1N[∑i=1Nu(xi)]\\\\color{#348FEA}{\\\\mathbb{E}u(x) = \\\\frac1N\\\\left[\\\\sum_{i=1}^Nu(x_i)\\\\right]}\\nEu(x)=N1\\u200b[i=1∑N\\u200bu(xi\\u200b)]Таким образом, теоретические матожидания всех компонент ui(x)u_i(x)ui\\u200b(x) должны совпадать с их эмпирическими оценками, а метод максимального правдоподобия совпадает с методом моментов для Eui(x)\\\\mathbb{E}u_i(x)Eui\\u200b(x) в качестве моментов.\\nИ в следующем пункте выяснится, что распределения из семейств, относящихся к экспоненциальному классу, это те самые распределения, которые имеют максимальную энтропию из тех, что имеют заданные моменты Eui(x)\\\\mathbb{E}u_i(x)Eui\\u200b(x).\\n**Пример.**Рассмотрим вновь логнормальное распределение:\\np(x)=1x2πσ2exp\\u2061(−(log\\u2061x−μ)22σ2)=p(x) = \\\\frac1{x\\\\sqrt{2\\\\pi\\\\sigma^2}}\\\\exp\\\\left(-\\\\frac{(\\\\log{x} - \\\\mu)^2}{2\\\\sigma^2}\\\\right) =\\np(x)=x2πσ2\\u200b1\\u200bexp(−2σ2(logx−μ)2\\u200b)==1x2πσ2exp\\u2061(−12σ2log\\u20612x+μσ2log\\u2061x−μ22σ2)==\\\\frac1{x\\\\sqrt{2\\\\pi\\\\sigma^2}}\\\\exp\\\\left(-\\\\frac1{2\\\\sigma^2}\\\\log^2{x} + \\\\frac{\\\\mu}{\\\\sigma^2}\\\\log{x} - \\\\frac{\\\\mu^2}{2\\\\sigma^2}\\\\right) =\\n=x2πσ2\\u200b1\\u200bexp(−2σ21\\u200blog2x+σ2μ\\u200blogx−2σ2μ2\\u200b)==1x2πσ2exp\\u2061(μ22σ2)exp\\u2061(μσ2⏟=θ1log\\u2061x⏟=u1(x)−12σ2⏟=θ2log\\u20612x⏟=u2(x))==\\\\frac1{x\\\\sqrt{2\\\\pi\\\\sigma^2}\\\\exp\\\\left(\\\\frac{\\\\mu^2}{2\\\\sigma^2}\\\\right)}\\\\exp\\\\left(\\\\underbrace{\\\\frac{\\\\mu}{\\\\sigma^2}}_{=\\\\theta_1}\\\\underbrace{\\\\log{x}}_{=u_1(x)} -\\\\underbrace{\\\\frac1{2\\\\sigma^2}}_{=\\\\theta_2}\\\\underbrace{\\\\log^2{x}}_{=u_2(x)}  \\\\right) =\\n=x2πσ2\\u200bexp(2σ2μ2\\u200b)1\\u200bexp\\u200b=θ1\\u200bσ2μ\\u200b\\u200b\\u200b=u1\\u200b(x)logx\\u200b\\u200b−=θ2\\u200b2σ21\\u200b\\u200b\\u200b=u2\\u200b(x)log2x\\u200b\\u200b\\u200b=1−πθ2−1⋅exp\\u2061−θ124θ2⋅1xexp\\u2061(θ1u1(x)+θ2u2(x))\\\\frac{1}{\\\\sqrt{-\\\\pi\\\\theta_2^{-1}}\\\\cdot\\\\exp{-\\\\frac{\\\\theta_1^2}{4\\\\theta_2}}}\\\\cdot\\\\frac1x\\\\exp\\\\left(\\\\theta_1u_1(x) + \\\\theta_2u_2(x)\\\\right)\\n−πθ2−1\\u200b\\u200b⋅exp−4θ2\\u200bθ12\\u200b\\u200b1\\u200b⋅x1\\u200bexp(θ1\\u200bu1\\u200b(x)+θ2\\u200bu2\\u200b(x))Как видим, логнормальное распределение тоже из экспоненциального класса. Вас может это удивить: ведь выше мы обсуждали, что для него метод моментов и метод максимального правдоподобия дают разные оценки.\\nНо никакого подвоха тут нет: мы просто брали не те моменты. В данном случае u1(x)=log\\u2061xu_1(x) = \\\\log{x}u1\\u200b(x)=logx, u2(x)=log\\u20612xu_2(x) = \\\\log^2{x}u2\\u200b(x)=log2x, их матожидания и надо брать; тогда для параметров, получаемых из MLE, должно выполняться\\nElog\\u2061x=1N∑ilog\\u2061xi,Elog\\u20612x=1N∑ilog\\u20612xi\\\\mathbb{E}\\\\log{x} = \\\\frac1N\\\\sum_i\\\\log{x_i},\\\\quad \\\\mathbb{E}\\\\log^2{x} = \\\\frac1N\\\\sum_i\\\\log^2{x_i}\\nElogx=N1\\u200bi∑\\u200blogxi\\u200b,Elog2x=N1\\u200bi∑\\u200blog2xi\\u200bМатожидания в левых частых мы должны выразить через параметры — и нам для этого совершенно не обязательно что-то интегрировать! В самом деле:\\nElog\\u2061x=∂∂θ1log\\u2061h(θ)=\\\\mathbb{E}\\\\log{x} = \\\\frac{\\\\partial}{\\\\partial\\\\theta_1}\\\\log{h(\\\\theta)} =\\nElogx=∂θ1\\u200b∂\\u200blogh(θ)==∂∂θ1(−12log\\u2061π+12log\\u2061θ2−θ124θ22)=−θ12θ22=\\\\frac{\\\\partial}{\\\\partial\\\\theta_1}\\\\left(-\\\\frac12\\\\log{\\\\pi} + \\\\frac12\\\\log{\\\\theta_2} - \\\\frac{\\\\theta_1^2}{4\\\\theta_2^2}\\\\right) = -\\\\frac{\\\\theta_1}{2\\\\theta_2^2}\\n=∂θ1\\u200b∂\\u200b(−21\\u200blogπ+21\\u200blogθ2\\u200b−4θ22\\u200bθ12\\u200b\\u200b)=−2θ22\\u200bθ1\\u200b\\u200bElog\\u20612x=∂∂θ2log\\u2061h(θ)=12θ2+θ122θ23\\\\mathbb{E}\\\\log^2{x} = \\\\frac{\\\\partial}{\\\\partial\\\\theta_2}\\\\log{h(\\\\theta)} = \\\\frac1{2\\\\theta_2} + \\\\frac{\\\\theta_1^2}{2\\\\theta_2^3}\\nElog2x=∂θ2\\u200b∂\\u200blogh(θ)=2θ2\\u200b1\\u200b+2θ23\\u200bθ12\\u200b\\u200bТеорема Купмана-Питмана-Дармуа\\nТеперь мы наконец готовы сформулировать одно из самых любопытных свойств семейств экспоненциального класса.\\nВ следующей теореме мы опустим некоторые не очень обременительные условия регулярности. Просто считайте, что для хороших дискретных и абсолютно непрерывных распределений, с которыми вы в основном и будете сталкиваться, это так.\\nТеорема. Пусть p(x)=1h(θ)exp\\u2061(θTu(x))p(x) = \\\\frac1{h(\\\\theta)}\\\\exp\\\\left(\\\\theta^Tu(x)\\\\right)p(x)=h(θ)1\\u200bexp(θTu(x)) — распределение, причём θ\\\\thetaθ — вектор длины nnn и Eui(x)=αi\\\\mathbb{E}u_i(x) = \\\\alpha_iEui\\u200b(x)=αi\\u200b для некоторых фиксированных αi\\\\alpha_iαi\\u200b, i=1,…,ni=1,\\\\ldots,ni=1,…,n. Тогда распределение p(x)p(x)p(x) обладает наибольшей энтропией среди распределений с тем же носителем, для которых Eui(x)=αi\\\\mathbb{E}u_i(x) = \\\\alpha_iEui\\u200b(x)=αi\\u200b, i=1,…,ni=1,\\\\ldots,ni=1,…,n. При этом оно  — единственное с таким свойством: в том смысле, что любое другое распределение, обладающее этим свойством, совпадает с ним почти всюду.\\nИдея обоснования через оптимизацию.Мы приведём рассуждение для дискретного случая; в абсолютно непрерывном рассуждения будут по сути теми же, только там придётся дифференцировать не по переменным, а по функциям, и мы решили не ввергать вас в мир вариационного исчисления.\\nВ дискретном случае у нас есть счётное семейство точек x1,x2,…x_1, x_2,\\\\ldotsx1\\u200b,x2\\u200b,…, и распределение определяется счётным набором вероятностей pip_ipi\\u200b принимать значение xix_ixi\\u200b. Мы будем решать задачу\\n{−∑jpjlog\\u2061pj⟶max\\u2061,∑jpjui(xj)=αi,i=1,…,n,∑jpj=1,pj⩾0\\\\begin{cases}\\n-\\\\sum_j p_j\\\\log{p_j}\\\\longrightarrow\\\\max,\\\\\\\\\\n\\\\sum_jp_ju_i(x_j) = \\\\alpha_i, i = 1,\\\\ldots,n,\\\\\\\\\\n\\\\sum_jp_j = 1,\\\\\\\\\\np_j\\\\geqslant0\\n\\\\end{cases}\\n⎩⎨⎧\\u200b−∑j\\u200bpj\\u200blogpj\\u200b⟶max,∑j\\u200bpj\\u200bui\\u200b(xj\\u200b)=αi\\u200b,i=1,…,n,∑j\\u200bpj\\u200b=1,pj\\u200b⩾0\\u200bЗапишем лагранжиан:\\nL=∑jpjlog\\u2061pj+∑iθi(αi−∑jpjui(xj))+\\\\mathcal{L} = \\\\sum_j p_j\\\\log{p_j} + \\\\sum_i\\\\theta_i\\\\left(\\\\alpha_i - \\\\sum_jp_ju_i(x_j)\\\\right)+\\nL=j∑\\u200bpj\\u200blogpj\\u200b+i∑\\u200bθi\\u200b(αi\\u200b−j∑\\u200bpj\\u200bui\\u200b(xj\\u200b))++θ0(∑jpj−1)−∑jλjpj+\\\\theta_0\\\\left(\\\\sum_jp_j - 1\\\\right) - \\\\sum_j\\\\lambda_jp_j\\n+θ0\\u200b(j∑\\u200bpj\\u200b−1)−j∑\\u200bλj\\u200bpj\\u200bПродифференцируем его по pjp_jpj\\u200b:\\n∂L∂pj=log\\u2061pj+1−∑iθiui(xj)+θ0−λj\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial p_j} = \\\\log{p_j} + 1 - \\\\sum_i\\\\theta_iu_i(x_j) + \\\\theta_0 - \\\\lambda_j\\n∂pj\\u200b∂L\\u200b=logpj\\u200b+1−i∑\\u200bθi\\u200bui\\u200b(xj\\u200b)+θ0\\u200b−λj\\u200bПриравнивая это к нулю, получаем\\npj=exp\\u2061(⟨θ,u(xj)⟩)exp\\u2061(λj−θ0−1)p_j = \\\\frac{\\\\exp\\\\left(\\\\langle \\\\theta, u(x_j)\\\\rangle\\\\right)}{\\\\exp\\\\left(\\\\lambda_j - \\\\theta_0 - 1\\\\right)}\\npj\\u200b=exp(λj\\u200b−θ0\\u200b−1)exp(⟨θ,u(xj\\u200b)⟩)\\u200bЧислитель уже ровно такой, как и должен быть у распределения из экспоненциального класса. Разберёмся со знаменателем.\\nЛегко видеть, что условие pj⩾0p_j\\\\geqslant0pj\\u200b⩾0 заведомо выполнено (ведь тут сплошные экспоненты), так что его можно было выкинуть из постановки задачи оптимизации или, что то же самое, положить λj=0\\\\lambda_j = 0λj\\u200b=0. Параметр θ0\\\\theta_0θ0\\u200b находится из условия ∑jpj=1\\\\sum_jp_j = 1∑j\\u200bpj\\u200b=1, а точнее, выражается через остальные θi\\\\theta_iθi\\u200b, что позволяет записать знаменатель в виде h(θ)h(\\\\theta)h(θ).\\nИдея доказательства «в лоб».Как и следовало ожидать, оно ничем не отличается от того, как мы доказывали максимальность энтропии у равномерного или нормального распределения. Пусть q(x)q(x)q(x) — ещё одно распределение, для которого\\n∫ui(x)q(x)dx=∫ui(x)p(x)dx\\\\int u_i(x)q(x)dx = \\\\int u_i(x)p(x)dx\\n∫ui\\u200b(x)q(x)dx=∫ui\\u200b(x)p(x)dxдля всех i=1,…,ni = 1,\\\\ldots,ni=1,…,n. Тогда\\n0⩽KL(q∣∣p)=∫q(x)log\\u2061(q(x)p(x))dx=0\\\\leqslant KL(q\\\\vert\\\\vert p) = \\\\int q(x)\\\\log\\\\left(\\\\frac{q(x)}{p(x)}\\\\right)dx = \\n0⩽KL(q∣∣p)=∫q(x)log(p(x)q(x)\\u200b)dx==∫q(x)log\\u2061q(x)dx⏟−H(q)−∫q(x)log\\u2061p(x)dx==\\\\underbrace{\\\\int q(x)\\\\log{q(x)}dx}_{-H(q)} - \\\\int q(x)\\\\log{p(x)}dx=\\n=−H(q)∫q(x)logq(x)dx\\u200b\\u200b−∫q(x)logp(x)dx==−H(q)−∫q(x)(−log\\u2061h(θ)+∑iθiui(x))dx==-H(q) - \\\\int q(x)\\\\left(-\\\\log{h(\\\\theta)} + \\\\sum_i\\\\theta_iu_i(x)\\\\right)dx =\\n=−H(q)−∫q(x)(−logh(θ)+i∑\\u200bθi\\u200bui\\u200b(x))dx==−H(q)−log\\u2061h(θ)∫q(x)dx⏟=1=∫p(x)dx−∑iθi∫q(x)ui(x)dx⏟=∫p(x)ui(x)dx==-H(q) - \\\\log{h(\\\\theta)}\\\\underbrace{\\\\int q(x)dx}_{=1=\\\\int p(x)dx} - \\\\sum_i\\\\theta_i\\\\underbrace{\\\\int q(x)u_i(x)dx}_{=\\\\int p(x)u_i(x)dx} =\\n=−H(q)−logh(θ)=1=∫p(x)dx∫q(x)dx\\u200b\\u200b−i∑\\u200bθi\\u200b=∫p(x)ui\\u200b(x)dx∫q(x)ui\\u200b(x)dx\\u200b\\u200b==−H(q)−∫p(x)(−log\\u2061h(θ)+∑iθiui(x))dx==-H(q) - \\\\int p(x)\\\\left(-\\\\log{h(\\\\theta)} + \\\\sum_i\\\\theta_iu_i(x)\\\\right)dx =\\n=−H(q)−∫p(x)(−logh(θ)+i∑\\u200bθi\\u200bui\\u200b(x))dx==−H(q)+∫p(x)log\\u2061p(x)dx=−H(q)+H(p)=-H(q) + \\\\int p(x)\\\\log{p(x)}dx = -H(q) + H(p)\\n=−H(q)+∫p(x)logp(x)dx=−H(q)+H(p)Таким образом, H(p)⩾H(q)H(p)\\\\geqslant H(q)H(p)⩾H(q), причём по уже не раз использованному нами свойству дивергенции Кульбака-Лейблера из H(p)=H(q)H(p) = H(q)H(p)=H(q) будет следовать то, что ppp и qqq совпадают почти всюду.\\nРассмотрим несколько примеров:\\nПример 1. Среди распределений на множестве {1,2,3,…}\\\\{1,2,3,\\\\ldots\\\\}{1,2,3,…} неотрицательных целых чисел с заданным математическим ожиданием μ\\\\muμ найдём распределение с максимальной энтропией.\\nВ данном случае у нас лишь одна функция u1(x)=xu_1(x) = xu1\\u200b(x)=x, которая соответствует фиксации матожидания Ex\\\\mathbb{E}xEx. Плотность будет вычисляться только в точках x=kx=kx=k, k=1,2,…k=1,2,\\\\ldotsk=1,2,… и будет иметь вид\\npk=p(k)=1h(θ)exp\\u2061(θk)p_k = p(k) = \\\\frac1{h(\\\\theta)}\\\\exp\\\\left(\\\\theta k\\\\right)\\npk\\u200b=p(k)=h(θ)1\\u200bexp(θk)В этой формуле уже безошибочно угадывается геометрическое распределение с p=1−eθp = 1 - e^{\\\\theta}p=1−eθ. Параметр ppp можно подобрать из соображений того, что математическое ожидание равно μ\\\\muμ. Матожидание геометрического распределения равно 1p\\\\frac1pp1\\u200b, так что p=1μp = \\\\frac1{\\\\mu}p=μ1\\u200b. Окончательно,\\npk=1μ(1−1μ)k−1p_k = \\\\frac1{\\\\mu}\\\\left(1 - \\\\frac1{\\\\mu}\\\\right)^{k-1}\\npk\\u200b=μ1\\u200b(1−μ1\\u200b)k−1Пример 2. Среди распределений на всей вещественной прямой с заданным математическим ожиданием μ\\\\muμ найдём распределение с максимальной энтропией.\\nА сможете ли вы его найти? Решение под катом.Теория говорит нам, что его плотность должна иметь вид\\np(x)=1h(θ)exp\\u2061(θx)p(x) = \\\\frac1{h(\\\\theta)}\\\\exp\\\\left(\\\\theta x\\\\right)\\np(x)=h(θ)1\\u200bexp(θx)но интеграла экспоненты не существует, то есть применение «в лоб» теоремы провалилось. И неспроста: если даже рассмотреть все нормально распределённые случайные величины со средним μ\\\\muμ, их энтропии, равные 12+12log\\u2061(2πσ2)\\\\frac12 + \\\\frac12\\\\log(2\\\\pi\\\\sigma^2)21\\u200b+21\\u200blog(2πσ2) не ограничены сверху, то есть величины с наибольшей энтропией не существует.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф4.1. Вероятностный подход в MLКак описать привычные модели на\\xa0языке статистики. Оптимизация функции потерь vs\\xa0оценка максимального правоподобияСледующий параграф4.3. Обобщённые линейные моделиКак прокачать линейную модель с\\xa0помощью распределений из\\xa0экспоненциального классаЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_43.html', 'title': 'Аналитика временных рядов'}, page_content=\"Аналитика временных рядовЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/510.1.Кластеризация10.2.Временные ряды10.3.Аналитика временных рядовАвтокорреляционная функцияСтационарные временные ряды10.4.Модели вида ARIMA10.5.Задача ранжирования11.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Аналитика временных рядов10.3. Аналитика временных рядовАвторыВолков НикитаАвтокорреляционная функция\\nВременной ряд – зависимые между собой наблюдения. Например, температура воздуха сегодня достаточно сильно зависит от вчерашнего показателя температуры. Эту зависимость хотелось бы описать численно. Для этого часто используют разные виды коэффициентов корреляции, например, корреляции Пирсона, Спирмена и Кендалла. Каждый из этих коэффициентов корреляции вычисляется по двум выборкам, корреляцию между которыми требуется посчитать. В данном случае мы имеем один временной ряд, и наша задача – оценить корреляцию между разными наблюдениями ряда, считая, что она не меняется со временем.\\nВ качестве оценки корреляции значений yty_tyt\\u200b и yt+τy_{t+\\\\tau}yt+τ\\u200b для любых ttt рассмотрим коэффициент корреляции Пирсона ряда с самим собой со сдвигом на τ\\\\tauτ. Тем самым мы получим численную оценку степени влияния значения yty_tyt\\u200b на значение yt+τy_{t+\\\\tau}yt+τ\\u200b:\\nrτ=corr^(yt,yt+τ)=∑t=1T−τ(yt−y‾)(yt+τ−y‾)∑t=1T(yt−y‾)2,r_{\\\\tau} = \\\\widehat{corr}(y_t, y_{t+\\\\tau})   = \\\\frac{\\\\sum\\\\limits_{t=1}^{T-\\\\tau} \\\\left(y_t - \\\\overline{y}\\\\right)\\\\left(y_{t+\\\\tau} - \\\\overline{y}\\\\right)}{\\\\sum\\\\limits_{t=1}^T \\\\left(y_t - \\\\overline{y}\\\\right)^2},\\nrτ\\u200b=corr(yt\\u200b,yt+τ\\u200b)=t=1∑T\\u200b(yt\\u200b−y\\u200b)2t=1∑T−τ\\u200b(yt\\u200b−y\\u200b)(yt+τ\\u200b−y\\u200b)\\u200b,где τ\\\\tauτ – лаг автокорреляции, а среднее вычисляется по всему ряду y‾=1T∑t=1Tyt\\\\overline{y} = \\\\frac{1}{T}\\\\sum\\\\limits_{t=1}^T y_ty\\u200b=T1\\u200bt=1∑T\\u200byt\\u200b.  Например, если мы хотим оценить степень влияния сегодняшней температуры на завтрашнюю, то посчитаем коэффициент корреляции исходного ряда и им же, сдвинутым на 1 день.\\nЗамечание. Формула содержит некоторые упрощения при оценке ковариации и дисперсий.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nСвойства коэффициента корреляции:\\n\\n∣rτ∣⩽1\\\\vert r_{\\\\tau}\\\\vert \\\\leqslant 1∣rτ\\u200b∣⩽1;\\nrτ=0r_{\\\\tau} = 0rτ\\u200b=0 – отсутствие автокорреляции, при этом значения могут быть зависимыми (см. подробнее про разницу между независимостью и некоррелированностью);\\nrτ>0r_{\\\\tau} > 0rτ\\u200b>0 – положительная корреляция, то есть чем больше было значение вчера, тем оно будет больше сегодня;\\nrτ<0r_{\\\\tau} < 0rτ\\u200b<0 – отрицательная корреляция, то есть чем больше было значение вчера, тем оно будет меньше сегодня;\\n∣rτ∣=1\\\\vert r_{\\\\tau} \\\\vert = 1∣rτ\\u200b∣=1 означает строгую линейную зависимость.\\n\\nПосмотреть простые примеры и потренировать свою интуицию вы можете в игре Guess The Correlation.\\nПусть мы посчитали значение автокорреляции. А как понять, значение 0.1 – это много или мало? На этот вопрос может ответить статистический критерий Льюнга-Бокса (Ljung–Box), который проверяет значимость отклонения rτr_{\\\\tau}rτ\\u200b от нуля. Основное правило, которое нужно здесь понять – если значение p-value критерия не превосходит 0.050.050.05 (или другого заранее фиксированного порога значимости), то автокорреляция с лагом τ\\\\tauτ значима. Это число вычисляется с помощью стандартных статистических пакетов (например, statsmodels в Питоне).\\nРассмотрим временной ряд дорожно-транспортных происшествий за 14 лет с дискретностью 1 месяц, то есть с 12 измерениями в год. На графике мы видим явную сезонность. На нижнем графике изображена коррелограмма – график, визуализирующий автокорреляционную функцию. Точками на графике показаны значения автокорреляционной функции. Ее значение в нуле всегда равно 1, так как это автокорреляция ряда с собой же. Также мы видим, что значение r12r_{12}r12\\u200b является локальным максимумом, что означает высокую положительную корреляцию значения ряда за текущий месяц с аналогичным значением год назад. Иными словами, ряд со сдвигом на год ведёт себя «похожим образом», и это подкрепляет наше наблюдение про наличие годичной сезонности. Наоборот, значение r6r_{6}r6\\u200b минимально в своей окрестности, что означает высокую отрицательную корреляцию значения ряда со значением полгода назад. Закрашенная область визуализирует границу незначимой автокорреляции, то есть тех значений автокорреляции, для которых не выявлена статистически значимое отличие от 0 (иначе говоря, доверительный интервал пересекает 0). Так мы видим, что последняя значимая сезонная автокорреляция это – это r24r_{24}r24\\u200b. Кроме нее также значима сезонная корреляция r12r_{12}r12\\u200b и корреляция за половину сезона r6r_6r6\\u200b. Из несезонных автокорреляций значимы оказались r1r_{1}r1\\u200b и r2r_{2}r2\\u200b. Отсюда можно сделать вывод, что для построения прогноза значения yty_tyt\\u200b имеет смысл рассматривать признаки yt−1,yt−2,yt−6,yt−12,yt−24y_{t-1}, y_{t-2}, y_{t-6}, y_{t-12}, y_{t-24}yt−1\\u200b,yt−2\\u200b,yt−6\\u200b,yt−12\\u200b,yt−24\\u200b.\\n\\n\\nРассмотрим временной ряд потребления электричества в Австралии с дискретностью 30 минут. По графику мы можем заметить две разных сезонности – суточную и недельную. Кроме того, при наличии большего количества данных мы смогли бы увидеть еще и годовую сезонность. На данном графике видно повышенное потребление электричества в январе-феврале, когда в Австралии жарко и работает много кондиционеров. По графику автокорреляций мы видим, что наибольшую корреляцию имеют соседние измерения, а также значения сутки назад, двое суток назад, и т.д. Наоборот, значения 12, 36, ... часов назад имеют отрицательную корреляцию.\\n\\n\\nСтационарные временные ряды\\nВременной ряд yty_tyt\\u200b называется стационарным\\n\\nв узком смысле, если для любых \\xa0t1,...,tn,τ\\\\ t_1, ..., t_n, \\\\tau\\xa0t1\\u200b,...,tn\\u200b,τ вектор (yt1+τ,...,ytn+τ)(y_{t_1+\\\\tau}, ..., y_{t_n+\\\\tau})(yt1\\u200b+τ\\u200b,...,ytn\\u200b+τ\\u200b) совпадает по распределению с (yt1,...,ytn)(y_{t_1}, ..., y_{t_n})(yt1\\u200b\\u200b,...,ytn\\u200b\\u200b), то есть при сдвиге всех моментов времени на одно и тоже число совместное распределение значений временного ряда в эти моменты времени не поменяется.\\nв широком смысле, если\\n\\nEyt2<+∞\\\\mathbb{E} y_t^2 < +\\\\inftyEyt2\\u200b<+∞ для любого ttt.\\nEyt\\\\mathbb{E} y_tEyt\\u200b не зависит от ttt, то есть в среднем значение временного ряда постоянно.\\ncov(yt+τ,ys+τ)=cov(yt,ys)cov(y_{t+\\\\tau}, y_{s+\\\\tau}) = cov(y_{t}, y_{s})cov(yt+τ\\u200b,ys+τ\\u200b)=cov(yt\\u200b,ys\\u200b) для любых t,s,τt, s, \\\\taut,s,τ, то есть значение автокорреляции зависит только от длины отрезка времени между двумя значениями.\\n\\n\\nдля гауссовских распределений, то есть для случая, когда все векторы вида (yt1,...,ytn)(y_{t_1}, ..., y_{t_n})(yt1\\u200b\\u200b,...,ytn\\u200b\\u200b) имеют нормальное распределение, определения эквивалентны. Это следует из того, что распределение гауссовского случайного вектора полностью определяется математическим ожиданием и ковариациями.\\n\\nПример. Рассмотрим временной ряд yt=ξ1cos\\u2061t+ξ2sin\\u2061ty_t = \\\\xi_1 \\\\cos t + \\\\xi_2 \\\\sin tyt\\u200b=ξ1\\u200bcost+ξ2\\u200bsint, где ξ1,ξ2\\\\xi_1, \\\\xi_2ξ1\\u200b,ξ2\\u200b и независимы и одинаково распределены, причем P(ξ1=1)=P(ξ1=−1)=1/2\\\\mathsf{P}(\\\\xi_1=1) = \\\\mathsf{P}(\\\\xi_1=-1) = 1/2P(ξ1\\u200b=1)=P(ξ1\\u200b=−1)=1/2.\\nМожем заметить, что Ey1=0\\\\mathbb{E} y_1 = 0Ey1\\u200b=0 и cov(yt,ys)=cos\\u2061tcos\\u2061s\\xa0Dξ1+sin\\u2061tsin\\u2061s\\xa0Dξ2=cos\\u2061(t−s)cov(y_t, y_s) = \\\\cos t\\\\cos s\\\\ \\\\mathsf{D} \\\\xi_1 + \\\\sin t\\\\sin s\\\\ \\\\mathsf{D} \\\\xi_2 = \\\\cos (t-s)cov(yt\\u200b,ys\\u200b)=costcoss\\xa0Dξ1\\u200b+sintsins\\xa0Dξ2\\u200b=cos(t−s). Тем самым имеем стационарность в широком смысле.\\nНо при t=0t=0t=0 получаем y0=ξ1∈{−1,1}y_0 = \\\\xi_1 \\\\in \\\\{-1, 1\\\\}y0\\u200b=ξ1\\u200b∈{−1,1}, а при t=π/4t = \\\\pi/4t=π/4 получаем\\nyπ/4=ξ1+ξ22∈{−2,0,2}.y_{\\\\pi/4} = \\\\frac{\\\\xi_1 + \\\\xi_2}{\\\\sqrt{2}} \\\\in \\\\left\\\\{-\\\\sqrt{2}, 0, \\\\sqrt{2}\\\\right\\\\}.\\nyπ/4\\u200b=2\\u200bξ1\\u200b+ξ2\\u200b\\u200b∈{−2\\u200b,0,2\\u200b}.Мы получили разные распределения, поэтому нет стационарности в узком смысле.\\nНекоторые примеры нестационарных временных рядов:\\n\\nСлучайное блуждание – пример: yt=yt−1+εty_t = y_{t-1} + \\\\varepsilon_tyt\\u200b=yt−1\\u200b+εt\\u200b, где εt\\\\varepsilon_tεt\\u200b – белый шум, то есть независимые одинаково распределенные случайные величины. Математическое ожидание постоянно, но ряд не является стационарным, поскольку Dyt\\\\mathsf{D} y_tDyt\\u200b бесконечно растет.\\nВременной ряд с трендом – пример: yt=α+βt+εty_t = \\\\alpha + \\\\beta t + \\\\varepsilon_tyt\\u200b=α+βt+εt\\u200b, где εt\\\\varepsilon_tεt\\u200b – белый шум. Ряд не стационарен, так как Eyt\\\\mathbb{E} y_tEyt\\u200b меняется с течением времени.\\nВременной ряд с сезонностью – пример: yt=sin\\u2061t+εty_t = \\\\sin t + \\\\varepsilon_tyt\\u200b=sint+εt\\u200b, где εt\\\\varepsilon_tεt\\u200b – белый шум. Не стационарен, так как\\n\\nEyt={−1,\\xa0при\\xa0t=−π/2+2πk;1,\\xa0при\\xa0t=t=π/2+2πk.\\\\mathbb{E} y_t=\\\\begin{cases}\\n-1, \\\\text{ при }t = -\\\\pi/2 + 2\\\\pi k; \\\\\\\\ \\n1, \\\\text{ при }t = t = \\\\pi/2 + 2\\\\pi k.\\n\\\\end{cases}Eyt\\u200b={−1,\\xa0при\\xa0t=−π/2+2πk;1,\\xa0при\\xa0t=t=π/2+2πk.\\u200bСтационарный ряд визуально не имеет предсказуемых закономерностей. Если посмотреть на график такого ряда издалека, то он будет горизонтален.\\nРяд можно проверить строго на станционарность с помощью различных статистических критериев. Наиболее популярны следующие критерии:\\n\\nкритерий KPSS (Kwiatkowski–Phillips–Schmidt–Shin): если p-value ⩽0.05\\\\leqslant 0.05⩽0.05, то отвергаем стационарность;\\nкритерий Дики-Фуллера: если p-value ⩽0.05\\\\leqslant 0.05⩽0.05, то отвергаем НЕ\\\\textbf{НЕ}НЕстационарность.\\n\\nРассмотрим несколько примеров временных рядов:\\n\\nряды а, c, d, e, f, i не стационарны, поскольку они имеют тренд;\\nряд b скорее всего стационарный, имеется выброс;\\nряды d, g, h, i не стационарны, потому что имеют сезонность.\\n\\n\\nКогда вы определяете, чем вызвано изменение данных – трендом или шумом – стоит учитывать природу данных. Например, колебания значений ряда f теоретически можно было бы объяснить шумом в данных, но по временной оси мы видим, что данные представлены за 15 лет, соответственно, понимаем данные колебания как изменения тренда. Аналогично, для ряда d мы говорим о наличии меняющегося тренда помимо годовой сезонности.\\nПриведение к стационарным: стабилизация дисперсии\\nЗачем?\\nДанные методы рекомендуется использовать, если задача требует некоторой аналитики временного ряда. Если же требуется только построить точечный прогноз на будущее без построения предсказательных интервалов, то стабилизация дисперсии не является необходимой процедурой. Если же нас интересует предсказательный интервал, то многие методы лучше обрабатывают именно стационарные ряды, поэтому имеет смысл стабилизировать дисперсию.\\nПреобразования:\\n\\nКласс преобразований Бокса-Кокса с параметром λ\\\\lambdaλ:\\n\\nzt={ln\\u2061yt,λ=0(ytλ−1)/λ,λ≠0.z_t = \\n\\\\begin{cases} \\\\ln y_t, & \\\\lambda = 0 \\\\\\\\\\n  (y_t^\\\\lambda - 1) / \\\\lambda, & \\\\lambda \\\\not=0 \\n\\\\end{cases}.zt\\u200b={lnyt\\u200b,(ytλ\\u200b−1)/λ,\\u200bλ=0λ\\ue020=0\\u200b.\\nЕсли есть предположения о зависимости Dyt\\\\mathsf{D} y_tDyt\\u200b от ttt, то можно рассмотреть ряд zt=yt/Dytz_t = y_t \\\\left/ \\\\sqrt{\\\\mathsf{D} y_t} \\\\right.zt\\u200b=yt\\u200b/Dyt\\u200b\\u200b.\\n\\nПосле построения прогноза для преобразованного ряда нужно сделать обратное преобразование.\\nПриведение к стационарным: тренд и сезонность\\nПреобразования:\\n\\nДифференцирование ряда, то есть переход к ряду (yt′,t∈{2,...,T})(y'_t, t \\\\in \\\\{2, ..., T\\\\})(yt′\\u200b,t∈{2,...,T}), где yt′=yt−yt−1y'_t = y_t - y_{t-1}yt′\\u200b=yt\\u200b−yt−1\\u200b. Данное преобразование используется для снятие тренда.\\nСезонное дифференцирование ряда, то есть переход к ряду (yt′,t∈{s+1,...,T})(y'_t, t \\\\in \\\\{s+1, ..., T\\\\})(yt′\\u200b,t∈{s+1,...,T}), где yt′=yt−yt−sy'_t = y_t - y_{t-s}yt′\\u200b=yt\\u200b−yt−s\\u200b, sss – длина сезона.\\n\\nПреобразования можно применять несколько раз. Обычно сначала применяют сезонное дифференцирование.\\nПосмотрим на пример. В критерии KPSS для исходного ряда pvalue<0.01pvalue < 0.01pvalue<0.01, то есть ряд можно считать нестационарным. После логарифмирования ряда pvalue<0.01pvalue < 0.01pvalue<0.01, а после ещё и сезонного дифференцированная pvalue>0.1pvalue > 0.1pvalue>0.1, тем самым полученный ряд мы уже не можем отличить от стационарного.\\n\\nМодели вида экспоненциального сглаживания\\nПростое экспоненциальное сглаживание\\nНе редко временной ряд выглядит довольно шумным, что может достаточно плохо сказаться на работе других моделей и подходов к анализу этого временного ряда. В таком случае можно попытаться сгладить значения ряда. Далее мы рассмотрим несколько моделей сглаживания ряда, в том числе при наличии тренда и сезонности ряда. Помимо сглаживания истории ряда, с помощью данных методов можно также осуществлять простое прогнозирование ряда.\\nПусть имеется временной ряд yty_tyt\\u200b. В результате экспоненциального сглаживания получается новый временной ряд y^\\\\widehat{y}y\\u200b по правилу\\ny^t+1∣t=αyt+(1−α)y^t∣t−1,\\\\widehat{y}_{t+1\\\\vert t} = \\\\alpha y_t + (1 - \\\\alpha) \\\\widehat{y}_{t\\\\vert t - 1},\\ny\\u200bt+1∣t\\u200b=αyt\\u200b+(1−α)y\\u200bt∣t−1\\u200b,где y^t+h∣t\\\\widehat{y}_{t+h\\\\vert t}y\\u200bt+h∣t\\u200b – прогноз значения yt+hy_{t+h}yt+h\\u200b в момент времени ttt, а α\\\\alphaα – параметр сглаживания.\\nСмысл преобразования следующий – сглаженное значение в момент времени t+1t+1t+1 есть взвешенная комбинация предыдущего значения ряда yty_tyt\\u200b и предыдущего сглаженного значения ряда y^t∣t−1\\\\widehat{y}_{t\\\\vert t - 1}y\\u200bt∣t−1\\u200b.\\nСвойства:\\n\\nпри α≈1\\\\alpha \\\\approx 1α≈1 больший вес последнему значению ряда, поэтому получается слабое сглаживание y^T+1∣T≈yT\\\\widehat{y}_{T+1\\\\vert T} \\\\approx y_Ty\\u200bT+1∣T\\u200b≈yT\\u200b;\\nпри α≈0\\\\alpha \\\\approx 0α≈0 больший вес отдается предыдущему сглаженному значению, и получается сильное сглаживание, что в пределе вырождается в среднее y^T+1∣T≈y‾\\\\widehat{y}_{T+1\\\\vert T} \\\\approx \\\\overline{y}y\\u200bT+1∣T\\u200b≈y\\u200b;\\nОптимальное значение α∗\\\\alpha^{\\\\ast}α∗ можно подобрать либо по графику, либо оптимизируя\\n\\n∑t=t0T(y^t(α)−yt)2→min\\u2061α.\\\\sum\\\\limits_{t=t_0}^T \\\\left(\\\\widehat{y}_{t}(\\\\alpha) - y_t\\\\right)^2 \\\\to \\\\min_{\\\\alpha}.\\nt=t0\\u200b∑T\\u200b(y\\u200bt\\u200b(α)−yt\\u200b)2→αmin\\u200b.Существуют следующие эмпирические правила:\\n\\nесли α∗∈(0,0.3)\\\\alpha^{\\\\ast} \\\\in (0, 0.3)α∗∈(0,0.3) то ряд стационарен, можно применять экспоненциальное сглаживание без риска большой потери информации;\\nесли α∗∈(0.3,1)\\\\alpha^{\\\\ast} \\\\in (0.3, 1)α∗∈(0.3,1) то ряд нестационарен, применение экспоненциального сглаживания может привести к потере информации или смещению.\\n\\nПримеры: на каждом из графиков изображен исходный ряд (синий) и сглаженный ряд (оранжевый) для разных значений параметра сглаживания. Если имеется тренд или сезонность, то при большом сглаживании полученный ряд начинает «запаздывать» за исходным рядом.\\n\\n\\nОткуда взялась эта формула экспоненциального сглаживания?\\nПокажем, что сглаженное значение соответствует прогнозу величины xxx в момент времени T+1T+1T+1, подбираемому по правилу\\n∑t=0TβT−t(yt−x)2→min\\u2061x.\\\\sum\\\\limits_{t = 0}^T\\\\beta^{T - t}(y_{t} - x)^2 \\\\rightarrow \\\\min\\\\limits_x.\\nt=0∑T\\u200bβT−t(yt\\u200b−x)2→xmin\\u200b.Иначе говоря, для прогнозирования мы берем взвешенный MSE с экспоненциально убывающими по времени весами.\\nПриравняем производную к нулю:\\n2∑t=0TβT−t(yt−x)=02\\\\sum\\\\limits_{t = 0}^T\\\\beta^{T - t}(y_{t} - x) = 0\\n2t=0∑T\\u200bβT−t(yt\\u200b−x)=0Отсюда выразим xxx и воспользуемся разложением функции 11−x\\\\frac{1}{1-x}1−x1\\u200b в ряд Тейлора\\nx=∑t=0TβT−tyt∑t=0Tβt≈∑t=0TβT−tyt1/(1−β)=(1−β)∑t=0TβT−tyt=x = \\\\dfrac{\\\\sum\\\\limits_{t = 0}^{T}\\\\beta^{T - t}y_t}{\\\\sum\\\\limits_{t = 0}^{T}\\\\beta^t} \\n\\t \\\\approx \\\\dfrac{\\\\sum\\\\limits_{t = 0}^{T}\\\\beta^{T - t}y_t}{1/(1 - \\\\beta)} \\n=(1 - \\\\beta) \\\\sum\\\\limits_{t = 0}^T\\\\beta^{T - t}y_t =x=t=0∑T\\u200bβtt=0∑T\\u200bβT−tyt\\u200b\\u200b≈1/(1−β)t=0∑T\\u200bβT−tyt\\u200b\\u200b=(1−β)t=0∑T\\u200bβT−tyt\\u200b==(1−β)yT+(1−β)β∑t=0T−1βT−1−tyt=(1−β)yT+βy^T∣T−1 =  (1 - \\\\beta)  y_T + (1 - \\\\beta) \\\\beta \\\\sum\\\\limits_{t=0}^{T - 1}\\\\beta^{T - 1 - t}y_t \\n= (1 - \\\\beta)y_T + \\\\beta \\\\widehat{y}_{T|T - 1}=(1−β)yT\\u200b+(1−β)βt=0∑T−1\\u200bβT−1−tyt\\u200b=(1−β)yT\\u200b+βy\\u200bT∣T−1\\u200bТем самым мы получили модель экспоненциального сглаживания для β=1−α\\\\beta = 1 - \\\\alphaβ=1−α.\\nМодель Хольта\\nАддитивный линейный тренд:\\nПрогноз на ddd шагов вперед выражается с помощью линейной функции от числа шагов, где коэффициенты меняются по формулам, аналогичным экспоненциальному сглаживанию\\ny^t+d∣t=at+bt⋅d,\\\\widehat{y}_{t+d|t} = a_t + b_t \\\\cdot d,\\ny\\u200bt+d∣t\\u200b=at\\u200b+bt\\u200b⋅d,at=αyt+(1−α)(at−1+bt−1)a_t = \\\\alpha y_t + (1-\\\\alpha) (a_{t-1} + b_{t-1})\\nat\\u200b=αyt\\u200b+(1−α)(at−1\\u200b+bt−1\\u200b)bt=β(at−at−1)+(1−β)bt−1b_t = \\\\beta (a_t - a_{t-1}) + (1-\\\\beta) b_{t-1}\\nbt\\u200b=β(at\\u200b−at−1\\u200b)+(1−β)bt−1\\u200bМодель для мультипликативного линейного тренда выглядит аналогично\\ny^t+d∣t=atbtd,\\\\widehat{y}_{t+d|t} = a_t b_t^d,\\ny\\u200bt+d∣t\\u200b=at\\u200bbtd\\u200b,at=αyt+(1−α)(at−1bt−1)a_t = \\\\alpha y_t + (1-\\\\alpha) (a_{t-1} b_{t-1})\\nat\\u200b=αyt\\u200b+(1−α)(at−1\\u200bbt−1\\u200b)bt=βatat−1+(1−β)bt−1b_t = \\\\beta \\\\frac{a_t}{a_{t-1}} + (1-\\\\beta) b_{t-1}\\nbt\\u200b=βat−1\\u200bat\\u200b\\u200b+(1−β)bt−1\\u200bМодель Хольта: пояснение формул\\nПоясним на примере аддитивного тренда, почему формулы для ata_tat\\u200b и btb_tbt\\u200b получаются именно такими.\\nЗаметим, что для d=0d=0d=0 и d=1d=1d=1 и момента времени t+1t+1t+1 получаем y^t+1∣t+1=at+1\\\\widehat{y}_{t + 1\\\\vert t + 1} = a_{t + 1}y\\u200bt+1∣t+1\\u200b=at+1\\u200b, y^t+1∣t=at+bt\\\\widehat{y}_{t + 1\\\\vert t} = a_{t} + b_{t}y\\u200bt+1∣t\\u200b=at\\u200b+bt\\u200b. Хотелось бы, чтобы эти прогнозы примерно совпадали, то есть чтобы имело место at+1−at≈bta_{t + 1} - a_{t} \\\\approx b_{t}at+1\\u200b−at\\u200b≈bt\\u200b.\\n\\nРассмотрим ряд разностей Δyt=at+1−at\\\\Delta y_t = a_{t + 1} - a_{t}Δyt\\u200b=at+1\\u200b−at\\u200b и задачу константного прогноза для него (то есть Δy^t∣t−1=b\\\\Delta \\\\widehat{y}_{t \\\\vert t - 1} = bΔy\\u200bt∣t−1\\u200b=b) методом простого экспоненциального сглаживания\\n\\n∑i=0t(1−β)t−i(Δyi−b)2→min\\u2061b.\\\\sum\\\\limits_{i = 0}^{t}(1 - \\\\beta)^{t - i}(\\\\Delta y_i - b)^2 \\\\rightarrow \\\\min_b.\\ni=0∑t\\u200b(1−β)t−i(Δyi\\u200b−b)2→bmin\\u200b.Ее решение мы уже получили ранее:\\nbt=βΔyt−1+(1−β)bt−1=β(at−at−1)+(1−β)bt−1.b_t = \\\\beta \\\\Delta y_{t - 1} + (1 - \\\\beta)b_{t - 1} = \\\\beta (a_t - a_{t-1}) + (1-\\\\beta) b_{t-1}.\\nbt\\u200b=βΔyt−1\\u200b+(1−β)bt−1\\u200b=β(at\\u200b−at−1\\u200b)+(1−β)bt−1\\u200b.Получилась формулу для btb_tbt\\u200b в модели аддитивного тренда.\\n\\n\\nДалее, мы хотим, чтобы имело место at+1≈bt+ata_{t + 1} \\\\approx b_{t} + a_{t}at+1\\u200b≈bt\\u200b+at\\u200b. Рассматривая для yty_tyt\\u200b экспоненциальное сглаживание, в котором в качестве предыдущего значения сглаженного ряда берется bt+atb_t+a_tbt\\u200b+at\\u200b, а в качестве нового – at+1a_{t + 1}at+1\\u200b, получаем\\nat=αyt+(1−α)(at−1+bt−1).a_t = \\\\alpha y_t + (1-\\\\alpha) (a_{t-1} + b_{t - 1}).\\nat\\u200b=αyt\\u200b+(1−α)(at−1\\u200b+bt−1\\u200b).\\n\\nМодель Хольта-Уинтерса\\nАддитивная сезонность с трендом:\\ny^t+d∣t=at+dbt+st−m+(d\\xa0mod\\xa0m),\\\\widehat{y}_{t+d|t} = a_t + db_t + s_{t-m+(d \\\\text{ mod } m)},\\ny\\u200bt+d∣t\\u200b=at\\u200b+dbt\\u200b+st−m+(d\\xa0mod\\xa0m)\\u200b,at=α(yt−st−m)+(1−α)(at−1+bt−1)a_t = \\\\alpha (y_t - s_{t-m}) + (1-\\\\alpha) (a_{t-1} + b_{t-1})\\nat\\u200b=α(yt\\u200b−st−m\\u200b)+(1−α)(at−1\\u200b+bt−1\\u200b)bt=β(at−at−1)+(1−β)bt−1b_t = \\\\beta (a_t - a_{t-1}) + (1-\\\\beta) b_{t-1}\\nbt\\u200b=β(at\\u200b−at−1\\u200b)+(1−β)bt−1\\u200bst=γ(yt−at)+(1−γ)st−ms_t = \\\\gamma(y_t - a_t) + (1-\\\\gamma)s_{t-m}\\nst\\u200b=γ(yt\\u200b−at\\u200b)+(1−γ)st−m\\u200bгде mmm – длина сезона\\nМультипликативная сезонность\\nБез тренда\\ny^t+d∣t=at⋅st−m+(d\\xa0mod\\xa0m),\\\\widehat{y}_{t+d|t} = a_t \\\\cdot s_{t-m+(d \\\\text{ mod } m)},\\ny\\u200bt+d∣t\\u200b=at\\u200b⋅st−m+(d\\xa0mod\\xa0m)\\u200b,at=α(yt/st−m)+(1−α)at−1a_t = \\\\alpha (y_t / s_{t-m}) + (1-\\\\alpha) a_{t-1}\\nat\\u200b=α(yt\\u200b/st−m\\u200b)+(1−α)at−1\\u200bst=γ(yt/at)+(1−γ)st−ms_t = \\\\gamma(y_t / a_t) + (1-\\\\gamma)s_{t-m}\\nst\\u200b=γ(yt\\u200b/at\\u200b)+(1−γ)st−m\\u200bС линейным трендом\\ny^t+d∣t=(at+dbt)st−m+(d\\xa0mod\\xa0m),\\\\widehat{y}_{t+d|t} = (a_t + db_t) s_{t-m+(d \\\\text{ mod } m)},\\ny\\u200bt+d∣t\\u200b=(at\\u200b+dbt\\u200b)st−m+(d\\xa0mod\\xa0m)\\u200b,at=αytst−m+(1−α)(at−1+bt−1)a_t = \\\\alpha \\\\frac{y_t}{s_{t-m}} + (1-\\\\alpha) (a_{t-1} + b_{t-1})\\nat\\u200b=αst−m\\u200byt\\u200b\\u200b+(1−α)(at−1\\u200b+bt−1\\u200b)bt=β(at−at−1)+(1−β)bt−1b_t = \\\\beta (a_t - a_{t-1}) + (1-\\\\beta) b_{t-1}\\nbt\\u200b=β(at\\u200b−at−1\\u200b)+(1−β)bt−1\\u200bst=γytat+(1−γ)st−ms_t = \\\\gamma\\\\frac{y_t}{a_t} + (1-\\\\gamma)s_{t-m}\\nst\\u200b=γat\\u200byt\\u200b\\u200b+(1−γ)st−m\\u200bРазные модели с трендом и сезонностью\\n\\nАдаптивное сглаживание\\nВ примерах выше мы видели, что при изменении локального тренда ряда экспоненциальное сглаживание запаздывает за значениями ряда при использовании сильного сглаживания. Если же использовать слабое сглаживание, то существенного запаздывания не происходит, но ряд остается шумным. Если для ряда предполагаются значительные структурные изменения, можно использовать модель адаптивного экспоненциального сглаживания, в которой параметр сглаживания может меняться для разных отрезков временного ряда.\\nПусть y^t\\\\widehat{y}_ty\\u200bt\\u200b – прогноз значения yty_tyt\\u200b в момент времени t−1t-1t−1 обычным экспоненциальным сглаживанием, а ε^t=yt−y^t\\\\widehat{\\\\varepsilon}_t = y_t - \\\\widehat{y}_tεt\\u200b=yt\\u200b−y\\u200bt\\u200b – ошибка прогноза, сделанного на шаге t−1t-1t−1.\\nОпределим следующие значения\\n\\nСреднее значение ошибки:\\n\\nEt=γε^t+(1−γ)Et−1.E_t = \\\\gamma \\\\widehat{\\\\varepsilon}_t + (1-\\\\gamma)E_{t-1}.\\nEt\\u200b=γεt\\u200b+(1−γ)Et−1\\u200b.\\nСредний разброс ошибки:\\n\\nAt=γ∣ε^t∣+(1−γ)At−1.A_t = \\\\gamma \\\\left|\\\\widehat{\\\\varepsilon}_t\\\\right| + (1-\\\\gamma)A_{t-1}.\\nAt\\u200b=γ∣εt\\u200b∣+(1−γ)At−1\\u200b.\\nKt=Et/AtK_t = E_t/A_tKt\\u200b=Et\\u200b/At\\u200b – статистика, которая сигнализирует, насколько адекватно модель работает в момент времени ttt.\\n\\nKt≈±1K_t \\\\approx \\\\pm 1Kt\\u200b≈±1 – модель систематически ошибается в одну сторону.\\nKt≈0K_t \\\\approx 0Kt\\u200b≈0 – модель работает адекватно.\\n\\n\\n\\nОбычно берут значения γ∈(0.05,0.1)\\\\gamma \\\\in (0.05, 0.1)γ∈(0.05,0.1).\\nЧтобы экспоненциальное сглаживание быстро приспосабливалось к резким структурным изменениям берут αt=min\\u2061(∣Kt∣,1)\\\\alpha_t = \\\\min \\\\left(|K_t|, 1\\\\right)αt\\u200b=min(∣Kt\\u200b∣,1).\\nОднако, у данного подхода есть и недостатки, например,\\n\\nплохо реагирует на одиночные выбросы;\\nтребует подбора γ\\\\gammaγ.\\n\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф10.2. Временные рядыСледующий параграф10.4. Модели вида ARIMAЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_55.html', 'title': 'Оптимизация в ML'}, page_content='Оптимизация в MLЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/414.1.Оптимизация в MLВведениеГрадиентный спуск (GD)Стохастический градиентный спуск (SGD)Использование дополнительной информации о функцииИспользование информации о предыдущих шагахАдаптивный подбор размера шагаОбъединяем все вместе...Практические аспекты14.2.Проксимальные методы14.3.Методы второго порядка14.4.Сходимость SGD15.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Оптимизация в ML14.1. Оптимизация в MLАвторыТяпкин ДаниилКак найти оптимум функции потерь: от\\xa0градиентного спуска до\\xa0AdamВведение\\nЗачастую задачи машинного обучения формулируются таким образом, что «веса» модели, которую мы строим, возникают, как решение оптимизационной задачи. В качестве VIP-примера рассмотрим задачу линейной регрессии:\\n∥y−Xw∥22→min\\u2061w,    \\\\Vert y - Xw \\\\Vert_2^2 \\\\to \\\\min_w,\\n∥y−Xw∥22\\u200b→wmin\\u200b,По сути, мы получили чистейшую задачу квадратичной оптимизации. В чем особенность конкретно этой задачи? Она выпуклая.\\nДля интересующихся определением.Функция f\\u2009\\u2063:Rd→Rf \\\\colon \\\\mathbb{R}^d \\\\to \\\\mathbb{R}f:Rd→R является (нестрого) выпуклой (вниз), если для любых x1,x2∈Rdx_1,x_2 \\\\in \\\\mathbb{R}^dx1\\u200b,x2\\u200b∈Rd верно, что\\n∀t∈[0,1]:f(tx1+(1−t)x2)≤tf(x1)+(1−t)f(x2).    \\\\forall t \\\\in [0,1] : f(tx_1 + (1-t)x_2) \\\\leq t f(x_1) + (1-t) f(x_2).\\n∀t∈[0,1]:f(tx1\\u200b+(1−t)x2\\u200b)≤tf(x1\\u200b)+(1−t)f(x2\\u200b).Чтобы запомнить, в какую сторону неравенство, всегда полезно рисовать следующую картинку с графическим определением выпуклой функции.\\n\\nЭквивалентное определение, если функция достаточно гладкая – гессиан неотрицательно определен в любой точке, то есть в каждой точке функция хорошо приближается параболоидом ветвями вверх. Отсюда по критерию минимальности второго порядка автоматически следует, что всякая точка локального оптимума является точкой локального минимума, то есть локальных максимумов и сёдел в выпуклом мире попросту не существует.\\nВажное свойство выпуклых функций – локальный минимум автоматически является глобальным (но не обязательно единственным!). Это позволяет избегать уродливых ситуаций, которые с теоретической точки зрения могут встретиться в невыпуклом случае, например, вот такой:\\nТеорема (No free lunch theorem) Пусть AAA – алгоритм оптимизации, использующий локальную информацию (все производные в точке). Тогда существует такая невыпуклая функция f\\u2009\\u2063:[0,1]d→[0,1]f \\\\colon [0,1]^d \\\\to [0,1]f:[0,1]d→[0,1], что для нахождения глобального минимума на квадрате [0,1]d[0,1]^d[0,1]d с точностью 1m\\\\frac{1}{m}m1\\u200b требуется совершить хотя бы mdm^dmd шагов.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nДля интересующихся доказательствами.Будем строить наш контрпример, пользуясь принципом сопротивляющегося оракула (или рассуждениями с противником, кому как привычнее называть).\\nРазделим нашу область на подкубики размера 1/m×…×1/m1/m \\\\times \\\\ldots \\\\times 1/m1/m×…×1/m. Зададим функцию следующим образом – она будет тождественно равна 111 на всех кубиках, кроме одного, в середине которого будет точка с значением 000 (мы не специфицируем, как значение будет гладко «снижаться» до 000; можно построить кусочно-линейную функцию, а потом сгладить её).\\nА именно поставим ноль в тот кубик, который наш алгоритм оптимизации AAA посетит последним. Так как кубиков у нас mdm^dmd, то алгоритм должен всегда совершить как минимум mdm^dmd шагов, попробовав все кубики. Итого у нас следующая картинка (m=3,d=2m=3, d=2m=3,d=2):\\n\\nОтметим дополнительно, что полученный контрпример можно сделать какой угодно гладкости (но не аналитическим).\\nМы видим, что в общем случае без выпуклости нас ожидает полное разочарование. Ничего лучше перебора по сетке придумать в принципе невозможно. В выпуклом случае же существуют алгоритмы, которые находят глобальный минимум за разумное время.\\nВстречаются ли в жизни функции невыпуклые? Повсеместно! Например, функция потерь при обучении нейронных сетей, как правило, не является выпуклой. Но отсюда не следует, что любой алгоритм их оптимизации будет обязательно неэффективным: ведь «контрпример» из теоремы довольно специфичен. И, как мы увидим, оптимизировать невыпуклые функции очень даже возможно.\\nНайти глобальный минимум невыпуклой функции – очень трудная задача, но зачастую нам хватает локального, который является, в частности, стационарной точкой: такой, в которой производная равна нулю. Все теоретические результаты в случае невыпуклых задач, как правило, касаются поиска таких точек, и алгоритмы тоже направлены на их отыскание.\\nЭтим объясняется и то, что большинство алгоритмов оптимизации, придуманных для выпуклого случая, дословно перешли в невыпуклый. Теоретическая причина в следующем: в выпуклом случае поиск стационарной точки и поиск минимума – буквально одна и та же задача, поэтому то, что хорошо ищет минимум в выпуклом случае, ожидаемо будет хорошо искать стационарные точки в невыпуклом. Практическая же причина в том, что оптимизаторы в библиотеках никогда не спрашивают, выпуклую ли им функцию подают на вход, а просто работают и работают хорошо.\\nВнимательный читатель мог возразить на моменте подмены задачи: подождите-ка, мы ведь хотим сделать функцию как можно меньше, а не стационарную точку искать какую-то непонятную. Доказать в невыпуклом случае тут, к сожалению, ничего невозможно, но на практике мы снова используем алгоритмы изначально для выпуклой оптимизации. Почему?\\nПричина номер 1: сойтись в локальный минимум лучше, чем никуда. Об этом речь уже шла.\\nПричина номер 2: в окрестности локального минимума функция становится выпуклой, и там мы сможем быстро сойтись.\\nПричина номер 3: иногда невыпуклая функция является в некотором смысле «зашумленной» версией выпуклой или похожей на выпуклую. Например, посмотрите на эту картинку (функция Леви):\\n\\nУ этой функции огромное количество локальных минимумов, но «глобально» она кажется выпуклой. Что-то отдаленно похожее наблюдается и в случае нейронных сетей. Нашей задачей становится не скатиться в маленький локальный минимум, который всегда рядом с нами, а в большую-большую ложбину, где значение функции минимально и в некотором смысле стабильно.\\nПричина номер 4: оказывается, что градиентные методы весьма часто сходятся именно к локальным минимумам.\\nСразу отметим важную разницу между выпуклой и невыпуклой задачами: в выпуклом случае работа алгоритма оптимизации не очень существенно зависит от начальной точки, поскольку мы всегда скатимся в точку оптимума. В невыпуклом же случае правильно выбранная точка старта – это уже половина успеха.\\nТеперь перейдём к разбору важнейших алгоритмов оптимизации.\\nГрадиентный спуск (GD)\\nОпишем самый простой метод, который только можно придумать – градиентный спуск. Для того, чтобы его определить, вспомним заклинание из любого курса матанализа: «градиент – это направление наискорейшего локального возрастания функции», тогда антиградиент – это направление наискорейшего локального убывания.\\nДля интересующихся формализмом.Воспользуемся формулой Тейлора для ∥h∥=1\\\\Vert h \\\\Vert = 1∥h∥=1 (направления спуска):\\nf(x+αh)=f(x)+α⟨∇f(x),h⟩+o(α).    f(x + \\\\alpha h) = f(x) + \\\\alpha \\\\langle \\\\nabla f(x), h \\\\rangle + o(\\\\alpha).\\nf(x+αh)=f(x)+α⟨∇f(x),h⟩+o(α).Мы хотим уменьшить значение функции, то есть\\nf(x)+α⟨∇f(x),h⟩+o(α)<f(x).    f(x) + \\\\alpha \\\\langle \\\\nabla f(x), h \\\\rangle + o(\\\\alpha) < f(x).\\nf(x)+α⟨∇f(x),h⟩+o(α)<f(x).При α→0\\\\alpha \\\\to 0α→0 имеем ⟨∇f(x),Δx⟩≤0\\\\langle \\\\nabla f(x), \\\\Delta x \\\\rangle \\\\leq 0⟨∇f(x),Δx⟩≤0. Более того, мы хотим наискорешйшего убывания, поэтому это скалярное произведение хочется минимизировать. Сделаем это при помощи неравенства Коши-Буняковского:\\n⟨∇f(x),h⟩≥−∥∇f(x)∥2∥h∥2=∥∇f(x)∥2.    \\\\langle \\\\nabla f(x), h \\\\rangle \\\\geq - \\\\Vert \\\\nabla f(x) \\\\Vert_2 \\\\Vert h \\\\Vert_2 = \\\\Vert \\\\nabla f(x) \\\\Vert_2. \\n⟨∇f(x),h⟩≥−∥∇f(x)∥2\\u200b∥h∥2\\u200b=∥∇f(x)∥2\\u200b.Равенство в неравенстве Коши-Буняковского достигается при пропорциональности аргументов, то есть\\nh=−∇f(x)∥∇f(x)∥2.    h = - \\\\frac{\\\\nabla f(x)}{\\\\Vert \\\\nabla f(x) \\\\Vert_2}.\\nh=−∥∇f(x)∥2\\u200b∇f(x)\\u200b.Тогда пусть x0x_0x0\\u200b – начальная точка градиентного спуска. Тогда каждую следующую точку мы выбираем следующим образом:\\nxk+1=xk−α∇f(xk),    x_{k+1} = x_k - \\\\alpha \\\\nabla f(x_k),\\nxk+1\\u200b=xk\\u200b−α∇f(xk\\u200b),где α\\\\alphaα – это размер шага (он же learning rate). Общий алгоритм градиентного спуска пишется крайне просто и элегантно:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1x = normal(0, 1)                # можно пробовать и другие виды инициализации\\n2repeat S times:                 # другой вариант: while abs(err) > tolerance\\n3   h = grad_f(x)                # вычисляем направление спуска\\n4   x -= alpha * h               # обновляем значение в точке\\n\\n\\nЭту схему в приложении к линейной регрессии можно найти в параграфе про линейные модели.\\nПосле всего этого начинаются тонкости:\\n\\nА как вычислять градиент?\\nА как выбрать размер шага?\\nА есть ли какие-то теоретические оценки сходимости?\\n\\nНачнем разбирать вопросы постепенно. Для вычисления градиентов современный человек может использовать инструменты автоматического дифференцирования. Идейно, это вариация на тему алгоритма обратного распространения ошибки (backpropagation), ведь как правило человек задает функции, составленные из элементарных при помощи умножений/делений/сложений/композиций. Такой метод реализован во всех общих фреймворках для нейронных сетей (Tensorflow, PyTorch, Jax).\\nНо, вообще говоря, возникает некоторая тонкость. Например, расмотрим задачу линейной регрессии. Запишем её следующим образом:\\nf(w)=1N∑i=1N(w⊤xi−yi)2.    f(w) = \\\\frac{1}{N} \\\\sum_{i=1}^N (w^\\\\top x_i - y_i)^2.\\nf(w)=N1\\u200bi=1∑N\\u200b(w⊤xi\\u200b−yi\\u200b)2.Видим, что слагаемых суммарно NNN – размер выборки. При NNN порядка 10610^6106 и ddd (это количество признаков) порядка 10410^4104 вычисление градиента за O(Nd)O(Nd)O(Nd) становится жутким мучением. Но если от ddd избавиться без дополнительных предположений (например, о разреженности) нельзя, то с зависимостью от NNN в каком-то смысле удастся разделаться при помощи метода стохастического градиентного спуска.\\nХранение градиентов тоже доставит нам проблемы. У градиента столько же компонент, сколько параметров у модели, и если мы имеем дело с глубокой нейросетью, это даст значительные затраты дополнительной памяти. Хуже того, метод обратного распространения ошибки устроен так, что нам приходится помнить все промежуточные представления для вычисления градиентов. Поэтому вычислить градиент целиком невозможно ни для какой нормальной нейросети, и от этой беды тоже приходится спасаться с помощью стохастического градиентного спуска.\\nТеперь перейдем к размеру шага. Теория говорит о том, что если функция гладкая, то можно брать достаточно маленький размер шага, где под достаточно маленьким подразумевается α≤1L\\\\alpha \\\\leq \\\\frac1Lα≤L1\\u200b, где LLL – некоторая константа, которая зависит от гладкости задачи (так называемая константа Липшица). Вычисление этой константы может быть задачей сложнее, чем изначальная задача оптимизации, поэтому этот вариант нам не годится. Более того, эта оценка крайне пессимистична – мы ведь хотим размер шага как можно больше, чтобы уменьшить функцию как можно больше, а тут мы будем изменять все очень мало.\\nСуществует так называемый метод наискорейшего спуска: выбираем размер шага так, чтобы как можно сильнее уменьшить функцию:\\nαk=arg\\u2061min\\u2061α≥0f(xk−α∇f(xk)).   \\\\alpha_k = \\\\arg\\\\min_{\\\\alpha \\\\geq 0} f(x_k - \\\\alpha \\\\nabla f(x_k)).\\nαk\\u200b=argα≥0min\\u200bf(xk\\u200b−α∇f(xk\\u200b)).Одномерная оптимизация является не сильно сложной задачей, поэтому теоретически мы можем её совершать (например, методом бинарного/тернарного поиска или золотого сечения), можно этот шаг также совершать неточно. Но сразу стоит заметить, что это можно делать, только если функция fff вычислима более-менее точно за разумное время, в случае линейной регрессии это уже не так (не говоря уже о нейронных сетях).\\nТакже есть всевозможные правила Армихо/Гольдштейна/Вульфа и прочее и прочее, разработанные в давние 60-е, и для их проверки требуется снова вычислять значения функции в точке. Желающие могут посмотреть на эти условия на википедии. Про более хитрые вариации выбора шагов мы поговорим позже, но сразу стоит сказать, что эта задача довольно сложная.\\nПо поводу теории: сначала скажем что-то про выпуклый случай.\\nВ максимально общем выпуклом случае без дополнительных предположений оценки для градиентного спуска крайне и крайне пессимистичные: чтобы достичь качества ε\\\\varepsilonε, то есть\\n∣f(xk)−f(x∗)∣≤ε\\\\vert f(x_k) - f(x^*) \\\\vert\\\\leq \\\\varepsilon \\n∣f(xk\\u200b)−f(x∗)∣≤εдостаточно сделать O(R2/ε2)O(R^2/\\\\varepsilon^2)O(R2/ε2) шагов, где R2R^2R2 — это расстояние от x0x_0x0\\u200b до x∗x^*x∗. Выглядит очень плохо: ведь чтобы достичь точности 10−210^{-2}10−2, необходимо сделать порядка 10410^4104 шагов градиентного спуска. Но на практике такого не происходит, потому что на самом деле верны разные предположения, дающие более приятные свойства. Для контраста, укажем оценку в случае гладкой и сильно выпуклой в точке оптимума функции: за kkk шагов будет достигнута точность\\nO(min\\u2061{R2exp\\u2061(−k4κ),R2k}),    O\\\\left( \\\\min\\\\left\\\\{R^2 \\\\exp\\n    \\\\left(-\\\\frac{k}{4\\\\kappa}\\\\right), \\\\frac{R^2}{k} \\\\right\\\\}\\\\right),\\nO(min{R2exp(−4κk\\u200b),kR2\\u200b}),где κ\\\\kappaκ – это так называемое число обусловленности задачи. По сути, это число измеряет, насколько линии уровня функции вытянуты в окрестности оптимума.\\nМорали две:\\n\\nСкорость сходимости градиентного спуска сильно зависит от обусловленности задачи;\\nТакже она зависит от выбора хорошей точки старта, ведь везде входит расстояние от точки старта до оптимума.\\n\\nВ качестве ссылки на доказательство укажем на работу Себастиана Стича, где оно довольно простое и общее.\\nВ невыпуклом же случае все куда хуже с точки зрения теории: требуется порядка O(1/ε2)O(1/\\\\varepsilon^2)O(1/ε2) шагов в худшем случае даже для гладкой функции, где ε\\\\varepsilonε – желаемая точность уменьшения нормы градиента.\\nСтохастический градиентный спуск (SGD)\\nТеперь мы попробуем сэкономить в случае регрессии и подобных ей задач. Будем рассматривать функционалы вида\\nf(x)=∑i=1NL(x,yi),    f(x) = \\\\sum_{i=1}^N \\\\mathcal{L}(x, y_i),\\nf(x)=i=1∑N\\u200bL(x,yi\\u200b),где сумма проходится по всем объектам выборки (которых может быть очень много).\\nТеперь сделаем следующий трюк: заметим, что это усреднение – это по сути взятие матожидания. Таким образом, мы говорим, что наша функция выглядит как\\nf(x)=E[L(x,ξ)],    f(x) = \\\\mathbb{E}[\\\\mathcal{L}(x, \\\\xi)],\\nf(x)=E[L(x,ξ)],где ξ\\\\xiξ равномерно распределена по обучающей выборке. Задачи такого вида возникают не только в машинном обучении; иногда встречаются и просто задачи стохастического программирования, где происходит минимизация матожидания по неизвестному (или слишком сложному) распределению.\\nДля функционалов такого вида мы также можем посчитать градиент, он будет выглядеть довольно ожидаемо:\\n∇f(x)=E∇L(x,ξ).    \\\\nabla f(x) = \\\\mathbb{E} \\\\nabla \\\\mathcal{L}(x, \\\\xi).\\n∇f(x)=E∇L(x,ξ).Будем считать, что вычисление матожидания напрямую невозможно.\\nНовый взгляд из статистики дает возможность воспользоваться классическим трюком: давайте подменим матожидание на его несмещенную Монте-Карло оценку. Получается то, что можно назвать стохастическим градиентом:\\n∇~f(x)=1B∑i=1B∇L(x,ξi).    \\\\tilde \\\\nabla f(x) = \\\\frac{1}{B} \\\\sum_{i=1}^B \\\\nabla \\\\mathcal{L}(x, \\\\xi_i).\\n∇~f(x)=B1\\u200bi=1∑B\\u200b∇L(x,ξi\\u200b).Говоря инженерным языком, мы подменили вычисление градиента по всей выборке вычислением по случайной подвыборке. Подвыборку ξ1,…,ξB\\\\xi_1,\\\\ldots,\\\\xi_Bξ1\\u200b,…,ξB\\u200b часто называют (мини)батчем, а число BBB – размером батча.\\nПо-хорошему, наука предписывает нам каждый раз независимо генерировать батчи, но это трудно с вычислительной точки зрения. Вместо этого воспользуемся следующим приёмом: сначала перемешаем нашу выборку (чтобы внести дополнительную случайность), а затем будем рассматривать последовательно блоки по BBB элементов выборки. Когда мы просмотрели всю выборку – перемешиваем еще раз и повторяем проход. Очередной прогон по обучающей выборке называется эпохой. И, хотя, казалось бы, независимо генерировать батчи лучше, чем перемешивать лишь между эпохами, есть несколько результатов, демонстрирующих обратное: одна работа и вторая (более новая); главное условие успеха – правильно изменяющийся размер шага.\\nПолучаем следующий алгоритм, называемый стохастическим градиентным спуском (stochastic gradient descent, SGD):\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1x = normal(0, 1)                    # инициализация\\n2repeat E times:                     # цикл по количеству эпох\\n3   for i = 0; i <= N; i += B:\\n4        batch = data[i:i+B]\\n5        h = grad_loss(batch).mean() # вычисляем оценку градиента как среднее по батчу\\n6        x -= alpha * h\\n\\n\\nДополнительное удобство такого подхода – возможность работы с внешней памятью, ведь выборка может быть настолько большой, что она помещается только на жёсткий диск. Сразу отметим, что в таком случае BBB стоит выбирать достаточно большим: обращение к данным с диска всегда медленнее, чем к данным из оперативной памяти, так что лучше бы сразу забирать оттуда побольше.\\nПоскольку стохастические градиенты являются лишь оценками истинных градиентов, SGD может быть довольно шумным:\\n\\nПоэтому если вы обучаете глубокую нейросеть и у вас в память влезает лишь батч размером с 2-4 картинки, модель, возможно, ничего хорошего не сможет выучить. Аппроксимация градиента и поведение SGD может стать лучше с ростом размера батча BBB – и обычно его действительно хочется подрастить, но парадоксальным образом слишком большие батчи могут порой испортить дело (об этом дальше в этом параграфе!).\\nТеоретический анализ\\nТеперь перейдем к теоретической стороне вопроса. Сходимость SGD обеспечивается несмещенностью стохастического градиента. Несмотря на то, что во время итераций копится шум, суммарно он зачастую оказывается довольно мал.\\nТеперь приведем оценки. Сначала, по традиции, в выпуклом случае. Для выпуклой функции потерь за kkk шагов будет достигнута точность порядка\\nO(min\\u2061{R2exp\\u2061(−k4κ)+σ2μk,R2k+σ2Rk}),    O\\\\left( \\\\min\\\\left\\\\{R^2 \\\\exp\\n    \\\\left(-\\\\frac{k}{4\\\\kappa} \\\\right)+ \\\\frac{\\\\sigma^2}{\\\\mu k}, \\\\frac{R^2}{k} + \\\\frac{\\\\sigma^2 R}{\\\\sqrt{k}} \\\\right\\\\}\\\\right),\\nO(min{R2exp(−4κk\\u200b)+μkσ2\\u200b,kR2\\u200b+k\\u200bσ2R\\u200b}),где σ2\\\\sigma^2σ2 – это дисперсия стохградиента, а μ\\\\muμ – константа сильной выпуклости, показывающая, насколько функция является «не плоской» в окрестности точки оптимума. Доказательство в том же препринте С. Стича.\\nМораль в следующем: дисперсия стохастического градиента, вычисленного по батчу размера BBB равна σ02/B\\\\sigma_0^2/Bσ02\\u200b/B, где σ02\\\\sigma_0^2σ02\\u200b – это дисперсия одного градиента. То есть увеличение размера батча помогает и с теоретической точки зрения.\\nВ невыпуклом случае оценка сходимости SGD просто катастрофически плохая: требуется O(1/ε4)O(1/\\\\varepsilon^4)O(1/ε4) шагов для того, чтобы сделать норму градиента меньше ε\\\\varepsilonε. В теории есть всевозможные дополнительные способы снижения дисперсии с лучшими теоретическими оценками (Stochastic Variance Reduced Gradient (SVRGD), Spider, etc), но на практике они активно не используются.\\nИспользование дополнительной информации о функции\\nМетоды второго порядка\\nОсновной раздел.\\nПостараемся усовершенствовать метод стохастического градиентного спуска. Сначала заметим, что мы используем явно не всю информацию об оптимизируемой функции.\\nВернемся к нашему VIP-примеру линейной регресии с ℓ2\\\\ell_2ℓ2\\u200b регуляризацией:\\n∥y−Xw∥22+λ∥w∥22→min\\u2061w.    \\\\Vert y - Xw \\\\Vert_2^2 + \\\\lambda \\\\Vert w \\\\Vert_2^2 \\\\to \\\\min_w.\\n∥y−Xw∥22\\u200b+λ∥w∥22\\u200b→wmin\\u200b.Эта функция достаточно гладкая, и может быть неплохой идеей использовать её старшие производные для ускорения сходимости алгоритма. В наиболее чистом виде этой философии следует метод Ньютона и подобные ему; о них вы можете прочитать в соответствующем разделе. Отметим, что все такие методы, как правило, довольно дорогие (исключая L-BFGS), и при большом размере задачи и выборки ничего лучше вариаций SGD не придумали.\\nПроксимальные методы\\nОсновной раздел.\\nК сожалению, не всегда функции такие красивые и гладкие. Для примера рассмотрим Lasso-регресию:\\n∥y−Xw∥22+λ∥w∥1→min\\u2061w.    \\\\Vert y - Xw \\\\Vert_2^2 + \\\\lambda \\\\Vert w \\\\Vert_1 \\\\to \\\\min_w.\\n∥y−Xw∥22\\u200b+λ∥w∥1\\u200b→wmin\\u200b.Второе, не гладкое слагаемое резко ломает все свойства этой задачи: теоретически оценки для градиентного спуска становятся гораздо хуже (и на практике тоже). С другой стороны, регуляризационное слагаемое устроено очень просто, и эту дополнительную структурную особенность можно и нужно эксплуатировать. Методы решения задачи вида\\nf(x)+h(x)→min\\u2061x,    f(x) + h(x) \\\\to \\\\min_x,\\nf(x)+h(x)→xmin\\u200b,где hhh – простая функция (в некотором смысле), а fff – гладкая, называются методами композитной оптимизации. Глубже погрузиться в них можно в соответствующем разделе, посвященном проксимальным методам.\\nИспользование информации о предыдущих шагах\\nСледующая претензия к методу градиентного спуска – мы не используем информацию о предыдущих шагах, хотя, кажется, там может храниться что-то полезное.\\nМетод инерции, momentum\\nНачнем с физической аналогии. Представим себе мячик, который катится с горы. В данном случае гора – это график функции потерь в пространстве параметров нашей модели, а мячик – её текущее значение. Реальный мячик не застрянет перед небольшой кочкой, так как у него есть некоторая масса и уже накопленный импульс – некоторое время он способен двигаться даже вверх по склону. Аналогичный прием может быть использован и в градиентной оптимизации. В англоязычной литературе он называется Momentum.\\n\\nС математической точки зрения, мы добавляем к градиентному шагу еще одно слагаемое:\\nxk+1=xk−αk∇f(xk)+βk(xk−xk−1).    x_{k+1} = x_k - \\\\alpha_k \\\\nabla f(x_k) + \\\\color{red}{\\\\beta_k (x_k - x_{k-1})}.\\nxk+1\\u200b=xk\\u200b−αk\\u200b∇f(xk\\u200b)+βk\\u200b(xk\\u200b−xk−1\\u200b).Сразу заметим, что мы немного усугубили ситуацию с подбором шага, ведь теперь нужно подбирать не только αk\\\\alpha_kαk\\u200b, но и βk\\\\beta_kβk\\u200b. Для обычного, не стохастического градиентного спуска мы можем адаптировать метод наискорейшего и получить метод тяжелого шарика:\\n(αk,βk)=arg\\u2061min\\u2061α,βf(xk−α∇f(xk)+β(xk−xk−1)).    (\\\\alpha_k, \\\\beta_k) = \\\\arg\\\\min_{\\\\alpha,\\\\beta} f(x_k - \\\\alpha \\\\nabla f(x_k) + \\\\beta (x_k - x_{k-1})).\\n(αk\\u200b,βk\\u200b)=argα,βmin\\u200bf(xk\\u200b−α∇f(xk\\u200b)+β(xk\\u200b−xk−1\\u200b)).Но, увы, для SGD это работать не будет.\\nВыгода в невыпуклом случае от метода инерции довольно понятна – мы будем пропускать паразитные локальные минимумы и седла и продолжать движение вниз. Но выгода есть также и в выпуклом случае. Рассмотрим плохо обусловленную квадратичную задачу, для которой линии уровня оптимизируемой функции будут очень вытянутыми эллипсами, и запустим на SGD с инерционным слагаемым и без него. Направление градиента будет иметь существенную вертикальную компоненту, а добавление инерции как раз «погасит» паразитное направление. Получаем следующую картинку:\\n\\nТакже удобно бывает представить метод моментума в виде двух параллельных итерационных процессов:\\nvk+1=βkvk−αk∇f(xk)xk+1=xk+vk+1.\\\\begin{align}\\n    v_{k+1} &= \\\\beta_k v_k - \\\\alpha_k \\\\nabla f(x_k)\\\\\\\\\\n    x_{k+1} &= x_k + v_{k+1}.\\n\\\\end{align}\\nvk+1\\u200bxk+1\\u200b\\u200b=βk\\u200bvk\\u200b−αk\\u200b∇f(xk\\u200b)=xk\\u200b+vk+1\\u200b.\\u200b\\u200bAccelerated Gradient Descent (Nesterov Momentum)\\nРассмотрим некоторую дополнительную модификацию, которая была предложена в качестве оптимального метода первого порядка для решения выпуклых оптимизационных задач.\\nМожно доказать, что в сильно выпуклом и гладком случае найти минимум с точностью ε\\\\varepsilonε нельзя быстрее, чем за\\nΩ(R2exp\\u2061(−kκ))    \\\\Omega\\\\left( R^2\\\\exp\\\\left(-\\\\frac{k}{\\\\sqrt{\\\\kappa}}\\\\right) \\\\right)\\nΩ(R2exp(−κ\\u200bk\\u200b))итераций, где κ\\\\kappaκ – число обусловленности задачи. Напомним, что для обычного градиентного спуска в экспоненте у нас был не корень из κ\\\\kappaκ, а просто κ\\\\kappaκ, то есть, градиентный спуск справляется с плохой обусловленностью задачи хуже, чем мог бы.\\nВ 1983 году Ю.Нестеровым был предложен алгоритм, имеющий оптимальную по порядку оценку. Для этого модифицируем немного моментум и будем считать градиент не в текущей точке, а как бы в точке, в которую мы бы пошли, следуя импульсу:\\nvk+1=βkvk−αk∇f(xk+βkvk)xk+1=xk+vk+1\\\\begin{align}\\n    v_{k+1} &= \\\\beta_k v_k - \\\\alpha_k \\\\nabla f(\\\\color{red}{x_k + \\\\beta_k v_k})\\\\\\\\\\n    x_{k+1} &= x_k + v_{k+1}\\n\\\\end{align}\\nvk+1\\u200bxk+1\\u200b\\u200b=βk\\u200bvk\\u200b−αk\\u200b∇f(xk\\u200b+βk\\u200bvk\\u200b)=xk\\u200b+vk+1\\u200b\\u200b\\u200bСравним с обычным momentum:\\n\\nКомментарий: иногда упоминается, что Nesterov Momentum «заглядывает в будущее» и исправляет ошибки на данном шаге оптимизации. Конечно, никто не заглядывает в будущее в буквальном смысле.\\nВ работе Нестерова были предложены конкретные (и довольно магические) константы для импульса, которые получаются из некоторой еще более магической последовательности. Мы приводить их не будем, поскольку мы в первую очередь заинтересованы невыпуклым случаем.\\nNesterov Momentum позволяет значительно повысить устойчивость и скорость сходимости в некоторых случаях. Но, конечно, он не является серебряной пулей в задачах оптимизации, хотя в выпуклом мире и является теоретически неулучшаемым.\\nТакже отметим, что ускоренный метод может напрямую примениться к проксимальному градиентному спуску. В частности, применение ускоренного метода к проксимальному алгоритму решения ℓ1\\\\ell_1ℓ1\\u200b регрессии (ISTA) называется FISTA (Fast ISTA).\\nОбщие выводы:\\n\\nДобавление momentum к градиентному спуску позволяет повысить его устойчивость и избегать маленьких локальных минимумов/максимумов;\\nВ выпуклом случае добавление моментного слагаемого позволяет доказуемо улучшить асимптотику и уменьшить зависимость от плохой обусловленности задачи.\\nИдея ускорения применяется к любым около-градиентным методам, в том числе и к проксимальным, позволяя получить, например, ускоренный метод для ℓ1\\\\ell_1ℓ1\\u200b-регрессии.\\n\\nАдаптивный подбор размера шага\\nВыше мы попытались эксплуатировать свойства градиентного спуска. Теперь же пришел момент взяться за больной вопрос: как подбирать размер шага? Он максимально остро встаёт в случае SGD: ведь посчитать значение функции потерь в точке очень дорого, так что методы в духе наискорейшего спуска нам не помогут!\\nНужно действовать несколько хитрее.\\nAdagrad\\nРассмотрим первый алгоритм, который является адаптацией стохастического градиентного спуска. Впервые он предложен в статье в JMLR 2011 года, но она написана в очень широкой общности, так что читать её достаточно сложно.\\nЗафиксируем α\\\\alphaα – исходный learning rate. Затем напишем следующую формулу обновления:\\nGk+1=Gk+(∇f(xk))2xk+1=xk−αGk+1+ε∇f(xk).\\\\begin{align}\\n    G_{k+1} &= G_k + (\\\\nabla f(x_k))^2 \\\\\\\\\\n    x_{k+1} &= x_k - \\\\frac{\\\\alpha}{\\\\sqrt{G_{k+1} + \\\\varepsilon}} \\\\nabla f(x_k).\\n\\\\end{align}\\nGk+1\\u200bxk+1\\u200b\\u200b=Gk\\u200b+(∇f(xk\\u200b))2=xk\\u200b−Gk+1\\u200b+ε\\u200bα\\u200b∇f(xk\\u200b).\\u200b\\u200bВозведение в квадрат и деления векторов покомпонентные. По сути, мы добавляем некоторую квазиньютоновость и начинаем динамически подбирать размер шага для каждой координаты по отдельности. Наш размера шага для фиксированной координаты – это какая-то изначальная константа α\\\\alphaα (learning rate), деленная на корень из суммы квадратов координат градиентов плюс дополнительный параметр сглаживания ε\\\\varepsilonε, предотвращающий деление на ноль. Добавка ε\\\\varepsilonε на практике оставляется дефолтными 1e-8 и не изменяется.\\nИдея следующая: если мы вышли на плато по какой-то координате и соответствующая компонента градиента начала затухать, то нам нельзя уменьшать размер шага слишком сильно, поскольку мы рискуем на этом плато остаться, но в то же время уменьшать надо, потому что это плато может содержать оптимум. Если же градиент долгое время довольно большой, то это может быть знаком, что нам нужно уменьшить размер шага, чтобы не пропустить оптимум. Поэтому мы стараемся компенсировать слишком большие или слишком маленькие координаты градиента.\\nНо довольно часто получается так, что размер шага уменьшается слишком быстро и для решения этой проблемы придумали другой алгоритм.\\nRMSProp\\nМодифицируем слегка предыдущую идею: будем не просто складывать нормы градиентов, а усреднять их в скользящем режиме:\\nGk+1=γGk+(1−γ)(∇f(xk))2xk+1=xk−αGk+1+ε∇f(xk).\\\\begin{align}\\n    G_{k+1} &= \\\\gamma G_k + (1 - \\\\gamma)(\\\\nabla f(x_k))^2 \\\\\\\\\\n    x_{k+1} &= x_k - \\\\frac{\\\\alpha}{\\\\sqrt{G_{k+1} + \\\\varepsilon}} \\\\nabla f(x_k).\\n\\\\end{align}\\nGk+1\\u200bxk+1\\u200b\\u200b=γGk\\u200b+(1−γ)(∇f(xk\\u200b))2=xk\\u200b−Gk+1\\u200b+ε\\u200bα\\u200b∇f(xk\\u200b).\\u200b\\u200bТакой выбор позволяет все еще учитывать историю градиентов, но при этом размер шага уменьшается не так быстро.\\nОбщие выводы:\\n\\nБлагодаря адаптивному подбору шага в современных оптимизаторах не нужно подбирать последовательность αk\\\\alpha_kαk\\u200b размеров всех шагов, а достаточно выбрать всего одно число – learning rate α\\\\alphaα, всё остальное сделает за вас сам алгоритм. Но learning rate все еще нужно выбирать крайне аккуратно: алгоритм может либо преждевременно выйти на плато, либо вовсе разойтись. Пример приведен на иллюстрации ниже.\\n\\n\\nОбъединяем все вместе...\\nAdam\\nТеперь покажем гвоздь нашей программы: алгоритм Adam, который считается решением по умолчанию и практически серебряной пулей в задачах стохастической оптимизации.\\nНазвание Adam = ADAptive Momentum намекает на то, что мы объединим идеи двух последних разделов в один алгоритм. Приведем его алгоритм, он будет немного отличаться от оригинальной статьи отсутствием коррекций смещения (bias correction), но идея останется той же самой:\\nvk+1=β1vk+(1−β1)∇f(xk)Gk+1=β2Gk+(1−β2)(∇f(xk))2xk+1=xk−αGk+1+εvk+1.\\\\begin{align}\\n    v_{k+1} &= \\\\beta_1 v_k + (1 - \\\\beta_1) \\\\nabla f(x_k) \\\\\\\\\\n    G_{k+1} &= \\\\beta_2 G_k + (1 - \\\\beta_2)(\\\\nabla f(x_k))^2 \\\\\\\\\\n    x_{k+1} &= x_k - \\\\frac{\\\\alpha}{\\\\sqrt{G_{k+1} + \\\\varepsilon}} v_{k+1}.\\n\\\\end{align}\\nvk+1\\u200bGk+1\\u200bxk+1\\u200b\\u200b=β1\\u200bvk\\u200b+(1−β1\\u200b)∇f(xk\\u200b)=β2\\u200bGk\\u200b+(1−β2\\u200b)(∇f(xk\\u200b))2=xk\\u200b−Gk+1\\u200b+ε\\u200bα\\u200bvk+1\\u200b.\\u200b\\u200bКак правило, в этом алгоритме подбирают лишь один гиперпараметр α\\\\alphaα – learning rate. Остальные же: β1\\\\beta_1β1\\u200b, β2\\\\beta_2β2\\u200b и ε\\\\varepsilonε – оставляют стандартными и равными 0.9, 0.99 и 1e-8 соответственно. Подбор α\\\\alphaα составляет главное искусство.\\nЗачастую, при начале работы с реальными данными начинают со значения learning rate равного 3e-4. История данного значения достаточно забавна: в 2016 году Андрей Карпатый (Andrej Karpathy) опубликовал шутливый пост в Twitter.\\n\\nПосле чего сообщество подхватило эту идею (до такой степени, что иногда число 3e-4 называют Karpathy constant).\\nОбращаем ваше внимание, что при работе с учебными данными зачастую полезно выбирать более высокий (на 1-2 порядка) начальный learning rate (например, при классификации MNIST, Fashion MNIST, CIFAR или при обучении языковой модели на примере поэзии выбранного поэта).\\nТакже стоит помнить, что Adam требует хранения как параметров модели, так и градиентов, накопленного импульса и нормировочных констант (cache). Т.е. достижение более быстрой (с точки зрения количества итераций/объема рассмотренных данных) сходимости требует больших объемов памяти. Кроме того, если вы решите продолжить обучение модели, остановленное на некоторой точке, необходимо восстановить из чекпоинта не только веса модели, но и накопленные параметры Adam. В противном случае оптимизатор начнёт сбор всех своих статистик с нуля, что может сильно сказаться на качестве дообучения. То же самое касается вообще всех описанных выше методов, так как каждый из них накапливает какие-то статистики во время обучения.\\nИнтересный факт: Adam расходится на одномерном контрпримере, что совершенно не мешает использовать его для обучения нейронных сетей. Этот факт отлично демонстрирует, насколько расходятся теория и практика в машинном обучении. В той же работе предложено исправление этого недоразумения, но его активно не применяют и продолжают пользоваться «неправильным» Adamом потому что он быстрее сходится на практике.\\nAdamW\\nА теперь давайте добавим ℓ2\\\\ell_2ℓ2\\u200b-регуляризацию неявным образом, напрямую в оптимизатор и минуя адаптивный размер шага:\\nvk+1=β1vk+(1−β1)∇f(xk)Gk+1=β2Gk+(1−β2)(∇f(xk))2xk+1=xk−(αGk+1+εvk+1+λxk).\\\\begin{align}\\n    v_{k+1} &= \\\\beta_1 v_k + (1 - \\\\beta_1) \\\\nabla f(x_k) \\\\\\\\\\n    G_{k+1} &= \\\\beta_2 G_k + (1 - \\\\beta_2)(\\\\nabla f(x_k))^2 \\\\\\\\\\n    x_{k+1} &= x_k - \\\\left( \\\\frac{\\\\alpha}{\\\\sqrt{G_{k+1} + \\\\varepsilon}} v_{k+1} \\\\color{red}{ + \\\\lambda x_{k}} \\\\right).\\n\\\\end{align}\\nvk+1\\u200bGk+1\\u200bxk+1\\u200b\\u200b=β1\\u200bvk\\u200b+(1−β1\\u200b)∇f(xk\\u200b)=β2\\u200bGk\\u200b+(1−β2\\u200b)(∇f(xk\\u200b))2=xk\\u200b−(Gk+1\\u200b+ε\\u200bα\\u200bvk+1\\u200b+λxk\\u200b).\\u200b\\u200bЭто сделано для того, чтобы эффект ℓ2\\\\ell_2ℓ2\\u200b-регуляризации не затухал со временем и обобщающая способность модели была выше. Оставим ссылку на одну заметку про этот эффект. Отметим, впрочем, что этот алгоритм особо не используется.\\nПрактические аспекты\\nРасписания\\nЧасто learning rate понижают итеративно: каждые условные 5 эпох (LRScheduler в Pytorch) или же при выходе функции потерь на плато. При этом лосс нередко ведет себя следующим схематичным образом:\\n\\nПомимо этого используют другие варианты «расписаний» для learning rate. Из часто применяемых неочевидных лайфхаков: сначала сделать warmup, то есть увеличивать learning rate, а затем начать постепенно понижать. Использовалось в известной статье про трансформеры. В ней предложили следующую формулу:\\nlr=dmodel−0.5⋅min\\u2061(step_num−0.5,step_num⋅warmup_steps−1.5).    lr = d^{-0.5}_{\\\\rm{model}} \\\\cdot \\\\min(step\\\\_ num^{-0.5}, step\\\\_ num \\\\cdot warmup\\\\_ steps^{-1.5}).\\nlr=dmodel−0.5\\u200b⋅min(step_num−0.5,step_num⋅warmup_steps−1.5).По сути, первые warmup_stepswarmup\\\\_ stepswarmup_steps шагов происходит линейный рост размера шага, а затем он начинает уменьшаться как 1/t1/\\\\sqrt{t}1/t\\u200b, где ttt — число итераций.\\nЕсть и вариант с косинусом из отдельной библиотеки для трансформеров.\\n\\nВ этой же библиотеке можно также почерпнуть идею рестартов: с какого-то момента мы снова включаем warmup, увеличивая размер шага.\\nБольшие батчи\\nПредставим ситуацию, что мы хотим обучить свою нейронную сеть на нескольких GPU. Одно из решений выглядит следующим образом: загружаем на каждую видеокарту нейронную сеть и свой отдельный батч, вычисляем стохастические градиенты, а затем усредняем их по всем видеокартам и делаем шаг. Что плохого может быть в этом?\\nПо факту, эта схема в некотором смысле эквивалентна работе с одним очень большим батчем. Хорошо же, нет разве?\\nНа самом деле существует так называемый generalization gap: использование большого размера батча может приводить к худшей обобщающей способности итоговой модели. О причине этого эффекта можно поспекулировать, базируясь на текущих знаниях о ландшафтах функций потерь при обучении нейронных сетей.\\nБольший размер батча приводит к тому, что оптимизатор лучше «видит» ландшафт функции потерь для конкретной выборки и может скатиться в маленькие «узкие» паразитные локальные минимумы, которые не имеют обобщающий способности — при небольшом шевелении этого ландшафта (distributional shift c тренировочной на тестовую выборку) значение функции потерь резко подскакивает. В свою очередь, широкие локальные минимумы дают модель с лучшей обобщающей способностью. Эту идею можно увидеть на следующей картинке:\\n\\nИными словами, большие батчи могут приводить к переобучению, но это можно исправить правильным динамическим подбором learning rate, как будет продемонстрировано далее. Сразу отметим, что совсем маленькие батчи – это тоже плохо, с ними ничего не получится выучить, так как каждая итерация SGD знает слишком мало о ландшафте функции потерь.\\nLARS\\nМы рассмотрим нестандартный оптимизатор для обучения нейронных сетей, которого нет в Pytorch по умолчанию, но который много где используется: Layer-wise Adaptive Rate Scaling (LARS). Он позволяет эффективно использовать большие размеры батчей, что очень важно при вычислении на нескольких GPU.\\nОсновная идея заключена в названии – нужно подбирать размер шага не один для всей сети или каждого нейрона, а отдельный для каждого слоя по правилу, похожему на RMSProp. По сравнению с оригинальным RMSProp подбор learning rate для каждого слоя дает большую стабильность обучения.\\nТеперь рассмотрим формулу пересчета: пусть wlw_lwl\\u200b – это веса слоя lll, l<Ll < Ll<L. Параметры алгоритма: базовый learning rate η\\\\etaη (на который запускается расписание), коэффициент инерции mmm, коэффециент затухания весов β\\\\betaβ (как в AdamW).\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1for l in range(L):                                              # Цикл по слоям\\n2    g_l = stochgrad(w_prev)[l]                                  # Вычисляем стохградиент из батча для текущего слоя\\n3    lr = eta * norm(w[l]) / (norm(g_l) + beta * norm(w[l]))     # Вычислеяем learning rate для текущего слоя\\n4    v[l] = m * v[l] + lr * (g_l + beta * w[l])                  # Обновляем momentum\\n5    w[l] -= v[l]                                                # Делаем градиентный шаг по всему слою сразу\\n6w_prev = w                                                      # Обновляем веса\\n\\n\\nLAMB\\nЭтот оптимизатор введен в статье Large Batch Optimization For Deep Learning и является идейным продолжателем LARS, более приближенным к Adam, чем к обычному RMSProp. Его параметры – это параметры Adam η,β1,β2,ε\\\\eta, \\\\beta_1, \\\\beta_2, \\\\varepsilonη,β1\\u200b,β2\\u200b,ε, которые берутся как в Adam, а также параметр λ\\\\lambdaλ, который отвечает за затухание весов (β\\\\betaβ в LARS).\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1for l in range(L):                                              # Цикл по слоям\\n2    g_l = stochgrad(w_prev)[l]                                  # Вычисляем стохградиент из батча для текущего слоя\\n3    m[l] = beta_1 * m[l] + (1 - beta_1) * g_l                   # Вычисляем моментум\\n4    v[l] = beta_2 * v[l] + (1 - beta_2) * g_l                   # Вычисляем новый размер шага\\n5    m[l] /= (1 - beta_1**t)                                     # Шаг для уменьшения смещения из Adam\\n6    v[l] /= (1 - beta_2**t)\\n7    r[l] = m[l] / sqrt(v[l] + eps)                              # Нормируем моментум как предписывает Adam\\n8    lr = eta * norm(w[l]) / norm(r[l] + llambda * w[l])         # Как в LARS\\n9    w[l] = w[l] - lr *  (r[l] + llambda * w[l])                 # Делаем шаг по моментуму\\n10w_prev = w                                                      # Обновляем веса\\n\\n\\nУсреднение\\nТеперь снова заглянем в теорию: на самом деле, все хорошие теоретические оценки для SGD проявляются, когда берётся усреднение по точкам.\\nЭтот эффект при обучении нейронных сетей был исследован в статье про алгоритм SWA. Суть очень проста: давайте усреднять веса модели по каждой ccc-й итерации; можно считать, что по эпохам. В итоге, веса финальной модели являются усреднением весов моделей, имевших место в конце каждой эпохи.\\nВ результате такого усреднения сильно повышается обобщающая способность модели: мы чаще попадаем в те самые широкие локальные минимумы, о которых мы говорили в разделе про большие батчи. Вдохновляющая картинка из статьи прилагается:\\n\\nНа второй и третьей картинке изображено сравнение SGD и SWA при обучении нейронной сети (Preactivation ResNet-164 on CIFAR-100) при одной и той же инициализации.\\nНа первой же картинке изображено, как идеологически должен работать SWA. Также мы видим тут демонстрацию эффекта концентрации меры: после обучения стохастический градиентный спуск становится случайным блужданием по области в окрестности локального минимума. Если, например, предположить, что итоговая точка – это нормальное распределение с центром в реальном минимуме в размерности d>106d > 10^6d>106, то все эти точки с большой вероятности будут находиться в окрестности сферы радиуса d\\\\sqrt{d}d\\u200b. Интуитивную демонстрацию многомерного нормального распределения можно увидеть на следующей картинке из книги Р.Вершинина \"High-Dimensional Probability\" (слева в размерности 2, справа в большой размерности):\\n\\nПоэтому, чтобы вычислить центральную точку этой гауссианы, усреднение просто необходимо, по такому же принципу работает и SWA.\\nПредобуславливание\\nТеперь мы снова обратимся к теории: скорость сходимости градиентного спуска (даже ускоренного) очень сильно зависит от числа обусловленности задачи. Разумной идеей будет попытаться использовать какие-то сведения о задаче и улучшить этот показатель, тем самым ускорив сходимость.\\nВ теории, здесь могут помочь техники предобуславливания. Но, к сожалению, попытки наивно воплотить эту идею приводят к чему-то, похожему на метод Ньютона, в котором нужно хранить большую-большую матрицу для обучения больших моделей. Способ обойти эту проблему рассмотрели в статье о методе Shampoo, который использует то, что веса нейронной сети зачастую удобно представлять как матрицу или даже многомерный тензор. Таким образом, Shampoo можно рассматривать как многомерный аналог AdaGrad.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф13.6. Implicit biasСледующий параграф14.2. Проксимальные методыКак оптимизировать функции потерь с\\xa0$L_1$-регуляризациейЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_8.html', 'title': 'Ансамбли в машинном обучении'}, page_content='Ансамбли в машинном обученииЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/52.1.Линейные модели2.2.Метрические методы2.3.Решающие деревья2.4.Ансамбли в машинном обученииСмещение и разбросБэггингRandom ForestБустингСтекингПочитать по теме2.5.Градиентный бустинг3.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Ансамбли в машинном обучении2.4. Ансамбли в машинном обученииАвторы Елистратова ЕвгенияГубко ПавелКак смешать несколько моделей в\\xa0одну. Стэкинг, бэггинг, случайные лесаПредставим, что у вас есть несколько моделей, обученных на ваших данных. Можно ли придумать процедуру, которая позволит использовать все имеющиеся модели и при этом получить на тестовых данных качество выше, чем могла показать каждая из этих моделей в отдельности?\\nДа. И в этом параграфе мы расскажем, как именно.\\nСмещение и разброс\\nПредположим, мы решаем задачу регрессии с квадратичной функцией потерь. При использовании квадратичной функции потерь для оценки качества работы алгоритма aaa можно воспользоваться следующим функционалом:\\nQ(a)=ExEX,ϵ[y(x,ϵ)−a(x,X)]2,Q(a) = \\\\mathbb{E}_x \\\\mathbb{E}_{X, \\\\epsilon} [y(x, \\\\epsilon) - a(x, X)]^2,\\nQ(a)=Ex\\u200bEX,ϵ\\u200b[y(x,ϵ)−a(x,X)]2,где\\n\\nXXX — обучающая выборка\\nxxx — точка из тестового множества\\ny=f(x)+ϵy = f(x) + \\\\epsilony=f(x)+ϵ — целевая зависимость, которую мы можем измерить с точностью до случайного шума ϵ\\\\epsilonϵ\\na(x,X)a(x, X)a(x,X) — значение алгоритма, обученного на выборке XXX, в точке xxx\\nEx\\\\mathbb{E}_xEx\\u200b — среднее по всем тестовым точкам и EX,ϵ\\\\mathbb{E}_{X, \\\\epsilon}EX,ϵ\\u200b — среднее по всем обучающим выборкам XXX и случайному шуму ϵ\\\\epsilonϵ\\nВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nДля Q(a)Q(a)Q(a) существует разложение на три компоненты — шум, смещение и разброс. Это разложение называется bias-variance decomposition, оно — одно из мощных средств для анализа работы ансамблей. О том, как его вывести, вы узнаете в соответствующем параграфе, а здесь мы приведём его формулировку.\\nСуществует представление Q(a)Q(a)Q(a) в виде трёх компонент:\\nQ(a)=ExbiasX2a(x,X)+ExVX[a(x,X)]+σ2,    Q(a) = \\\\mathbb{E}_x \\\\text{bias}_X^2 a(x, X) + \\\\mathbb{E}_x \\\\mathbb{V}_X[a(x, X)] + \\\\sigma^2,\\nQ(a)=Ex\\u200bbiasX2\\u200ba(x,X)+Ex\\u200bVX\\u200b[a(x,X)]+σ2,где\\nbiasXa(x,X)=f(x)−EX[a(x,X)]    \\\\text{bias}_X a(x, X) = f(x) - \\\\mathbb{E}_X[a(x, X)]\\nbiasX\\u200ba(x,X)=f(x)−EX\\u200b[a(x,X)]\\nэто смещение предсказания алгоритма в точке xxx, усреднённого по всем возможным обучающим выборкам, относительно истинной зависимости fff,\\n\\nVX[a(x,X)]=EX[a(x,X)−EX[a(x,X)]]2    \\\\mathbb{V}_X[a(x, X)] = \\\\mathbb{E}_X \\\\left[ a(x, X) - \\\\mathbb{E}_X[a(x, X)] \\\\right]^2\\nVX\\u200b[a(x,X)]=EX\\u200b[a(x,X)−EX\\u200b[a(x,X)]]2\\nэто дисперсия (разброс) предсказаний алгоритма в зависимости от обучающей выборки XXX,\\n\\nσ2=ExEϵ[y(x,ϵ)−f(x)]2    \\\\sigma^2 = \\\\mathbb{E}_x \\\\mathbb{E}_\\\\epsilon[y(x, \\\\epsilon) - f(x)]^2\\nσ2=Ex\\u200bEϵ\\u200b[y(x,ϵ)−f(x)]2\\nэто неустранимый шум в данных.\\n\\nРаз нам известно, что ошибка алгоритма раскладывается на шум, смещение и разброс, можно подумать над способом сократить ошибку. Будет разумно попытаться сначала уменьшить одну из составляющих. Понятно, что с шумом уже ничего не сделать — это минимально возможная ошибка. Какую можно придумать процедуру, чтобы, например, сократить разброс, не увеличивая смещение?\\nПример приходит из жизни древних греков: если много человек проголосуют независимо друг от друга, то вместе они придут к разумному решению несмотря на то, что опыт каждого из них субъективен. Аналогом голосования в мире машинного обучения является бэггинг.\\nБэггинг\\nИдея бэггинга (bagging, bootstrap aggregation) заключается в следующем. Пусть обучающая выборка состояла из nnn объектов. Выберем из неё nnn примеров равновероятно, с возвращением. Получим новую выборку X1X^1X1, в которой некоторых элементов исходной выборки не будет, а какие-то могут войти несколько раз. С помощью некоторого алгоритма bbb обучим на этой выборке модель b1(x)=b(x,X1)b_1(x) = b(x, X^1)b1\\u200b(x)=b(x,X1). Повторим процедуру: сформируем вторую выборку X2X^2X2 из nnn элементов с возвращением и с помощью того же алгоритма обучим на ней модель b2(x)=b(x,X2)b_2(x) = b(x, X^2)b2\\u200b(x)=b(x,X2). Повторив процедуру kkk раз, получим kkk моделей, обученных на kkk выборках. Чтобы получить одно предсказание, усредним предсказания всех моделей:\\na(x)=1k(b1(x)+⋯+bk(x)).a(x) = \\\\frac{1}{k}(b_1(x) + \\\\dots + b_k(x)).\\na(x)=k1\\u200b(b1\\u200b(x)+⋯+bk\\u200b(x)).Процесс генерации подвыборок с помощью семплирования с возвращением называется бутстрепом (bootstrap), а модели b1(x),…,bk(x)b_1(x), \\\\ldots, b_k(x)b1\\u200b(x),…,bk\\u200b(x) часто называют базовыми алгоритмами (хотя, наверное, лучше было бы назвать их базовыми моделями). Модель a(x)a(x)a(x) называется ансамблем этих моделей.\\nПосмотрим, что происходит с качеством предсказания при переходе от одной модели к ансамблю. Сначала убедимся, что смещение ансамбля не изменилось по сравнению со средним смещением отдельных моделей. Будем считать, что когда мы берём матожидание по всем обучающим выборкам XXX, то в эти выборки включены также все подвыборки, полученные бутстрепом.\\nbiasXa(x,X)=f(x)−EX[a(x,X)]=f(x)−EX[1k∑i=1kb(x,Xi)]=    \\\\color{#348FEA}{\\\\text{bias}_X a(x, X) =} f(x) - \\\\mathbb{E}_X[a(x, X)] = f(x) - \\\\mathbb{E}_X \\\\left[ \\\\frac{1}{k} \\\\sum_{i = 1}^k b(x, X^i) \\\\right] =  \\nbiasX\\u200ba(x,X)=f(x)−EX\\u200b[a(x,X)]=f(x)−EX\\u200b[k1\\u200bi=1∑k\\u200bb(x,Xi)]==f(x)−1k∑i=1kEX[b(x,Xi)]=f(x)−1k∑i=1kEX[b(x,X)]=f(x)−EXb(x,X)    = f(x) - \\\\frac{1}{k} \\\\sum_{i = 1}^k \\\\mathbb{E}_X \\\\left[ b(x, X^i) \\\\right] = f(x) - \\\\frac{1}{k} \\\\sum_{i = 1}^k \\\\mathbb{E}_X \\\\left[ b(x, X) \\\\right] = f(x) - \\\\mathbb{E}_X b(x, X)\\n=f(x)−k1\\u200bi=1∑k\\u200bEX\\u200b[b(x,Xi)]=f(x)−k1\\u200bi=1∑k\\u200bEX\\u200b[b(x,X)]=f(x)−EX\\u200bb(x,X)=f(x)−EXb(x,X)=biasXb(x,X)    = f(x) - \\\\mathbb{E}_X b(x, X) \\\\color{#348FEA}{= \\\\text{bias}_X b(x, X)}\\n=f(x)−EX\\u200bb(x,X)=biasX\\u200bb(x,X)Получили, что смещение композиции равно смещению одного алгоритма. Теперь посмотрим, что происходит с разбросом.\\nVX[a(x,X)]=EX[a(x,X)−EX[a(x,X)]]2=\\\\mathbb{V}_X[a(x, X)] = \\\\mathbb{E}_X \\\\left[ a(x, X) - \\\\mathbb{E}_X[a(x, X)] \\\\right]^2 = \\nVX\\u200b[a(x,X)]=EX\\u200b[a(x,X)−EX\\u200b[a(x,X)]]2==EX[1k∑i=1kb(x,Xi)−EX[1k∑i=1kb(x,Xi)]]2== \\\\mathbb{E}_X \\\\left[ \\\\frac{1}{k} \\\\sum_{i = 1}^k b(x, X^i)- \\\\mathbb{E}_X \\\\left[ \\\\frac{1}{k} \\\\sum_{i = 1}^k b(x, X^i) \\\\right] \\\\right]^2 = \\n=EX\\u200b[k1\\u200bi=1∑k\\u200bb(x,Xi)−EX\\u200b[k1\\u200bi=1∑k\\u200bb(x,Xi)]]2==1k2EX[∑i=1k(b(x,Xi)−EXb(x,Xi))]2== \\\\frac{1}{k^2} \\\\mathbb{E}_X \\\\left[ \\\\sum_{i = 1}^k \\\\left( b(x, X^i) - \\\\mathbb{E}_X b(x, X^i) \\\\right) \\\\right]^2 = \\n=k21\\u200bEX\\u200b[i=1∑k\\u200b(b(x,Xi)−EX\\u200bb(x,Xi))]2==1k2∑i=1kEX(b(x,Xi)−EXb(x,Xi))2+= \\\\frac{1}{k^2} \\\\sum_{i = 1}^k \\\\mathbb{E}_X (b(x, X^i) - \\\\mathbb{E}_X b(x, X^i))^2 + \\n=k21\\u200bi=1∑k\\u200bEX\\u200b(b(x,Xi)−EX\\u200bb(x,Xi))2++1k2∑k1≠k2EX[(b(x,Xk1)−EXb(x,Xk1))(b(x,Xk2)−EXb(x,Xk2))]=+ \\\\frac{1}{k^2} \\\\sum_{k_1 \\\\ne k_2} \\\\mathbb{E}_X \\\\left[ \\\\left( b(x, X^{k_1}) - \\\\mathbb{E}_X b(x, X^{k_1}) \\\\right) \\\\left( b(x, X^{k_2}) - \\\\mathbb{E}_X b(x, X^{k_2}) \\\\right) \\\\right] = \\n+k21\\u200bk1\\u200b\\ue020=k2\\u200b∑\\u200bEX\\u200b[(b(x,Xk1\\u200b)−EX\\u200bb(x,Xk1\\u200b))(b(x,Xk2\\u200b)−EX\\u200bb(x,Xk2\\u200b))]==1k2∑i=1kVXb(x,Xi)+1k2∑k1≠k2cov(b(x,Xk1),b(x,Xk2))= \\\\frac{1}{k^2} \\\\sum_{i = 1}^k \\\\mathbb{V}_X b(x, X^i) + \\\\frac{1}{k^2} \\\\sum_{k_1 \\\\ne k_2} \\\\text{cov} \\\\left( b(x, X^{k_1}), b(x, X^{k_2}) \\\\right)\\n=k21\\u200bi=1∑k\\u200bVX\\u200bb(x,Xi)+k21\\u200bk1\\u200b\\ue020=k2\\u200b∑\\u200bcov(b(x,Xk1\\u200b),b(x,Xk2\\u200b))Если предположить, что базовые алгоритмы некоррелированы, то:\\nVX[a(x,X)]=1k2∑i=1kVXb(x,Xi)=\\\\color{#348FEA}{\\\\mathbb{V}_X[a(x, X)] =} \\\\frac{1}{k^2} \\\\sum_{i = 1}^k \\\\mathbb{V}_X b(x, X^i) = \\nVX\\u200b[a(x,X)]=k21\\u200bi=1∑k\\u200bVX\\u200bb(x,Xi)==1k2∑i=1kVXb(x,X)=1kVXb(x,X)= \\\\frac{1}{k^2} \\\\sum_{i = 1}^k \\\\mathbb{V}_X b(x, X) \\\\color{#348FEA}{= \\\\frac{1}{k} \\\\mathbb{V}_X b(x, X)}\\n=k21\\u200bi=1∑k\\u200bVX\\u200bb(x,X)=k1\\u200bVX\\u200bb(x,X)Получилось, что в этом случае дисперсия композиции в kkk раз меньше дисперсии отдельного алгоритма.\\nПример: бэггинг над решающими деревьями\\nПусть наша целевая зависимость f(x)f(x)f(x) задаётся как\\nf(x)=xsin\\u2061x,    f(x) = x \\\\sin x,\\nf(x)=xsinx,и к ней добавляется нормальный шум ϵ∼N(0,9)\\\\epsilon \\\\sim \\\\mathcal{N}(0, 9)ϵ∼N(0,9).  Пример семпла из таких данных:\\n\\nПопробуем посмотреть, как выглядят предсказания решающих деревьев глубины 7 и бэггинга над такими деревьями в зависимости от обучающей выборки. Обучим решающие деревья 100 раз на различных случайных семплах размера 20. Возьмём также бэггинг над 10 решающими деревьями глубины 7 в качестве базовых классификаторов и тоже 100 раз обучим его на случайных выборках размера 20. Если изобразить предсказания обученных моделей на каждой из 100 итераций, то можно увидеть примерно такую картину:\\n\\nПо этому рисунку видно, что общая дисперсия предсказаний в зависимости от обучающего множества у бэггинга значительно ниже, чем у отдельных деревьев, а в среднем предсказания деревьев и бэггинга не отличаются.\\nЧтобы подтвердить это наблюдение, мы можем изобразить смещение и разброс случайных деревьев и бэггинга в зависимости от максимальной глубины:\\n\\nНа графике видно, как значительно бэггинг сократил дисперсию. На самом деле, дисперсия уменьшилась практически в 10 раз, что равняется числу базовых алгоритмов (kkk), которые бэггинг использовал для предсказания:\\n\\nКод для отрисовки картинок и подсчёта смещения и разброса можно найти тут.\\nRandom Forest\\nВ предыдущем разделе мы сделали предположение, что базовые алгоритмы некоррелированы, и за счёт этого получили очень сильное уменьшение дисперсии у ансамбля относительно входящих в него базовых алгоритмов. Однако в реальной жизни добиться этого сложно: ведь базовые алгоритмы учили одну и ту же зависимость на пересекающихся выборках. Поэтому будет странно, если корреляция на самом деле нулевая. Но на практике оказывается, что строгое выполнение этого предположения не обязательно. Достаточно, чтобы алгоритмы были в некоторой степени не похожи друг на друга. На этом строится развитие идеи бэггинга для решающих деревьев — случайный лес.\\nПостроим ансамбль алгоритмов, где базовый алгоритм — это решающее дерево. Будем строить по следующей схеме:\\n\\n\\nДля построения iii-го дерева:\\n\\n\\nСначала, как в обычном бэггинге, из обучающей выборки XXX выбирается с возвращением случайная подвыборка XiX^iXi того же размера, что и XXX.\\n\\n\\nВ процессе обучения каждого дерева в каждой вершине случайно выбираются n<Nn < Nn<N признаков, где NNN — полное число признаков (метод случайных подпространств), и среди них ищется оптимальный сплит. Такой приём как раз позволяет управлять степенью скоррелированности базовых алгоритмов.\\n\\n\\n\\n\\nЧтобы получить предсказание ансамбля на тестовом объекте, усредняем отдельные ответы деревьев (для регрессии) или берём самый популярный класс (для классификации).\\n\\n\\nProfit. Мы построили Random Forest (случайный лес) — комбинацию бэггинга и метода случайных подпространств над решающими деревьями.\\n\\n\\nВнимательный читатель мог заметить, что при построении случайного леса у специалиста по машинному обучению есть несколько степеней свободы. Давайте обсудим их подробнее.\\nКакая должна быть глубина деревьев в случайном лесу?\\nОшибка модели (на которую мы можем повлиять) состоит из смещения и разброса. Разброс мы уменьшаем с помощью процедуры бэггинга. На смещение бэггинг не влияет, а хочется, чтобы у леса оно было небольшим. Поэтому смещение должно быть небольшим у самих деревьев, из которых строится ансамбль.\\nУ неглубоких деревьев малое число параметров, то есть дерево способно запомнить только верхнеуровневые статистики обучающей подвыборки. Они во всех подвыборках будут похожи, но будут не очень подробно описывать целевую зависимость. Поэтому при изменении обучающей подвыборки предсказание на тестовом объекте будет стабильным, но не точным (низкая дисперсия, высокое смещение).\\nНаоборот, у глубоких деревьев нет проблем запомнить подвыборку подробно. Поэтому предсказание на тестовом объекте будет сильнее меняться в зависимости от обучающей подвыборки, зато в среднем будет близко к истине (высокая дисперсия, низкое смещение).\\nВывод: используем глубокие деревья.\\nСколько признаков надо подавать дереву для обучения?\\nОграничивая число признаков, которые используются в обучении одного дерева, мы также управляем качеством случайного леса. Чем больше признаков, тем больше корреляция между деревьями и тем меньше чувствуется эффект от ансамблирования. Чем меньше признаков, тем слабее сами деревья.\\nПрактическая рекомендация — брать корень из числа всех признаков для классификации и треть признаков для регрессии.\\nСколько должно быть деревьев в случайном лесе?\\nВыше было показано, что увеличение числа элементарных алгоритмов в ансамбле не меняет смещения и уменьшает разброс. Так как число признаков и варианты подвыборок, на которых строятся деревья в случайном лесе, ограничены, уменьшать разброс до бесконечности не получится. Поэтому имеет смысл построить график ошибки от числа деревьев и ограничить размер леса в тот момент, когда ошибка перестанет значимо уменьшаться.\\nВторым практическим ограничением на количество деревьев может быть время работы ансамбля. Однако есть положительное свойство случайного леса: случайный лес можно строить и применять параллельно, что сокращает время работы, если у нас есть несколько процессоров. Но процессоров, скорее всего, всё же сильно меньше числа деревьев, а сами деревья обычно глубокие. Поэтому на большом числе деревьев Random Forest может работать дольше желаемого и количество деревьев можно сократить, немного пожертвовав качеством.\\nБустинг\\nБустинг (boosting) — это ансамблевый метод, в котором так же, как и в методах выше, строится множество базовых алгоритмов из одного семейства, объединяющихся затем в более сильную модель. Отличие состоит в том, что в бэггинге и случайном лесе базовые алгоритмы учатся независимо и параллельно, а в бустинге — последовательно.\\n\\n\\n\\n    Автор изображения –\\n    Joseph Rocca.\\n  \\n\\nКаждый следующий базовый алгоритм в бустинге обучается так, чтобы уменьшить общую ошибку  всех своих предшественников. Как следствие, итоговая композиция будет иметь меньшее смещение, чем каждый отдельный базовый алгоритм (хотя уменьшение разброса также может происходить).\\nПоскольку основная цель бустинга — уменьшение смещения, в качестве базовых алгоритмов часто выбирают алгоритмы с высоким смещением и небольшим разбросом. Например, если в качестве базовых классификаторов выступают деревья, то их глубина должна быть небольшой — обычно не больше 2-3 уровней.\\nЕщё одной важной причиной для выбора моделей с высоким смещением в качестве базовых является то, что такие модели, как правило, быстрее учатся.  Это важно для их последовательного обучения, которое может стать очень дорогим по времени, если на каждой итерации будет учиться сложная модель. На текущий момент основным видом бустинга с точки зрения применения на практике является градиентный бустинг, о котором подробно рассказывается в соответствующем параграфе.\\nХотя случайный лес — мощный и достаточно простой для понимания и реализации алгоритм, на практике он чаще всего уступает градиентному бустингу. Поэтому градиентный бустинг сейчас — основное продакшн-решение, если работа происходит с табличными данными (в работе с однородными данными — картинками, текстами — доминируют нейросети).\\nСтекинг\\nСтекинг (stacking) — алгоритм ансамблирования, основные отличия которого от предыдущих состоят в следующем:\\n\\nон может использовать алгоритмы разного типа, а не только из какого-то фиксированного семейства. Например, в качестве базовых алгоритмов могут выступать метод ближайших соседей и линейная регрессия\\nрезультаты базовых алгоритмов объединяются в один с помощью обучаемой мета-модели, а не с помощью какого-либо обычного способа агрегации (суммирования или усреднения)\\n\\nОбучение стекинга проходит в несколько этапов:\\n\\nобщая выборка разделяется на тренировочную и тестовую\\nтренировочная выборка делится на nnn фолдов. Затем эти фолды перебираются тем же способом, что используется при кросс-валидации: на каждом шаге фиксируются (n−1)(n - 1)(n−1) фолдов для обучения базовых алгоритмов и один — для их предсказаний (вычисления мета-факторов). Такой подход нужен для того, чтобы можно было использовать всё тренировочное множество, и при этом базовые алгоритмы не переобучались\\n\\n\\n\\n\\n    Автор изображения — \\n    Steven Yu.  \\n  \\n\\n\\nна полученных мета-факторах обучается мета-модель. Кроме мета-факторов, она может принимать на вход и фичи из исходного датасета. Выбор зависит от решаемой задачи\\n\\n\\n\\n\\n    Автор изображения — \\n    Steven Yu. \\n  \\n\\nДля получения мета-факторов на тестовом множестве базовые алгоритмы можно обучить на всём тренировочном множестве — переобучения в данном случае возникнуть не должно.\\nЕсли данных достаточно много, то можно просто разделить обучающие данные на две непересекающиеся части: ту, на которой учатся базовые алгоритмы, и ту, на которой они делают свои предсказания и обучается мета-модель. Использование такого простого разбиения вместо кросс-валидации на тренировочных данных иногда называют блендингом (blending). Если данных совсем много, то тестовое множество тоже можно разделить на две части: тестовую и валидационную, и использовать последнюю для подбора гиперпараметров моделей-участников.\\nС точки зрения смещения и разброса стекинг не имеет прямой интерпретации, так как не минимизирует напрямую ни ту, ни другую компоненту ошибки. Удачно работающий стекинг просто уменьшает ошибку, и, как следствие, её компоненты тоже будут убывать.\\nПочитать по теме\\n\\nЛекция Евгения Соколова про bias-variance decomposition и бэггинг\\nБлог-пост про ансамбли от Joseph Rocca\\nБлог-пост про стекинг и блендинг от Steven Yu\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф2.3. Решающие деревьяОбучение древесных моделей для классификации и\\xa0регрессии. Эффективное построение решающих деревьевСледующий параграф2.5. Градиентный бустингКак устроено самое мощное семейство не-нейросетевых моделей: градиентный бустинг над решающими деревьямиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_38.html', 'title': 'Рекомендации на основе матричных разложений'}, page_content='Рекомендации на основе матричных разложенийЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/49.1.Введение в рекомендательные системы9.2.Рекомендации на основе матричных разложенийВведениеСвязь с задачей матричной факторизацииПостановка задачиAlternating Least Squares (ALS)IALS (Implicit ALS)Обобщения ALS и IALSFunkSVDSingular Value Decomposition with implicit feedback (SVD++)Collaborative Filtering with Temporal Dynamics (timeSVD++)SLIM (Sparse Linear Methods)ИтогиСписок литературы9.3.Контентные рекомендации9.4.Хорошие свойства рекомендательных систем10.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Рекомендации на основе матричных разложений9.2. Рекомендации на основе матричных разложенийАвторыАби ПалагашвилиВведение\\nДопустим, мы работаем в сервисе рекомендаций фильмов и перед нами стоит задача подобрать для каждого пользователя набор наиболее релевантных фильмов. Пользователь может разными способами провзаимодействовать с фильмом: посмотреть его, оставить отзыв, поставить оценку (например, от 1 до 5).\\nВ этом параграфе мы будем строить рекомендации на основе матрицы оценок user-item. Её строки соответствуют объектам, а столбцы – пользователям. На (i,j)(i, j)(i,j)-й позиции матрицы мы ставим либо пропуск, либо оценку, выставленную iii-му объекту jjj-м пользователем. Разумеется, не все оценки нам известны: вряд ли каждый пользователь имел возможность ознакомиться с каждым объектом. В процессе решения задачи мы будем пытаться восстановить оценки на местах пропусков. Сделав это, мы сможем, например, порекомендовать пользователю те объекты, которые он ещё не смотрел, но предсказанная оценка которых для этого пользователя максимальна.\\n\\nВсе типы взаимодействия пользователей с объектами мы можем рассматривать как пользовательский фидбек. Обычно различают явный (explicit) и неявный (implicit) виды фидбека. Фидбек называется явным, если он отражает степень интереса пользователя к объекту. Например, к этому типу относят рейтинги, лайки и дизлайки. Такого фидбека обычно мало, он поступает только от тех пользователей, которые соглашаются нам его дать.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nОбычно гораздо больше информации имеется о неявных предпочтениях – просмотры, клики, добавление в закладки. Но если пользователь, например, посмотрел фильм, мы ещё не можем сделать вывод, что он ему понравился. Мы можем лишь утверждать, что до просмотра этот фильм казался ему достаточно интересным. Поэтому обычно неявная обратная связь более шумная, чем явная.\\nДля начала научимся работать с явным фидбеком.\\nСвязь с задачей матричной факторизации\\nВернёмся к задаче восстановления матрицы оценок и предположим, что каждый пользователь и объект можно закодировать набором из SSS скрытых признаков, а оценка iii-го объекта uuu-м пользователем равна скалярному произведению соответствующих векторов скрытых представлений xux_uxu\\u200b и yiy_iyi\\u200b. Тогда если бы наша матрица оценок была заполнена полностью, её можно было бы представить в виде произведений двух матриц XXX и YYY, составленных по столбцам из скрытых представлений пользователей и объектов:\\nU=XT⋅YU = X^T \\\\cdot Y\\nU=XT⋅Y\\nПравда, в таком случае нам бы и не требовалось ничего решать: мы могли бы просто рекомендовать пользователю объекты с самыми высокими оценками в соответствующей строке. Но суровая реальность такова, что зачастую матрица оценок сильно разрежена. Мы можем поступить следующим образом: восстановить латентные векторы для пользователей и объектов по имеющемуся набору оценок, после чего предсказать оценки для всех отсутствующих позиций. В параграфе, посвящённом матричной факторизации, мы уже обсуждали способы решения данной задачи с помощью SVD и стохастического градиентного спуска. У SVD есть существенные недостатки: из-за большого количества пропусков в матрице полученное решение будет слишком шумным, а кроме того, его придется каждый раз рассчитывать заново при добавлении новых пользователей или объектов. Градиентный спуск не имеет данных проблем, но тоже не очень практичен. В этом параграфе мы рассмотрим более эффективный алгоритм, называемый Alternating Least Squares (ALS).\\nПостановка задачи\\nПусть, как и раньше, xu,yix_u, y_ixu\\u200b,yi\\u200b – скрытые представления пользователей и объектов соответственно размерности TTT. Запишем эти векторы по строкам в матрицы XXX и YYY размера S×NS\\\\times NS×N и S×DS\\\\times DS×D соответственно, где NNN – количество пользователей, а DDD – количество объектов.\\nОбозначим через RRR множество таких пар (u,i)(u, i)(u,i) пользователей и объектов, для которых имеются явно проставленные оценки.\\nПредсказывать рейтинги мы будем как скалярное произведение скрытых представлений:\\nr^ui=xuTyi\\\\hat{r}_{ui} = x_u^Ty_i\\nr^ui\\u200b=xuT\\u200byi\\u200bВ результате мы приходим к следующей задаче оптимизации. Мы хотим научиться как можно лучше приближать известные рейтинги:\\nmin\\u2061xu,yi∑(u,i)∈R(rui−xuTyi)2\\\\min_{x_u, y_i} \\\\sum\\\\limits_{(u,i) \\\\in R} (r_{ui} - x_u^Ty_i)^2  \\nxu\\u200b,yi\\u200bmin\\u200b(u,i)∈R∑\\u200b(rui\\u200b−xuT\\u200byi\\u200b)2Добавив регуляризацию получаем следующую функцию потерь:\\nmin\\u2061xu,yi∑(u,i)∈R(rui−xuTyi)2+λ∑∀u∣∣xu∣∣2Cu+λ∑∀i∣∣yi∣∣2Ci\\\\min_{x_u, y_i} \\\\sum\\\\limits_{(u,i) \\\\in R}(r_{ui}-x_{u}^{T}y_i)^2+\\\\lambda\\\\sum\\\\limits_{\\\\forall u}||x_u||^2C_u+\\\\lambda\\\\sum\\\\limits_{\\\\forall i}||y_i||^2C_i\\nxu\\u200b,yi\\u200bmin\\u200b(u,i)∈R∑\\u200b(rui\\u200b−xuT\\u200byi\\u200b)2+λ∀u∑\\u200b∣∣xu\\u200b∣∣2Cu\\u200b+λ∀i∑\\u200b∣∣yi\\u200b∣∣2Ci\\u200bAlternating Least Squares (ALS)\\nОптимальные параметры можно найти с помощью хорошо знакомого нам градиентного спуска, но есть более быстрые и надёжные способы. Если мысленно заморозить параметры, соответствующие латентным факторам пользователей, задача оптимизации латентных представлений объектов сведётся к задаче наименьших квадратов, для которой мы знаем точное решение.\\nИтоговый процесс оптимизации функции потерь будет иметь следующий вид.\\nВ цикле до сходимости:\\n\\nФиксируем матрицу XXX (скрытые представления пользователей);\\nРешаем задачу L2-регуляризованной регрессии для каждого товара и находим оптимальную матрицу YYY;\\nФиксируем матрицу YYY (скрытые представления объектов);\\nРешаем задачу L2-регуляризованной регрессии для каждого пользователя и находим оптимальную матрицу XXX;\\n\\nРешение, получаемое путём попеременного вычисления точных аналитических решений, обычно точнее тех, что получаются с помощью наивного градиентного спуска. Более того, данное решение имеет эффективную реализацию, позволяющую использовать преимущества параллельных вычислений.\\nДля лучшего понимания распишем каждый шаг данного алгоритма оптимизации:\\nALS - шаг по (одному) xux_uxu\\u200b:\\nargminxu∑(u,i)∈R(rui−xuTyi)2+λ∑∀u∣∣xu∣∣2Cu+λ∑∀i∣∣yi∣∣2Ci\\\\underset{x_u}{\\\\mathrm{argmin}} \\\\sum\\\\limits_{(u,i) \\\\in R}^{}(r_{ui}-x_{u}^{T}y_i)^2+\\\\lambda\\\\sum\\\\limits_{\\\\forall u}||x_u||^2C_u + \\\\lambda\\\\sum\\\\limits_{\\\\forall i}||y_i||^2C_i\\nxu\\u200bargmin\\u200b(u,i)∈R∑\\u200b(rui\\u200b−xuT\\u200byi\\u200b)2+λ∀u∑\\u200b∣∣xu\\u200b∣∣2Cu\\u200b+λ∀i∑\\u200b∣∣yi\\u200b∣∣2Ci\\u200bРаскроем квадратичный член:\\nargminxu∑(u,i)∈Rrui2−2∑(u,i)∈RruixuTyi+∑(u,i)∈R(xuTyi)2+λ∑∀u∣∣xu∣∣2Cu+λ∑∀i∣∣yi∣∣2Ci\\\\underset{x_u}{\\\\mathrm{argmin}} \\\\sum\\\\limits_{(u, i) \\\\in R}^{} r_{ui}^2 - 2 \\\\sum\\\\limits_{(u,i) \\\\in R}^{} r_{ui}x_{u}^{T}y_i + \\\\sum\\\\limits_{(u,i) \\\\in R}^{} (x_{u}^{T}y_i)^2\\n+\\\\lambda\\\\sum\\\\limits_{\\\\forall u}||x_u||^2C_u + \\\\lambda\\\\sum\\\\limits_{\\\\forall i}||y_i||^2C_i\\nxu\\u200bargmin\\u200b(u,i)∈R∑\\u200brui2\\u200b−2(u,i)∈R∑\\u200brui\\u200bxuT\\u200byi\\u200b+(u,i)∈R∑\\u200b(xuT\\u200byi\\u200b)2+λ∀u∑\\u200b∣∣xu\\u200b∣∣2Cu\\u200b+λ∀i∑\\u200b∣∣yi\\u200b∣∣2Ci\\u200bВ первой сумме константы, они уходят. Из второй и третьей возьмём только те слагаемые, в которых участвует xux_uxu\\u200b. Из четвёртой остается только член с xux_uxu\\u200b, так как все xvx_vxv\\u200b независимы. Последняя сумма пропадает, так как xvx_vxv\\u200b и yjy_jyj\\u200b независимы:\\nargminxu−2∑i:\\u2009(u,i)∈RruixuTyi+∑i:\\u2009(u,i)∈R(xuTyi)2+λCuxuTxu=\\\\underset{x_u}{\\\\mathrm{argmin}}  - 2 \\\\sum\\\\limits_{i:\\\\,(u,i) \\\\in R}^{} r_{ui}x_{u}^{T}y_i + \\\\sum\\\\limits_{i:\\\\,(u,i) \\\\in R}^{} (x_{u}^{T}y_i)^2 + \\\\lambda C_u x_u^T x_u = \\nxu\\u200bargmin\\u200b−2i:(u,i)∈R∑\\u200brui\\u200bxuT\\u200byi\\u200b+i:(u,i)∈R∑\\u200b(xuT\\u200byi\\u200b)2+λCu\\u200bxuT\\u200bxu\\u200b=В первой сумме индекс uuu фиксирован, поэтому xux_uxu\\u200b можно вынести за знак суммы:\\nargminxu−2xuT∑(u,i)∈Rruiyi+∑(u,i)∈RxuTyi⋅xuTyi+λCu(xu,xu)=\\\\underset{x_u}{\\\\mathrm{argmin}} -2x_{u}^{T} \\\\sum\\\\limits_{(u, i) \\\\in R}^{} r_{ui}y_i + \\\\sum\\\\limits_{(u, i) \\\\in R}^{} x_{u}^{T}y_i \\\\cdot x_{u}^{T}y_i + \\\\lambda C_u (x_u, x_u)  = \\nxu\\u200bargmin\\u200b−2xuT\\u200b(u,i)∈R∑\\u200brui\\u200byi\\u200b+(u,i)∈R∑\\u200bxuT\\u200byi\\u200b⋅xuT\\u200byi\\u200b+λCu\\u200b(xu\\u200b,xu\\u200b)=Объединим второй и третий члены формулы, вынесем умножение на xux_uxu\\u200b за скобки:\\nargminxu−2xuT(∑(u,i)∈Rruiyi)+xuT(∑(u,i)∈RyiyiT+λCu)xu=\\\\underset{x_u}{\\\\mathrm{argmin}} -2x_{u}^{T} \\\\Bigl( \\\\sum\\\\limits_{(u,i) \\\\in R}^{} r_{ui}y_i \\\\Bigr) + x_u^{T} \\\\Bigl( \\\\sum\\\\limits_{(u,i) \\\\in R}^{} y_i y_i^T + \\\\lambda C_u \\\\Bigr) x_u = \\nxu\\u200bargmin\\u200b−2xuT\\u200b((u,i)∈R∑\\u200brui\\u200byi\\u200b)+xuT\\u200b((u,i)∈R∑\\u200byi\\u200byiT\\u200b+λCu\\u200b)xu\\u200b=Теперь воспользуемся тем, что\\nargminxu−2xuTBu+xuTAuxu=Au−1Bu\\\\underset{x_u}{\\\\mathrm{argmin}} -2x_u^TB_u + x_u^TA_ux_u = A_u^{-1}B_u\\nxu\\u200bargmin\\u200b−2xuT\\u200bBu\\u200b+xuT\\u200bAu\\u200bxu\\u200b=Au−1\\u200bBu\\u200bи выпишем ответ:\\nxu∗=(∑i:(u,i)∈RyjyjT+λCiI)−1(∑j:(i,j)∈Rrijyj)x_u^* = \\n\\\\Bigl( \\\\sum\\\\limits_{i: (u, i) \\\\in R}^{} y_jy_j^T + \\\\lambda C_i I \\\\Bigr)^{-1} \\\\Bigl( \\\\sum\\\\limits_{j: (i,j) \\\\in R}^{} r_{ij}y_j \\\\Bigr)\\nxu∗\\u200b=(i:(u,i)∈R∑\\u200byj\\u200byjT\\u200b+λCi\\u200bI)−1(j:(i,j)∈R∑\\u200brij\\u200byj\\u200b)Таким образом, мы получили аналитическое выражение для вычисления каждого xux_uxu\\u200b на шаге алгоритма. Отметим, что каждый вектор xux_uxu\\u200b мы можем вычислить независимо от других xvx_vxv\\u200b. Данное наблюдение позволяет нам использовать всю мощь параллельных вычислений для эффективного решения оптимизационной задачи. Распределив данные так, что на каждой вычислительной машине хранятся все yiy_iyi\\u200b для некоторого подмножества xux_uxu\\u200b, на одной итерации алгоритма мы можем параллельно вычислить все xux_uxu\\u200b. На следующей итерации аналогичным образом вычисляем все yiy_iyi\\u200b.\\nIALS (Implicit ALS)\\nОригинальная статья\\nРаньше мы работали с матрицей RRR как с матрицей рейтингов, явно проставленных пользователем. Как мы говорили выше, такого фидбека обычно довольно мало, а куда больше неявного фидбека. При этом количество данных может быть критичным при работе с такими разреженными структурами, как матрицы рейтингов, поэтому хочется научиться работать и с неявным фидбеком тоже.\\nНеявным фидбеком является в том числе и факт взаимодействия, поэтому мы можем заполнить всю матрицу user-item целиком: на тех позициях, где пользователь положительно взаимодействовал с объектом, поставим 111, а на тех, где взаимодействие было негативным или его вообще не произошло, поставим 000. Эта компонента фидбека называется предпочтением (preference):\\npui={1,rui>00,rui≤0\\xa0или\\xa0rui\\xa0не\\xa0определено\\\\begin{equation}\\n    p_{ui} = \\n    \\\\begin{cases}\\n        1, & r_{ui} > 0 \\\\\\\\\\n        0, & r_{ui} \\\\le 0 \\\\text{ или } r_{ui} \\\\text{ не определено}\\n    \\\\end{cases}\\n\\\\end{equation}\\npui\\u200b={1,0,\\u200brui\\u200b>0rui\\u200b≤0\\xa0или\\xa0rui\\u200b\\xa0не\\xa0определено\\u200b\\u200b\\u200bТем самым мы избавились от пропусков в матрице, но использовали не всю информацию. Согласитесь, если один пользователь посмотрел часовое видео польностью, а другой выключил после 5 минут, несправедливо считать, что это видео им понравилось в одинаковой степени. Введём ещё степень уверенности (confidence), отражающую уверенность в оценке пользователя:\\ncui=1+α∣rui∣\\xa0(\\xa0степень\\xa0уверенности\\xa0в\\xa0pui),c_{ui} = 1 + \\\\alpha |r_{ui}|\\\\ (\\\\text{ степень уверенности в } p_{ui}),\\ncui\\u200b=1+α∣rui\\u200b∣\\xa0(\\xa0степень\\xa0уверенности\\xa0в\\xa0pui\\u200b),где α\\\\alphaα – некоторая константа.\\nНа местах пропусков мы явно проставляем pij=0p_{ij} = 0pij\\u200b=0. На остальных позициях мы можем сами регулировать степень уверенности в зависимости от фидбека пользователя.\\nРассмотрим следующую функцию потерь:\\n∑∀u,icui(pui−xuTyi)2+λ∑∀u∣∣xu∣∣2Cu+λ∑∀i∣∣yi∣∣2Ci,\\\\sum\\\\limits_{\\\\forall u,i}^{} c_{ui} (p_{ui} - x_{u}^{T}y_i) ^ 2 + \\\\lambda \\\\sum\\\\limits_{\\\\forall u}^{} ||x_u||^2C_u + \\\\lambda \\\\sum\\\\limits_{\\\\forall i}^{} ||y_i||^2C_i, \\\\\\\\\\n∀u,i∑\\u200bcui\\u200b(pui\\u200b−xuT\\u200byi\\u200b)2+λ∀u∑\\u200b∣∣xu\\u200b∣∣2Cu\\u200b+λ∀i∑\\u200b∣∣yi\\u200b∣∣2Ci\\u200b,Она позволяет:\\n\\nУчитывать неявный фидбек, которого обычно на порядок больше, чем явного,\\nРегулировать степень уверенности в действиях пользователей.\\n\\nIALS: оптимизация\\nРаспишем нашу функцию потерь по аналогии с ALS и приведем к форме −2xuTBu+xuTAuxu-2x_u^TB_u + x_u^TA_ux_u−2xuT\\u200bBu\\u200b+xuT\\u200bAu\\u200bxu\\u200b:\\nargminxu∑u,icui(pui−xuTyi)2+λ∑u∣∣xu∣∣2Cu+λ∑i∣∣yi∣∣2Ci=argminxu∑icuipui2−2∑icuipuixuTyi+∑icui(xuTyi)2+λCuxuTxu=argminxu−2xuT∑∀icuipuiyi+∑∀icuixuTyi⋅xuTyi+λCuxuTxu=argminxi−2xuT(∑∀icuipuiyi)+xuT(∑∀icuiyiyiT+λCu)xu=(∑∀icuiyiyiT+λCuI)−1(∑∀icuipuiyi)=\\\\underset{x_u}{\\\\mathrm{argmin}} \\\\sum\\\\limits_{u,i}^{} c_{ui}(p_{ui} - x_u^{T}y_i)^2 + \\\\lambda \\\\sum\\\\limits_{u}^{} ||x_u||^2C_u + \\\\lambda \\\\sum\\\\limits_{i}^{} ||y_i||^2C_i = \\\\\\\\\\n\\\\underset{x_u}{\\\\mathrm{argmin}} \\\\sum\\\\limits_{i}^{} c_{ui} p_{ui}^2 - \\n2 \\\\sum\\\\limits_{i}^{} c_{ui} p_{ui} x_u^{T} y_i +\\n\\\\sum\\\\limits_{i}^{} c_{ui} (x_u^{T}y_i)^2 + \\\\lambda C_u x_u^Tx_u = \\\\\\\\\\n\\\\underset{x_u}{\\\\mathrm{argmin}} - \\n2 x_u^{T} \\\\sum\\\\limits_{\\\\forall i}^{} c_{ui} p_{ui} y_i +\\n\\\\sum\\\\limits_{\\\\forall i}^{} c_{ui} x_u^{T}y_i \\\\cdot x_u^{T}y_i + \\\\lambda C_u x_u^{T}x_u = \\\\\\\\\\n\\\\underset{x_i}{\\\\mathrm{argmin}} - \\n2 x_u^{T} \\n    \\\\Bigl(\\n        \\\\sum\\\\limits_{\\\\forall i}^{} c_{ui} p_{ui} y_i\\n    \\\\Bigr)  +\\nx_u^{T} \\n    \\\\Bigl(\\n        \\\\sum\\\\limits_{\\\\forall i}^{} c_{ui} y_i y_i ^{T} + \\\\lambda C_u\\n    \\\\Bigr) \\nx_u = \\\\\\\\\\n\\\\Bigl( \\n    \\\\sum\\\\limits_{\\\\forall i}^{} c_{ui}y_{i}y_{i}^{T} + \\\\lambda C_u I \\n\\\\Bigr)^{-1}\\n\\\\Bigl(\\n    \\\\sum\\\\limits_{\\\\forall i}^{} c_{ui}p_{ui}y_{i}\\n\\\\Bigr) = \\nxu\\u200bargmin\\u200bu,i∑\\u200bcui\\u200b(pui\\u200b−xuT\\u200byi\\u200b)2+λu∑\\u200b∣∣xu\\u200b∣∣2Cu\\u200b+λi∑\\u200b∣∣yi\\u200b∣∣2Ci\\u200b=xu\\u200bargmin\\u200bi∑\\u200bcui\\u200bpui2\\u200b−2i∑\\u200bcui\\u200bpui\\u200bxuT\\u200byi\\u200b+i∑\\u200bcui\\u200b(xuT\\u200byi\\u200b)2+λCu\\u200bxuT\\u200bxu\\u200b=xu\\u200bargmin\\u200b−2xuT\\u200b∀i∑\\u200bcui\\u200bpui\\u200byi\\u200b+∀i∑\\u200bcui\\u200bxuT\\u200byi\\u200b⋅xuT\\u200byi\\u200b+λCu\\u200bxuT\\u200bxu\\u200b=xi\\u200bargmin\\u200b−2xuT\\u200b(∀i∑\\u200bcui\\u200bpui\\u200byi\\u200b)+xuT\\u200b(∀i∑\\u200bcui\\u200byi\\u200byiT\\u200b+λCu\\u200b)xu\\u200b=(∀i∑\\u200bcui\\u200byi\\u200byiT\\u200b+λCu\\u200bI)−1(∀i∑\\u200bcui\\u200bpui\\u200byi\\u200b)=Разобьём сумму на 2 части. В первой будет сумма по тем элементам, с которыми у пользователя не было положительного взаимодействия. Во второй – сумма по всем остальным элементам. Также заметим, что во втором множителе суммирование имеет смысл только по ненулевым элементам:\\n(∑∀i:pui=0cuiyi⋅yiT+∑∀i:pui≠0cuiyi⋅yiT+λCuI)−1(∑∀i:pui≠0cuipuiyi)=\\\\Bigl(\\n    \\\\sum\\\\limits_{\\\\forall i: p_{ui}=0}^{}c_{ui}y_i \\\\cdot y_{i}^{T} + \\\\sum\\\\limits_{\\\\forall i: p_{ui}\\\\neq 0}^{} c_{ui}y_i \\\\cdot y_{i}^{T} + \\\\lambda C_u I\\n\\\\Bigr)^{-1} \\n\\\\Bigl(\\n    \\\\sum\\\\limits_{\\\\forall i: p_{ui}\\\\neq 0}^{} c_{ui}p_{ui}y_i\\n\\\\Bigr) = \\n(∀i:pui\\u200b=0∑\\u200bcui\\u200byi\\u200b⋅yiT\\u200b+∀i:pui\\u200b\\ue020=0∑\\u200bcui\\u200byi\\u200b⋅yiT\\u200b+λCu\\u200bI)−1(∀i:pui\\u200b\\ue020=0∑\\u200bcui\\u200bpui\\u200byi\\u200b)=Заметим, что в первой сумме все cuic_{ui}cui\\u200b будут равны 1 (так как везде pui=0p_{ui} = 0pui\\u200b=0). Прибавим и вычтем единицу к cuic_{ui}cui\\u200b во второй сумме и разобьем её на две компоненты. Вторый из них будет сумма по всем yiyiTy_iy_i^Tyi\\u200byiT\\u200b, где pui≠0p_{ui} \\\\neq 0pui\\u200b\\ue020=0. Объединив её с первой суммой, получимпросто YTYY^{T}YYTY:\\n(YTY+λCuI+∑∀i:pui≠0(cui−1)yiyiT)−1(∑∀i:pui≠0cuipuiyi)\\\\Bigl(\\n    Y^{T}Y + \\\\lambda C_u I + \\\\sum\\\\limits_{\\\\forall i: p_{ui}\\\\neq 0}^{} (c_{ui} - 1)y_{i}y_{i}^{T}\\n\\\\Bigr)^{-1}\\n\\\\Bigl(\\n    \\\\sum\\\\limits_{\\\\forall i: p_{ui}\\\\neq 0}^{} c_{ui}p_{ui}y_{i}\\n\\\\Bigr)\\n(YTY+λCu\\u200bI+∀i:pui\\u200b\\ue020=0∑\\u200b(cui\\u200b−1)yi\\u200byiT\\u200b)−1(∀i:pui\\u200b\\ue020=0∑\\u200bcui\\u200bpui\\u200byi\\u200b)Заметим, что произведение YTYY^{T}YYTY никак не зависит от uuu. Мы можем посчитать его один раз для всех xux_uxu\\u200b перед очередной итерацией. В остальном же мы точно так же, как и в случае с обычным ALS, можем распределить данные так, чтобы на одной машине содержались все yjy_jyj\\u200b, необходимые для обновления xvx_vxv\\u200b, хранящихся на этой машине, и сделать следующий шаг оптимизации нашей функции потерь.\\nОбобщения ALS и IALS\\n\\nОбе модели: и ALS, и Imlicit ALS – можно несколько усложнить, вместо rui≈xuyir_{ui} \\\\approx x_{u}y_{i}rui\\u200b≈xu\\u200byi\\u200b рассмотрев rui≈xuyi+bu+bi+μr_{ui} \\\\approx x_{u}y_{i} + b_{u} + b_{i} + \\\\murui\\u200b≈xu\\u200byi\\u200b+bu\\u200b+bi\\u200b+μ. В таком случае bib_ibi\\u200b и bjb_jbj\\u200b играют роль некоторых априорных усреднённых оценок пользователя и объекта соответственно, а μ\\\\muμ является глобальной априорной константой.\\nВ модели IALS мы обычно полагаем элементы puip_{ui}pui\\u200b равными 111 во всех случаях, когда имело место взаимодействие, но можем использовать и другие значения, в том числе зависящие от того, что ещё нам известно о пользователях и объектах.\\nДля уверенности cuv=1+α∥rui∥c_{uv} = 1 + \\\\alpha \\\\|r_{ui}\\\\|cuv\\u200b=1+α∥rui\\u200b∥ для IALS необязательно использовать 111 в качестве значения по умолчанию. Например, события «пользователь не посмотрел популярный фильм» и «пользователь не посмотрел редкий фильм» могут иметь для нас разный вес.\\n\\nFunkSVD\\nЭтот подход получил широкую известность после конкурса Netflix Prize в 2006 году. Пост Саймона Фанка про участие в Netflize Prize\\nФанк предложил моделировать рейтинг как r^ui=μ+bu+bi+xuyi\\\\hat{r}_{ui} = \\\\mu + b_u + b_i + x_{u}y_{i}r^ui\\u200b=μ+bu\\u200b+bi\\u200b+xu\\u200byi\\u200b. Однако, в отличие от ALS, оптимизация производилась с помощью стохастического градиентного спуска. Правила обновления весов выглядели следующим образом:\\n{eui=r^ui−ruixu←xu+η(euiyi−λxu)yi←yi+η(euixu−λyi)bu←bu+η(eui−λbu)bi←bi+η(eui−λbi)\\\\begin{cases}\\n    e_{ui} = \\\\hat{r}_{ui} - r_{ui} \\\\\\\\ \\n    x_u \\\\xleftarrow{} x_u + \\\\eta (e_{ui}y_{i} - \\\\lambda x_{u}) \\\\\\\\\\n    y_i \\\\xleftarrow{} y_i + \\\\eta (e_{ui}x_{u} - \\\\lambda y_{i}) \\\\\\\\\\n    b_u \\\\xleftarrow{} b_u + \\\\eta (e_{ui} - \\\\lambda b_{u}) \\\\\\\\\\n    b_i \\\\xleftarrow{} b_i + \\\\eta (e_{ui} - \\\\lambda b_{i}) \\\\\\\\\\n\\\\end{cases}\\n⎩⎨⎧\\u200beui\\u200b=r^ui\\u200b−rui\\u200bxu\\u200b\\u200bxu\\u200b+η(eui\\u200byi\\u200b−λxu\\u200b)yi\\u200b\\u200byi\\u200b+η(eui\\u200bxu\\u200b−λyi\\u200b)bu\\u200b\\u200bbu\\u200b+η(eui\\u200b−λbu\\u200b)bi\\u200b\\u200bbi\\u200b+η(eui\\u200b−λbi\\u200b)\\u200bЭтот подход не получил большой популярности, так как градиентный спуск, в отличие от ALS, намного сложнее распараллелить.\\nSingular Value Decomposition with implicit feedback (SVD++)\\nОригинальная статья\\nРанее мы отдельно рассматривали факторизации для явного и неявного фидбека. Но, ограничиваясь только одним типом фидбека, мы теряем много информации. Если мы работаем над стриминговым сервисом, то в качестве неявного фидбека мы можем взять, например, историю фильмов, взятых в прокат. Такие данные не предоставляют нам явных оценок пользователей, но позволяют выявить неявные предпочтения. Учесть неявный фидбек в модели можно следующим образом:\\nrui≈(xu+1∣{j∣puj≠0}∣∑∀j:puj≠0y^j)Tyi+bu+bi+μr_{ui} \\\\approx \\n\\\\Bigl(\\n    x_{u} + \\\\frac{1}{\\\\sqrt{|\\\\{j|p_{uj} \\\\neq 0\\\\}|}} \\n    \\\\sum\\\\limits_{\\\\forall j: p_{uj}\\\\neq 0}^{} \\\\widehat{y}_j\\n\\\\Bigr)^T\\ny_{i} + b_{u} + b_{i} + \\\\mu\\nrui\\u200b≈(xu\\u200b+∣{j∣puj\\u200b\\ue020=0}∣\\u200b1\\u200b∀j:puj\\u200b\\ue020=0∑\\u200by\\u200bj\\u200b)Tyi\\u200b+bu\\u200b+bi\\u200b+μВ данной модели пользователь представлен скрытым представлением xux_uxu\\u200b, а также слагаемым, отражающим историю неявных взаимодей с айтемами: 1∣{j∣puj≠0}∣∑∀j:puj≠0y^j\\\\frac{1}{\\\\sqrt{|\\\\{j|p_{uj} \\\\neq 0\\\\}|}} \\\\sum\\\\limits_{\\\\forall j: p_{uj}\\\\neq 0}^{} \\\\widehat{y}_j∣{j∣puj\\u200b\\ue020=0}∣\\u200b1\\u200b∀j:puj\\u200b\\ue020=0∑\\u200by\\u200bj\\u200b.\\nВажно отметить, что вектора y^j\\\\widehat{y}_jy\\u200bj\\u200b не совпадают с векторами yiy_{i}yi\\u200b. Это своего рода «неявные» вектора айтемов.\\nCollaborative Filtering with Temporal Dynamics (timeSVD++)\\nОригинальная статья\\nОсобенностью всех рассмотренных на данный момент разложений является отсутствие учёта порядка просмотра объектов.\\nОднако, как показывает практика, со временем пользователь может менять своё мнение о тех или иных айтемах. Тогда, отсортировав взаимодействия по времени, мы можем разбить события на бакеты и модифицировать приведённую выше функцию потерь, в которой таргет выражается следующим образом:\\nrui(t)≈(xu(t)+1∣{j∣puj≠0}∣∑∀j:puj≠0y^j)yi+bu(t)+bi(t)+μr_{ui}(t) \\\\approx \\n\\\\Bigl(\\n    x_{u}(t) + \\\\frac{1}{\\\\sqrt{|\\\\{j|p_{uj} \\\\neq 0\\\\}|}} \\n    \\\\sum\\\\limits_{\\\\forall j: p_{uj}\\\\neq 0}^{} \\\\widehat{y}_j\\n\\\\Bigr)\\ny_{i} + b_{u}(t) + b_{i}(t) + \\\\mu\\nrui\\u200b(t)≈(xu\\u200b(t)+∣{j∣puj\\u200b\\ue020=0}∣\\u200b1\\u200b∀j:puj\\u200b\\ue020=0∑\\u200by\\u200bj\\u200b)yi\\u200b+bu\\u200b(t)+bi\\u200b(t)+μSLIM (Sparse Linear Methods)\\nОригинальная статья\\nОписанные выше методы демонстрируют хорошее качество, однако требуют больших усилий для эффективной работы в онлайн сервисах. Возникает потребность в лёгких моделях, эффективность которых значительно выше, но качество которых не сильно хуже. Для этого была предложена линейная разреженная модель.\\nИтак, пусть AAA – бинарная матрица N×DN \\\\times DN×D user-item взаимодействий, например, матрица кликов/показов. Будем определять ответ алгоритма auia_{ui}aui\\u200b как взвешивание событий из истории пользователя:\\na^ui=∑jwijauj\\\\hat{a}_{ui} = \\\\sum\\\\limits_j w_{ij}a_{uj}\\na^ui\\u200b=j∑\\u200bwij\\u200bauj\\u200bПри этом наложим ограничение wij≥0w_{ij} \\\\ge 0wij\\u200b≥0. В такой постановке мы будем учить модель находить «похожие» объекты. Добавим ещё условие wii=0w_{ii} = 0wii\\u200b=0, которое позволит нам избежать элементарного решения – единичной матрицы W=IW = IW=I.В результате вес wijw_{ij}wij\\u200b выступает в качестве некоторой меры схожести iii-го и jjj-го объектов. Осталось определиться с методом оптимизации данных параметров.\\nДля оптимизации используется функция потерь MSE с L1L_{1}L1\\u200b- и L2L_{2}L2\\u200b-регуляризаторами:\\n12∑u,i(aui−∑jwijauj)2+λ∑i,j∣wij∣+β2∑i,j(wij)2→min\\u2061W\\\\frac{1}{2}\\\\sum\\\\limits_{u, i}^{}(a_{ui} - \\\\sum\\\\limits_{j}^{}w_{ij}a_{uj})^{2} + \\n\\\\lambda \\\\sum\\\\limits_{i,j}^{}|w_{ij}| +\\n\\\\frac{\\\\beta}{2}\\\\sum\\\\limits_{i, j}^{}(w_{ij})^2 \\\\xrightarrow{} \\\\min_{W}\\n21\\u200bu,i∑\\u200b(aui\\u200b−j∑\\u200bwij\\u200bauj\\u200b)2+λi,j∑\\u200b∣wij\\u200b∣+2β\\u200bi,j∑\\u200b(wij\\u200b)2\\u200bWmin\\u200bМожно заметить, что задачу можно разбить на DDD независимых по строкам матрицы WWW:\\n12∑u(aui−∑jwijauj)2+λ∑j∣wij∣+β2∑j(wij)2→min\\u2061wi1,...,wiD(∀i)\\\\frac{1}{2}\\\\sum\\\\limits_{u}^{}(a_{ui} - \\\\sum\\\\limits_{j}^{}w_{ij}a_{uj})^{2} + \\n\\\\lambda \\\\sum\\\\limits_{j}^{}|w_{ij}| +\\n\\\\frac{\\\\beta}{2}\\\\sum\\\\limits_{j}^{}(w_{ij})^2 \\\\xrightarrow{} \\\\min_{w_{i1}, ..., w_{iD}} (\\\\forall i)\\n21\\u200bu∑\\u200b(aui\\u200b−j∑\\u200bwij\\u200bauj\\u200b)2+λj∑\\u200b∣wij\\u200b∣+2β\\u200bj∑\\u200b(wij\\u200b)2\\u200bwi1\\u200b,...,wiD\\u200bmin\\u200b(∀i)Данную задачу можно решать покоординатным спуском:\\n\\nФиксируем все строки WWW, кроме одной координаты wijw_{ij}wij\\u200b;\\nпереходим в оптимум по wijw_{ij}wij\\u200b;\\nпереходим к следующей координате;\\nповторять до сходимости.\\n\\nПрименение данной модели выглядит следующим образом:\\n\\nРассчитываем вектор взаимодействий пользователя (uui)i=1D(u_{ui})_{i=1}^D(uui\\u200b)i=1D\\u200b;\\nСчитаем atauiat{a}_{ui}ataui\\u200b для всех непросмотренных объектов;\\nОтбираем топ kkk непросмотренных объектов по a^ui\\\\hat{a}_{ui}a^ui\\u200b.\\n\\nТак как в задаче оптимизации мы пользуемся L1L_{1}L1\\u200b-регуляризацией, матрица WWW получается разреженной. Матрица просмотров AAA тоже разреженная (по определению). Эти обстоятельства позволяют заметно улучшить эффективность применения модели.\\nИтоги\\nВ этом параграфе мы рассмотрели некоторые рекомендательные модели на основе матричных факторизаций. Такие модели редко используется в чистом виде для формирования рекомендательной выдачи. Обычно результаты матричной факторизации используются для генерации кандидатов в рекомендации, когда из сотен тысяч и миллионов объектов необходимо отобрать небольшое количество (например, сотни) самых релевантных. Для генерации кандидатов требуется перемножить вектор пользователя с вектором каждого из сотен тысяч объектов и отобрать топ самых релевантных.\\nВ онлайн-сервисах, когда время формирования рекомендаций составляет несколько сотен миллисекунд, нет возможности при каждом запросе рассчитывать релевантность каждого объекта для данного пользователя. Оптимизировать поиск можно с помощью инструментов для поиска ближайших соседей. Для любой функции близости, в том числе и для скалярного произведения, можно построить индекс – структуру данных, с помощью которой для любого пользователя мы сможем быстро приближённо, но зато быстро искать «ближайшие» объекты. В результате, принцип работы выглядит следующим образом:\\n\\nобучаются эмбеддинги объектов и пользователей;\\nдля представлений эмбеддингов строится индекс;\\nв рантайме по вектору пользователя происходит приближённый поиск nnn самых релевантных объектов; таким образом генерируется список кандидатов в рекомендации;\\nдальше список кандидатов обрабатывается с помощью более хитрых методов машинного обучения.\\n\\nПодробнее о том, как быстро искать ближайших соседей, вы можете узнать в параграфе посвященном метрическим методам\\nПомимо генерации кандидатов, полученные представления можно использовать в качестве признаков в более сложных моделях.\\nОсновной недостаток методов, основанных на матричной факторизации, состоит в том, что они используют лишь информацию о взаимодействии пользователей и объектов, но не о них самих. В следующем параграфе мы рассмотрим контентные методы, которые используют атрибуты объектов и пользователей.\\nСписок литературы\\n\\nСтатья про Implicit ALS\\nСтатья про SVD++\\nСтатья про TimeSVD++\\nСтатья про SLIM\\nПост Саймона Фанка про участие в конкурсе Netflix Prize\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф9.1. Введение в рекомендательные системыСледующий параграф9.3. Контентные рекомендацииЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_18.html', 'title': 'Байесовский подход к оцениванию'}, page_content='Байесовский подход к оцениваниюЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/74.1.Вероятностный подход в ML4.2.Экспоненциальный класс распределений и принцип максимальной энтропии4.3.Обобщённые линейные модели4.4.Как оценивать вероятности4.5.Генеративный подход к классификации4.6.Байесовский подход к оцениваниюАприорное знаниеОцениваем не значение параметра, а его распределениеПостроение апостериорного распределенияСопряжённые распределенияОценка апостериорного максимума (MAP)Связь MAP- и MLE-оценокБайесовские оценки для условных распределенийПример: линейная регрессия с -регуляризацией как модель с гауссовским априорным распределением на весаПример: линейная регрессия с -регуляризацией как модель с лапласовским априорным распределением на весаКак делать предсказанияБайесовский подход и дообучение моделейБайесовский подход к выбору модели: мотивацияБайесовский подход к выбору модели: формализацияФреквентисты против байесиан: кто кого?4.7.Модели с латентными переменными5.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Байесовский подход к оцениванию4.6. Байесовский подход к оцениваниюАвторыФедотов СтаниславБайесовская статистика. Априорные и\\xa0апостериорные распределения на\\xa0параметры моделей. MAP-оценки. Байесовский подход к\\xa0выбору моделей. Байесовский подход для задачи линейной регресииАприорное знание\\nНачнём с простого вопроса: как нам внести в модель априорные знания.\\nПредставьте, что мы обучаем модель линейной регрессии y∼⟨x,w⟩+εy\\\\sim \\\\langle x, w\\\\rangle + \\\\varepsilony∼⟨x,w⟩+ε, ε∼N(0,σ2)\\\\varepsilon\\\\sim\\\\mathcal{N}(0,\\\\sigma^2)ε∼N(0,σ2). С помощью MLE мы получили некоторую оценку w^\\\\widehat{w}w на веса www — всякие ли их значения мы встретим с покорностью и смирением? Наверное, мы удивимся, если какие-то компоненты вектора w^\\\\widehat{w}w будут очень большими по сравнению с элементами XXX: пожалуй, наша физическая интуиция будет бунтовать против этого, мы задумаемся о том, что из-за потенциальных ошибок сокращения вычисление предсказаний (xi,w^)(x_i, \\\\widehat{w})(xi\\u200b,w) окажутся неточным — в общем, хотелось бы по возможности избежать этого. Но как?\\nБудь мы приверженцами чисто инженерного подхода, мы бы сделали просто: прибавили бы к функции потерь слагаемое α∥ω∥22\\\\alpha \\\\left\\\\|\\\\omega  \\\\right\\\\|_{2}^{2}α∥ω∥22\\u200b, или α∥ω∥1\\\\alpha \\\\left\\\\|\\\\omega  \\\\right\\\\|_1α∥ω∥1\\u200b, или ещё что-то такое — тогда процедура обучения стала бы компромиссом между минимизацией исходного лосса и этой добавки, что попортило бы слегка близость y∼⟨x,w⟩y\\\\sim \\\\langle x, w \\\\rangley∼⟨x,w⟩, но зато позволило бы лучше контролировать масштаб w^\\\\widehat{w}w. Надо думать, вы узнали в этой конструкции старую добрую регуляризацию.\\nНо наша цель — зашить наше априорное знание о том, что компоненты www не слишком велики по модулю, в вероятностную модель. Введение в модель априорного знания соответствует введению априорного распределения на www. Какое распределение выбрать? Ну, наверное, компоненты www будут независимыми (ещё нам не хватало задавать взаимосвязи между ними!), а каждая из них будет иметь какое-то непрерывное распределение, в котором небольшие по модулю значения более правдоподобны, а совсем большие очень неправдоподобны.\\nМы знаем такие распределения? Да, и сразу несколько. Например, нормальное. Логично было бы определитьВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\np(w)=∏i=1DN(wi∣0,τ2)p(w) = \\\\prod_{i=1}^D\\\\mathcal{N}(w_i \\\\vert 0,\\\\tau^2)\\np(w)=i=1∏D\\u200bN(wi\\u200b∣0,τ2)где τ2\\\\tau^2τ2 — какая-то дисперсия, которую мы возьмём с потолка или подберём по валидационной выборке. Отметим, что выбор нормального распределение следует и из принципа максимальной энтропии: ведь у него наибольшая энтропия среди распределений на всей числовой оси с нулевым матожиданием и дисперсией τ2\\\\tau^2τ2.\\nКонтроль масштаба весов — это, вообще говоря, не единственное, что мы можем потребовать. Например, мы можем из каких-то физических соображений знать, что тот или иной вес в линейной модели непременно должен быть неотрицательным. Тогда в качестве априорного на этот вес мы можем взять, например, показательное распределение (которое, напомним, обладает максимальной энтропией среди распределений на положительных числах с данным матожиданием).\\nОцениваем не значение параметра, а его распределение\\nРаз уж мы начали говорить о распределении на веса www, то почему бы не пойти дальше. Решая задачу классификации, мы уже столкнулись с тем, что может быть важна не только предсказанная метка класса, но и вероятности. Аналогичное верно и для задачи регрессии. Давайте рассмотрим две следующих ситуации, в каждой из которых мы пытаемся построить регрессию y∼ax+by\\\\sim ax + by∼ax+b:\\n\\nНесмотря на то, что в каждом из случаев «точная формула» или градиентный спуск выдадут нам что-то, степень нашей уверенности в ответе совершенно различная. Один из способов выразить (не)уверенность — оценить распределение параметров. Так, для примеров выше распределения на параметр aaa могли бы иметь какой-то такой вид:\\n\\nДальше мы постараемся формализовать процесс получения таких оценок.\\nПостроение апостериорного распределения\\nДавайте ненадолго забудем про линейную регрессию и представим, что мы подобрали с пола монету, которая выпадает орлом с некоторой неизвестной пока вероятностью θ\\\\thetaθ. До тех пор, пока мы не начали её подкидывать, мы совершенно ничего не знаем о θ\\\\thetaθ, эта вероятность может быть совершенно любой — то есть априорное распределение на θ\\\\thetaθ является равномерным (на отрезке [0,1][0,1][0,1]):\\np(θ)=I[0;1](θ)p(\\\\theta) = \\\\mathbb{I}_{[0;1]}(\\\\theta)\\np(θ)=I[0;1]\\u200b(θ)Теперь представим, что мы подкинули её nnn раз, получив результаты Y=(y1,…,yn)Y = (y_1,\\\\ldots,y_n)Y=(y1\\u200b,…,yn\\u200b) (000 — решка, 111 — орёл), среди которых n0=n−∑i=1nyin_0 = n - \\\\sum_{i=1}^ny_in0\\u200b=n−∑i=1n\\u200byi\\u200b решек и n1=∑i=1nyin_1=\\\\sum_{i=1}^ny_in1\\u200b=∑i=1n\\u200byi\\u200b орлов. Определённо наши познания о числе ppp стали точнее: так, если n1n_1n1\\u200b мало, то можно заподозрить, что и ppp невелико (уже чувствуете, запахло распределением!).\\nРаспределение мы посчитаем с помощью формулы Байеса:\\np(θ∣Y)=p(θ,Y)p(Y)=p(Y∣θ)p(θ)∫p(Y∣ψ)p(ψ)dψ\\\\color{#348FEA}{p(\\\\theta \\\\vert Y) = \\\\frac{p(\\\\theta , Y)}{p(Y)} = \\\\frac{p(Y \\\\vert \\\\theta)p(\\\\theta)}{\\n\\\\int p(Y \\\\vert \\\\psi)p(\\\\psi)d\\\\psi}}\\np(θ∣Y)=p(Y)p(θ,Y)\\u200b=∫p(Y∣ψ)p(ψ)dψp(Y∣θ)p(θ)\\u200bв нашем случае:\\np(θ∣Y)=∏i=1nθyi(1−θ)1−yiI[0,1](θ)∫01∏i=1nψyi(1−ψ)1−yidψ=p(\\\\theta \\\\vert Y) = \\\\frac{\\\\prod_{i=1}^n\\\\theta^{y_i}(1 - \\\\theta)^{1 - y_i}\\\\mathbb{I}_{[0,1]}(\\\\theta)}{\\n\\\\int_0^1\\\\prod_{i=1}^n\\\\psi^{y_i}(1 - \\\\psi)^{1 - y_i}d\\\\psi} =\\np(θ∣Y)=∫01\\u200b∏i=1n\\u200bψyi\\u200b(1−ψ)1−yi\\u200bdψ∏i=1n\\u200bθyi\\u200b(1−θ)1−yi\\u200bI[0,1]\\u200b(θ)\\u200b==θn1(1−θ)n0I[0,1](θ)∫01ψn1(1−ψ)n0dψ=\\\\frac{\\\\theta^{n_1}(1 - \\\\theta)^{n_0}\\\\mathbb{I}_{[0,1]}(\\\\theta)}{\\n\\\\int_0^1\\\\psi^{n_1}(1 - \\\\psi)^{n_0}d\\\\psi}\\n=∫01\\u200bψn1\\u200b(1−ψ)n0\\u200bdψθn1\\u200b(1−θ)n0\\u200bI[0,1]\\u200b(θ)\\u200bВ этом выражении нетрудно узнать бета-распределение: Beta(n1+1,n0+1)\\\\text{Beta}(n_1 + 1, n_0 + 1)Beta(n1\\u200b+1,n0\\u200b+1). Давайте нарисует графики его плотности для нескольких конкретных значений n0n_0n0\\u200b и n1n_1n1\\u200b:\\n\\nКак можно заметить, с ростом nnn мы всё лучше понимаем, каким может быть θ\\\\thetaθ, при этом если орёл выпадал редко, то пик оказывается ближе к нулю, и наоборот. Ширина пика в каком-то смысле отражает нашу уверенность в том, какими могут быть значения параметра, и не случайно чем больше у нас данных — тем уже будет пик, то есть тем больше уверенности.\\nРаспределение p(θ∣Y)p(\\\\theta\\\\vert Y)p(θ∣Y) параметра, полученное с учётом данных, называется апостериорным. Переход от априорного распределения к апостериорному отражает обновление нашего представления о параметрах распределения с учётом полученной информации, и этот процесс является сердцем байесовского подхода. Отметим, что если нам придут новые данные Y′=(y1′,…,ym′)Y\\' = (y_1\\',\\\\ldots,y_m\\')Y′=(y1′\\u200b,…,ym′\\u200b), в которых m0m_0m0\\u200b решек и m1m_1m1\\u200b орлов, мы сможем ещё раз обновить распределение по той же формуле Байеса:\\np(θ∣Y∪Y′)=p([θ∣Y]∣Y′)=p(Y′∣θ)p(θ∣Y)p(Y′)=p(\\\\theta \\\\vert Y\\\\cup Y\\') = p([\\\\theta\\\\vert Y]\\\\vert Y\\') = \\\\frac{p(Y\\'\\\\vert\\\\theta)p(\\\\theta\\\\vert Y)}{p(Y\\')}=\\np(θ∣Y∪Y′)=p([θ∣Y]∣Y′)=p(Y′)p(Y′∣θ)p(θ∣Y)\\u200b==θm1(1−θ)m0θn1(1−θ)n0B(n1+1,n0+1)I[0,1](θ)злой\\xa0интеграл==\\\\frac{\\\\theta^{m_1}(1 - \\\\theta)^{m_0}\\\\frac{\\\\theta^{n_1}(1 - \\\\theta)^{n_0}}{B(n_1 + 1, n_0 + 1)}\\\\mathbb{I}_{[0,1]}(\\\\theta)}{\\n\\\\text{злой интеграл}} =\\n=злой\\xa0интегралθm1\\u200b(1−θ)m0\\u200bB(n1\\u200b+1,n0\\u200b+1)θn1\\u200b(1−θ)n0\\u200b\\u200bI[0,1]\\u200b(θ)\\u200b==θn1+m1(1−θ)n0+m0константа∼Beta(n1+m1+1,n0+m0+1)=\\\\frac{\\\\theta^{n_1 + m_1}(1 - \\\\theta)^{n_0 + m_0}}{\\\\text{константа}}\\\\sim\\\\text{Beta}(n_1 + m_1 + 1, n_0 + m_0 + 1)\\n=константаθn1\\u200b+m1\\u200b(1−θ)n0\\u200b+m0\\u200b\\u200b∼Beta(n1\\u200b+m1\\u200b+1,n0\\u200b+m0\\u200b+1)Вопрос на подумать. Пусть p(y∣μ)=N(y∣μ,σ2)p(y\\\\vert\\\\mu) = \\\\mathcal{N}(y\\\\vert\\\\mu,\\\\sigma^2)p(y∣μ)=N(y∣μ,σ2) — нормальное распределение с фиксированной дисперсией σ2\\\\sigma^2σ2, а для параметра μ\\\\muμ в качестве априорного выбрано также нормальное распределение N(μ∣λ,θ2)\\\\mathcal{N}(\\\\mu\\\\vert \\\\lambda,\\\\theta^2)N(μ∣λ,θ2). Каким будет апостериорное распределение при условии данных Y=(y1,…,yn)Y = (y_1,\\\\ldots,y_n)Y=(y1\\u200b,…,yn\\u200b)?\\nОтвет (не открывайте сразу; сначала подумайте сами!)p(θ∣Y)=[∏i=1n12πσ2e−(yi−μ)22σ2]⋅12πθ2e−(μ−λ)22θ2злой\\xa0интеграл=p(\\\\theta \\\\vert Y) = \\\\frac{\\\\left[\\\\prod_{i=1}^n\\\\frac1{\\\\sqrt{2\\\\pi\\\\sigma^2}}e^{-\\\\frac{(y_i - \\\\mu)^2}{2\\\\sigma^2}}\\\\right]\\\\cdot\\\\frac1{\\\\sqrt{2\\\\pi\\\\theta^2}}e^{-\\\\frac{(\\\\mu - \\\\lambda)^2}{2\\\\theta^2}}}{\\n\\\\text{злой интеграл}} =\\np(θ∣Y)=злой\\xa0интеграл[∏i=1n\\u200b2πσ2\\u200b1\\u200be−2σ2(yi\\u200b−μ)2\\u200b]⋅2πθ2\\u200b1\\u200be−2θ2(μ−λ)2\\u200b\\u200b==1(2πσ2)n2⋅12πθ2e−12σ2∑i=1n(yi−μ)2−12θ2(μ−λ)2злой\\xa0интеграл=\\\\frac{\\\\frac1{(2\\\\pi\\\\sigma^2)^{\\\\frac{n}2}}\\\\cdot\\\\frac1{\\\\sqrt{2\\\\pi\\\\theta^2}}e^{-\\\\frac1{2\\\\sigma^2}\\\\sum_{i=1}^n(y_i - \\\\mu)^2-\\\\frac{1}{2\\\\theta^2}(\\\\mu - \\\\lambda)^2}}{\\n\\\\text{злой интеграл}}\\n=злой\\xa0интеграл(2πσ2)2n\\u200b1\\u200b⋅2πθ2\\u200b1\\u200be−2σ21\\u200b∑i=1n\\u200b(yi\\u200b−μ)2−2θ21\\u200b(μ−λ)2\\u200bС точностью до множителей, не зависящих от μ\\\\muμ, это экспонента квадратичной формы, то есть распределение будет нормальным. Давайте найдём его параметры. Для этого нам нужно выделить в показателе полный квадрат:\\n=e−12(nσ2+1θ2)μ2+(1σ2∑i=1nyi+1θ2λ)μ+свободный\\xa0членчто-то,\\xa0где\\xa0нет\\xa0μ==\\\\frac{e^{-\\\\frac12\\\\left(\\\\frac{n}{\\\\sigma^2} + \\\\frac{1}{\\\\theta^2}\\\\right)\\\\mu^2 + \\\\left(\\\\frac1{\\\\sigma^2}\\\\sum_{i=1}^ny_i + \\\\frac1{\\\\theta^2}\\\\lambda\\\\right)\\\\mu + \\\\text{свободный член}}}{\\n\\\\text{что-то, где нет }\\\\mu} =\\n=что-то,\\xa0где\\xa0нет\\xa0μe−21\\u200b(σ2n\\u200b+θ21\\u200b)μ2+(σ21\\u200b∑i=1n\\u200byi\\u200b+θ21\\u200bλ)μ+свободный\\xa0член\\u200b=Обозначим ρ2=(nσ2+1θ2)−1\\\\rho^2 = \\\\left(\\\\frac{n}{\\\\sigma^2} + \\\\frac{1}{\\\\theta^2}\\\\right)^{-1}ρ2=(σ2n\\u200b+θ21\\u200b)−1\\n=frace−12ρ2[μ2−ρ2(1σ2∑i=1nyi+1θ2λ)]2+остальноечто-то,\\xa0где\\xa0нет\\xa0μ=frac{e^{-\\\\frac1{2\\\\rho^2}\\\\left[\\\\mu^2 - \\\\rho^2\\\\left(\\\\frac1{\\\\sigma^2}\\\\sum_{i=1}^ny_i + \\\\frac1{\\\\theta^2}\\\\lambda\\\\right)\\\\right]^2 + \\\\text{остальное}}}{\\n\\\\text{что-то, где нет }\\\\mu}\\n=frace−2ρ21\\u200b[μ2−ρ2(σ21\\u200b∑i=1n\\u200byi\\u200b+θ21\\u200bλ)]2+остальноечто-то,\\xa0где\\xa0нет\\xa0μТеперь уже хорошо видно, что получилось нормальное распределение с параметрами\\nλnew=(nσ2+1θ2)−1(1σ2∑i=1nyi+1θ2λ)\\\\lambda_{new} = \\\\left(\\\\frac{n}{\\\\sigma^2} + \\\\frac{1}{\\\\theta^2}\\\\right)^{-1}\\\\left(\\\\frac1{\\\\sigma^2}\\\\sum_{i=1}^ny_i + \\\\frac1{\\\\theta^2}\\\\lambda\\\\right)\\nλnew\\u200b=(σ2n\\u200b+θ21\\u200b)−1(σ21\\u200bi=1∑n\\u200byi\\u200b+θ21\\u200bλ)θnew2=(nσ2+1θ2)−1\\\\theta_{new}^2 = \\\\left(\\\\frac{n}{\\\\sigma^2} + \\\\frac{1}{\\\\theta^2}\\\\right)^{-1}\\nθnew2\\u200b=(σ2n\\u200b+θ21\\u200b)−1Формулы жутковатые, но проинтерпретировать их можно. Например, мы видим, что появление больших yiy_iyi\\u200b будет сдвигать среднее, а дисперсия уменьшается с ростом nnn.\\nСопряжённые распределения\\nВ двух предыдущих примерах нам очень сильно повезло, что апостериорные распределения оказались нашими добрыми знакомыми. Если же взять случайную пару распределений p(y∣θ)p(y\\\\vert\\\\theta)p(y∣θ) и p(θ)p(\\\\theta)p(θ), результат может оказаться совсем не таким приятным.\\nВ самом деле, нет никакой проблемы в том, чтобы посчитать числитель формулы Байеса, но вот интеграл в знаменателе может и не найтись. Поэтому выбирать распределения нужно с умом. Более того, поскольку апостериорное распределение само станет априорным, когда придут новые данные, хочется, чтобы априорное и апостериорное распределения были из одного семейства; пары (семейств) распределений p(y∣θ)p(y\\\\vert\\\\theta)p(y∣θ) и p(θ)p(\\\\theta)p(θ), для которых это выполняется, называются сопряжёнными p(θ)p(\\\\theta)p(θ) называется сопряжённым к p(y∣θ)p(y\\\\vert\\\\theta)p(y∣θ). Полезно помнить несколько наиболее распространённых пар сопряжённых распределений:\\n\\np(y∣θ)p(y\\\\vert\\\\theta)p(y∣θ) — распределение Бернулли с вероятностью успеха θ\\\\thetaθ, p(θ)p(\\\\theta)p(θ) — бета распределение;\\np(y∣μ)p(y\\\\vert\\\\mu)p(y∣μ) — нормальное с матожиданием μ\\\\muμ и фиксированной дисперсией σ2\\\\sigma^2σ2, p(θ)p(\\\\theta)p(θ) также нормальное;\\np(y∣λ)p(y\\\\vert\\\\lambda)p(y∣λ) — показательное с параметром λ\\\\lambdaλ, p(λ)p(\\\\lambda)p(λ) — гамма распределение;\\np(y∣λ)p(y\\\\vert\\\\lambda)p(y∣λ) — пуассоновское с параметром λ\\\\lambdaλ, p(λ)p(\\\\lambda)p(λ) — гамма распределение;\\np(y∣θ)p(y\\\\vert\\\\theta)p(y∣θ) — равномерное на отрезке [0,θ][0,\\\\theta][0,θ], p(θ)p(\\\\theta)p(θ) — Парето;\\n\\nВозможно, вы заметили, что почти все указанные выше семейства распределений (кроме равномерного и Парето) относятся к экспоненциальному классу. И это не случайность! Экспоненциальный класс и тут лучше всех: оказывается, что для p(y∣θ)p(y\\\\vert\\\\theta)p(y∣θ) из экспоненциального класса можно легко подобрать сопряжённое p(θ)p(\\\\theta)p(θ). Давайте же это сделаем.\\nПусть p(y∣θ)p(y\\\\vert\\\\theta)p(y∣θ) имеет вид\\np(y∣θ)=1h(θ)g(y)exp\\u2061(θTu(y))p(y\\\\vert\\\\theta) = \\\\frac1{h(\\\\theta)}g(y)\\\\exp(\\\\theta^Tu(y))\\np(y∣θ)=h(θ)1\\u200bg(y)exp(θTu(y))Положим\\np(θ)=1hν(θ)exp\\u2061(ηTθ)⋅f(η,ν)p(\\\\theta) = \\\\frac1{h^{\\\\nu}(\\\\theta)}\\\\exp(\\\\eta^T\\\\theta)\\\\cdot f(\\\\eta, \\\\nu)\\np(θ)=hν(θ)1\\u200bexp(ηTθ)⋅f(η,ν)где f(η,ν)f(\\\\eta, \\\\nu)f(η,ν) — множитель, обеспечивающий равенство единице интеграла от этой функции. Найдём апостериорное распределение:\\np(θ∣Y)=[∏i=1n1h(θ)g(yi)exp\\u2061(θTu(yi))]1hν(θ)exp\\u2061(ηTθ)⋅f(η,ν)злой\\xa0интеграл=p(\\\\theta\\\\vert Y) = \\\\frac{\\\\left[\\\\prod_{i=1}^n\\\\frac1{h(\\\\theta)}g(y_i)\\\\exp(\\\\theta^Tu(y_i))\\\\right]\\\\frac1{h^{\\\\nu}(\\\\theta)}\\\\exp(\\\\eta^T\\\\theta)\\\\cdot f(\\\\eta, \\\\nu)}{\\\\text{злой интеграл}} =\\np(θ∣Y)=злой\\xa0интеграл[∏i=1n\\u200bh(θ)1\\u200bg(yi\\u200b)exp(θTu(yi\\u200b))]hν(θ)1\\u200bexp(ηTθ)⋅f(η,ν)\\u200b==1hν+n(θ)exp\\u2061(θT[η+∑i=1nu(yi)])что-то,\\xa0где\\xa0нет\\xa0θ= \\\\frac{\\\\frac1{h^{\\\\nu + n}(\\\\theta)}\\\\exp\\\\left(\\\\theta^T\\\\left[\\\\eta + \\\\sum_{i=1}^nu(y_i)\\\\right]\\\\right)}{\\\\text{что-то, где нет }\\\\theta}\\n=что-то,\\xa0где\\xa0нет\\xa0θhν+n(θ)1\\u200bexp(θT[η+∑i=1n\\u200bu(yi\\u200b)])\\u200bЭто распределение действительно из того же семейства, что и p(θ)p(\\\\theta)p(θ), только с новыми параметрами:\\nηnew=η+∑i=1nu(yi),νnew=ν+n\\\\eta_{new} = \\\\eta + \\\\sum_{i=1}^nu(y_i),\\\\quad\\\\nu_{new} = \\\\nu + n\\nηnew\\u200b=η+i=1∑n\\u200bu(yi\\u200b),νnew\\u200b=ν+nПример. Пусть p(y∣q)=qy(1−q)1−yp(y\\\\vert q) = q^y(1 - q)^{1 - y}p(y∣q)=qy(1−q)1−y подчиняется распределению Бернулли. Напомним, что оно следующим образом представляется в привычном для экспоненциального класса виде:\\np(y∣q)=(1−q)⏟=1h(q)exp\\u2061(y⏟=u1(y)log\\u2061q1−q⏟=θ)p(y\\\\vert q) = \\\\underbrace{(1 - q)}_{=\\\\frac1{h(q)}}\\\\exp\\\\left(\\\\underbrace{y}_{=u_1(y)}\\\\underbrace{\\\\log{\\\\frac{q}{1 - q}}}_{=\\\\theta}\\\\right)\\np(y∣q)==h(q)1\\u200b(1−q)\\u200b\\u200bexp\\u200b=u1\\u200b(y)y\\u200b\\u200b=θlog1−qq\\u200b\\u200b\\u200b\\u200bПредлагается брать априорное распределение вида\\np(q)=(1−q)νexp\\u2061(ηlog\\u2061q1−q)что-то,\\xa0где\\xa0нетqp(q) = \\\\frac{(1 - q)^{\\\\nu}\\\\exp\\\\left(\\\\eta\\\\log{\\\\frac{q}{1-q}}\\\\right)}{\\\\text{что-то, где нет}q}\\np(q)=что-то,\\xa0где\\xa0нетq(1−q)νexp(ηlog1−qq\\u200b)\\u200bТогда апостериорное распределение будет иметь вид (проверьте, посчитав по формуле Байеса!)\\np(q∣Y)=(1−q)ν+nexp\\u2061([η+∑i=1nyi]log\\u2061q1−q)что-то,\\xa0где\\xa0нетqp(q\\\\vert Y) = \\\\frac{(1 - q)^{\\\\nu + n}\\\\exp\\\\left(\\\\left[\\\\eta + \\\\sum_{i=1}^ny_i\\\\right]\\\\log{\\\\frac{q}{1-q}}\\\\right)}{\\\\text{что-то, где нет}q}\\np(q∣Y)=что-то,\\xa0где\\xa0нетq(1−q)ν+nexp([η+∑i=1n\\u200byi\\u200b]log1−qq\\u200b)\\u200bПревратив логарифм частного в сумму, а экспоненту суммы в произведение, легко убедиться, что получается то самое бета распределение, которое мы уже получали выше.\\nОценка апостериорного максимума (MAP)\\nАпостериорное распределение — это очень тонкий инструмент анализа данных, но иногда надо просто сказать число (или же интеграл в знаменателе не берётся и мы не можем толком посчитать распределение). В качестве точечной оценки логично выдать самое вероятное значение θ∣Y\\\\theta\\\\vert Yθ∣Y (интеграл в знаменателе от θ\\\\thetaθ не зависит, поэтому на максимизацию не влияет):\\nθ^MAP=argmax\\u2061θp(θ∣Y)=argmax\\u2061θp(Y∣θ)p(θ)\\\\color{blue}{\\\\widehat{\\\\theta}_{MAP} = \\\\underset{\\\\theta}{\\\\operatorname{argmax}}{p(\\\\theta \\\\vert Y)} = \\\\underset{\\\\theta}{\\\\operatorname{argmax}}{p(Y \\\\vert \\\\theta)p(\\\\theta)}}\\nθMAP\\u200b=θargmax\\u200bp(θ∣Y)=θargmax\\u200bp(Y∣θ)p(θ)Это число называется оценкой апостериорного максимума (MAP).\\nЕсли же в формуле выше перейти к логарифмам, то мы получим кое-что, до боли напоминающее старую добрую регуляризацию (и не просто так, как мы вскоре убедимся!):\\nargmax\\u2061θp(Y∣θ)p(θ)=argmax\\u2061θlog\\u2061(p(Y∣θ)p(θ))=\\\\underset{\\\\theta}{\\\\operatorname{argmax}}{p(Y \\\\vert \\\\theta)p(\\\\theta)} = \\\\underset{\\\\theta}{\\\\operatorname{argmax}}\\\\log(p(Y \\\\vert \\\\theta)p(\\\\theta)) = \\nθargmax\\u200bp(Y∣θ)p(θ)=θargmax\\u200blog(p(Y∣θ)p(θ))==argmax\\u2061θ(12log\\u2061p(Y∣θ)+log\\u2061p(θ))=\\\\underset{\\\\theta}{\\\\operatorname{argmax}}\\\\left(\\\\vphantom{\\\\frac12}\\\\log{p(Y \\\\vert \\\\theta)} + \\\\log{p(\\\\theta)}\\\\right)\\n=θargmax\\u200b(21\\u200blogp(Y∣θ)+logp(θ))Пример. Рассмотрим снова распределение Бернулли p(y∣q)p(y\\\\vert q)p(y∣q) и априорное распределение p(q)∼Beta(q∣a,b)p(q)\\\\sim\\\\text{Beta}(q\\\\vert a, b)p(q)∼Beta(q∣a,b). Тогда MAP-оценка будет равна\\nargmax\\u2061qp(Y∣q)p(q)=argmax\\u2061qq∑i=1nyi(1−q)n−∑i=1nyi⋅qa−1(1−q)b−1=\\\\underset{q}{\\\\operatorname{argmax}}{p(Y \\\\vert q)p(q)} = \\\\underset{q}{\\\\operatorname{argmax}}{q^{\\\\sum_{i=1}^ny_i}(1 - q)^{n - \\\\sum_{i=1}^ny_i}\\\\cdot q^{a - 1}(1 - q)^{b - 1}} = \\nqargmax\\u200bp(Y∣q)p(q)=qargmax\\u200bq∑i=1n\\u200byi\\u200b(1−q)n−∑i=1n\\u200byi\\u200b⋅qa−1(1−q)b−1=argmax\\u2061q((a−1+∑i=1nyi)log\\u2061q+(b−1+n−∑i=1nyi)log\\u2061(1−q))\\\\underset{q}{\\\\operatorname{argmax}}\\\\left((a - 1 + \\\\sum_{i=1}^ny_i)\\\\log{q} + (b - 1 + n - \\\\sum_{i=1}^ny_i)\\\\log(1 - q)\\\\right)\\nqargmax\\u200b((a−1+i=1∑n\\u200byi\\u200b)logq+(b−1+n−i=1∑n\\u200byi\\u200b)log(1−q))Дифференцируя по qqq и приравнивая производную к нулю, мы получаем\\nq=a+∑i=1nyi−1a+b+n−2q = \\\\frac{a + \\\\sum_{i=1}^ny_i - 1}{a + b + n - 2}\\nq=a+b+n−2a+∑i=1n\\u200byi\\u200b−1\\u200bВ отличие от оценки максимального правдоподобия ∑i=1nyin\\\\frac{\\\\sum_{i=1}^ny_i}{n}n∑i=1n\\u200byi\\u200b\\u200b мы здесь используем априорное знание: параметры (a−1)(a - 1)(a−1) и (b−1)(b - 1)(b−1) работают как «память о воображаемых испытаниях», как будто бы до того, как получить данные yiy_iyi\\u200b, мы уже имели (a−1)(a - 1)(a−1) успехов и (b−1)(b - 1)(b−1) неудач.\\nСвязь MAP- и MLE-оценок\\nОценка максимального правдоподобия является частным случаем апостериорной оценки.\\nВ самом деле, если априорное распределение является равномерным, то есть p(θ)p(\\\\theta)p(θ) не зависит θ\\\\thetaθ (если веса θ\\\\thetaθ вещественные, могут потребоваться дополнительные усилия, чтобы понять, как такое вообще получается), и тогда\\nθ^MAP=argmax\\u2061θlog\\u2061p(Y∣θ)p(θ)=argmax\\u2061θ(log\\u2061p(Y∣θ)+log\\u2061p(θ)⏟=const)=\\\\widehat{\\\\theta}_{MAP} = \\\\underset{\\\\theta}{\\\\operatorname{argmax}}\\\\log{p(Y \\\\vert \\\\theta)p(\\\\theta)} = \\\\underset{\\\\theta}{\\\\operatorname{argmax}}\\\\left(\\\\log{p(Y \\\\vert \\\\theta)} + \\\\underbrace{\\\\log{p(\\\\theta)}}_{=const}\\\\right) =\\nθMAP\\u200b=θargmax\\u200blogp(Y∣θ)p(θ)=θargmax\\u200b\\u200blogp(Y∣θ)+=constlogp(θ)\\u200b\\u200b\\u200b==argmax\\u2061θlog\\u2061p(y∣θ)=θ^MLE= \\\\underset{\\\\theta}{\\\\operatorname{argmax}}\\\\log{p(y \\\\vert \\\\theta)} = \\\\widehat{\\\\theta}_{MLE}\\n=θargmax\\u200blogp(y∣θ)=θMLE\\u200bБайесовские оценки для условных распределений\\nВ предыдущих разделах мы разобрали, как байесовский подход работает для обычных, не условных распределений. Теперь вернёмся к чему-то более близкому к машинному обучению, а именно к распределениям вида y∣x,wy\\\\vert x,wy∣x,w, и убедимся, что для них байесовских подход работает точно так же, как и для обычных распределений.\\nИмея некоторое распределение p(y∣x,w)p(y\\\\vert x, w)p(y∣x,w), мы подбираем для него априорное распределение на веса p(w)p(w)p(w) (и да, оно не зависит от xxx: ведь априорное распределение существует ещё до появления данных) и вычисляем апостериорное распределение на веса:\\np(w∣X,y)p(w \\\\vert X, y)\\np(w∣X,y)Вычислять его мы будем по уже привычной формуле Байеса:\\np(w∣X,y)=p(y,w∣X)p(y)=p(y∣X,w)p(w)p(y)\\\\color{blue}{p(w \\\\vert X, y) = \\\\frac{p(y, w \\\\vert X)}{p(y)} = \\\\frac{p(y \\\\vert X, w)p(w)}{p(y)}}\\np(w∣X,y)=p(y)p(y,w∣X)\\u200b=p(y)p(y∣X,w)p(w)\\u200bПовторим ещё разок, в чём суть байесовского подхода: у нас было некоторое априорное представление p(w)\\\\color{blue}{p(w)}p(w) о распределении весов w\\\\color{blue}{w}w, а теперь, посмотрев на данные (xi,yi)i=1n(x_i, y_i)_{i=1}^n(xi\\u200b,yi\\u200b)i=1n\\u200b, мы уточняем своё понимание, формулируя апостериорное представление p(w∣X,y)p(w \\\\vert X, y)p(w∣X,y).\\nЕсли же нам нужна только точечная оценка, мы можем ограничиться оценкой апостериорного максимума (MAP):\\nw^MAP=argmax\\u2061wp(w∣X,y)=argmax\\u2061wp(y∣X,w)p(w)=\\\\color{blue}{\\\\widehat{w}_{MAP} = \\\\underset{w}{\\\\operatorname{argmax}}{p(w \\\\vert X,y)} = \\\\underset{w}{\\\\operatorname{argmax}}{p(y \\\\vert X, w)p(w)}} = \\nwMAP\\u200b=wargmax\\u200bp(w∣X,y)=wargmax\\u200bp(y∣X,w)p(w)==argmax\\u2061w(12log\\u2061p(y∣X,w)+log\\u2061p(w))=\\\\underset{w}{\\\\operatorname{argmax}}\\\\left(\\\\vphantom{\\\\frac12}\\\\log{p(y \\\\vert X, w)} + \\\\log{p(w)}\\\\right)\\n=wargmax\\u200b(21\\u200blogp(y∣X,w)+logp(w))что уже до неприличия напоминает регуляризованную модель\\nПример: линейная регрессия с L2L^2L2-регуляризацией как модель с гауссовским априорным распределением на веса\\nВ модели линейной регрессии y=⟨x,w⟩+εy = \\\\langle x, w\\\\rangle + \\\\varepsilony=⟨x,w⟩+ε, ε∼N(0,σ2)\\\\varepsilon\\\\sim\\\\mathcal{N}(0, \\\\sigma^2)ε∼N(0,σ2) введём априорное распределение на веса вида\\np(w)=N(w∣0,τ2I)=∏j=1DN(wj∣0,τ2)=∏j=1Dp(wj)\\\\color{blue}{p(w) = \\\\mathcal{N}(w  \\\\vert  0, \\\\tau^2I) = \\\\prod_{j=1}^D \\\\mathcal{N}(w_j \\\\vert  0, \\\\tau^2) = \\\\prod_{j=1}^D p(w_j)}\\np(w)=N(w∣0,τ2I)=j=1∏D\\u200bN(wj\\u200b∣0,τ2)=j=1∏D\\u200bp(wj\\u200b)Тогда w^MAP\\\\widehat{w}_{MAP}wMAP\\u200b — точка минимума следующего выражения:\\n−log\\u2061p(y∣X,w)−log\\u2061p(w)=−∑i=1Np(yi∣xi,w)−∑j=1Dp(wj)=-\\\\log{p(y \\\\vert X, w)} - \\\\log{p(w)} =-\\\\sum_{i=1}^Np(y_i \\\\vert x_i, w) - \\\\sum_{j=1}^Dp(w_j) =\\n−logp(y∣X,w)−logp(w)=−i=1∑N\\u200bp(yi\\u200b∣xi\\u200b,w)−j=1∑D\\u200bp(wj\\u200b)==−∑i=1N(−12log\\u2061(2πσ2)−(yi−(w,xi))22σ2)−∑j=1D(−12log\\u2061(2πτ2)−wj22τ2)==-\\\\sum_{i=1}^N\\\\left(-\\\\frac12\\\\log(2\\\\pi\\\\sigma^2) - \\\\frac{(y_i - (w, x_i))^2}{2\\\\sigma^2}\\\\right)\\n-\\\\sum_{j=1}^D\\\\left(-\\\\frac12\\\\log(2\\\\pi\\\\tau^2) - \\\\frac{w_j^2}{2\\\\tau^2}\\\\right)==−i=1∑N\\u200b(−21\\u200blog(2πσ2)−2σ2(yi\\u200b−(w,xi\\u200b))2\\u200b)−j=1∑D\\u200b(−21\\u200blog(2πτ2)−2τ2wj2\\u200b\\u200b)==12σ2∑i=1N(yi−(w,xi))2+12τ2∑j=1Dwj2+\\xa0не\\xa0зависящие\\xa0от\\xa0w\\xa0члены= \\\\frac1{2\\\\sigma^2}\\\\sum_{i=1}^N(y_i - (w, x_i))^2 + \\\\frac1{2\\\\tau^2}\\\\sum_{j=1}^D w_j^2+\\\\text{ не зависящие от }w\\\\text{ члены}\\n=2σ21\\u200bi=1∑N\\u200b(yi\\u200b−(w,xi\\u200b))2+2τ21\\u200bj=1∑D\\u200bwj2\\u200b+\\xa0не\\xa0зависящие\\xa0от\\xa0w\\xa0членыПолучается, что\\nw^MAP=argmin\\u2061w(12∑i=1N(yi−(w,xi))2+σ2τ2∥w∥2)\\\\color{blue}{\\\\widehat{w}_{MAP} = \\\\underset{w}{\\\\operatorname{argmin}}\\\\left(\\\\vphantom{\\\\frac12}\\\\sum_{i=1}^N(y_i - (w, x_i))^2 + \\\\frac{\\\\sigma^2}{\\\\tau^2}\\\\|w\\\\|^2\\\\right)}\\nwMAP\\u200b=wargmin\\u200b(21\\u200bi=1∑N\\u200b(yi\\u200b−(w,xi\\u200b))2+τ2σ2\\u200b∥w∥2)а это же функция потерь для линейной регрессии с L2L^2L2-регуляризацией! Напомним на всякий случай, что у этой задачи есть «точное» решение\\nw^MAP=(XTX+σ2τ2I)−1XTy\\\\color{blue}{\\\\widehat{w}_{MAP} = \\\\left(X^TX + \\\\frac{\\\\sigma^2}{\\\\tau^2}I\\\\right)^{-1}X^Ty}\\nwMAP\\u200b=(XTX+τ2σ2\\u200bI)−1XTyДля этого примера мы можем вычислить и апостериорное распределение p(w∣X,y)p(w \\\\vert X, y)p(w∣X,y). В самом деле, из написанного выше мы можем заключить, что\\nlog\\u2061p(w∣X,y)=log\\u2061(p(y∣X,w)p(w))−log\\u2061p(y)=\\\\log{p(w \\\\vert X, y)} = \\\\log(p(y \\\\vert X, w)p(w)) - \\\\log{p(y)} = \\nlogp(w∣X,y)=log(p(y∣X,w)p(w))−logp(y)==12σ2(y−Xw)T(y−Xw)+12τ2wTw+\\xa0не\\xa0зависящие\\xa0от\\xa0w\\xa0члены=\\\\frac1{2\\\\sigma^2}(y - Xw)^T(y - Xw) + \\\\frac1{2\\\\tau^2}w^Tw+\\\\text{ не зависящие от }w\\\\text{ члены}\\n=2σ21\\u200b(y−Xw)T(y−Xw)+2τ21\\u200bwTw+\\xa0не\\xa0зависящие\\xa0от\\xa0w\\xa0членыТаким образом, log\\u2061p(w∣X,y)\\\\log{p(w \\\\vert X, y)}logp(w∣X,y) — это квадратичная функция от www, откуда следует, что апостериорное распределение является нормальным. Чтобы найти его параметры, нужно немного преобразовать полученное выражение:\\n…=12σ2(yTy−wTXTy−yTWx+wTXTXw)+12τ2wTw+const(w)=\\\\ldots=\\\\frac1{2\\\\sigma^2}(y^Ty - w^TX^Ty - y^TWx + w^TX^TXw) + \\\\frac1{2\\\\tau^2}w^Tw+\\\\mathrm{const}(w) =\\n…=2σ21\\u200b(yTy−wTXTy−yTWx+wTXTXw)+2τ21\\u200bwTw+const(w)==wT(12σ2XTX+12τ2I)w−12σ2wTXTy−12σ2yTWx+const(w)==w^T\\\\left(\\\\frac1{2\\\\sigma^2}X^TX + \\\\frac1{2\\\\tau^2}I\\\\right)w - \\\\frac{1}{2\\\\sigma^2}w^TX^Ty - \\\\frac1{2\\\\sigma^2}y^TWx + \\\\mathrm{const}(w) =\\n=wT(2σ21\\u200bXTX+2τ21\\u200bI)w−2σ21\\u200bwTXTy−2σ21\\u200byTWx+const(w)==12(w−w^MAP)T(1σ2XTX+1τ2I)(w−w^MAP)+const(w)==\\\\frac12\\\\left(w - \\\\widehat{w}_{MAP}\\\\right)^T\\\\left(\\\\frac1{\\\\sigma^2}X^TX + \\\\frac1{\\\\tau^2}I\\\\right)\\\\left(w - \\\\widehat{w}_{MAP}\\\\right) + \\\\mathrm{const}(w)=\\n=21\\u200b(w−wMAP\\u200b)T(σ21\\u200bXTX+τ21\\u200bI)(w−wMAP\\u200b)+const(w)=Таким образом,\\np(w∣X,y)=N(w^MAP,(1σ2XTX+1τ2I)−1)\\\\color{blue}{p(w \\\\vert X,y) = \\\\mathcal{N}\\\\left(\\\\widehat{w}_{MAP}, \\\\left(\\\\frac1{\\\\sigma^2}X^TX + \\\\frac1{\\\\tau^2}I\\\\right)^{-1}  \\\\right)}\\np(w∣X,y)=N(wMAP\\u200b,(σ21\\u200bXTX+τ21\\u200bI)−1)Как видим, от априорного распределения оно отличается корректировкой как матожидания 0↦w^MAP0\\\\mapsto\\\\widehat{w}_{MAP}0↦wMAP\\u200b, так и ковариационной матрицы (1τ2I)−1↦(1σ2XTX+1τ2I)−1\\\\left(\\\\frac1{\\\\tau^2}I\\\\right)^{-1}\\\\mapsto\\\\left(\\\\frac1{\\\\sigma^2}X^TX + \\\\frac1{\\\\tau^2}I\\\\right)^{-1}(τ21\\u200bI)−1↦(σ21\\u200bXTX+τ21\\u200bI)−1. Отметим, что XTXX^TXXTX — это, с точностью до численного множителя, оценка ковариационной матрицы признаков нашего датасета (элементы матрицы XTXX^TXXTX — это скалярные произведения столбцов XXX, то есть столбцов значений признаков).\\nИллюстрация. Давайте на простом примере (датасет с двумя признаками) посмотрим, как меняется апостериорное распределение www с ростом размера обучающей выборки:\\n\\nКак видим, не только мода распределения, то есть w^MAP\\\\widehat{w}_{MAP}wMAP\\u200b приближается к своему истинному значению, но и дисперсия распределения постепенно уменьшается.\\nЕщё иллюстрация. Теперь рассмотрим задачу аппроксимации неизвестной функции одной переменной (чьи значения в обучающей выборке искажены нормальным шумом) многочленом третьей степени. Её, разумеется, тоже можно решать, как задачу линейной регрессии на коэффициенты многочлена. Давайте нарисуем, как будут выглядеть функции, сгенерированные из распределения p(w∣X,y){p(w \\\\vert X,y)}p(w∣X,y) для разного объёма обучающей выборки:\\n\\nТут тоже видим, что функции не только становятся ближе к истинной, но и разброс их уменьшается.\\nПример: линейная регрессия с L1L^1L1-регуляризацией как модель с лапласовским априорным распределением на веса\\nДругое распределение, которое тоже может кодировать наше желание, чтобы небольшие по модулю значения wjw_jwj\\u200b были правдоподобными, а большие не очень, — распределение Лапласа. Посмотрим, что будет, если его взять в качестве априорного распределения на веса.\\np(w)=∏j=1Dp(wj)=∏j=1Dλ2exp\\u2061(−λ∣wm∣)\\\\color{blue}{p(w) = \\\\prod_{j=1}^D p(w_j) = \\\\prod_{j=1}^D\\\\frac{\\\\lambda}{2}\\\\exp(-\\\\lambda|w_m|)}\\np(w)=j=1∏D\\u200bp(wj\\u200b)=j=1∏D\\u200b2λ\\u200bexp(−λ∣wm\\u200b∣)Проводя такое же вычисление, получаем, что\\nw^MAP=argmin\\u2061w(12∑i=1N(yi−(w,xi))2+λ∑j=1D∣wj∣)\\\\color{blue}{\\\\widehat{w}_{MAP} = \\\\underset{w}{\\\\operatorname{argmin}}\\\\left(\\\\vphantom{\\\\frac12}\\\\sum_{i=1}^N(y_i - (w, x_i))^2 + \\\\lambda\\\\sum_{j=1}^D|w_j|\\\\right)}\\nwMAP\\u200b=wargmin\\u200b(21\\u200bi=1∑N\\u200b(yi\\u200b−(w,xi\\u200b))2+λj=1∑D\\u200b∣wj\\u200b∣)а это же функция потерь для линейной регрессии с L1L^1L1-регуляризацией!\\nКак делать предсказания\\nВсе изложенные выше рассуждения проводились в ситуации, когда X=XtrainX = X_{train}X=Xtrain\\u200b — обучающая выборка. Для неё мы можем посчитать\\np(w∣Xtrain,ytrain)=(y∣X,w)p(w)p(y)p(w \\\\vert X_{train}, y_{train}) = \\\\frac{(y \\\\vert X,w)p(w)}{p(y)}\\np(w∣Xtrain\\u200b,ytrain\\u200b)=p(y)(y∣X,w)p(w)\\u200bи точечную апостериорную оценку w^MAP=argmax\\u2061wp(y∣X,w)p(y)\\\\widehat{w}_{MAP} = \\\\underset{w}{\\\\operatorname{argmax}}{p(y \\\\vert X,w)p(y)}wMAP\\u200b=wargmax\\u200bp(y∣X,w)p(y). А теперь пусть нам дан новый объект x0∈Xx_0\\\\in\\\\mathbb{X}x0\\u200b∈X. Какой таргет y0y_0y0\\u200b мы для него предскажем?\\nБыло бы естественным, раз уж мы предсказываем распределение для www, и для y0y_0y0\\u200b тоже предсказывать распределение. Делается это следующим образом:\\np(y0∣x0,Xtrain,ytrain)=∫p(y0∣x0,w)p(w∣Xtrain,ytrain)dwp(y_0 \\\\vert x_0, X_{train}, y_{train}) = \\\\int{p(y_0 \\\\vert x_0,w)p(w \\\\vert X_{train}, y_{train})}dw\\np(y0\\u200b∣x0\\u200b,Xtrain\\u200b,ytrain\\u200b)=∫p(y0\\u200b∣x0\\u200b,w)p(w∣Xtrain\\u200b,ytrain\\u200b)dwНадо признать, что вычисление этого интеграла не всегда посильная задача, поэтому зачастую приходится «просто подставлять w^MAP\\\\widehat{w}_{MAP}wMAP\\u200b». В вероятностных терминах это можно описать так: вместо сложного апостериорного распределения p(w∣Xtrain,ytrain)p(w \\\\vert X_{train}, y_{train})p(w∣Xtrain\\u200b,ytrain\\u200b) мы берём самое грубое на свете приближение\\np(w∣Xtrain,ytrain)≈δ(w−w^MAP),p(w \\\\vert X_{train}, y_{train})\\\\approx\\\\delta(w - \\\\widehat{w}_{MAP}),\\np(w∣Xtrain\\u200b,ytrain\\u200b)≈δ(w−wMAP\\u200b),где δ(t)\\\\delta(t)δ(t) — дельта-функция, которая не является честной функцией (а является тем, что математики называют обобщёнными функциями), которая определяется тем свойством, что ∫f(t)δ(t)dt=f(0)\\\\int f(t)\\\\delta(t)dt = f(0)∫f(t)δ(t)dt=f(0) для достаточно разумных функций fff. Если не мудрствовать лукаво, то это всё значит, что\\np(y0∣x0,Xtrain,ytrain)≈p(y0∣x0,w^MAP)p(y_0 \\\\vert x_0,X_{train}, y_{train})\\\\approx p(y_0 \\\\vert x_0,\\\\widehat{w}_{MAP})\\np(y0\\u200b∣x0\\u200b,Xtrain\\u200b,ytrain\\u200b)≈p(y0\\u200b∣x0\\u200b,wMAP\\u200b)Пример. Пусть y∼Xw+εy\\\\sim Xw + \\\\varepsilony∼Xw+ε, ε∼N(0,σ)2\\\\varepsilon\\\\sim\\\\mathcal{N}(0,\\\\sigma)^2ε∼N(0,σ)2 — модель линейной регрессии с априорным распределением p(w)=N(0,τ2)p(w) = \\\\mathcal{N}(0,\\\\tau^2)p(w)=N(0,τ2) на параметры. Тогда, как мы уже видели раньше,\\np(w∣X,y)=N(w∣w^MAP,(1σ2XTX+1τ2I)−1)p(w \\\\vert X,y) = \\\\mathcal{N}\\\\left(w \\\\vert \\\\widehat{w}_{MAP}, \\\\left(\\\\frac1{\\\\sigma^2}X^TX + \\\\frac1{\\\\tau^2}I\\\\right)^{-1}  \\\\right)\\np(w∣X,y)=N(w∣wMAP\\u200b,(σ21\\u200bXTX+τ21\\u200bI)−1)Попробуем для новой точки x0x_0x0\\u200b посчитать распределение на y0y_0y0\\u200b. Рекомендуем читателю попробовать самостоятельно посчитать интеграл или же обратиться к пункту 7.6.2 книжки «Machine Learning A Probabilistic Perspective» автора Kevin P. Murphy, убедившись, что\\np(y0∣x0,Xtrain,ytrain)=N(y0∣x0w^MAP,σ2+σ2x0T(XTX+σ2τ2I)−1x0)p(y_0 \\\\vert x_0, X_{train}, y_{train}) = \\\\mathcal{N}\\\\left(y_0 \\\\vert x_0\\\\widehat{w}_{MAP},\\n\\\\sigma^2 + \\\\sigma^2x_0^T\\\\left(X^TX + \\\\frac{\\\\sigma^2}{\\\\tau^2}I\\\\right)^{-1}x_0\\\\right)p(y0\\u200b∣x0\\u200b,Xtrain\\u200b,ytrain\\u200b)=N(y0\\u200b∣x0\\u200bwMAP\\u200b,σ2+σ2x0T\\u200b(XTX+τ2σ2\\u200bI)−1x0\\u200b)что, очевидно, более содержательно, чем оценка, полученная с помощью приближения p(w∣Xtrain,ytrain)≈δ(w−w^MAP)p(w \\\\vert X_{train}, y_{train})\\\\approx\\\\delta(w - \\\\widehat{w}_{MAP})p(w∣Xtrain\\u200b,ytrain\\u200b)≈δ(w−wMAP\\u200b):\\np(y0∣x0,w^MAP)=N(y0∣x0w^MAP,σ2)p(y_0 \\\\vert x_0, \\\\widehat{w}_{MAP}) = \\\\mathcal{N}\\\\left(y_0\\\\left \\\\vert x_0\\\\widehat{w}_{MAP},\\n\\\\sigma^2\\\\right.\\\\right)p(y0\\u200b∣x0\\u200b,wMAP\\u200b)=N(y0\\u200b\\u200bx0\\u200bwMAP\\u200b,σ2)Собственно, видно, что в этом случае\\nПример в примере. Рассмотрим полюбившуюся уже нам задачу приближения функции многочленом степени не выше 333 (в которой мы строим модели с σ2=τ2=1\\\\sigma^2 = \\\\tau^2 = 1σ2=τ2=1). Для N=8N = 8N=8 мы получали такую картинку:\\n\\nЕсли оценить по приведённым выше формулам p(y0∣x0,Xtrain,ytrain)p(y_0 \\\\vert x_0, X_{train}, y_{train})p(y0\\u200b∣x0\\u200b,Xtrain\\u200b,ytrain\\u200b) для разных x0x_0x0\\u200b, то можно убедиться, что модель в большей степени уверена в предсказаниях для точек из областей, где было больше точек из обучающей выборки:\\n\\nБайесовский подход и дообучение моделей\\nДо сих пор мы в основном рассуждали о моделях машинного обучения как о чём-то, что один раз обучается и дальше навсегда застывает в таком виде, но в жизни такое скорее редкость. Мы пока не будем обсуждать изменчивость истинных зависимостей во времени, но даже если истина неизменна, к нам могут поступать новые данные, которые очень хотелось бы использовать для дообучения модели.\\nОбычные, не байесовские вероятностные модели не предоставляют таких инструментов. Оценку максимального правдоподобия придётся пересчитывать заново (хотя, конечно, можно схитрить, использовав старое значение в качестве начального приближения при итеративной оптимизации). Байесовский же подход позволяет оформить дообучения в виде простой и элегантной формулы: при добавлении новых данных (xN+1,yN+1),…,(xM,yM)(x_{N+1}, y_{N+1}),\\\\ldots,(x_M, y_M)(xN+1\\u200b,yN+1\\u200b),…,(xM\\u200b,yM\\u200b) имеем\\np(w∣(xi,yi)i=1M)=p((yi)i=N+1M∣(xi)i=N+1M)p(w∣(xi,yi)i=1N)p((yi)i=N+1M)p\\\\left(w\\\\vert (x_i, y_i)_{i=1}^M\\\\right) = \\\\frac{p\\\\left((y_i)_{i=N+1}^M\\\\vert (x_i)_{i=N+1}^M\\\\right) p\\\\left(w\\\\vert (x_i, y_i)_{i=1}^N\\\\right)}{p\\\\left( (y_i)_{i=N+1}^M \\\\right)}\\np(w∣(xi\\u200b,yi\\u200b)i=1M\\u200b)=p((yi\\u200b)i=N+1M\\u200b)p((yi\\u200b)i=N+1M\\u200b∣(xi\\u200b)i=N+1M\\u200b)p(w∣(xi\\u200b,yi\\u200b)i=1N\\u200b)\\u200bБайесовский подход к выбору модели: мотивация\\nНам часто приходится выбирать: дерево или случайный лес, линейная модель или метод ближайших соседей; да, собственно, и внутри наших вероятностных моделей есть параметры (скажем, дисперсия шума σ2\\\\sigma^2σ2 и τ2\\\\tau^2τ2), которые надо бы подбирать. Но как?\\nВ обычной ситуации мы выбираем модель, обученную на выборке (Xtrain,ytrain)(X_{train}, y_{train})(Xtrain\\u200b,ytrain\\u200b) в зависимости от того, как она себя ведёт на валидационной выборке (Xval,yval)(X_{val}, y_{val})(Xval\\u200b,yval\\u200b) (сравниваем правдоподобие или более сложные метрики) — или же делаем кросс-валидацию. Но как сравнивать модели, выдающие распределение?\\nОтветим вопросом на вопрос: а как вообще сравнивать модели? Назначение любой модели — объяснять мир вокруг нас, и её качество определяется именно тем, насколько хорошо она справляется с этой задачей. Тестовая выборка — это хороший способ оценки, потому что она показывает, насколько вписываются в модель новые данные. Но могут быть и другие соображения, помогающие оценить качество модели.\\nПример №1\\nАналитик Василий опоздал на работу. Своему руководителю он может предложить самые разные объяснения — и это будет выработанная на одном обучающем примере модель, описывающая причины опоздания и потенциально позволяющая руководителю принять решение о том, карать ли Василия.\\nКонечно, руководитель мог бы принять изложенную Василием модель к сведению, подождать, пока появятся другие опоздавшие, и оценить её, так скажем, на тестовой выборке, но стоит ли? Давайте рассмотрим несколько конкретных примеров:\\n\\n\\nМодель «Василий опоздал, потому что так получилось», то есть факт опоздания — это просто ни от чего не зависящая случайная величина. Такая модель плоха тем, что (а) не предлагает, на самом деле, никакого объяснения тому факту, что Василий опоздал, а его коллега Надежда не опоздала и (б) совершенно не помогает решить, наказывать ли за опоздание. Наверное, такое не удовлетворит руководителя.\\n\\n\\nМодель «Василий опоздал, потому что рядом с его домом открылся портал в другой мир, где шла великая битва орков с эльфами, и он почувствовал, что просто обязан принять в ней участие на стороне орков, которых привёл к победе, завоевав руку и сердце орочьей принцессы, после чего был перенесён обратно в наш скучный мир завистливым шаманом». Чем же она плоха? Битва с эльфами — это, безусловно, важное и нужное дело, и на месте руководителя мы бы дружно согласились, что причина уважительная. Но заметим, что в рамках этой модели можно объяснить множество потенциальных исходов, среди которых довольно маловероятным представляется наблюдаемый: тот, в котором Василий не погиб в бою, не остался со своей принцессой и не был порабощён каким-нибудь завистливым шаманом. Отметим и другой недостаток этой модели: её невозможно провалидировать. Если в совершенно случайной модели можно оценить вероятность опоздания и впоследствии, когда накопятся ещё примеры, проверить, правильно ли мы её посчитали, то в мире, где открываются порталы и любой аналитик может завоевать сердце орочьей принцессы, возможно всё, и даже если больше никто не попадёт в такую ситуацию, Василий всё равно сможет бить себя в грудь кулаком и говорить, что он избранный. Так что, наверное, это тоже не очень хорошая модель.\\n\\n\\nМодель «Василий опоздал, потому что проспал» достаточно проста, чтобы в неё поверить, и в то же время даёт руководителю возможность принять решение, что делать с Василием.\\n\\n\\nПример №2\\nОбратимся к примеру из машинного обучения. Сравним три модели линейной регрессии:\\n\\nДаже и не запрашивая тестовую выборку, мы можем сделать определённые выводы о качестве этих моделей. Средняя (квадратичная) явно лучше левой (линейной), потому что она лучше объясняет то, что мы видим: тот факт, что облако точек обучающей выборки выглядит вогнутым вниз.\\nА что с правым, почему мы можем утверждать, что он хуже? Есть много причин критиковать его. Остановимся вот на какой. На средней картинке у нас приближение квадратичной функцией, а на правой — многочленом довольно большой степени (на самом деле, десятой). А ради интереса: как выглядит график квадратичной функции и как — многочлена десятой степени со случайно сгенерированными коэффициентами? Давайте сгенерируем несколько и отметим их значения в точках обучающей выборки:\\n\\nОбратите внимание на масштаб на графиках справа. И какова вероятность, что нам достался именно тот многочлен десятой степени, у которого значения в обучающих точках по модулю в пределах сотни? Очевидно, она очень мала. Поэтому мы можем сказать, что выбор в качестве модели многочлена десятой степени не очень обоснован.\\nПопробуем резюмировать\\nСлишком простая модель плохо объясняет наблюдаемые нами данные, тогда как слишком сложная делает это хорошо, но при этом описывает слишком многообразный мир, в котором имеющиеся у нас данные оказываются уже слишком частным случаем. В каком-то смысле наш способ выбора модели оказывается переформулировкой бритвы Оккама: из моделей, пристойно описывающих наблюдаемые явления, следует выбирать наиболее минималистичную.\\nБайесовский подход к выбору модели: формализация\\nПусть у нас есть некоторое семейство моделей J\\\\mathcal{J}J и для каждого j∈Jj\\\\in\\\\mathcal{J}j∈J задана какая-то своя вероятностная модель. В духе байесовского подхода было бы оценить условное распределение моделей\\np(j∣y,X)=p(y∣X,j)p(j)∑j∈Jp(j,y∣X)p(j \\\\vert y, X) = \\\\frac{p(y \\\\vert X,j)p(j)}{\\\\sum\\\\limits_{j\\\\in\\\\mathcal{J}}p(j, y \\\\vert X)}\\np(j∣y,X)=j∈J∑\\u200bp(j,y∣X)p(y∣X,j)p(j)\\u200bи в качестве наилучшей модели взять её моду. Если же считать все модели равновероятными, то мы сводим всё к максимизации только лишь p(y∣X,j)=pj(y∣X)p(y \\\\vert X,j) = p_j(y \\\\vert X)p(y∣X,j)=pj\\u200b(y∣X):\\nȷ^=argmax\\u2061j∫pj(y∣X,w)pj(w)dw=argmax\\u2061jpj(y∣X)\\\\color{blue}{\\\\widehat{\\\\jmath} = \\\\underset{j}{\\\\operatorname{argmax}}\\\\int{p_j(y \\\\vert X,w)p_j(w)}dw =\\\\underset{j}{\\\\operatorname{argmax}}p_j(y \\\\vert X)}\\n\\ue237\\u200b=jargmax\\u200b∫pj\\u200b(y∣X,w)pj\\u200b(w)dw=jargmax\\u200bpj\\u200b(y∣X)Величина pj(y∣X)p_j(y \\\\vert X)pj\\u200b(y∣X) называется обоснованностью (evidence, marginal likelihood) модели.\\nОтметим, что такое определение вполне согласуется с мотивацией из предыдущего подраздела. Слишком простая модель плохо описывает наблюдаемые данные, и потому будет отвергнута. В свою очередь, слишком сложная модель способна описывать гораздо большее многообразие явлений, чем нам было бы достаточно. Таким образом, компромисс между качеством описания и сложностью и даёт нам оптимальную модель.\\nПример\\nВернёмся к нашей любимой задаче аппроксимации функции одной переменной многочленом небольшой степени по нескольким точкам, значение в которых было искажено нормальным шумом. Построим несколько моделей, приближающих многочленом степени не выше некоторого deg\\\\mathrm{deg}deg (будет принимать значения 1, 3 и 6), положив в вероятностной модели σ2=τ2=1\\\\sigma^2 = \\\\tau^2 = 1σ2=τ2=1.\\nМы не будем приводить полный вывод обоснованности для задачи регрессии p(y∣X,w)=N(y∣Xw,σ2I)p(w∣τ2I)p(y \\\\vert X,w) = \\\\mathcal{N}(y \\\\vert Xw,\\\\sigma^2I)p(w \\\\vert \\\\tau^2I)p(y∣X,w)=N(y∣Xw,σ2I)p(w∣τ2I), а сразу выпишем ответ:\\np(y∣X)=N(0,σ2I+τ2XXT)p(y \\\\vert X) = \\\\mathcal{N}\\\\left(0, \\\\sigma^2I + \\\\tau^2XX^T\\\\right)\\np(y∣X)=N(0,σ2I+τ2XXT)Посмотрим, какой будет обоснованность для разного числа обучающих точек:\\n\\nМожно убедиться, что для регрессии по двум точкам наиболее обоснованная — линейная модель (и неудивительно), тогда как с ростом числа точек более обоснованной становится модель с многочленом третьей степени; слишком сложная же модель шестой степени всегда плетётся в хвосте.\\nАппроксимация обоснованности и байесовский информационный критерий\\nТочно вычислить обоснованность может быть трудной задачей (попробуйте проделать это сами хотя бы для линейной регрессии!). Есть разные способы посчитать её приближённо; мы рассмотрим самый простой.\\nНапомним, что\\np(y∣X)=∫p(y∣X,w)p(w)dwp(y \\\\vert X) = \\\\int{p(y \\\\vert X,w)p(w)}dw \\np(y∣X)=∫p(y∣X,w)p(w)dwВоспользуемся приближением Лапласа, то есть разложим p(y∣X,w)p(y \\\\vert X,w)p(y∣X,w) (как функцию от www) вблизи своего максимума, то есть вблизи w^:=w^MLE\\\\widehat{w} := \\\\widehat{w}_{MLE}w:=wMLE\\u200b в ряд Тейлора:\\nlog\\u2061p(y∣X,w)≈log\\u2061p(y∣X,w^)−12(w−w^)TIN(w^)(w−w^),\\\\log{p(y \\\\vert X,w)} \\\\approx \\\\log{p(y \\\\vert X,\\\\widehat{w})} - \\\\frac12(w - \\\\widehat{w})^TI_N(\\\\widehat{w})(w - \\\\widehat{w}),\\nlogp(y∣X,w)≈logp(y∣X,w)−21\\u200b(w−w)TIN\\u200b(w)(w−w),где линейный член отсутствует, поскольку разложение делается в точке локального экстремума, а I(w^)I(\\\\widehat{w})I(w) — знакомая нам матрица Фишера IN(w^)=−E∇w2log\\u2061p(y∣X,w)∣w^=NI1(w^)I_N(\\\\widehat{w}) = -\\\\mathbb{E}\\\\nabla^2_w\\\\log{p(y \\\\vert X,w)}\\\\vert_{\\\\widehat{w}} = NI_1(\\\\widehat{w})IN\\u200b(w)=−E∇w2\\u200blogp(y∣X,w)∣w\\u200b=NI1\\u200b(w).\\nДалее, p(w)p(w)p(w) мы можем с точностью до второго порядка приблизить p(w^MAP)p(\\\\widehat{w}_{MAP})p(wMAP\\u200b). Получается, что\\np(y∣X)≈∫elog\\u2061p(y∣X,w^)−N2(w−w^)TI1(w^)(w−w^)p(w^MAP)dw=p(y \\\\vert X)\\\\approx\\\\int e^{\\\\log{p(y \\\\vert X,\\\\widehat{w})} - \\\\frac{N}2(w - \\\\widehat{w})^TI_1(\\\\widehat{w})(w - \\\\widehat{w})}p(\\\\widehat{w}_{MAP})dw =\\np(y∣X)≈∫elogp(y∣X,w)−2N\\u200b(w−w)TI1\\u200b(w)(w−w)p(wMAP\\u200b)dw==elog\\u2061p(y∣X,w^)p(w^MAP)∫e−N2(w−w^)TI1(w^)(w−w^)dw== e^{\\\\log{p(y \\\\vert X,\\\\widehat{w})}}p(\\\\widehat{w}_{MAP})\\\\int e^{ - \\\\frac{N}{2}(w - \\\\widehat{w})^TI_1(\\\\widehat{w})(w - \\\\widehat{w})}dw = \\n=elogp(y∣X,w)p(wMAP\\u200b)∫e−2N\\u200b(w−w)TI1\\u200b(w)(w−w)dw==elog\\u2061p(y∣X,w^)p(w^MAP)⋅(2π)D/2∣I1(w^)∣−12ND/2== e^{\\\\log{p(y \\\\vert X,\\\\widehat{w})}}p(\\\\widehat{w}_{MAP})\\\\cdot (2\\\\pi)^{D/2}\\\\frac{|I_1(\\\\widehat{w})|^{-\\\\frac12}}{N^{D/2}} =\\n=elogp(y∣X,w)p(wMAP\\u200b)⋅(2π)D/2ND/2∣I1\\u200b(w)∣−21\\u200b\\u200b==exp\\u2061(log\\u2061p(y∣X,w^)−D2log\\u2061N+всякие\\xa0штуки)=\\\\exp\\\\left(\\\\log{p(y \\\\vert X,\\\\widehat{w})} - \\\\frac{D}2\\\\log{N} + \\\\text{всякие штуки}\\\\right)\\n=exp(logp(y∣X,w)−2D\\u200blogN+всякие\\xa0штуки)Несмотря на то, что p(w^MAP)p(\\\\widehat{w}_{MAP})p(wMAP\\u200b) и ∣I1(w^)∣−12\\\\vert I_1(\\\\widehat{w})\\\\vert^{-\\\\frac12}∣I1\\u200b(w)∣−21\\u200b, сгруппированные нами во «всякие штуки», существенным образом зависят от модели, при больших NNN они вносят в показатель гораздо меньше вклада, чем первые два слагаемых. Таким образом, мы можем себе позволить вместо трудновычисляемых p(y∣X)p(y \\\\vert X)p(y∣X) использовать для сравнения модели байесовский\\xa0информационный\\xa0критерий(BIC):\\\\color{blue}байесовский\\\\ информационный\\\\ критерий (BIC):байесовский\\xa0информационный\\xa0критерий(BIC):\\nBIC=Dlog\\u2061N−2log\\u2061p(y∣X,w^)\\\\color{blue}{BIC = D\\\\log{N} - 2\\\\log{p(y \\\\vert X,\\\\widehat{w})}}\\nBIC=DlogN−2logp(y∣X,w)Фреквентисты против байесиан: кто кого?\\nМы с вами познакомились с двумя парадигмами оценивания:\\n\\nфреквентистской (frequentist, от слова \"frequency\", частота) — в которой считается, что данные являются случайным (настоящая случайность!) семплом из некоторого фиксированного распределения, которое мы стараемся оценить по этому семплу, и\\nбайесовской — в которой данные считаются данностью и в которой мы используем данные для обновления наших априорных представлений о распределении параметров (здесь случайности нет, а есть лишь нехватка знания).\\n\\nУ обеих есть свои достоинства и недостатки, поборники и гонители. К недостаткам байесовской относится, безусловно, её вычислительная сложность: возможно, вы помните, в пучину вычислений сколь мрачных нас низвергла банальная задача линейной регрессии, и дальше становится только ещё трудней. Если мы захотим байесовский подход применять к более сложным моделям, например, нейросетям, нам придётся прибегать к упрощениям, огрублениям, приближениям, что, разумеется, ухудшает наши оценки. Но, если простить ему эту вынужденную неточность, он логичнее и честней, и мы продемонстрируем это на следующем примере.\\nОдно известное свойство оценки максимального правдоподобия — асимптотическая нормальность. Если оценивать наши веса www по различным наборам из NNN обучающих примеров, причём считать, что наборы выбираются случайно (не будем уточнять, как именно), то оценка w^MLE\\\\widehat{w}_{MLE}wMLE\\u200b тоже превращается в случайную величину, которая как-то распределена. Теория утверждает, что при N→∞N\\\\rightarrow\\\\inftyN→∞\\nw^MLE∼N(w∗,IN(w∗)−1)\\\\quad \\\\widehat{w}_{MLE}\\\\sim\\\\mathcal{N}\\\\left(w^{\\\\ast}, I_N({w}^{\\\\ast})^{-1}\\\\right)\\nwMLE\\u200b∼N(w∗,IN\\u200b(w∗)−1)где w∗w^{\\\\ast}w∗ — истинное значение весов, а IN(w∗)I_N({w}^{\\\\ast})IN\\u200b(w∗) — матрица информации Фишера, которая определяется как\\nIN(w∗)=E[(∂∂wilog\\u2061p(y∣X,w)∣w∗)(∂∂wjlog\\u2061p(y∣X,w)∣w∗)]I_N({w}^{\\\\ast}) = \\\\mathbb{E}\\\\left[\\\\left(\\\\left.\\\\frac{\\\\partial}{\\\\partial w_i}\\\\log{p(y \\\\vert X,w)}\\\\right|_{w^{\\\\ast}}\\\\right)\\\\left(\\\\left.\\\\frac{\\\\partial}{\\\\partial w_j}\\\\log{p(y \\\\vert X,w)}\\\\right|_{w^{\\\\ast}}\\\\right)\\\\right]\\nIN\\u200b(w∗)=E[(∂wi\\u200b∂\\u200blogp(y∣X,w)\\u200bw∗\\u200b)(∂wj\\u200b∂\\u200blogp(y∣X,w)\\u200bw∗\\u200b)]что при некоторых не слишком обременительных ограничениях равно\\nIN(w∗)=−E[∂2∂wi∂wjlog\\u2061p(y∣X,w)∣w∗]I_N({w}^{\\\\ast}) = -\\\\mathbb{E}\\\\left[\\\\left.\\\\frac{\\\\partial^2}{\\\\partial w_i\\\\partial w_j}\\\\log{p(y \\\\vert X,w)}\\\\right|_{w^{\\\\ast}}\\\\right]\\nIN\\u200b(w∗)=−E[∂wi\\u200b∂wj\\u200b∂2\\u200blogp(y∣X,w)\\u200bw∗\\u200b]При этом поскольку log\\u2061p(y∣X,w)=∑i=1Nlog\\u2061p(y∣X,w)\\\\log{p(y \\\\vert X,w)} = \\\\sum_{i=1}^N\\\\log{p(y \\\\vert X,w)}logp(y∣X,w)=∑i=1N\\u200blogp(y∣X,w), матрица тоже распадается в сумму, и получается, что IN(w∗)=NI1(w∗)I_N({w}^{\\\\ast}) = NI_1(w^{\\\\ast})IN\\u200b(w∗)=NI1\\u200b(w∗), то есть с ростом NNN ковариация (NI1(w∗))−1(NI_1(w^{\\\\ast}))^{-1}(NI1\\u200b(w∗))−1 оценки максимального правдоподобия стремится к нулю.\\nНа интуитивном уровне можно сказать, что матрица информации Фишера показывает, сколько информации о весах www содержится в XXX.\\nПоговорим о проблемах. В реальной ситуации мы не знаем w∗w^{\\\\ast}w∗ и тем более не можем посчитать матрицу Фишера, то есть мы с самого начала вынуждены лукавить. Ясно, что вместо w∗w^{\\\\ast}w∗ можно взять просто w^\\\\widehat{w}w, а вместо IN(w∗)I_N(w^{\\\\ast})IN\\u200b(w∗) — матрицу IN(w^)I_N(\\\\widehat{w})IN\\u200b(w), которую можно даже при желании определить как\\n−(∂2∂wi∂wjlog\\u2061(p(y∣X,w))∣w∗)-\\\\left(\\\\left.\\\\frac{\\\\partial^2}{\\\\partial w_i\\\\partial w_j}\\\\log(p(y \\\\vert X, w))\\\\right|_{w^{\\\\ast}}\\\\right)\\n−(∂wi\\u200b∂wj\\u200b∂2\\u200blog(p(y∣X,w))\\u200bw∗\\u200b)безо всякого математического ожидания. Итак, хотя мы можем теперь построить доверительный интервал для оцениваемых параметров, по ходу нами было сделано много упрощений: мы предположили, что асимптотическая оценка распределения уже достигнута, от w∗w^{\\\\ast}w∗ перешли к w^\\\\widehat{w}w, а для полноты чувств ещё и избавились от математического ожидания. В байесовском подходе мы такого себе не позволяем.\\nЕсли вам интересно, посмотрите, как это будет выглядеть для линейной регрессии.Рассмотрим модель линейной регрессии y∼Xw+εy\\\\sim Xw + \\\\varepsilony∼Xw+ε, ε∼N(0,σ2)\\\\varepsilon\\\\sim\\\\mathcal{N}(0, \\\\sigma^2)ε∼N(0,σ2). Для неё\\nlog\\u2061p(y∣X,w)=−N2log\\u2061(2πσ2)−12σ2(y−Xw)T(y−Xw)\\\\log{p(y \\\\vert X,w)} = -\\\\frac{N}{2\\\\log(2\\\\pi\\\\sigma^2)} - \\\\frac{1}{2\\\\sigma^2}(y - Xw)^T(y - Xw)\\nlogp(y∣X,w)=−2log(2πσ2)N\\u200b−2σ21\\u200b(y−Xw)T(y−Xw)Нетрудно убедиться, что\\n∇wlog\\u2061p(y∣X,w)=1σ2XT(y−Xw)\\\\nabla_w\\\\log{p(y \\\\vert X,w)} = \\\\frac{1}{\\\\sigma^2}X^T(y - Xw)\\n∇w\\u200blogp(y∣X,w)=σ21\\u200bXT(y−Xw)∇w2log\\u2061p(y∣X,w)=−1σ2XTX\\\\nabla^2_w\\\\log{p(y \\\\vert X,w)} = -\\\\frac{1}{\\\\sigma^2}X^TX\\n∇w2\\u200blogp(y∣X,w)=−σ21\\u200bXTXСоответственно,\\nIN(w^)=1σ2XTXI_N(\\\\widehat{w}) = \\\\frac{1}{\\\\sigma^2}X^TX\\nIN\\u200b(w)=σ21\\u200bXTXгде w^\\\\widehat{w}w — это полученная по датасету XXX оценка весов. Заметим, что WTXW^TXWTX — это с точностью до коэффициента 1N\\\\frac{1}{N}N1\\u200b оценка ковариационной матрицы признаков нашего датасета (элементы XTXX^TXXTX — это скалярные произведения столбцов XXX, то есть столбцов признаков). Можно легко убедиться, что\\n1σ2XTX=1σ2∑i=1NxiTxi\\\\frac{1}{\\\\sigma^2}X^TX = \\\\frac{1}{\\\\sigma^2}\\\\sum_{i=1}^Nx_i^Tx_i\\nσ21\\u200bXTX=σ21\\u200bi=1∑N\\u200bxiT\\u200bxi\\u200bПо-хорошему, нам надо было бы ещё взять математическое ожидание. Найти его мы не можем, но можем очень наивно оценить как C=1N∑i=1NxiTxiC = \\\\frac1N\\\\sum_{i=1}^Nx_i^Tx_iC=N1\\u200b∑i=1N\\u200bxiT\\u200bxi\\u200b. Тогда получаем, что IN(w^)=Nσ2CI_N(\\\\widehat{w}) = \\\\frac{N}{\\\\sigma^2}CIN\\u200b(w)=σ2N\\u200bC. Таким образом, имея один датасет XXX и одну посчитанную по нему оценку w^\\\\widehat{w}w, мы можем довольно грубо оценить распределение оценок максимального правдоподобия для заданного NNN как\\nN(w^,Nσ2C)\\\\mathcal{N}\\\\left(\\\\widehat{w}, \\\\frac{N}{\\\\sigma^2}C\\\\right)\\nN(w,σ2N\\u200bC)Пример в примере. Давайте рассмотрим задачу аппроксимации функции одной переменной (чьи значения в обучающей выборке искажены нормальным шумом) многочленом степени не выше 333. Положим в вероятностной модели σ2=1\\\\sigma^2 = 1σ2=1. Тогда различный выбор обучающих датасетов будет приводить к различным результатам:\\n\\nНо разброс результатов падает с ростом NNN.\\nЕщё пример в примере. Рассмотрим ещё одну задачу регрессии с двумя признаками (в которой всё так же будем полагать σ2=1\\\\sigma^2 = 1σ2=1), для которой оценим распределение w0w_0w0\\u200b через первую компоненту N(w^,IN(w^)−1)\\\\mathcal{N}(\\\\widehat{w}, I_N(\\\\widehat{w})^{-1})N(w,IN\\u200b(w)−1) для одного конкретного w^\\\\widehat{w}w и нарисуем несколько различных w^\\\\widehat{w}w, полученных из других датасетов той же мощности:\\n\\nВидим, что средние оцененного распределения сходятся к истинному значению −1-1−1; при этом дисперсия падает. Красные крестики не вполне подчиняются синему распределению, но мы от них ждём лишь приближённой согласованности, которая имеет место.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф4.5. Генеративный подход к классификацииКак использовать распределение меток классов в\\xa0задаче классификации. LDA, QDA и\\xa0наивный байесСледующий параграф4.7. Модели с латентными переменнымиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_59.html', 'title': 'Введение в онлайн-обучение'}, page_content='Введение в онлайн-обучениеЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/415.1.Введение в онлайн-обучениеО чём раздел про онлайн-обучение, кому и зачем его читать?ОглавлениеПостановка задачиВыпуклая онлайн-оптимизацияFollow the LeaderFollow The Regularized LeaderAdaptive FTRLЛинеаризация и вычислительно эффективный FTRLСубдифференциал и субградиентные методы15.2.Адаптивный FTRL15.3.Регуляризация в онлайн-обучении15.4.Методы оптимизации в Deep Learning16.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Введение в онлайн-обучение15.1. Введение в онлайн-обучениеАвторыАлексей МорозовО чём раздел про онлайн-обучение, кому и зачем его читать?\\nВо многих случаях обучение ML-модели ― это однократный процесс, после которого она не меняется и только используется для предсказания. А что, если к нам постоянно поступает новая информация и мы должны её учитывать? Тогда модель должна уметь обновляться при поступлении нового объекта или батча объектов. Грубо говоря, этим и занимается онлайн-оптимизация. Можно заметить, что обновление модели на батче объектов проходит и в процессе стохастической оптимизации, ― и это сходство не случайно.\\nОказывается, что все известные вам методы стохастической оптимизации первого порядка ― такие как SGD, AdaGrad, Adam, AMSgrad и другие ― являются в первую очередь алгоритмами онлайн-обучения. Чтобы в этом убедиться, достаточно открыть эти статьи и увидеть, для какой задачи выводятся гарантии на сходимость. Постановка задачи онлайн-обучения является одновременно математически простой и очень общей, соединяя три больших темы:\\n\\n«Классическое» онлайн обучение.\\nСтохастическую оптимизацию на фиксированном датасете. Мы покажем, что любой алгоритм онлайн обучения можно переформулировать, как алгоритм стохастической оптимизации; при этом из гарантий на сходимость, полученных для онлайн обучения, автоматически будет следовать сходимость на фиксированном датасете.\\nAdversarial обучение.\\n\\nДанный текст является в первую очередь систематизирующим. Мы постараемся достичь следующих целей:\\n\\nПодведем единую математическую базу, необходимую для вдумчивого чтения статей по оптимизации. Это будет полезно ML-теоретикам.\\nПокажем, как исторически развивались методы оптимизации, как из одного метода получался другой, какие проблемы они решали и ― главное ― актуальны ли эти проблемы сейчас.\\nРазберём все «именные» методы оптимизации на набор базовых концепций и покажем, как вы можете самостоятельно их сочетать, создавая оптимальный метод для решения своей задачи. Спойлер: базовых концепций намного меньше, чем наименований методов. Эти знания будут полезны ML-инженерам.\\nПройдемся по относительно нишевым темам, таким как разреженные методы регуляризации L1L_1L1\\u200b и L1/2L_{1/2}L1/2\\u200b, и рассмотрим наилучшие методы оптимизации для них. Такие методы невозможно получить в стандартной постановке стохастической оптимизации. Эти знания будут полезны ML-инженерам, занимающимся рекомендательными системами.\\n\\nВ параграфе «Введение в онлайн-обучение», которую вы читаете сейчас, вы познакомитесь с общей постановкой задачи онлайн-обучения, а также с семейством алгоритмов Follow the Regularized Leader (FTRL), которое включает в себя все методы первого порядка. Кроме того, вы узнаете, как сводить задачи стохастической оптимизации к задачам онлайн-обучения и увидите, что этот переход позволяет строить более эффективные методы стохастической оптимизации, особенно для разреженных регуляризаторов вроде L1L_1L1\\u200b.\\nВ параграфе «Адаптивный FTRL» вы узнаете, как улучшить сходимость алгоритмов стохастической оптимизации с помощью регуляризаторов и каковы гарантии сходимости для регуляризованных задач. Это позволит вывести AdaGrad как наилучший адаптивный метод для онлайн-оптимизации.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nВ параграфе «Регуляризация в онлайн-обучении» мы снова поговорим о регуляризации, но на этот раз речь пойдёт о регуляризаторах, которые накладывают на решение определённые органичения, например, разреженность. Вы сможете с новой стороны взглянуть на разреживающие свойства L1L_1L1\\u200b-регуляризаторов. Кроме того, мы получим не достижимые с помощью обычных SGD/AdaGrad результаты для разреженных L1L_1L1\\u200b и L1/2L_{1/2}L1/2\\u200b регуляризаторов.\\nВ параграфе «Стохастическая оптимизация в Deep Learning» мы перейдём к методам оптимизации в глубоких нейросетях. Вас ждёт краткий исторический обзор и мотивация появления двух важных модификаций AdaGrad ― Adam и RMSprop. Мы покажем, что эти методы ломаются вокруг критических точек, и поговорим о том, как починить это и достичь более точной сходимости (этого эффекта можно достичь либо прямой модификацией алгоритмов (AMSgrad и RAdam), либо косвенно с помощью Learning Rate Scheduler\\'ов).\\nВ конце параграфа мы соберём воедино все рассмотренные концепции и покажем, как можно комбинировать лучшее из разных методов оптимизации в один новый метод.\\nОглавление\\nЧасть 1. Введение\\nПостановка задачи\\nВыпуклая онлайн-оптимизация\\nFollow the Leader\\nFollow The Regularized Leader (FTRL)\\nЛинеаризация и вычислительно эффективный FTRL\\nСубдифференциал и субградиентные методы\\n\\nЧасть 2. Адаптивные методы оптимизации\\nАддитивные регуляризаторы\\nКлассы алгоритмов FTRL\\nГарантии сходимости для алгоритмов FTRL\\nПостроение эффективного адаптивного FTRL\\n\\nO(1)O(1)O(1) learning rate\\nO(1t)O(\\\\frac{1}{\\\\sqrt{t}})O(t\\u200b1\\u200b) learning rate\\nData-Adaptive learning rate.\\nAdaGrad ― оптимальный data-adaptive метод\\n\\n\\n\\nЧасть 3. Продвинутые методы регуляризации\\nИдея неразложения регуляризаторов в субградиентную оценку\\n\\nСвязь между Composite-Objective FTRL и Proximal Gradient Descent. Lazy vs Greedy представления\\n\\n\\nL1L_1L1\\u200b-регуляризация\\n\\nОтбор параметров разреженных моделей\\nLinear Incremental L1L_1L1\\u200b. Аналог L1L_1L1\\u200b в Greedy методах\\nGlobal L1L_1L1\\u200b. Лучший метод L1L_1L1\\u200b для онлайн обучения\\n\\n\\nL2L_2L2\\u200b регуляризация\\n\\nWeight Decay. Decoupled Weight Decay\\n\\n\\nПроекция на выпуклое множество χ\\\\chiχ\\n\\nЧасть 4. Методы оптимизации в Deep Learning\\nRMSprop и Adam\\n\\nМотивация их создания\\nRMSprop\\nAdam\\nМотивация для bias correction\\nПочему Adam ошибочно считают лучшим методом стохастической оптимизации\\n\\n\\nКак сломать адаптивные методы RMSprop и Adam со скользящим средним\\nЧиним RMSprop и Adam\\n\\nМетод AMSgrad\\n\\nРеализация без дополнительной памяти\\nДобавление Bias Correction\\n\\n\\nLearning Rate Scheduling\\n\\nВлияние learning rate decay на сходимость\\nПрактические рекомендации\\nLearning rate scheduling vs AdaGrad\\n\\n\\n\\n\\nSGD vs Adam. Методы AdamW/SGDW. Улучшение методов с помощью проксимального L2L_2L2\\u200b\\nMomentum\\n\\nNesterov Momentum\\nAdan\\n\\n\\n\\nЧасть 5. Заключение\\nТаблицы формул. Примеры комбинирования идей\\n\\nПостановка задачи\\nЛитература. Отсюда и далее, пока явно не скажем о переходе на другие источники информации, используется материал из книги Shai Shalev-Shwartz  Online Learning and Online\\nConvex Optimization\\nОнлайн-обучение ― это процесс предсказания ответов на последовательность вопросов с учётом знания (возможно, неполного) о предыдущих правильных ответах.\\nПредставим себе следующую игру (назовём её игра (1)). На каждом раунде игры ttt мы:\\n\\nПолучаем xtx_txt\\u200b ― частичную информацию о текущем «вопросе»;\\nВыбираем модель wtw_twt\\u200b, которой будем делать прогноз;\\nПрогнозируем pt(wt,xt)p_t(w_t, x_t)pt\\u200b(wt\\u200b,xt\\u200b);\\nПолучаем истинный ответ yty_tyt\\u200b;\\nПолучаем обратную связь-лосс l(pt,yt)l(p_t, y_t)l(pt\\u200b,yt\\u200b). Лоссы обычно имеют семантику функции ошибки: больше ― хуже, меньше ― лучше.\\n\\nЦель любого алгоритма онлайн обучения ― минимизация суммарной ошибки прогнозов Loss(T)=∑t=1Tl(pt,yt)Loss(T) = \\\\sum\\\\limits_{t=1}^Tl(p_t, y_t)Loss(T)=t=1∑T\\u200bl(pt\\u200b,yt\\u200b) для любого количества раундов TTT.\\nПока рассмотрим интуитивный пример: линейная регрессия (обозначения взяты из параграфа про линейные модели). Пусть у нас уже сыграны раунды 1,…,t−11,\\\\ldots,t-11,…,t−1 и есть выборка данных x1,…,xt−1x_1,\\\\ldots,x_{t-1}x1\\u200b,…,xt−1\\u200b и ответов y1,…,yt−1y_1,\\\\ldots,y_{t-1}y1\\u200b,…,yt−1\\u200b.\\n\\nПолучаем новый xtx_txt\\u200b. В данном случае просто получаем и пока не используем;\\nВыбираем модель wtw_twt\\u200b, которая наилучшим образом объясняет всю предыдущую имеющуюся выборку x1..tx_{1..t}x1..t\\u200b (алгоритм обучения можем выбирать любой, какой нам нравится);\\nПрогнозируем pt=<wt,xt>p_t = <w_t, x_t>pt\\u200b=<wt\\u200b,xt\\u200b>;\\nПолучаем правильный ответ yty_tyt\\u200b;\\nСчитаем loss (yt−pt)2(y_t - p_t)^2(yt\\u200b−pt\\u200b)2;\\n\\nДействуя таким образом, мы делаем интуитивное предположение, что ответы yty_tyt\\u200b как-то зависят от наших xtx_txt\\u200b и что эту зависимость мы можем выучить из предыдущей выборки, улучшив прогноз на новых объектах.\\nПредположения\\nТеория онлайн обучения выгодно отличается от классической теории статистического обучения довольно расслабленными и гораздо более простыми (с точки зрения математических формулировок) условиями. Мы не делаем предположений о некой статистической зависимости между xtx_txt\\u200b, yty_tyt\\u200b. Зависимость может быть детерминированной, стохастической или даже adversarial:\\n\\nДетерминированная: в самом начала игры делается выбор детерминированной зависимости xt→ytx_t \\\\rightarrow y_txt\\u200b→yt\\u200b\\nСтохастическая: xtx_txt\\u200b может быть реализациями случайной величины, зависящей от yty_tyt\\u200b\\nAdversarial: мы играем против активного противника, который может на каждом раунде игры по своему усмотрению менять зависимость xt→ytx_t \\\\rightarrow y_txt\\u200b→yt\\u200b и/или подбирать l(pt,yt)l(p_t, y_t)l(pt\\u200b,yt\\u200b), имея на руках в том числе текущий ответ yty_tyt\\u200b, не доступный алгоритму онлайн-обучения\\n\\nAdversarial постановка включает в себя все остальные как частные случаи, так что сразу будем строить теорию для наиболее общего случая.\\nПоведение алгоритма на шаге T\\nНачнем с введения метрики качества алгоритма на некотором раунде игры TTT, а затем расширим ее на все раунды игры.\\nЕсли у противника нет никаких ограничений, то противник всегда выигрывает. Поскольку l(pt,yt)l(p_t, y_t)l(pt\\u200b,yt\\u200b) выбирается после нашего прогноза, он может выбрать любую функцию с сколь угодно большим штрафом.\\nЧтобы такого не случалось, мы предположим, что все ответы на шаге TTT должны быть сгенерированы некоторым отображением h∗:X→Y,h∗∈Hh^*: X\\\\rightarrow Y, h^* \\\\in Hh∗:X→Y,h∗∈H, где HHH ― пространство возможных решений, известное и онлайн-алгоритму, и противнику.\\nС учетом введенного ограничения на поведение противника, введем понятие regret:\\nRegretT(h)=∑t=1Tl(pt,yt)−∑t=1Tl(h(xt),yt)Regret_T(h) = \\\\sum\\\\limits_{t=1}^T l(p_t, y_t) - \\\\sum\\\\limits_{t=1}^Tl(h(x_t), y_t)\\nRegretT\\u200b(h)=t=1∑T\\u200bl(pt\\u200b,yt\\u200b)−t=1∑T\\u200bl(h(xt\\u200b),yt\\u200b)Regret ― это метрика того, насколько онлайн алгоритм работает хуже, чем некоторая фиксированная модель-бейзлайн h (regret переводится как «сожаление»: насколько мы пожалели о том, что взяли онлайн алгоритм, а не модель h). Поскольку мы работаем в adversarial случае, то логично сравнивать наш онлайн алгоритм с сильнейшим возможным противником, а именно: противник всегда выбирает не «некоторую», а наилучшую модель-бейзлайн h∗∈Hh^* \\\\in Hh∗∈H:\\nmaxRegret(T)=max\\u2061h∗∈H[∑t=1Tl(pt,yt)−∑t=1Tl(h∗(xt),yt)]maxRegret(T) = \\\\max\\\\limits_{h^* \\\\in H}\\\\left[ \\\\sum\\\\limits_{t=1}^T l(p_t, y_t) - \\\\sum\\\\limits_{t=1}^Tl(h^*(x_t), y_t)\\\\right]\\nmaxRegret(T)=h∗∈Hmax\\u200b[t=1∑T\\u200bl(pt\\u200b,yt\\u200b)−t=1∑T\\u200bl(h∗(xt\\u200b),yt\\u200b)]Поведение алгоритма на всей последовательности раундов игры\\nВспомним, что вообще-то мы играем игру с бесконечным числом раундов. В таком случае, естественно будет анализировать поведение ряда maxRegret(T),T∈N,T→∞maxRegret(T), T \\\\in \\\\mathbb{N}, T \\\\rightarrow \\\\inftymaxRegret(T),T∈N,T→∞. Здесь хочется еще раз подчеркнуть, в чем заключается adversarial поведение: на каждом шаге t maxRegret будет иметь свою наилучшую модель ht∗h^*_tht∗\\u200b в бейзлайне:\\nmaxRegret(T1)=max\\u2061h∗∈H∑t=1T1l(pt,yt)−∑t=1T1l(h∗(xt),yt)=∑t=1T1l(pt,yt)−∑t=1T1l(hT1∗(xt),yt)maxRegret(\\\\color{#348FEA}{T_1}) = \\\\max\\\\limits_{h^* \\\\in H} \\\\sum\\\\limits_{t=1}^{\\\\color{#348FEA}{T_1}} l(p_t, y_t) - \\\\sum\\\\limits_{t=1}^{\\\\color{#348FEA}{T_1}}l(h^*(x_t), y_t) = \\\\sum\\\\limits_{t=1}^{\\\\color{#348FEA}{T_1}} l(p_t, y_t) - \\\\sum\\\\limits_{t=1}^{\\\\color{#348FEA}{T_1}}l(h^*_{\\\\color{#348FEA}{T_1}}(x_t), y_t)\\nmaxRegret(T1\\u200b)=h∗∈Hmax\\u200bt=1∑T1\\u200b\\u200bl(pt\\u200b,yt\\u200b)−t=1∑T1\\u200b\\u200bl(h∗(xt\\u200b),yt\\u200b)=t=1∑T1\\u200b\\u200bl(pt\\u200b,yt\\u200b)−t=1∑T1\\u200b\\u200bl(hT1\\u200b∗\\u200b(xt\\u200b),yt\\u200b)maxRegret(T2)=max\\u2061h∗∈H∑t=1T2l(pt,yt)−∑t=1T2l(h∗(xt),yt)=∑t=1T2l(pt,yt)−∑t=1T2l(hT2∗(xt),yt)maxRegret(\\\\color{#E06A27}{T_2}) = \\\\max\\\\limits_{h^* \\\\in H} \\\\sum\\\\limits_{t=1}^{\\\\color{#E06A27}{T_2}} l(p_t, y_t) - \\\\sum\\\\limits_{t=1}^{\\\\color{#E06A27}{T_2}}l(h^*(x_t), y_t) = \\\\sum\\\\limits_{t=1}^{\\\\color{#E06A27}{T_2}} l(p_t, y_t) - \\\\sum\\\\limits_{t=1}^{\\\\color{#E06A27}{T_2}}l(h^*_{\\\\color{#E06A27}{T_2}}(x_t), y_t)\\nmaxRegret(T2\\u200b)=h∗∈Hmax\\u200bt=1∑T2\\u200b\\u200bl(pt\\u200b,yt\\u200b)−t=1∑T2\\u200b\\u200bl(h∗(xt\\u200b),yt\\u200b)=t=1∑T2\\u200b\\u200bl(pt\\u200b,yt\\u200b)−t=1∑T2\\u200b\\u200bl(hT2\\u200b∗\\u200b(xt\\u200b),yt\\u200b)Качество онлайн алгоритма на протяжении всей игры\\nКогда мы говорим про adversarial setting и игру с противником, мы хотим не просто как-то минимизировать кумулятивный Loss(T)=∑t=1Tl(pt,yt)Loss(T) = \\\\sum\\\\limits_{t=1}^Tl(p_t, y_t)Loss(T)=t=1∑T\\u200bl(pt\\u200b,yt\\u200b), но еще и хотим быть не хуже нашего противника. Потребуем, чтобы\\nlim\\u2061T→∞1TmaxRegret(T)=0\\\\lim\\\\limits_{T \\\\rightarrow \\\\infty} \\\\frac{1}{T}maxRegret(T) = 0\\nT→∞lim\\u200bT1\\u200bmaxRegret(T)=0Такое условие означает, что regret должен расти медленнее чем линейно (в таком случае говорят, что алгоритм имеет сублинейный regret).\\nСублинейности бывают разные. Так, RegretTRegret_TRegretT\\u200b может быть ограничен сверху рядом с асимптотикой T\\\\sqrt{T}T\\u200b или же рядом с асимптотикой log\\u2061T\\\\log{T}logT\\nАсимптотика log\\u2061T\\\\log{T}logT, очевидно, приводит к намного лучшей сходимости. Но достичь этого не всегда возможно. Стандартной асимптотикой regret в большинстве используемых на практике алгоритмов является T\\\\sqrt{T}T\\u200b, для этой асимптотики условия на задачу наименее жесткие. Все рассматриваемые нами ниже алгоритмы будут иметь асимптотику T\\\\sqrt{T}T\\u200b и отличаться в основном константами в оценках (но, конечно, отличия в константах при оценке Regret часто приводят к существенно разному поведению на практике). Любые более мощные асимптотики требуют условий, которые крайне редко выполняются в практических задачах\\nOnline to batch conversion\\nВ данном обзоре мы будем анализировать методы, которые гораздо чаще используются для оптимизации в классической постановке: есть фиксированный датасет (xi,yi)i=1N(x_i, y_i)_{i=1}^N(xi\\u200b,yi\\u200b)i=1N\\u200b, модель pw(x)p_w(x)pw\\u200b(x) с обучаемыми параметрами www и функция потерь fff, задача ― найти минимум функции\\n1N∑i=1Nf(pw(xi),yi)\\\\frac1N \\\\sum\\\\limits_{i=1}^N f(p_w(x_i), y_i)\\nN1\\u200bi=1∑N\\u200bf(pw\\u200b(xi\\u200b),yi\\u200b)Если представить, что все наши f(pw(xi),yi)f(p_w(x_i), y_i)f(pw\\u200b(xi\\u200b),yi\\u200b) ― независимые одинаково распределенные случайные величины, то можно считать, что на самом деле мы оптимизируем\\n1N∑i=1Nf(pw(xi),yi)≈E(x,y)f(pw(x),y)\\\\frac1N \\\\sum\\\\limits_{i=1}^N f(p_w(x_i), y_i) \\\\approx \\\\mathbb{E}_{(x,y)}f(p_w(x), y)\\nN1\\u200bi=1∑N\\u200bf(pw\\u200b(xi\\u200b),yi\\u200b)≈E(x,y)\\u200bf(pw\\u200b(x),y)Такую постановку задачи часто называть батчевой (англ. batch). Это означает, что мы можем использовать два класса методов оптимизации:\\n\\nметоды, которые на каждом шаге смотрят сразу на всю выборку (например, градиентный спуск или метод Ньютона);\\nметоды, которые на каждом шаге смотрят на случайное подмножество данных в надежде, что, итерируясь по таким подмножествам, мы сможем соптимизировать матожидание Ef(w)\\\\mathbb{E}f(w)Ef(w) (например, SGD). Такие методы называют стохастическими.\\n\\nСуществует специальный класс методов анализа сходимости, называемый online to batch conversion. Они позволяют адаптировать алгоритм онлайн-обучения к постановке задачи стохастической оптимизации на фиксированном датасете; при этом оценка на regret транслируется в асимптотику сходимости стохастической оптимизации. Математически строгий вывод этих методов обычно довольно громоздкий и не дарит более глубокого понимания идей в современных стохастических методах, это чисто технические выкладки, поэтому мы здесь ограничимся интуитивным описанием. Строгий вывод можно найти, например, в упомянутой выше книге Shai Shalev-Schwartz.\\nПроцесс стохастической оптимизации на фиксированном датасете можно представить в виде задачи онлайн обучения, если вытянуть все эпохи (проходы по датасетам) в единую последовательность. Мы получим задачу онлайн обучения, в которой (xt,yt)(x_t, y_t)(xt\\u200b,yt\\u200b) сэмплируются из фиксированного множества (x1,y1),…,(xN,yN)(x_1,y_1),\\\\ldots,(x_N,y_N)(x1\\u200b,y1\\u200b),…,(xN\\u200b,yN\\u200b). Строго говоря, тут сэмлпирование двухстадийное:\\n\\nБерем исходное множество функций\\nСэмплируем из него без возвращения, пока множество не станет пустым\\nКак только оно стало пустым ― заново заполняем его\\n\\nТаким образом, деление на \"эпохи\" отчетливо видно и в вытянутой последовательности.\\nЛегко видеть, что эта задача является корректной задачей онлайн обучения. Тут мы активно пользуемся тем, что постановка задачи онлайн обучения математически простая и очень общая. Из корректности данной задачи следует, что все алгоритмы онлайн обучения будут иметь на такой последовательности сублинейный regret.\\nСледующим шагом давайте взглянем на regret в момент смены эпохи. Обозначим за MMM―число эпох, тогда:\\nmaxRegret(T)=max\\u2061h∗∈H[∑t=1Tl(pt,yt)−∑t=1Tl(h∗(xt),yt)]=max\\u2061h∗∈H[∑m=1M∑i=1Nl(pm,i,yi)−∑m=1M∑i=1Nl(h∗(xi),yi)]=max\\u2061h∗∈H[∑m=1M∑i=1Nl(pm,i,yi)−M∑i=1Nl(h∗(xi),yi)]maxRegret(T) = \\\\max\\\\limits_{h^* \\\\in H}\\\\left[ \\\\sum\\\\limits_{t=1}^T l(p_t, y_t) - \\\\sum\\\\limits_{t=1}^Tl(h^*(x_t), y_t)\\\\right] = \\\\max\\\\limits_{h^* \\\\in H}\\\\left[ \\\\sum\\\\limits_{m=1}^M \\\\sum\\\\limits_{i=1}^N l(p_{m,i}, y_i) - \\\\sum\\\\limits_{m=1}^M \\\\sum\\\\limits_{i=1}^N l(h^*(x_i), y_i)\\\\right] = \\\\max\\\\limits_{h^* \\\\in H}\\\\left[ \\\\sum\\\\limits_{m=1}^M \\\\sum\\\\limits_{i=1}^N l(p_{m,i}, y_i) - M\\\\sum\\\\limits_{i=1}^N l(h^*(x_i), y_i) \\\\right]\\nmaxRegret(T)=h∗∈Hmax\\u200b[t=1∑T\\u200bl(pt\\u200b,yt\\u200b)−t=1∑T\\u200bl(h∗(xt\\u200b),yt\\u200b)]=h∗∈Hmax\\u200b[m=1∑M\\u200bi=1∑N\\u200bl(pm,i\\u200b,yi\\u200b)−m=1∑M\\u200bi=1∑N\\u200bl(h∗(xi\\u200b),yi\\u200b)]=h∗∈Hmax\\u200b[m=1∑M\\u200bi=1∑N\\u200bl(pm,i\\u200b,yi\\u200b)−Mi=1∑N\\u200bl(h∗(xi\\u200b),yi\\u200b)]Из сходимости последовательности следует сходимость любой ее подпоследовательности, а значит, последовательность regret\\'ов в моменты смены эпох тоже ведет себя сублинейно:\\n1MNmax\\u2061h∗∈H[∑m=1M∑i=1Nl(pm,i,yi)−M∑i=1Nl(h∗(xi),yi)]→0\\\\frac{1}{MN}\\\\max\\\\limits_{h^* \\\\in H}\\\\left[ \\\\sum\\\\limits_{m=1}^M \\\\sum\\\\limits_{i=1}^N l(p_{m,i}, y_i) - M\\\\sum\\\\limits_{i=1}^N l(h^*(x_i), y_i) \\\\right] \\\\rightarrow 0\\nMN1\\u200bh∗∈Hmax\\u200b[m=1∑M\\u200bi=1∑N\\u200bl(pm,i\\u200b,yi\\u200b)−Mi=1∑N\\u200bl(h∗(xi\\u200b),yi\\u200b)]→0max\\u2061h∗∈H[1MN∑m=1M∑i=1Nl(pm,i,yi)−1N∑i=1Nl(h∗(xi),yi)]→0\\\\max\\\\limits_{h^* \\\\in H}\\\\left[ \\\\frac{1}{MN}\\\\sum\\\\limits_{m=1}^M \\\\sum\\\\limits_{i=1}^N l(p_{m,i}, y_i) - \\\\frac1N\\\\sum\\\\limits_{i=1}^N l(h^*(x_i), y_i) \\\\right] \\\\rightarrow 0\\nh∗∈Hmax\\u200b[MN1\\u200bm=1∑M\\u200bi=1∑N\\u200bl(pm,i\\u200b,yi\\u200b)−N1\\u200bi=1∑N\\u200bl(h∗(xi\\u200b),yi\\u200b)]→01MN∑m=1M∑i=1Nl(pm,i,yi)−min\\u2061h∗∈H[1N∑i=1Nl(h∗(xi),yi)]→0\\\\frac{1}{MN}\\\\sum\\\\limits_{m=1}^M \\\\sum\\\\limits_{i=1}^N l(p_{m,i}, y_i) - \\\\min\\\\limits_{h^* \\\\in H}\\\\left[ \\\\frac1N\\\\sum\\\\limits_{i=1}^N l(h^*(x_i), y_i) \\\\right] \\\\rightarrow 0\\nMN1\\u200bm=1∑M\\u200bi=1∑N\\u200bl(pm,i\\u200b,yi\\u200b)−h∗∈Hmin\\u200b[N1\\u200bi=1∑N\\u200bl(h∗(xi\\u200b),yi\\u200b)]→0Последнее слагаемое уже выглядит практически как постановка задачи стохастической оптимизации на фиксированном датасете! Интуиция на данный момент подсказывает нам, что разрыв между решениями, даваемыми онлайн обучением, и точным решением задачи батч-оптимизации, будет постепенно сокращаться.\\nВ этот момент интуицию можно выключать―остаются только строгие технические выкладки по ссылкам выше.\\nВыпуклая онлайн-оптимизация\\nВыпуклая оптимизация играет центральную роль в анализе алгоритмов онлайн-обучения и позволяет получать эффективные алгоритмы. Вот примеры задач, в которых она хорошо работает:\\n\\nЛинейная оптимизация;\\nExpert Advice problem;\\nЛинейная/логистическая регрессия.\\n\\nДля задач, возникающих в глубинном обучении, мы поступим согласно рекомендациям ведущих ученых: возьмем теоретически обоснованный алгоритм выпуклой оптимизации, воткнем в нейросеть и помолимся, чтобы он сохранил свои хорошие свойства. С методами первого порядка, как правило, работает (а здесь мы будем рассматривать только такие методы)\\nВведём в нашу игру предположение о выпуклости, а заодно попробуем сделать вычисления менее громоздкими. Для этого определим упрощённую игру (2):\\n\\nВыбираем параметрическую модель wtw_twt\\u200b;\\nПолучаем извне выпуклую функцию потерь ft(w)f_t(w)ft\\u200b(w);\\nСчитаем ftf_tft\\u200b в точке wtw_twt\\u200b и получаем наш loss ft(wt)f_t(w_t)ft\\u200b(wt\\u200b).\\n\\nПервое упрощение состоит в том, что прогноз hth_tht\\u200b и бейзлайн ht∗h^*_tht∗\\u200b мы теперь берём не из абстрактного функционального множества HHH, а из некоторого параметризованного семейства. Говоря «модель wtw_twt\\u200b», мы имеем в виду «модель, заданная параметрами wtw_twt\\u200b». Скажем, для линейной регрессии это может быть вектор весов и bias. Regret будет записываться следующим образом:\\nmaxRegretT=∑t=1Tft(wt)−∑t=1Tft(wT∗)maxRegret_T = \\\\sum\\\\limits_{t=1}^T f_t(w_t) - \\\\sum\\\\limits_{t=1}^T f_t(w_T^*)\\nmaxRegretT\\u200b=t=1∑T\\u200bft\\u200b(wt\\u200b)−t=1∑T\\u200bft\\u200b(wT∗\\u200b)Второе упрощение в том, что мы не думаем о признаках xtx_txt\\u200b и таргетах yty_tyt\\u200b. Вся эта информация спрятана в определение функции ft(w)f_t(w)ft\\u200b(w). Например, для линейной регрессии ft(w)=(xtTw−yt)2f_t(w) = (x_t^Tw - y_t)^2ft\\u200b(w)=(xtT\\u200bw−yt\\u200b)2. При этом теперь у нас нет частичной информации о текущем раунде игры перед выбором новой модели wtw_twt\\u200b: ведь мы сначала выбираем wtw_twt\\u200b и лишь потом получаем ft(w)f_t(w)ft\\u200b(w).\\nОбратите внимание: если вы попробуете себе представить онлайн алгоритм на практике, то, как правило, частичная информация о функции ft(w)f_t(w)ft\\u200b(w) перед выбором wtw_twt\\u200b вам доступна. Например, рассмотрим рекомендательную систему с онлайн-дообучаемой ранжирующей моделью:\\n\\nПользователь пришел, мы сразу пошли в базу данных за его историей покупок и получили признаковое описание (возможно частичное) xtx_txt\\u200b;\\nС учётом этого признакового описания мы выбираем модель wtw_twt\\u200b и с её помощью оцениваем релевантность товаров этому пользователю;\\nСмотрим, что купил пользователь и купил ли, это даёт нам ft(wt)f_t(w_t)ft\\u200b(wt\\u200b).\\n\\nТем не менее, в этом параграфе мы будем считать, что частичной информации нет, потому что хотим разрабатывать наиболее общий фреймворк, а не ad-hoc алгоритмы, использующие конкретный вид этой частичной информации. Если даже для какой-то узкой проблемы можно сформулировать специфический алгоритм, учитывающий частичную информацию, с высокой вероятностью он не будет работать значимо лучше стандартного решения. Если знаете контрпримеры ― напишите, добавим сюда для полноты.\\nFollow the Leader\\nПредположим, что мы провели ttt шагов игры (2) и теперь выбираем модель wt+1w_{t+1}wt+1\\u200b (как условились, без информации о ft+1(w)f_{t+1}(w)ft+1\\u200b(w)). Наиболее естественным выбором будет алгоритм, минимизирующий ошибку на всех предыдущих раундах\\nwt+1=argmin\\u2061w∑s=1tfs(w)w_{t+1} = arg\\\\min\\\\limits_w \\\\sum\\\\limits_{s=1}^{t}f_s(w)\\nwt+1\\u200b=argwmin\\u200bs=1∑t\\u200bfs\\u200b(w)Такой алгоритм называется Follow The Leader (FTL), потому что мы идем вплотную за наилучшим возможным алгоритмом-бейзлайном в regret (лидером), который учитывает ещё и информацию с (t+1)(t+1)(t+1)-го шага:\\nw∗t+1=argmin\\u2061w∑s=1t+1fs(w)w*_{t+1} = arg\\\\min\\\\limits_w \\\\sum\\\\limits_{s=1}^{\\\\color{#E06A27}{t+1}}f_s(w)\\nw∗t+1\\u200b=argwmin\\u200bs=1∑t+1\\u200bfs\\u200b(w)К сожалению, для алгоритма в таком виде есть важные примеры выпуклых задач, когда он не работает. Допустим, наши функции потерь линейны ft(w)=gtTwf_t(w) = g_t^Twft\\u200b(w)=gtT\\u200bw. Вам может показаться, что линейная функция не особенно похожа на функцию потерь, но, забегая вперед, именно такие функции потерь встретятся дальше при изучении градиентных онлайн-алгоритмов (gt=∇ft(wt)g_t = \\\\nabla f_t(w_t)gt\\u200b=∇ft\\u200b(wt\\u200b)).\\nРассмотрим одномерную задачу ft(w)=gtwtf_t(w) = g_tw_tft\\u200b(w)=gt\\u200bwt\\u200b, gt∈Rg_t \\\\in \\\\mathbb{R}gt\\u200b∈R, wt∈[−1;1]w_t \\\\in[-1;1]wt\\u200b∈[−1;1]. Пусть\\ngt={−0.5t=11t%2=0−1t%2=1g_t = \\\\begin{cases}\\n      -0.5 & t=1 \\\\\\\\\\n      1 & t\\\\%2 = 0 \\\\\\\\\\n      -1 & t\\\\%2 = 1\\n\\\\end{cases}gt\\u200b=⎩⎨⎧\\u200b−0.51−1\\u200bt=1t%2=0t%2=1\\u200bАлгоритм FTL выглядит так:\\nwT+1=argmin\\u2061w∑t=1Tgtw=argmin\\u2061ww(∑t=1Tgt)w_{T+1} = arg\\\\min\\\\limits_w\\\\sum\\\\limits_{t=1}^T g_tw = arg\\\\min\\\\limits_w w\\\\Big(\\\\sum\\\\limits_{t=1}^T g_t\\\\Big)\\nwT+1\\u200b=argwmin\\u200bt=1∑T\\u200bgt\\u200bw=argwmin\\u200bw(t=1∑T\\u200bgt\\u200b)Такие осциллирующие суммы коэффициентов будут заставлять FTL выбирать наихудшее возможное решение в каждом раунде. Функция потерь в каждом раунде будет равна 0.50.50.5, а кумулятивная функция потерь примет вид ∑t=1T0.5=0.5T\\\\sum\\\\limits_{t=1}^T0.5 = 0.5Tt=1∑T\\u200b0.5=0.5T. При этом кумулятивная функция потерь константного решения w∗=0w^*=0w∗=0 будет равна 0. Получаем линейный regret 0.5T0.5T0.5T относительно бейзлайна wT∗=w∗=0w_T^* = w^* = 0wT∗\\u200b=w∗=0, алгоритм не сходится.\\nFollow The Regularized Leader\\nЧтобы стабилизировать алгоритм, мы добавим регуляризаторы, и назовем получившийся алгоритм Follow The Regularized Leader (FTRL):\\nwT=argmin\\u2061w[∑t=1Tft(w)+R(w)]w_T = arg\\\\min\\\\limits_w\\\\Big[ \\\\sum\\\\limits_{t=1}^T f_t(w) + R(w)\\\\Big]\\nwT\\u200b=argwmin\\u200b[t=1∑T\\u200bft\\u200b(w)+R(w)]Упражнение. Проверьте, что в примере из предыдущего параграфа добавление регуляризатора стабилизирует осцилляцию решения www.\\nДобавка R(w)R(w)R(w) должна быть выпуклой и неотрицательной. При этом различный выбор R(w)R(w)R(w) будет приводить к различным алгоритмам и различным оценкам на regret.\\nПервое, что приходит в голову ― это L2L_2L2\\u200b регуляризатор R(w)=∣∣w∣∣22R(w) = \\\\vert\\\\vert w\\\\vert\\\\vert_2^2R(w)=∣∣w∣∣22\\u200b. Он даёт алгоритм\\nwT=argmin\\u2061w[∑t=1Tft(w)+12λ∣∣w∣∣22]w_T = arg\\\\min\\\\limits_w\\\\Big[ \\\\sum\\\\limits_{t=1}^T f_t(w) +\\\\frac{1}{2\\\\lambda}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2\\\\Big]\\nwT\\u200b=argwmin\\u200b[t=1∑T\\u200bft\\u200b(w)+2λ1\\u200b∣∣w∣∣22\\u200b]Adaptive FTRL\\nСледующая идея―сделать регуляризатор зависящим от данных (то есть от ftf_tft\\u200b) и своим на каждом раунде T:\\nwT=argmin\\u2061w[∑t=1Tft(w)+RT(w)]w_T = arg\\\\min\\\\limits_w\\\\Big[ \\\\sum\\\\limits_{t=1}^T f_t(w) + R_T(w)\\\\Big]\\nwT\\u200b=argwmin\\u200b[t=1∑T\\u200bft\\u200b(w)+RT\\u200b(w)]Забегая вперед―все современные градиентные алгоритмы Adam, RMSProp, AdaGrad и т.д. попадают в это семейство data-dependent регуляризаторов и работают значительно эффективнее любых алгоритмов с константными регуляризаторами R(w)R(w)R(w).\\nОбратите внимание: регуляризаторы являются частью алгоритма FTRL, они не входят в формулу для regret, которая по-прежнему имеет вид\\nRegretT(w∗)=∑t=1Tft(wt)−∑t=1Tft(w∗)Regret_T(w^*) = \\\\sum\\\\limits_{t=1}^T f_t(w_t) - \\\\sum\\\\limits_{t=1}^T f_t(w^*)\\nRegretT\\u200b(w∗)=t=1∑T\\u200bft\\u200b(wt\\u200b)−t=1∑T\\u200bft\\u200b(w∗)Таким образом, мы не изменили постановку решаемой нами задачи, изменили лишь метод ее решения.\\nОбратите внимание: введение регуляризаторов влияет только на онлайн-алгоритм и выбор wtw_twt\\u200b. Бейзлайны wT∗w_T^*wT∗\\u200b выбираются как и раньше:\\nwT∗=argmin\\u2061w∑t=1Tft(w)w_T^* = arg\\\\min\\\\limits_w \\\\sum\\\\limits_{t=1}^Tf_t(w)\\nwT∗\\u200b=argwmin\\u200bt=1∑T\\u200bft\\u200b(w)Линеаризация и вычислительно эффективный FTRL\\nРассмотрим пример с логистической регрессией ft(w)=log\\u2061(1+e−ytwtTxt)f_t(w) = \\\\log(1 + e^{-y_tw_t^Tx_t})ft\\u200b(w)=log(1+e−yt\\u200bwtT\\u200bxt\\u200b) и  константным L2L_2L2\\u200b регуляризатором:\\n∑t=1Tlog\\u2061(1+e−ytwtTxt)+12η∣∣w∣∣22⟶min\\u2061w\\\\sum\\\\limits_{t=1}^T \\\\log(1 + e^{-y_tw_t^Tx_t}) + \\\\frac{1}{2\\\\eta}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2\\\\longrightarrow\\\\min\\\\limits_w\\nt=1∑T\\u200blog(1+e−yt\\u200bwtT\\u200bxt\\u200b)+2η1\\u200b∣∣w∣∣22\\u200b⟶wmin\\u200bКлассический пример использования онлайн логистической регрессии ― предсказание CTR в рекламе. Миллионы запросов в секунду => миллионы решений этой оптимизационной задачи в секунду (если разбивать на батчи ― тысячи, но сути это не меняет). Успех онлайн-алгоритма в таких задачах определяется его вычислительной эффективностью, как по памяти, так и по скорости. Увы, с этим у нашего алгоритма не всё так хорошо:\\n\\nСкорость: аналитически задача не решается => FAIL\\nПамять: нужно хранить все предыдущие запросы xtx_txt\\u200b, t∈1..Tt\\\\in{1..T}t∈1..T => FAIL\\n\\nЗдесь нам на помощь приходит линеаризация задачи. Если фунции ft(w)f_t(w)ft\\u200b(w) выпуклые (вниз) и гладкие (на негладкие посмотрим позже), то они удовлетворяют основному свойству выпуклых функций\\nf(w)≥f(wt)+[∇f(wt)]T(w−wt)f(w) \\\\geq f(w_t) + [\\\\nabla f(w_t)]^T(w - w_t)\\nf(w)≥f(wt\\u200b)+[∇f(wt\\u200b)]T(w−wt\\u200b)Разложим все функции ft(w)f_t(w)ft\\u200b(w) в точках wtw_twt\\u200b:\\nft(w)≥ft(wt)+[∇f(wt)]T(w−wt)f_t(w) \\\\geq f_t(w_t) + [\\\\nabla f(w_t)]^T(w - w_t)\\nft\\u200b(w)≥ft\\u200b(wt\\u200b)+[∇f(wt\\u200b)]T(w−wt\\u200b)ft(wt)−ft(w)≤[∇f(wt)]T(w−wt)f_t(w_t) - f_t(w) \\\\leq [\\\\nabla f(w_t)]^T(w - w_t)\\nft\\u200b(wt\\u200b)−ft\\u200b(w)≤[∇f(wt\\u200b)]T(w−wt\\u200b)Просуммируем от 1 до TTT\\n∑t=1T(ft(wt)−ft(w))≤∑t=1T([∇ft(wt)]Twt−[∇ft(wt)]TwT)\\\\sum\\\\limits_{t=1}^T \\\\Big(f_t(w_t) - f_t(w)\\\\Big) \\\\leq \\\\sum\\\\limits_{t=1}^T \\\\Big([\\\\nabla f_t(w_t)]^Tw_t - [\\\\nabla f_t(w_t)]^Tw^T\\\\Big)\\nt=1∑T\\u200b(ft\\u200b(wt\\u200b)−ft\\u200b(w))≤t=1∑T\\u200b([∇ft\\u200b(wt\\u200b)]Twt\\u200b−[∇ft\\u200b(wt\\u200b)]TwT)Теперь обозначим gt=∇ft(wt)g_t = \\\\nabla f_t(w_t)gt\\u200b=∇ft\\u200b(wt\\u200b) и рассмотрим выпуклую линейную задачу онлайн обучения с функцией потерь f~t(w)=gtTw\\\\widetilde{f}_t(w) = g_t^Twf\\u200bt\\u200b(w)=gtT\\u200bw. Regret для нее выглядит как\\nLinearizedRegretT(w∗)=∑t=1TgtTwt−∑t=1TgtTw∗LinearizedRegret_T(w^*) = \\\\sum\\\\limits_{t=1}^T g_t^Tw_t - \\\\sum\\\\limits_{t=1}^T g_t^Tw^*\\nLinearizedRegretT\\u200b(w∗)=t=1∑T\\u200bgtT\\u200bwt\\u200b−t=1∑T\\u200bgtT\\u200bw∗Неравенство выше позволяет нам оценить regret исходной задачи через regret линеаризованной:\\nRegretT(w∗)≤LinearizedRegretT(w∗)Regret_T(w^*) \\\\leq LinearizedRegret_T(w^*)\\nRegretT\\u200b(w∗)≤LinearizedRegretT\\u200b(w∗)Минимизируя правую часть неравенства, мы, безусловно, будем минимизировать и левую, так что мы можем выбирать wtw_twt\\u200b алгоритмом, решающим линеаризованную задачу, и получать хорошо сходящийся метод для исходной задачи.\\nПосмотрим, будет ли линеаризованный алгоритм вычислительно эффективнее. Посмотрим на линеаризацию задачи с data-depedent регуляризатором:\\nwT=argmin\\u2061w[∑t=1T∇[ft(wt)]Tw+RT(w)]w_T = arg\\\\min\\\\limits_w\\\\Big[ \\\\sum\\\\limits_{t=1}^T \\\\nabla [f_t(w_t)]^Tw + R_T(w)\\\\Big]\\nwT\\u200b=argwmin\\u200b[t=1∑T\\u200b∇[ft\\u200b(wt\\u200b)]Tw+RT\\u200b(w)]Линейные задачи имеют аналитическое решение для широкого спектра RT(w)R_T(w)RT\\u200b(w). Собственно, это и есть основное, что нужно помнить на практике ― выбирать регуляризатор так, чтобы эта задача решалась аналитически. Мы рассмотрим простейший случай RT(w)=R(w)=12η∣∣w∣∣22R_T(w) = R(w) = \\\\frac{1}{2\\\\eta}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2RT\\u200b(w)=R(w)=2η1\\u200b∣∣w∣∣22\\u200b:\\nwT=argmin\\u2061w[∑t=1T∇ft(wt)Tw+12η∣∣w∣∣22]w_T = arg\\\\min\\\\limits_w\\\\Big[ \\\\sum\\\\limits_{t=1}^T \\\\nabla f_t(w_t)^Tw + \\\\frac{1}{2\\\\eta}\\\\vert\\\\vert w\\\\vert\\\\vert_2^2\\\\Big]\\nwT\\u200b=argwmin\\u200b[t=1∑T\\u200b∇ft\\u200b(wt\\u200b)Tw+2η1\\u200b∣∣w∣∣22\\u200b]Справа дифференцируемая функция, так что мы можем найти wTw_TwT\\u200b, приравняв к нулю градиент:\\nwT=−η∑t=1T∇ft(wt)=−ηzT,w_T = -\\\\eta\\\\sum\\\\limits_{t=1}^T \\\\nabla f_t(w_t) = -\\\\eta z_T,\\nwT\\u200b=−ηt=1∑T\\u200b∇ft\\u200b(wt\\u200b)=−ηzT\\u200b,где zT=∑t=1T∇ft(wt)z_T = \\\\sum\\\\limits_{t=1}^T \\\\nabla f_t(w_t)zT\\u200b=t=1∑T\\u200b∇ft\\u200b(wt\\u200b) ― это сумма векторов, которую не нужно пересчитывать заново на каждом шаге, а можно инкрементально обновлять. Благодаря этому нам больше не нужно помнить все предыдущие объекты выборки, достаточно хранить лишь некоторую статистику.\\nГотово, мы построили наш первый вычислительно эффективный алгоритм онлайн обучения! В дальнейшем мы займемся тем, чтобы найти наилучший вычислительно эффективный алгоритм.\\nОбратите внимание: теперь вы понимете, почему пример с линейной функцией потерь был так важен: линейные функции соответствуют линеаризованному regret. При этом, как мы уже выяснили, без регуляризатора такие линеаризованные задачи нестабильны.\\nОбратите внимание: если переписать немного формулу для wTw_TwT\\u200b, мы получим:\\nwT=−η∑t=1T∇ft(wt)=wT−1−η∇ft(wt)w_T = -\\\\eta\\\\sum\\\\limits_{t=1}^T \\\\nabla f_t(w_t) = w_{T-1} - \\\\eta \\\\nabla f_t(w_t)\\nwT\\u200b=−ηt=1∑T\\u200b∇ft\\u200b(wt\\u200b)=wT−1\\u200b−η∇ft\\u200b(wt\\u200b)Таким образом, формулы FTRL c константным регуляризатором эквивалентны формулам обычного стохастического градиентного спуска. Забегая вперед, скажем, что различия в формулах градиентного спуска и FTRL будут только в разделе Composite objective FTRL. В этих отличиях и будет заключаться преимущество FTRL перед привычным SGD.\\nОбратите внимание: концепции FTRL и gradient descent в литературе часто называют lazy (ленивая) и greedy (жадная) соответственно.\\nGradient descent жадный, потому что алгоритм для обновления wt+1w_{t+1}wt+1\\u200b использует только текущий wtw_twt\\u200b и текущий градиент gtg_tgt\\u200b. Всё, что было на предыдущих шагах, алгоритм забывает.\\nFTRL ленивый, потому что алгоритм в явном виде сохраняет всю информацию с начала обучения и рассчитывает wt+1w_{t+1}wt+1\\u200b, исходя из всей истории g1,…,gtg_1,\\\\ldots,g_tg1\\u200b,…,gt\\u200b, и только после этого применяет все регуляризаторы. Подробнее мы расскажем об этом в разделе «Сравнение Composite Objective FTRL-Proximal и Adaptive Gradient Descent».\\nСубдифференциал и субградиентные методы\\nВыше мы рассматривали гладкие функции ft(w)f_t(w)ft\\u200b(w). Гладкость ― сильное ограничение, и оно на самом деле необязательно, можно ослабить условие, если использовать субградиенты.\\nКогда мы переходили от исходной задачи к линеаризованной, мы использовали основное свойство гладких выпуклых функций\\nf(w)≥f(wt)+[∇f(wt)]T(w−wt),∀wtf(w) \\\\geq f(w_t) + [\\\\nabla f(w_t)]^T(w - w_t), \\\\quad \\\\forall w_t\\nf(w)≥f(wt\\u200b)+[∇f(wt\\u200b)]T(w−wt\\u200b),∀wt\\u200bГладкость обеспечивает существование ∇f(wt)\\\\nabla f(w_t)∇f(wt\\u200b) для всех wtw_twt\\u200b. Но нам ведь не нужно, чтобы существовал именно градиент функции. Нам достаточно, чтобы существовал какой-то вектор gtg_tgt\\u200b, для которого выполнено неравенство\\nf(w)≥f(wt)+gT(w−wt)f(w) \\\\geq f(w_t) + g^T(w - w_t)\\nf(w)≥f(wt\\u200b)+gT(w−wt\\u200b)И в этом помогают следующие два понятия.\\nСубдифференциалом функции f(w)f(w)f(w) в точке wtw_twt\\u200b называется множество\\n∂wtf(w)={gt∣f(w)≥f(wt)+fT(w−wt),∀w}\\\\partial_{w_t} f(w) = \\\\left\\\\{ g_t \\\\mid f(w) \\\\geq f(w_t) + f^T(w - w_t),\\\\forall w\\\\right\\\\}\\n∂wt\\u200b\\u200bf(w)={gt\\u200b∣f(w)≥f(wt\\u200b)+fT(w−wt\\u200b),∀w}Субградиентом функции f(w)f(w)f(w) в точке wtw_twt\\u200b называется любой элемент множества ∂f(wt)\\\\partial f(w_t)∂f(wt\\u200b).\\nПотребуем, чтобы для любой точки был непустой субдифференциал, и дело в шляпе, можно вместо ∇ft(w)\\\\nabla f_t(w)∇ft\\u200b(w) везде подставлять субградиент gtg_tgt\\u200b и обобщить все выкладки выше на негладкий случай.\\nПримеры. Для гладких функций субдифференциал состоит из одной точки: градиента функции, а субградиент равен градиенту. В качестве примера функции с нетривиальным субградиентом рассмотрим функцию f(x)=∣x∣f(x) = \\\\vert x \\\\vertf(x)=∣x∣, где xxx ― скаляр. Субградиент в точке 000 ― это можество\\n∂0∣x∣={λ∣∣x∣≥αx}\\\\partial_0|x| = \\\\left\\\\{ \\\\lambda\\\\mid |x| \\\\geq \\\\alpha x \\\\right\\\\}\\n∂0\\u200b∣x∣={λ∣∣x∣≥αx}Легко видеть, что ∂0∣x∣\\\\partial_0\\\\vert x\\\\vert∂0\\u200b∣x∣ ― это отрезок [−1,1][-1, 1][−1,1].\\nЗамечание. На практике субдифференциал используют не так часто. Оптимизационные задачи с популярными негладкими регуляризаторами L1L_1L1\\u200b решают «в лоб», без перехода к субградиентной оценке, например, с помощью проксимальных методов.\\nОбратите внимание. В литературе очень часто используется термин Online Mirror Descent. Mirror descent ― это оптимизационная процедура вида\\nwt+1=argmin\\u2061w[12gtTw+λψ(w)+∣∣w−wt∣∣22],w_{t+1} = arg\\\\min\\\\limits_w \\\\left[\\\\vphantom{\\\\frac12}g_t^Tw + \\\\lambda \\\\psi(w) + \\\\vert\\\\vert w-w_t\\\\vert\\\\vert_2^2\\\\right],\\nwt+1\\u200b=argwmin\\u200b[21\\u200bgtT\\u200bw+λψ(w)+∣∣w−wt\\u200b∣∣22\\u200b],в которой ψ\\\\psiψ ― дополнительный негладкий регуляризатор (например, тот же L1L_1L1\\u200b), который мы как раз таки не заменяем на субградиентную оценку, а вместо этого оптимизируем всё «в лоб». Заметьте, что эти формулы идентичны формулам Proximal Gradient Descent. Если у нас нет регуляризатора ψ\\\\psiψ, то формулы эквивалентны обычному gradient descent.\\nКак вы увидите дальше, Mirror Descent ― это частный случай общего фреймворка, который мы описываем.\\nСубградиентные методы оптимизации.\\nПочти все градиентные методы оптимизации обобщаются на негладкие функции. Модифицируется необходимое и достаточное условие минимума для выпуклых функций: точка w∗w^*w∗ является минимумом, если субдифференциал содержит ноль: 0∈∂f(w∗)0 \\\\in \\\\partial f(w^*)0∈∂f(w∗). Очевидно, это прямое обобщение условия для гладких функций, где субдифференциал состоит только из градиента функции.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф14.4. Сходимость SGDПочему он\\xa0всё-таки сходитсяСледующий параграф15.2. Адаптивный FTRLЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_4.html', 'title': 'Машинное обучение'}, page_content='Машинное обучениеЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/31.1.Об этой книге1.2.Первые шаги1.3.Машинное обучениеКритерии качестваДанныеМодель и алгоритм обучения2.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Машинное обучение1.3. Машинное обучениеАвторыФедотов СтаниславСиницин ФилиппЧто такое машинное обучение и каким оно бывает. Основные понятия машинного обучения: признаки, таргеты, метрики, переобучениеМашинное обучение — это наука, изучающая алгоритмы, автоматически улучшающиеся благодаря опыту.\\nКогда Алан Тьюринг работал над первыми (компьютерами), он пытался расшифровать сообщения немецких военных, закодированные машиной Энигма. Поиск расшифровки требовал перебора массы вариантов. Люди с этой задачей справлялись плохо, зато машина могла решить её сравнительно быстро. Очевидно, далеко не для каждой задачи, с которой люди справляются с трудом, можно написать программу для эффективного поиска решения. Более того, есть целый класс задач (так называемые NP-трудные задачи), которые нельзя решить за разумное время. Можно даже явно доказать, что никакой компьютер здесь чуда тоже не совершит. Самое интересное это то, что бывают и задачи, которые для людей особенного труда не составляют, но которые почему-то крайне трудно запрограммировать, например:\\n\\n\\nперевести текст с одного языка на другой;\\n\\n\\nдиагностировать болезнь по симптомам;\\n\\n\\nсравнить, какой из двух документов в интернете лучше подходит под данный поисковый запрос;\\n\\n\\nсказать, что изображено на картинке;\\n\\n\\nоценить, по какой цене удастся продать квартиру.\\n\\n\\nУ всех этих задач есть много общего. Во-первых, их решение можно записать как функцию, которая отображает объекты или примеры (samples) в предсказания (targets). Например, больных надо отобразить в диагнозы, а документы в оценку релевантности. Во-вторых, вряд ли у этих задач есть единственно верное, идеальное решение. Даже профессиональные переводчики могут по-разному перевести один и тот же текст, и оба перевода будут верными. Так что лучшее в этих задачах — враг хорошего. В конце концов, и доктора иногда делают ошибки в диагнозах, и вы не всегда можете сказать, что же именно изображено на картинке. В-третьих, у нас есть много примеров правильных ответов (скажем, переводов предложения на другой язык или подписей к заданной картинке), а примеры неправильных ответов (если они нужны), как правило, не составляет труда сконструировать. Мы назовём функцию, отображающую объекты в предсказания, — моделью, а имеющийся у нас набор примеров — обучающей выборкой или датасетом. Обучающая выборка состоит из:\\n\\n\\nобъектов (к примеру, скачанные из интернета картинки, истории больных, активность пользователей сервиса и так далее);\\n\\n\\nи ответов (подписи к картинкам, диагнозы, информация об уходе пользователей с сервиса), которые мы также будем иногда называть таргетами.\\n\\n\\nПостановка задачи\\nОписанные выше задачи являются примерами задач обучения с учителем (supervised learning), так как правильные ответы для каждого объекта обучающей выборки заранее известны. Задачи обучения с учителем делятся на следующие виды в зависимости от того, каким может быть множество Y\\\\mathbb{Y}Y всех возможных ответов (таргетов):\\n\\nY=R\\\\mathbb{Y} = \\\\mathbb{R}Y=R или Y=RM\\\\mathbb{Y} = \\\\mathbb{R}^MY=RM — регрессия. Примерами задач регрессии является предсказание продолжительности поездки на каршеринге, спрос на конкретный товар в конкретный день или погода в вашем городе на завтра (температура, влажность и давление — это несколько вещественных чисел, которые формируют вектор нашего предсказания).\\nY=0,1\\\\mathbb{Y} = {0, 1}Y=0,1 — бинарная классификация. Например, мы можем предсказывать, кликнет ли пользователь по рекламному объявлению, вернёт ли клиент кредит в установленный срок, сдаст ли студент сессию, случится ли определённое заболевание у пациента, есть ли на картинке банан.\\nY=1,…,K\\\\mathbb{Y} = {1, \\\\dots, K}Y=1,…,K — многоклассовая (multiclass) классификация. Например, определение предметной области для научной статьи (математика, биология, психология и т. д.).\\nY=0,1K\\\\mathbb{Y} = {0, 1}^KY=0,1K — многоклассовая классификация с пересекающимися классами (multilabel classification). Например, задача автоматического проставления тегов для ресторанов (логично, что ресторан может одновременно иметь несколько тегов).\\nY\\\\mathbb{Y}Y — конечное упорядоченное множество — ранжирование. Основным примером является задача ранжирования поисковой выдачи, где для любого запроса нужно отсортировать все возможные документы по релевантности этому запросу; при этом оценка релевантности имеет смысл только в контексте сравнения двух документов между собой, её абсолютное значение информации не несёт.\\n\\nОтвет может быть и более сложным. Так, в задаче сегментации изображения требуется для каждого пикселя предсказать, к какому объекту или типу объектов он относится, а в задаче машинного перевода мы должны сгенерировать предложение (или целый текст), являющееся переводом исходного. Интерес представляют и задачи порождения новых объектов, то есть генерации правдоподобных объектов, из ничего или на основе уже существующих. С помощью такой модели также можно научиться увеличивать разрешение изображения и применять любимые всеми маски в Snapchat или Instagram.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nЕсть и относительно небольшой класс задач, относящихся к обучению без учителя (unsupervised learning), — это задачи, для которых нам известны только данные, а ответы неизвестны или вообще не существуют. Более того, часто поиск \"правильных\" ответов не является самоцелью. Классическим примером обучения без учителя является кластеризация — задача разделения объектов на группы, обладающие некоторыми неизвестными нам, но, как мы в глубине души надеемся, интерпретируемыми свойствами. Примером может служить кластеризация документов из электронной библиотеки по темам или кластеризация новостей с целью выделения крупных сюжетов.\\nБывают и другие виды (и даже парадигмы) машинного обучения, так что если вы встретите задачу, которую никак не получается отнести к одному из перечисленных выше типов, не расстраивайтесь и знайте, что где-то дальше в учебнике вас ждёт рассказ про такие задачи.\\nВопрос на подумать. Определите тип следующих задач. По возможности попробуйте отнести их к более узким видам задач.\\n\\n\\nПредсказание курса евро к доллару на следующий день.\\n\\n\\nСтилизация текста. Например, перевод на бюрократический язык: «Пиппина и Мерри похитили!» ↦\\\\mapsto↦ «Граждане Тук, Перегрин Паладинович, 2990 года рождения, и Брендибак, Мериадок Сарадокович, 2982 года рождения, были похищены неустановленными лицами».\\n\\n\\nДетектирование котиков на изображении.\\n\\n\\nОбучение робокота запрыгивать на стол из произвольной позы.\\n\\n\\nПоиск наборов товаров, которые посетители супермаркета часто покупают вместе.\\n\\n\\nОтвет (не открывайте сразу; сначала подумайте сами!)\\n\\nЭто задача регрессии. Модель предсказывает вещественное число, пусть и с небольшим количеством знаков после запятой.\\n\\n\\nЭто задача генерации новых объектов на основе уже существующих.\\n\\n\\nВ зависимости от того, для чего мы детектируем котиков, это может быть задача регрессии (предсказание координат вершин прямоугольника, в котором находится котик) или классификации (если нас просто интересует, есть котик или нет).\\n\\n\\nЭту задачу можно решать по-разному. Например, создав физическую модель движения робокота и рассчитав оптимальную последовательность движений. Если мы всё-таки хотим решать её с помощью машинного обучения, то можно поступить следующим образом. Создадим компьютерную симуляцию (чтобы не ломать настоящего робота) и модель, которая будет в каждый момент на основе конфигурации сочленений, высоты от пола, расстояния до стола, фазы Луны и других важных параметров предсказывать, как нужно дальше поворачивать лапы, изгибать спину кота и так далее. Эту модель будем прогонять в симуляции, так или иначе меняя её в зависимости от того, насколько удачно робот справляется со своей задачей. Такая парадигма называется обучением с подкреплением (reinforcement learning), и о ней мы поговорим в отдельном параграфе.\\n\\n\\nВы можете спросить: а почему это не обучение с учителем? Ведь у нас есть объекты — последовательности движений и ответы — запрыгнул кот на стол или нет. Проблема в том, что перебрать кучу траекторий (ввиду сложности задачи — действительно огромную кучу) и для каждой получить ответ — это очень долго и сложно; кроме того, нам хотелось бы иметь фреймворк, в котором можно было бы относительно легко адаптироваться, скажем, к изменению высоты стола.\\n\\nЭто задача обучения без учителя.\\n\\nВопрос на подумать. Ранжирование — это задача с таргетом из конечного упорядоченного множества (1,…,K)(1,\\\\ldots,K)(1,…,K). Казалось бы, её запросто можно было бы рассматривать как задачу классификации на KKK классов или задачу регрессии. В чём же проблема? Почему так не делают?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Для решения задач ранжирования обычно строят модель, предсказывающую некоторое вещественное число, по которому затем сортируют объекты, — так почему бы не считать её регрессией? Дело в том, что функции потерь и метрики в этой задаче совсем другие. Нам неважно, какие именно вещественные числа мы предсказываем. Мы просто хотим, чтобы более релевантным объектам сопоставлялись числа побольше.\\nЗадача «предскажите 10 самых релевантных объектов» не похожа на задачу классификации. Мир меняется, появляются новые объекты, и если к нам в руки попадёт объект более релевантный, чем текущий топ-1, все номера позиций поедут, и выученное нами соответствие объектов и номеров можно будет выкидывать на помойку.\\nКритерии качества\\nПо обучающей выборке мы хотим построить модель, предсказания которой достаточно хороши. Что вообще значит «достаточно хороши»? Не понимая, чего мы хотим добиться, мы не предложим хорошего решения, поэтому нужно внимательно отнестись к выбору метрик качества.\\nВозможно, вы уже участвовали в соревнованиях по анализу данных. На таких соревнованиях метрику организатор выбирает за вас, и она, как правило, непосредственным образом связана с результатами предсказаний. Но на практике всё бывает намного сложнее.\\nНапример, мы хотим:\\n\\nрешить, сколько коробок с бананами нужно завтра привезти в конкретный магазин, чтобы минимизировать количество товара, который не будет выкуплен, и минимизировать вероятность того, что покупатель к концу дня не найдёт желаемый продукт на полке;\\nувеличить счастье пользователей от работы с нашим сервисом, чтобы пользователи стали лояльнее, а сервис мог получать стабильный прогнозируемый доход;\\nрешить, нужно ли направить пациента на дополнительное медицинское обследование.\\n\\nВ каждом конкретном случае может возникать целая иерархия метрик.\\n\\n\\nСамый верхний уровень – это бизнес-метрики, например, будущий доход сервиса. Их трудно измерить в моменте, они сложным образом зависят от совокупности всех наших усилий, не только связанных с машинным обучением.\\n\\n\\nОнлайн (online) метрики – это характеристики работающей системы, с помощью которых мы надеемся оценить, что будет с бизнес-метриками. Например, это может быть:\\n– Медианная длина сессии в онлайн-игре. Можно предположить, что пользователь, который долго сидит в игре – это довольный пользователь.\\n– Среднее количество бананов на полках во всех магазинах торговой сети в конце дня.\\n\\n\\nНе всегда плоды наших трудов оцениваются числами. Многое может зависеть от субъективного восприятия людей, и для того, чтобы оценить их реакцию до выпуска в продакшен, применяется оценка специально нанятыми людьми – асессорами. Например, так можно оценивать, получилось ли у нас улучшить качество машинного перевода или релевантность выдачи в поисковой системе.\\n\\n\\nОфлайн (offline) метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным. В задачах, в которых нужно предсказывать какой-то конкретный таргет, офлайн метрики обычно оценивают отклонение предсказаний модели от истинных значений таргета. Например, это может быть точность предсказания, то есть число верно угаданных значений, или среднеквадратичное отклонение.\\n\\n\\nАсессорскую оценку тоже можно считать офлайн-метрикой\\nВ этой книге речь в основном пойдёт об офлайновых метриках и о функциях потерь. И прежде, чем вы начнёте знакомиться с методами решения задач обучения с учителем, полезно посмотреть, какими бывают метрики качества. Вот несколько примеров:\\n\\n\\nдля задачи постановки диагноза хорошими метриками могут быть, например, доля правильно поставленных диагнозов или доля больных, которым удалось поставить правильный диагноз (а вы поняли разницу?);\\n\\n\\nдля задачи предсказания цены квартиры метрикой качества может быть доля квартир, для которых разница между предсказанным и истинным значением цены не превысила какого-то порога, или средний модуль разницы между предсказанным и истинным значением;\\n\\n\\nдля задачи ранжирования поисковых документов по запросу — доля пар документов, которые мы упорядочили неправильно.\\n\\n\\nЦель обычно в том, чтобы найти модель, для которой значение метрики будет оптимальным.\\nВопрос на подумать. Важно помнить, что разные нужды заказчика могут диктовать самые разные метрики. Вернёмся к задаче постановки диагноза пациентам больницы. Какие метрики вы предложили бы использовать в каждом из следующих случаев:\\n\\n\\nобычный год в обычном терапевтическом отделении обычной больницы;\\n\\n\\nопределение очень неприятной болезни, которая жутким клеймом падёт на каждого, кому поставили такой диагноз;\\n\\n\\nопределение опасной и очень заразной болезни.\\n\\n\\nОтвет (не открывайте сразу; сначала подумайте сами!)Конечно, даже в каждом из этих довольно частных случаев могут быть разные ситуации и разные метрики, но вот как, например, можно было бы ответить:\\n\\n\\nОбычный год в обычном терапевтическом отделении обычной больницы — тогда главного врача вполне устроит, если доля правильно поставленных диагнозов будет высокой (эта метрика называется accuracy).\\n\\n\\nОпределение очень неприятной болезни, которая жутким клеймом падёт на каждого, кому поставили такой диагноз, — тогда нам важно максимизировать долю действительно больных среди тех, кому мы имели несчастье поставить этот диагноз (эта метрика называется точностью, или precision).\\n\\n\\nОпределение опасной и очень заразной болезни — тогда нам важно не пропустить ни одного заражённого, и метрика будет иметь вид доли правильно определённых носителей (эта метрика называется полнотой, или recall).\\n\\n\\nРазумеется, это самые простые метрики, и в реальной жизни вам придётся работать с более сложной иерархией метрик; немного подробнее мы поговорим об этом в параграфе про измерение качества моделей.\\nВопрос на подумать. Рассмотрим задачу детектирования людей на изображении. Чаще всего под детектированием понимают указание местоположения человека на картинке. Например, модель пытается выделить прямоугольник, в котором, по её мнению, есть человеческая фигура. Подумайте, какие метрики можно было бы использовать в различных ситуациях для измерения качества решения этой задачи. Не забудьте, что метрики — это способ численно измерить то, насколько модель помогает нам в жизни, так что важно думать о том, зачем нам вообще детектировать людей.\\nОтвет (не открывайте сразу; сначала подумайте сами!)Вот несколько вариантов, которые можно было бы придумать:\\n\\n\\nМы разрабатываем программу для проведения видеоконференций и хотим добавить эффект, который облачает участника в рыцарские доспехи, — в этом случае нам важно корректно определять местоположение и в качестве метрики мы могли бы брать среднеквадратичное отклонение координат каких-нибудь опорных точек тела от истинных.\\n\\n\\nМы строим систему безопасности вокруг какого-то важного объекта, и нам важно обнаруживать вторжение — в этом случае нам не очень принципиально, насколько точно отмечено местоположение человека в кадре, но все люди должны быть обнаружены. Таким образом, в качестве метрики можно рассмотреть полноту: на какой доле кадров, где действительно были люди, наша модель отметила их наличие.\\n\\n\\nМы строим систему, определяющую, не превышает ли количество людей в помещении некоторый порог (например, в рамках борьбы с пандемией), — в этом случае метрикой может быть, скажем, среднеквадратичное отклонение числа детектированных моделью людей от истинного их количества.\\n\\n\\nКритерии качества не всегда сводятся к метрикам. Бизнес или общество могут накладывать и другие требования, например:\\n\\n\\nМодель может выдавать предсказания в режиме реального времени. Заметим, что это требование не только к модели, но и к её реализации, а также к тому железу или к тем серверам, на которых она работает.\\n\\n\\nМодель достаточно компактна, чтобы помещаться на мобильном телефоне или другом устройстве.\\n\\n\\nМожно объяснить, на основании чего модель сделала то или иное предсказание для конкретного объекта. Это может быть важным в случае, если модель решает что-то важное в жизни человека, например, дадут ли кредит или будет ли согласовано дорогостоящее лечение. Такое требование является частным случаем более общего понятия интерпретируемости модели.\\n\\n\\nПредсказания модели не дискриминируют какую-либо категорию пользователей. Например, если двум людям с одинаковой и достаточно длинной историей просмотров онлайн-кинотеатр рекомендует разные фильмы только из-за того, что у них разный пол, то это не здорово.\\n\\n\\nДанные\\nМашинное обучение начинается с данных. Важно, чтобы их было достаточно много и чтобы они были достаточно качественными. Некоторые проекты приходится откладывать на неопределённый срок из-за того, что просто невозможно собрать данные.\\nЧем сложнее задача, тем больше данных нужно, чтобы её решить. Например, существенные успехи в задачах распознавания изображений были достигнуты лишь с появлением очень больших датасетов (и, стоит добавить, вычислительных мощностей). Вычислительные ресурсы продолжают совершенствовать, но во многих ситуациях размеченных данных (то есть объектов, которым кто-то сопоставил ответ) было бы по-прежнему слишком мало: например, для решения задачи аннотирования изображений (image captioning) потребовалось бы огромное количество пар (изображение, описание). В некоторых случаях можно воспользоваться открытыми датасетами. Сейчас их доступно довольно много и некоторые весьма велики, но чаще всего они создаются для довольно простых задач, например, для задачи классификации изображений. Иногда датасет можно купить. Но для каких-то задач вы нигде не найдёте данных. Скажем, авторам неизвестно больших и качественных корпусов телефонных разговоров с расшифровками – в том числе и по причинам конфиденциальности таких данных.\\nБороться с проблемой нехватки данных можно двумя способами.\\nПервый – использование краудсорсинга, то есть привлечение людей, готовых разметить много данных. Во многих ситуациях (например, когда речь заходит об оценке поисковой выдачи) без дополнительной разметки никак не обойтись. Мы расскажем про краудсорсинг подробнее в соответствующем параграфе. Некоторые проекты, в первую очередь научные и социальные, используют также citizen science – разметку данных волонтёрами без какого-либо вознаграждения, просто за чувство причастности к доброму делу исследования животных Африки или формы галактик.\\nВторой же способ состоит в использовании неразмеченных данных. К примеру, в задаче аннотирования изображений у нас есть огромное количество никак не связанных друг с другом изображений и текстов. Однако, мы можем использовать их для того, чтобы помочь компьютеру понять, какие слова в принципе могут стоять рядом в предложении. Подходы, связанные с использованием неразмеченных данных для решения задач обучения с учителем, объединяются термином self-supervised learning и очень активно используются сейчас. Важной составляющей является обучение представлений (representation learning) — задача построения компактных векторов небольшой размерности из сложных по структуре данных, например, изображений, звука, текстов, графов, так, чтобы близкие по структуре или семантике данные получали метрически близкие представления. Делать это можно разными способами — например, используя фрагменты моделей, обученных для решения какой-либо другой задачи, или строя модель, предсказывающую скрытую часть объекта по оставшейся его части — например, пропущенное слово в предложении. Этому будет посвящен отдельный параграф нашего учебника.\\nНо кроме количества данных важно ещё и то, насколько они хороши и удобны для анализа. Давайте разберёмся, что это значит и какие с этим бывают проблемы.\\nДля работы с объектом модель должна опираться на какие-то его свойства, например, доход человека, цвет левого верхнего пикселя на изображении или частоту встречаемости слова «интеграл» в тексте. Эти свойства обычно называются признаками, а совокупность свойств, которые мы выделили у объекта – его признаковым описанием.\\nВот несколько простых и распространённых разновидностей признаков:\\n\\n\\nЧисленные – например, рост или доход. Иногда отдельно выделяют вещественные и целочисленные признаки.\\n\\n\\nКатегориальные признаки принимают значения из некоторого дискретного множества. Например, профессия человека или день недели.\\n\\n\\nБинарные признаки принимают два значения: 000 и 111 или «да» и «нет». С ними можно работать и как с численными, и как с категориальными.\\n\\n\\nСреди категориальных признаков иногда выделяют ординальные. Они принимают значения из некоторого упорядоченного дискретного множества. Например, класс опасности химического вещества (бывает от 1-го до 4-го) или год обучения для студента являются ординальными.\\n\\n\\nПриходится иметь дело и с более сложно устроенными признаками. Например, описание ресторана может содержать тексты отзывов или фотографии, а профиль человека в социальной сети – список его друзей. Для многих однородных типов данных, таких как изображения, видео, тексты, звук, графы, разработано большое количество методов извлечения признаков – сейчас в первую очередь нейросетевых. О них вы сможете прочитать в разделах про нейросетевые архитектуры для соответствующих типов данных. Если же попадаются какие-то более сложно устроенные данные, могут потребоваться дополнительные усилия для извлечения из них признаков – этот процесс называют feature engineering.\\nУдобно бывает записать данные в виде таблицы, строки которой соответствуют объектам, а столбцы – признакам. Например:\\n\\n\\n\\n\\nОбъекты\\n\\n\\nВозраст\\n\\n\\nОценка по ML\\n\\n\\n\\n\\nНаташа\\n\\n\\n21\\n\\n\\nотл\\n\\n\\n\\n\\nВася\\n\\n\\nN/A\\n\\n\\nудовл\\n\\n\\n\\n\\nИгорь\\n\\n\\n47\\n\\n\\nхор\\n\\n\\n\\n\\nДанные, представленные в таком виде, называются табличными. Табличные данные – один из самых удобных для анализа форматов. Свои успешные пайплайны работы есть также для уже упомянутых текстов, звука, изображений, видео, графов.\\nЛучше всего, если все признаки являются численными. Тогда с таблицей можно работать как с объектом линейной алгебры – матрицей объекты-признаки.\\nСоздание информативного признакового описания очень важно для дальнейшего анализа. Но нужно также следить за качеством полученных данных. Вам могут встретиться, например, следующие проблемы:\\n\\n\\nПропуски (пропущенные значения). Так, в примере табличных данных выше нам неизвестен возраст Васи. Объекты или признаки, в которых есть пропуски, можно удалять из выборки, но если пропусков довольно много, мы можем потерять таким образом слишком много информации. Кроме того, наличие пропуска само по себе может нести информацию: скажем, это может говорить о систематической проблеме в сборе данных для того или иного сегмента выборки. Некоторые модели, например, решающие деревья, обладают собственными средствами для работы с пропусками, другие же – например, линейные модели или нейросети – требуют, чтобы пропуски были вычищены или заменены на что-то.\\n\\n\\nВыбросы, то есть объекты, которые резко отличаются от большинства остальных. Например, в датасете с информацией о клиентах банка 140-летний человек, очевидно, будет весьма нетипичным. Выбросы могут возникать из-за ошибок при сборе данных или представлять собой реально существующие аномалии. Обычно выбросы лучше удалять, но в некоторых случаях выбросами могут быть важные объекты (например, очень богатые клиенты банка), и тогда их, возможно, стоит отлавливать и обрабатывать отдельно.\\n\\n\\nОшибки разметки. Если, например, вы собираете данные с помощью разметчиков-людей, то вы должны быть готовы к тому, что часть таргетов будет отмечена неправильно. Даже если не думать о том, что не все из разметчиков совершенно честные и старательные, задача может оказаться для них сложной.\\n\\n\\nData drift. С течением времени данные могут меняться. Например, может измениться схема сбора данных, и они начнут приходить в формате, который вообще не обрабатывается моделью. Или же может поменяться распределение данных: скажем, если вы делали образовательный сервис для студентов, а к вам стали приходить и более зрелые люди. Data drift – это суровая реальность для любой системы, которая решает не сиюминутную задачу, поэтому нужно уметь мониторить распределение данных и, если нужно, обновлять модель.\\n\\n\\nВстречаются и другие проблемы. Нередко существенную часть данных приходится выкидывать, потому что в процессе сбора что-нибудь сломалось или потому, что полгода назад в сервисе изменили систему логирования и более старые данные невозможно склеить с более новыми.\\nМодель и алгоритм обучения\\nМодель — это некоторый способ описания мира. Например, «Земля плоская» — это модель, и не такая плохая, как вам может показаться. Ей активно пользуются, когда всё происходит в масштабах одного города и кривизной поверхности можно пренебрегать. С другой стороны, если мы попробуем рассчитать кратчайший путь из Парижа в Лос-Анджелес, модель плоской Земли выдаст неадекватный ответ, она войдёт в противоречие с имеющимися данными, и её придётся заменить на «Земля круглая», «Земля имеет форму эллипсоида» и так далее — в той мере, в которой нам важна точность и в какой нам это позволяет (не)совершенство измерительной техники. Так, модель «Земля — это похожая на геоид с шершавостями на месте горных хребтов» очень точная и замечательная, но, возможно, будет избыточно сложной для большинства практических задач и при этом слишком тяжёлой в плане вычислений.\\nВ первых параграфах мы будем рассматривать в основном предсказательные модели, то есть модели вида y=f(x)y = f(x)y=f(x), которые пытаются уловить зависимость между признаковым описанием xxx объекта и таргетом yyy. Но порой мы будем иметь дело и с моделями данных: например, «такой-то признак имеет нормальное распределение».\\nЧаще всего предсказательные модели мы будем брать из некоторого параметрического семейства y=fw(x)y = f_w(x)y=fw\\u200b(x), где www — параметры, которые мы будем подбирать по данным.\\nДля примера давайте возьмём задачу предсказания цены квартиры. В качестве класса моделей выберем константные функции f(x)=cf(x) = cf(x)=c (то есть будем для всех квартир предсказывать одно и то же значение цены). Поскольку значение не зависит от xxx, нам не очень важно, в каком виде получено признаковое описание: это может быть набор совершенно любых сведений о квартире. Не забудем зафиксировать метрику качества — среднее абсолютное отклонение (mean absolute error, она же MAE).\\nMAE(f,X,y)=L(f,X,y)=1N∑i=1N∣f(xi)−yi∣→min\\u2061f,MAE(f, X, y) = L(f, X, y) = \\\\frac1N\\\\sum\\\\limits_{i=1}^N \\\\vert f(x_i) - y_i\\\\vert \\\\rightarrow \\\\min\\\\limits_f,\\nMAE(f,X,y)=L(f,X,y)=N1\\u200bi=1∑N\\u200b∣f(xi\\u200b)−yi\\u200b∣→fmin\\u200b,где fff — это модель (та самая, f(x)=cf(x) = cf(x)=c), X=(x1,…,xN)X = (x_1,\\\\ldots,x_N)X=(x1\\u200b,…,xN\\u200b) — обучающие примеры (данные о квартирах, которые мы смогли достать), y=(y1,…,yN)y = (y_1,\\\\ldots,y_N)y=(y1\\u200b,…,yN\\u200b) — правильные ответы (то есть цены на известные нам квартиры). Чтобы найти минимум MAE, возьмём производную от выражения\\n1N∑i=1N∣c−yi∣→min\\u2061f,\\\\frac1N\\\\sum\\\\limits_{i=1}^N \\\\vert c - y_i\\\\vert \\\\rightarrow \\\\min\\\\limits_f,\\nN1\\u200bi=1∑N\\u200b∣c−yi\\u200b∣→fmin\\u200b,и приравняем её к нулю:\\n∇cL(f,X,y)=1N∑i=1Nsign(c−yi)=0\\\\nabla_cL(f, X, y) = \\\\frac1N\\\\sum\\\\limits_{i=1}^N sign(c - y_i) = 0\\n∇c\\u200bL(f,X,y)=N1\\u200bi=1∑N\\u200bsign(c−yi\\u200b)=0#{i∣yi<c}−#{i∣yi>c}=0\\\\#\\\\left\\\\{i\\\\mid y_i < c\\\\right\\\\} - \\\\#\\\\left\\\\{i\\\\mid y_i > c\\\\right\\\\} = 0\\n#{i∣yi\\u200b<c}−#{i∣yi\\u200b>c}=0Нам подходят точки ccc, для которых число yiy_iyi\\u200b, строго меньших ccc, равно числу yiy_iyi\\u200b, строго больших ccc. Таким образом, нам подходит медиана набора (y1,…,yN)(y_1,\\\\ldots,y_N)(y1\\u200b,…,yN\\u200b):\\nf(x)=median(y).f(x) = \\\\mathrm{median}(y).\\nf(x)=median(y).Вопрос на подумать. Давайте теперь в задаче предсказания цены квартиры рассмотрим метрику среднеквадратичное отклонение (MSE):\\nMSE(f,X,y)=1N∑i=1N(f(xi)−yi)2MSE(f, X, y) = \\\\frac1N\\\\sum_{i=1}^N(f(x_i) - y_i)^2\\nMSE(f,X,y)=N1\\u200bi=1∑N\\u200b(f(xi\\u200b)−yi\\u200b)2Каким будет оптимальное значение параметра ccc для константной модели f(x)=cf(x) = cf(x)=c?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Это будет среднее значение:\\ny‾=1N∑i=1Nyi\\\\overline{y} = \\\\frac1N\\\\sum_{i=1}^Ny_i\\ny\\u200b=N1\\u200bi=1∑N\\u200byi\\u200bПрекрасно, значит, в классе константных функций мы можем найти оптимальную модель. Может быть, это можно сделать и в каком-нибудь более интересном классе? Этому вопросу и будет посвящена большая часть нашей книги. Классический курс ML состоит из описания классов моделей и способов работы с ними. Несмотря на то что для решения большинства практических задач на сегодня достаточно знать только два типа моделей — градиентный бустинг на решающих деревьях и нейросетевые модели — мы постараемся рассказать и про другие, чтобы развить у вас глубинное понимание предмета и дать возможность не только использовать лучшие сложившиеся практики, но и, при вашем желании, участвовать в проработке новых идей и поиске новых методов — уже в роли исследователя, а не просто инженера.\\nКроме выбора модели важен также выбор алгоритма обучения. Алгоритм обучения — это процедура, которая превращает обучающую выборку в обученную модель. Скажем, в примере выше для константной модели мы в качестве алгоритма обучения использовали поиск нуля градиента. Как мы увидим дальше, градиентные методы используются для обучения многих моделей, и это очень богатый класс методов оптимизации, из которого порой не так просто выбрать лучший.\\nВ качестве примера рассмотрим задачу бинарной классификации точек на плоскости, для которой выберем линейную модель:\\n\\nМетрикой будет accuracy, то есть доля верных предсказаний.\\nТеперь нам нужно по обучающей выборке подобрать оптимальную разделяющую прямую y=w1x+w0y = w_1x + w_0y=w1\\u200bx+w0\\u200b. Числа w1w_1w1\\u200b и w0w_0w0\\u200b являются настраиваемыми (обучаемыми) параметрами модели, именно их будет по выборке восстанавливать алгоритм обучения. Но есть проблема: метрика accuracy не дифференцируема. Поэтому мы должны подобрать другую дифференцируемую функцию L(X,y,w)\\\\mathcal{L}(X, y, w)L(X,y,w), минимизация которой будет более или менее соответствовать оптимизации вероятности. Такая функция называется функцией потерь, лоссом (от слова loss) или лосс-функцией. О том, как могут выглядеть лосс-функции для бинарной линейной классификации, вы можете почитать в параграфе про линейные модели.\\nВ качестве алгоритма обучения мы можем взять теперь градиентный спуск:\\nwt+1=wt−α∇wL,w_{t+1} = w_t - \\\\alpha\\\\nabla_w\\\\mathcal{L},\\nwt+1\\u200b=wt\\u200b−α∇w\\u200bL,где α\\\\alphaα — шаг оптимизации — коэффициент, влияющий на скорость и устойчивость алгоритма. Отметим, что разный выбор коэффициента α\\\\alphaα, вообще говоря, даёт разные алгоритмы обучения, которые могут приводить к разным результатам: если α\\\\alphaα слишком мал, то спуск может не дойти до оптимума, а если слишком велик, то алгоритм будет «скакать» вокруг оптимума и никогда туда не попадёт. Мы видим, что важен не только выбор модели, но и выбор алгоритма обучения.\\nЧисло α\\\\alphaα является гиперпараметром алгоритма, то есть задаётся до начала обучения — но его тоже можно подбирать по данным. Более подробно о подборе гиперпараметров вы можете узнать в соответствующем параграфе.\\nВыбор модели, переобучение\\nМожет показаться, что мы вас обманули, когда пугали сложностями: очевидно, что для любой задачи машинного обучения можно построить идеальную модель, надо всего лишь запомнить всю обучающую выборку с ответами. Такая модель может достичь идеального качества по любой метрике, но радости от неё довольно мало, ведь мы хотим, чтобы она выявила какие-то закономерности в данных и помогла нам с ответами там, где мы их не знаем. Важно понимать, какая у построенной модели обобщающая способность, то есть насколько она способна выучить общие закономерности, присущие не только обучающей выборке, и давать адекватные предсказания на новых данных. Для того чтобы предохранить себя от конфуза, поступают обычно так: делят выборку с данными на две части: обучающую выборку и тестовую выборку (train и test). Обучающую выборку используют для собственно обучения модели, а метрики считают на тестовой.\\nТакой подход позволяет отделить модели, которые просто удачно подстроились к обучающим данным, от моделей, в которых произошла генерализация (generalization), то есть от таких, которые на самом деле кое-что поняли о том, как устроены данные, и могут выдавать полезные предсказания для объектов, которых не видели.\\nНапример, рассмотрим три модели регрессионной зависимости, построенные на одном и том же синтетическом датасете с одним-единственным признаком. Жёлтым нарисованы точки обучающей выборки. Здесь мы представим, что есть «истинная» закономерность (пунктир), которая искажена шумом (погрешности измерения, влияние других факторов и т.д.).\\n\\nЛевая, линейная модель недостаточно хороша: она сделала, что могла, но плохо приближает зависимость, особенно при маленьких и при больших xxx. Правая «запомнила» всю обучающую выборку (и в самом деле, чтобы вычислить значение этой функции, нам надо знать координаты всех исходных точек) вместо того, чтобы моделировать исходную зависимость. Наконец, центральная, хоть и не проходит через точки обучающей выборки, довольно неплохо моделирует истинную зависимость.\\nАлгоритм, избыточно подстроившийся под данные, называют переобученным.\\nС увеличением сложности модели ошибка на обучающей выборке падает. Во многих задачах очень сложная модель будет работать примерно так же, как модель, «просто запомнившая всю обучающую выборку», но с генерализацией всё будет плохо: ведь выученные закономерности будут слишком специфическими, подогнанными под то, что происходит на обучающей выборке. Мы видим это на трёх графиках сверху: линейная функция очень проста, но и закономерность приближает лишь очень грубо; на правом же графике мы видим довольно хитрую функцию, которая точно подобрана под значения из обучающей выборки, но явно слишком эксцентрична, чтобы соответствовать какой-то природной зависимости. Оптимальная же генерализация достигается на модели не слишком сложной и не слишком простой.\\nВ качестве иллюстрации для того же самого датасета рассмотрим модели вида\\ny\\xa0=\\xa0\\xa0многочлен\\xa0степени\\xa0Dy\\\\ =\\\\ \\\\text{ многочлен степени }D\\ny\\xa0=\\xa0\\xa0многочлен\\xa0степени\\xa0DЯсно, что с ростом DDD сложность модели растёт, и она достигает всё лучшего качества на обучающей выборке. А что, если у нас есть ещё тестовая выборка? Каким будет качество на ней? Вот так могут выглядеть графики среднеквадратичного отклонения (MSE) для обучающей и тестовой выборок:\\n\\nМы видим здесь типичную для классических моделей картину: MSE на обучающей выборке падает (может быть, даже до нуля), а на тестовой сперва падает, а затем начинает снова расти.\\nЗамечание. Для моделей глубинного обучения всё немного интереснее: в некоторых ситуациях есть грань, за которой метрика на тестовой выборке снова начинает падать. Но об этом в своё время. Пока давайте запомним, что слишком сложная модель — это вредно, а переобучение — боль.\\nТочный способ выбрать алгоритм оптимальной сложности по данной задаче нам пока неизвестен, хотя какую-то теоретическую базу имеющимся философским наблюдениям мы дадим в главе про теорию обучения; при этом есть хорошо продуманная методология сравнения разных моделей и выбора среди них оптимальной — об этом мы обязательно расскажем вам в следующих главах. А пока дадим самый простой и неизменно ценный совет: не забывайте считать метрики на тестовой выборке и никогда не смешивайте её с обучающей!\\nВопрос на подумать. Обсуждая переобучение, мы упоминали про сложность модели, но не сказали, что это такое. Как бы вы её определили? Как описать / сравнить сложность моделей для двух приведённых ниже задач? Почему, кстати, мы решили, что средняя модель ОК, а правая переобученная?\\n\\n\\nОтвет (не открывайте сразу; сначала подумайте сами!)Сложность модели можно очень грубо охарактеризовать числом настраиваемых параметров модели, то есть тех, которые мы можем определить по данным в процессе обучения. Это не имеет никакого математического смысла, и о каких-то более серьёзных оценках мы поговорим в главе про теорию машинного обучения, но никто не бросит в вас камень, если вы скажете, что модель с 10 тысячами параметров сложнее, чем модель с 1000 параметров.\\nВ первой задаче левая модель — это, судя по всему, линейная функция, у неё два параметра, вторая — наверное, квадратичная с тремя параметрами, а правая — многочлен какой-то высокой степени (на самом деле 11-й), у неё параметров намного больше. Центральная модель явно лучше, чем левая, справляется с тем, чтобы приблизить истинную закономерность; правая тоже вроде неплохо справляется с тем, чтобы приблизить её для обучающих данных, но вот два резких провала и крутое пике слева никак не объясняются имеющимися данными, и на двух тестовых точках в районе 0,50,50,5 модель отчаянно врёт — так что есть причины считать, что она переобучилась.\\nСо второй задачей ситуация во многом похожая. Центральная модель явно лучше разделяет жёлтые и серые точки. На правой же картинке мы видим довольно неестественные выпячивания жёлтой и серой областей: например, к серой точке в центре картинки (которая наверняка была выбросом) протянулось серое «щупальце», захватившее и несколько тестовых (и даже обучающих) точек другого класса. В целом можно поспорить о том, плох ли правый классификатор, но он явно рисует слишком сложные границы, чтобы можно было поверить, что они отражают что-то из реальной жизни.\\nПосле обучения\\nВ момент, когда подобраны все обучаемые параметры и гиперпараметры модели, работа специалиста по машинному обучению не заканчивается.\\nВо-первых, модель чаще всего создают для того, чтобы она работала в некотором продакшене. И чтобы она там оказалась, нужно эффективно её закодить, научить работать параллельно и подружить с используемыми вами фреймворками. Процесс выкатки в продакшен называется словом деплой или деплоймент (от deploy). После деплоя можно посчитать онлайн-метрики. Также имеет смысл провести АБ-тестирование, то есть сравнение с предыдущей версией модели на случайно выбранных подмножествах пользователей или сессий. Более подробно об АБ-тестировании вы сможете почитать в соответствующем параграфе. Если новая модель работает не очень здорово, должна быть возможность откатиться к старой.\\nПосле деплоймента модели важно продолжать дообучать или переобучать её при поступлении новых данных, а также мониторить качество. Мы уже обсуждали data drift, но бывает также и concept drift — изменение зависимости между признаками и таргетом. Например, если вы делаете музыкальные рекомендации, вам нужно будет учитывать и появление новых треков, и изменение вкусов аудитории. О мониторинге качества моделей мы подробнее расскажем в соответствующем параграфе.\\n\\nТеперь предлагаем вам потренировать изученный материал на практике. Скачайте ноутбук с лабораторной работой. В нём вы найдете описания заданий и дополнительные материалы. Задания из лабораторной прикреплены к этому параграфу в виде задач в системе Яндекс Контест. Чтобы проверить себя, отправляйте решения по соответствующим задачам в систему. Успехов в практике!\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьВыполните задачи урока0 / 11 выполненоВыполнять задачиСообщить об ошибкеПредыдущий параграф1.2. Первые шагиВ этой главе мы поговорим о рабочем окружении ML-специалиста — какие сервисы и библиотеки в него входят, как его развернуть, на что обратить внимание.\\xa0\\nА кроме того, в качестве быстрой практики обучим собственную модель генерировать ответы в стиле Льва Толстого.Следующий параграф2.1. Линейные моделиЛинейные модели от\\xa0линейной до\\xa0логистической регрессии. Регуляризация, работа с\\xa0категориальными признаками, многоклассовая классификацияЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_22.html', 'title': 'Метод обратного распространения ошибки'}, page_content=\"Метод обратного распространения ошибкиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/45.1.Нейронные сети5.2.Первое знакомство с полносвязными нейросетями5.3.Метод обратного распространения ошибкиМетод обратного распространения ошибки (backward propagation)Backward propagation в одномерном случаеПочему же нельзя просто пойти и начать везде вычислять производные?Градиент сложной функцииГрадиенты для типичных слоёвBackward propagation в общем видеАвтоматизация и autogradНо это лишь начало5.4.Тонкости обучения6.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Метод обратного распространения ошибки5.3. Метод обратного распространения ошибкиАвторыФедотов СтаниславНейчев РадославКак эффективно посчитать градиенты по\\xa0весам нейронной сетиНейронные сети обучаются с помощью тех или иных модификаций градиентного спуска, а чтобы применять его, нужно уметь эффективно вычислять градиенты функции потерь по всем обучающим параметрам. Казалось бы, для какого-нибудь запутанного вычислительного графа это может быть очень сложной задачей, но на помощь спешит метод обратного распространения ошибки.\\nМетод обратного распространения ошибки (backward propagation)\\nОткрытие метода обратного распространения ошибки стало одним из наиболее значимых событий в области искусственного интеллекта. В актуальном виде он был предложен в 1986 году Дэвидом Э. Румельхартом, Джеффри Э. Хинтоном и Рональдом Дж. Вильямсом, а также независимо и одновременно красноярскими математиками С. И. Барцевым и В. А. Охониным.\\nС тех пор для нахождения градиентов параметров нейронной сети используется метод вычисления производной сложной функции, и оценка градиентов параметров сети стала хоть и сложной инженерной задачей, но уже не искусством. Несмотря на простоту используемого математического аппарата, появление этого метода привело к значительному скачку в развитии искусственных нейронных сетей.\\nСуть метода можно записать одной формулой, тривиально следующей из формулы производной сложной функции: если f(x)=gm(gm−1(…(g1(x))…))f(x) = g_m(g_{m-1}(\\\\ldots (g_1(x)) \\\\ldots))f(x)=gm\\u200b(gm−1\\u200b(…(g1\\u200b(x))…)), то ∂f∂x=∂gm∂gm−1∂gm−1∂gm−2…∂g2∂g1∂g1∂x\\\\frac{\\\\partial f}{\\\\partial x} = \\\\frac{\\\\partial g_m}{\\\\partial g_{m-1}}\\\\frac{\\\\partial g_{m-1}}{\\\\partial g_{m-2}}\\\\ldots \\\\frac{\\\\partial g_2}{\\\\partial g_1}\\\\frac{\\\\partial g_1}{\\\\partial x}∂x∂f\\u200b=∂gm−1\\u200b∂gm\\u200b\\u200b∂gm−2\\u200b∂gm−1\\u200b\\u200b…∂g1\\u200b∂g2\\u200b\\u200b∂x∂g1\\u200b\\u200b. Уже сейчас мы видим, что градиенты можно вычислять последовательно, в ходе одного обратного прохода, начиная с ∂gm∂gm−1\\\\frac{\\\\partial g_m}{\\\\partial g_{m-1}}∂gm−1\\u200b∂gm\\u200b\\u200b и умножая каждый раз на частные производные предыдущего слоя.\\nBackward propagation в одномерном случае\\nВ одномерном случае всё выглядит особенно просто. Пусть w0w_0w0\\u200b — переменная, по которой мы хотим продифференцировать , причём сложная функция имеет видВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nf(w0)=gm(gm−1(…g1(w0)…)),f(w_0) = g_m(g_{m-1}(\\\\ldots g_1(w_0)\\\\ldots)),\\nf(w0\\u200b)=gm\\u200b(gm−1\\u200b(…g1\\u200b(w0\\u200b)…)),где все gig_igi\\u200b скалярные. Тогда\\nf′(w0)=gm′(gm−1(…g1(w0)…))⋅gm−1′(gm−2(…g1(w0)…))⋅…⋅g1′(w0)f'(w_0) = g_m'(g_{m-1}(\\\\ldots g_1(w_0)\\\\ldots))\\\\cdot g'_{m-1}(g_{m-2}(\\\\ldots g_1(w_0)\\\\ldots))\\\\cdot\\\\ldots \\\\cdot g'_1(w_0)\\nf′(w0\\u200b)=gm′\\u200b(gm−1\\u200b(…g1\\u200b(w0\\u200b)…))⋅gm−1′\\u200b(gm−2\\u200b(…g1\\u200b(w0\\u200b)…))⋅…⋅g1′\\u200b(w0\\u200b)Суть этой формулы такова. Если мы уже совершили прямой проход (forward propagation), значит мы уже знаем\\ng1(w0),g2(g1(w0)),…,gm−1(…g1(w0)…),g_1(w_0), g_2(g_1(w_0)),\\\\ldots,g_{m-1}(\\\\ldots g_1(w_0)\\\\ldots),\\ng1\\u200b(w0\\u200b),g2\\u200b(g1\\u200b(w0\\u200b)),…,gm−1\\u200b(…g1\\u200b(w0\\u200b)…),Поэтому мы можем действовать следующим образом:\\n\\n\\nберём производную gmg_mgm\\u200b в точке gm−1(…g1(w0)…)g_{m-1}(\\\\ldots g_1(w_0)\\\\ldots)gm−1\\u200b(…g1\\u200b(w0\\u200b)…);\\n\\n\\nумножаем на производную gm−1g_{m-1}gm−1\\u200b в точке gm−2(…g1(w0)…)g_{m-2}(\\\\ldots g_1(w_0)\\\\ldots)gm−2\\u200b(…g1\\u200b(w0\\u200b)…);\\n\\n\\nи так далее, пока не дойдём до производной g1g_1g1\\u200b в точке w0w_0w0\\u200b.\\n\\n\\nПроиллюстрируем это на картинке, расписав по шагам дифференцирование по весам wiw_iwi\\u200b функции потерь логистической регрессии на одном объекте (то есть для батча размера 1):\\n\\nСобирая все множители вместе, получаем:\\n∂f∂w0=(−y)⋅e−y(w0+w1x1+w2x2)⋅−11+e−y(w0+w1x1+w2x2)\\\\frac{\\\\partial f}{\\\\partial w_0} = (-y)\\\\cdot e^{-y(w_0 + w_1x_1 + w_2x_2)}\\\\cdot\\\\frac{-1}{1 + e^{-y(w_0 + w_1x_1 + w_2x_2)}}\\n∂w0\\u200b∂f\\u200b=(−y)⋅e−y(w0\\u200b+w1\\u200bx1\\u200b+w2\\u200bx2\\u200b)⋅1+e−y(w0\\u200b+w1\\u200bx1\\u200b+w2\\u200bx2\\u200b)−1\\u200b∂f∂w1=x1⋅(−y)⋅e−y(w0+w1x1+w2x2)⋅−11+e−y(w0+w1x1+w2x2)\\\\frac{\\\\partial f}{\\\\partial w_1} = x_1\\\\cdot(-y)\\\\cdot e^{-y(w_0 + w_1x_1 + w_2x_2)}\\\\cdot\\\\frac{-1}{1 + e^{-y(w_0 + w_1x_1 + w_2x_2)}}\\n∂w1\\u200b∂f\\u200b=x1\\u200b⋅(−y)⋅e−y(w0\\u200b+w1\\u200bx1\\u200b+w2\\u200bx2\\u200b)⋅1+e−y(w0\\u200b+w1\\u200bx1\\u200b+w2\\u200bx2\\u200b)−1\\u200b∂f∂w2=x2⋅(−y)⋅e−y(w0+w1x1+w2x2)⋅−11+e−y(w0+w1x1+w2x2)\\\\frac{\\\\partial f}{\\\\partial w_2} = x_2\\\\cdot(-y)\\\\cdot e^{-y(w_0 + w_1x_1 + w_2x_2)}\\\\cdot\\\\frac{-1}{1 + e^{-y(w_0 + w_1x_1 + w_2x_2)}}\\n∂w2\\u200b∂f\\u200b=x2\\u200b⋅(−y)⋅e−y(w0\\u200b+w1\\u200bx1\\u200b+w2\\u200bx2\\u200b)⋅1+e−y(w0\\u200b+w1\\u200bx1\\u200b+w2\\u200bx2\\u200b)−1\\u200bТаким образом, сперва совершается forward propagation для вычисления всех промежуточных значений (да, все промежуточные представления нужно будет хранить в памяти), а потом запускается backward propagation, на котором в один проход вычисляются все градиенты.\\nПочему же нельзя просто пойти и начать везде вычислять производные?\\nВ параграфе, посвящённом матричным дифференцированиям, мы поднимаем вопрос о том, что вычислять частные производные по отдельности — это зло, лучше пользоваться матричными вычислениями. Но есть и ещё одна причина: даже и с матричной производной в принципе не всегда хочется иметь дело.\\nРассмотрим простой пример. Допустим, что XrX^rXr и Xr+1X^{r+1}Xr+1 — два последовательных промежуточных представления N×MN\\\\times MN×M и N×KN\\\\times KN×K, связанных функцией Xr+1=fr+1(Xr)X^{r+1} = f^{r+1}(X^r)Xr+1=fr+1(Xr). Предположим, что мы как-то посчитали производную ∂L∂Xijr+1\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial X^{r+1}_{ij}}∂Xijr+1\\u200b∂L\\u200b функции потерь L\\\\mathcal{L}L, тогда\\n∂L∂Xstr=∑i,j∂fijr+1∂Xstr∂L∂Xijr+1\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial X^{r}_{st}} = \\\\sum_{i,j}\\\\frac{\\\\partial f^{r+1}_{ij}}{\\\\partial X^{r}_{st}}\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial X^{r+1}_{ij}}\\n∂Xstr\\u200b∂L\\u200b=i,j∑\\u200b∂Xstr\\u200b∂fijr+1\\u200b\\u200b∂Xijr+1\\u200b∂L\\u200bИ мы видим, что, хотя оба градиента ∂L∂Xijr+1\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial X_{ij}^{r+1}}∂Xijr+1\\u200b∂L\\u200b и ∂L∂Xstr\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial X_{st}^{r}}∂Xstr\\u200b∂L\\u200b — это просто матрицы, в ходе вычислений возникает «четырёхмерный кубик» ∂fijr+1∂Xstr\\\\frac{\\\\partial f_{ij}^{r+1}}{\\\\partial X_{st}^{r}}∂Xstr\\u200b∂fijr+1\\u200b\\u200b. Его болезненно даже хранить: уж больно много памяти он требует —мN2MKN^2MKN2MK по сравнению с безобидными NM+NKNM + NKNM+NK, требуемыми для хранения градиентов.\\nПоэтому хочется промежуточные производные ∂fr+1∂Xr\\\\frac{\\\\partial f^{r+1}}{\\\\partial X^{r}}∂Xr∂fr+1\\u200b рассматривать не как вычисляемые объекты ∂fijr+1∂Xstr\\\\frac{\\\\partial f_{ij}^{r+1}}{\\\\partial X_{st}^{r}}∂Xstr\\u200b∂fijr+1\\u200b\\u200b, а как преобразования, которые превращают ∂L∂Xijr+1\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial X_{ij}^{r+1}}∂Xijr+1\\u200b∂L\\u200b в ∂L∂Xstr\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial X_{st}^{r}}∂Xstr\\u200b∂L\\u200b.\\nЦелью следующих параграфов будет именно это: понять, как преобразуется градиент в ходе error backward propagation при переходе через тот или иной слой.\\nВы спросите себя: надо ли мне сейчас пойти и прочитать параграф учебника про матричное дифференцирование?\\nКороткий ответ: Зависит от ваших знаний.\\nДлинный ответ: Найдите производную функции по вектору xxx:\\nf(x)=xTAx,\\xa0A∈MatnR\\xa0—\\xa0матрица\\xa0размера\\xa0n×nf(x) = x^TAx,\\\\ A\\\\in Mat_{n}{\\\\mathbb{R}}\\\\text{ — матрица размера }n\\\\times n\\nf(x)=xTAx,\\xa0A∈Matn\\u200bR\\xa0—\\xa0матрица\\xa0размера\\xa0n×nА как всё поменяется, если AAA тоже зависит от xxx? Чему равен градиент функции, если AAA является скаляром?\\nЕсли вы готовы прямо сейчас взять ручку и бумагу и посчитать всё, то вам, вероятно, не надо читать про матричные дифференцирования. Но мы советуем всё-таки заглянуть в этот параграф, если обозначения, которые мы будем дальше использовать, покажутся вам непонятными: единой нотации для матричных дифференцирований человечество пока, увы, не изобрело, и переводить с одной на другую не всегда легко.\\nА мы же сразу перейдём к интересующей нас вещи: к вычислению градиентов сложных функций.\\nГрадиент сложной функции\\nНапомним, что формула производной сложной функции выглядит следующим образом:\\n[Dx0(u∘v)](h)=[Dv(x0)u]([Dx0v](h))\\\\left[D_{x_0} (\\\\color{#5002A7}{u} \\\\circ \\\\color{#4CB9C0}{v}) \\\\right](h) = \\\\color{#5002A7}{\\\\left[D_{v(x_0)} u \\\\right]} \\\\left( \\\\color{#4CB9C0}{\\\\left[D_{x_0} v\\\\right]} (h)\\\\right)\\n[Dx0\\u200b\\u200b(u∘v)](h)=[Dv(x0\\u200b)\\u200bu]([Dx0\\u200b\\u200bv](h))Теперь разберёмся с градиентами. Пусть f(x)=g(h(x))f(x) = g(h(x))f(x)=g(h(x)) – скалярная функция. Тогда\\n[Dx0f](x−x0)=⟨∇x0f,x−x0⟩.\\\\left[D_{x_0} f \\\\right] (x-x_0) = \\\\langle\\\\nabla_{x_0} f, x-x_0\\\\rangle.\\n[Dx0\\u200b\\u200bf](x−x0\\u200b)=⟨∇x0\\u200b\\u200bf,x−x0\\u200b⟩.С другой стороны,\\n[Dh(x0)g]([Dx0h](x−x0))=⟨∇hx0g,[Dx0h](x−x0)⟩=⟨[Dx0h]∗∇h(x0)g,x−x0⟩.\\\\left[D_{h(x_0)} g \\\\right] \\\\left(\\\\left[D_{x_0}h \\\\right] (x-x_0)\\\\right) = \\\\langle\\\\nabla_{h_{x_0}} g, \\\\left[D_{x_0} h\\\\right] (x-x_0)\\\\rangle = \\\\langle\\\\left[D_{x_0} h\\\\right]^* \\\\nabla_{h(x_0)} g, x-x_0\\\\rangle.\\n[Dh(x0\\u200b)\\u200bg]([Dx0\\u200b\\u200bh](x−x0\\u200b))=⟨∇hx0\\u200b\\u200b\\u200bg,[Dx0\\u200b\\u200bh](x−x0\\u200b)⟩=⟨[Dx0\\u200b\\u200bh]∗∇h(x0\\u200b)\\u200bg,x−x0\\u200b⟩.То есть ∇x0f=[Dx0h]∗∇h(x0)g\\\\color{#FFC100}{\\\\nabla_{x_0} f} = \\\\color{#348FEA}{\\\\left[D_{x_0} h \\\\right]}^* \\\\color{#FFC100}{\\\\nabla_{h(x_0)}}g∇x0\\u200b\\u200bf=[Dx0\\u200b\\u200bh]∗∇h(x0\\u200b)\\u200bg — применение сопряжённого к Dx0hD_{x_0} hDx0\\u200b\\u200bh линейного отображения к вектору ∇h(x0)g\\\\nabla_{h(x_0)} g∇h(x0\\u200b)\\u200bg.\\nЭта формула — сердце механизма обратного распространения ошибки. Она говорит следующее: если мы каким-то образом получили градиент функции потерь по переменным из некоторого промежуточного представления XkX^kXk нейронной сети и при этом знаем, как преобразуется градиент при проходе через слой fkf^kfk между Xk−1X^{k-1}Xk−1 и XkX^kXk (то есть как выглядит сопряжённое к дифференциалу слоя между ними отображение), то мы сразу же находим градиент и по переменным из Xk−1X^{k-1}Xk−1:\\n\\nТаким образом слой за слоем мы посчитаем градиенты по всем XiX^iXi вплоть до самых первых слоёв.\\nДалее мы разберёмся, как именно преобразуются градиенты при переходе через некоторые распространённые слои.\\nГрадиенты для типичных слоёв\\nРассмотрим несколько важных примеров.\\nПример №1\\nf(x)=u(v(x))f(x) = u(v(x))f(x)=u(v(x)), где xxx — вектор, а v(x)v(x)v(x) – поэлементное применение vvv:\\nv(x1⋮xN)=(v(x1)⋮v(xN))v\\\\begin{pmatrix}\\nx_1 \\\\\\\\\\n\\\\vdots\\\\\\\\\\nx_N\\n\\\\end{pmatrix}\\n= \\\\begin{pmatrix}\\nv(x_1)\\\\\\\\\\n\\\\vdots\\\\\\\\\\nv(x_N)\\n\\\\end{pmatrix}v\\u200bx1\\u200b⋮xN\\u200b\\u200b\\u200b=\\u200bv(x1\\u200b)⋮v(xN\\u200b)\\u200b\\u200bТогда, как мы знаем,\\n[Dx0f](h)=⟨∇x0f,h⟩=[∇x0f]Th.\\\\left[D_{x_0} f\\\\right] (h) = \\\\langle\\\\nabla_{x_0} f, h\\\\rangle = \\\\left[\\\\nabla_{x_0} f\\\\right]^T h.\\n[Dx0\\u200b\\u200bf](h)=⟨∇x0\\u200b\\u200bf,h⟩=[∇x0\\u200b\\u200bf]Th.Следовательно,\\n[Dv(x0)u]([Dx0v](h))=[∇v(x0)u]T(v′(x0)⊙h)==∑i[∇v(x0)u]iv′(x0i)hi=⟨[∇v(x0)u]⊙v′(x0),h⟩.\\\\begin{aligned}\\\\left[D_{v(x_{0})}u\\\\right]([D_{x_{0}}v](h))&=\\\\left[\\\\nabla_{v(x_{0})}u\\\\right]^{T}\\\\left(v^{\\\\prime}(x_{0})\\\\odot h\\\\right)=\\\\\\\\&=\\\\sum_{i}\\\\left[\\\\nabla_{v(x_{0})}u\\\\right]_{i}v^{\\\\prime}(x_{0i})h_{i}=\\\\langle\\\\left[\\\\nabla_{v(x_{0})}u\\\\right]\\\\odot v^{\\\\prime}(x_{0}),h\\\\rangle.\\\\end{aligned}\\n[Dv(x0\\u200b)\\u200bu]([Dx0\\u200b\\u200bv](h))\\u200b=[∇v(x0\\u200b)\\u200bu]T(v′(x0\\u200b)⊙h)==i∑\\u200b[∇v(x0\\u200b)\\u200bu]i\\u200bv′(x0i\\u200b)hi\\u200b=⟨[∇v(x0\\u200b)\\u200bu]⊙v′(x0\\u200b),h⟩.\\u200bгде ⊙\\\\odot⊙ означает поэлементное перемножение. Окончательно получаем\\n∇x0f=[∇v(x0)u]⊙v′(x0)=v′(x0)⊙[∇v(x0)u]\\\\color{#348FEA}{\\\\nabla_{x_0} f = \\\\left[\\\\nabla_{v(x_0)}u\\\\right] \\\\odot v'(x_0) = v'(x_0) \\\\odot \\\\left[\\\\nabla_{v(x_0)} u\\\\right]}\\n∇x0\\u200b\\u200bf=[∇v(x0\\u200b)\\u200bu]⊙v′(x0\\u200b)=v′(x0\\u200b)⊙[∇v(x0\\u200b)\\u200bu]Отметим, что если xxx и h(x)h(x)h(x) — это просто векторы, то мы могли бы вычислять всё и по формуле ∂f∂xi=∑j(∂zj∂xi)⋅(∂h∂zj)\\\\frac{\\\\partial f}{\\\\partial x_i} = \\\\sum_j\\\\big(\\\\frac{\\\\partial z_j}{\\\\partial x_i}\\\\big)\\\\cdot\\\\big(\\\\frac{\\\\partial h}{\\\\partial z_j}\\\\big)∂xi\\u200b∂f\\u200b=∑j\\u200b(∂xi\\u200b∂zj\\u200b\\u200b)⋅(∂zj\\u200b∂h\\u200b).\\nВ этом случае матрица (∂zj∂xi)\\\\big(\\\\frac{\\\\partial z_j}{\\\\partial x_i}\\\\big)(∂xi\\u200b∂zj\\u200b\\u200b) была бы диагональной (так как zjz_jzj\\u200b зависит только от xjx_jxj\\u200b: ведь hhh берётся поэлементно), и матричное умножение приводило бы к тому же результату. Однако если xxx и h(x)h(x)h(x) — матрицы, то (∂zj∂xi)\\\\big(\\\\frac{\\\\partial z_j}{\\\\partial x_i}\\\\big)(∂xi\\u200b∂zj\\u200b\\u200b) представлялась бы уже «четырёхмерным кубиком», и работать с ним было бы ужасно неудобно.\\nПример №2\\nf(X)=g(XW)f(X) = g(XW)f(X)=g(XW), где XXX и WWW — матрицы. Как мы знаем,\\n[DX0f](X−X0)=tr\\u2009([∇X0f]T(X−X0)).\\\\left[D_{X_0} f \\\\right] (X-X_0) = \\\\text{tr}\\\\, \\\\left(\\\\left[\\\\nabla_{X_0} f\\\\right]^T (X-X_0)\\\\right).\\n[DX0\\u200b\\u200bf](X−X0\\u200b)=tr([∇X0\\u200b\\u200bf]T(X−X0\\u200b)).Тогда\\n[DX0Wg]([DX0(∗W)](H))=[DX0Wg](HW)==tr([∇X0Wg]T⋅(H)W)==tr(W[∇X0W(g)]T⋅(H))=tr([[∇X0Wg]WT]T(H))\\\\begin{gathered}\\n[D_{X_{0}W}g]\\\\left([D_{X_{0}}\\\\left(*W\\\\right)](H)\\\\right)=[D_{X_{0}W}g]\\\\left(HW\\\\right)= \\\\\\\\\\n=\\\\mathrm{tr} \\\\left(\\\\left[\\\\nabla_{X_{0}W}g\\\\right]^{T}\\\\cdot(H)W\\\\right)= \\\\\\\\\\n=\\\\mathrm{tr} \\\\left(W[\\\\nabla_{X_{0}W}(g)]^{T}\\\\cdot(H)\\\\right)=\\\\mathrm{tr} \\\\left(\\\\left[[\\\\nabla_{X_{0}W}g]W^{T}\\\\right]^{T}(H)\\\\right) \\n\\\\end{gathered}\\n[DX0\\u200bW\\u200bg]([DX0\\u200b\\u200b(∗W)](H))=[DX0\\u200bW\\u200bg](HW)==tr([∇X0\\u200bW\\u200bg]T⋅(H)W)==tr(W[∇X0\\u200bW\\u200b(g)]T⋅(H))=tr([[∇X0\\u200bW\\u200bg]WT]T(H))\\u200bЗдесь через ∗W\\\\ast W∗W мы обозначили отображение Y↪YWY \\\\hookrightarrow  YWY↪YW, а в предпоследнем переходе использовалось следующее свойство следа:\\ntr\\u2009(ABC)=tr\\u2009(CAB),\\t\\\\text{tr} \\\\, (A B C) = \\\\text{tr} \\\\, (C A B),\\ntr(ABC)=tr(CAB),где A,B,CA, B, CA,B,C — произвольные матрицы подходящих размеров (то есть допускающие перемножение в обоих приведённых порядках). Следовательно, получаем\\n∇X0f=[∇X0W(g)]⋅WT\\\\color{#348FEA}{\\\\nabla_{X_0} f = \\\\left[\\\\nabla_{X_0W} (g) \\\\right] \\\\cdot W^T}\\n∇X0\\u200b\\u200bf=[∇X0\\u200bW\\u200b(g)]⋅WTПример №3\\nf(W)=g(XW)f(W) = g(XW)f(W)=g(XW), где WWW и XXX — матрицы. Для приращения H=W−W0H = W - W_0H=W−W0\\u200b имеем\\n[DW0f](H)=tr\\u2009([∇W0f]T(H))\\\\left[D_{W_0} f \\\\right] (H) = \\\\text{tr} \\\\, \\\\left( \\\\left[\\\\nabla_{W_0} f \\\\right]^T (H)\\\\right)\\n[DW0\\u200b\\u200bf](H)=tr([∇W0\\u200b\\u200bf]T(H))Тогда\\n[DXW0g]([DW0(X∗)](H))=[DXW0g](XH)==tr\\u2009([∇XW0g]T⋅X(H))=tr\\u2009([XT[∇XW0g]]T(H))\\\\left[D_{XW_0} g \\\\right] \\\\left( \\\\left[D_{W_0} \\\\left(X \\\\ast\\\\right) \\\\right] (H)\\\\right) = \\\\left[D_{XW_0} g \\\\right] \\\\left( XH \\\\right) = \\\\\\\\\\n= \\\\text{tr} \\\\, \\\\left( \\\\left[\\\\nabla_{XW_0} g \\\\right]^T \\\\cdot X (H)\\\\right) =\\n\\t\\\\text{tr}\\\\, \\\\left(\\\\left[X^T \\\\left[\\\\nabla_{XW_0} g \\\\right] \\\\right]^T (H)\\\\right)\\n[DXW0\\u200b\\u200bg]([DW0\\u200b\\u200b(X∗)](H))=[DXW0\\u200b\\u200bg](XH)==tr([∇XW0\\u200b\\u200bg]T⋅X(H))=tr([XT[∇XW0\\u200b\\u200bg]]T(H))Здесь через X∗X \\\\astX∗ обозначено отображение Y↪XYY \\\\hookrightarrow XYY↪XY. Значит,\\n∇X0f=XT⋅[∇XW0(g)]\\\\color{#348FEA}{\\\\nabla_{X_0} f = X^T \\\\cdot \\\\left[\\\\nabla_{XW_0} (g)\\\\right]}\\n∇X0\\u200b\\u200bf=XT⋅[∇XW0\\u200b\\u200b(g)]Пример №4\\nf(X)=g(softmax(X))f(X) = g(softmax(X))f(X)=g(softmax(X)), где XXX — матрица N×KN\\\\times KN×K, а softmaxsoftmaxsoftmax — функция, которая вычисляется построчно, причём для каждой строки xxx:\\nsoftmax(x)=(ex1∑text,…,exK∑text)softmax(x) = \\\\left(\\\\frac{e^{x_1}}{\\\\sum_te^{x_t}},\\\\ldots,\\\\frac{e^{x_K}}{\\\\sum_te^{x_t}}\\\\right)\\nsoftmax(x)=(∑t\\u200bext\\u200bex1\\u200b\\u200b,…,∑t\\u200bext\\u200bexK\\u200b\\u200b)В этом примере нам будет удобно воспользоваться формализмом с частными производными. Сначала вычислим ∂sl∂xj\\\\frac{\\\\partial s_l}{\\\\partial x_j}∂xj\\u200b∂sl\\u200b\\u200b для одной строки xxx, где через sls_lsl\\u200b мы для краткости обозначим softmax(x)l=exl∑textsoftmax(x)_l = \\\\frac{e^{x_l}}\\t{\\\\sum_te^{x_t}}softmax(x)l\\u200b=∑t\\u200bext\\u200bexl\\u200b\\u200b. Нетрудно проверить, что\\n∂sl∂xj={sj(1−sj),\\xa0j=l,−slsj,\\xa0j≠l\\\\frac{\\\\partial s_l}{\\\\partial x_j} = \\\\begin{cases}\\ns_j(1 - s_j),\\\\ & j = l,\\\\\\\\\\n-s_ls_j,\\\\ & j\\\\ne l\\n\\\\end{cases}∂xj\\u200b∂sl\\u200b\\u200b={sj\\u200b(1−sj\\u200b),\\xa0−sl\\u200bsj\\u200b,\\xa0\\u200bj=l,j\\ue020=l\\u200bТак как softmax вычисляется независимо от каждой строчки, то\\n∂srl∂xij={sij(1−sij),\\xa0r=i,j=l,−silsij,\\xa0r=i,j≠l,0,\\xa0r≠i,\\\\frac{\\\\partial s_{rl}}{\\\\partial x_{ij}} = \\\\begin{cases}\\ns_{ij}(1 - s_{ij}),\\\\ & r=i, j = l,\\\\\\\\\\n-s_{il}s_{ij},\\\\ & r = i, j\\\\ne l,\\\\\\\\\\n0,\\\\ & r\\\\ne i\\n\\\\end{cases},∂xij\\u200b∂srl\\u200b\\u200b=⎩⎨⎧\\u200bsij\\u200b(1−sij\\u200b),\\xa0−sil\\u200bsij\\u200b,\\xa00,\\xa0\\u200br=i,j=l,r=i,j\\ue020=l,r\\ue020=i\\u200b,где через srls_{rl}srl\\u200b мы обозначили для краткости softmax(X)rlsoftmax(X)_{rl}softmax(X)rl\\u200b.\\nТеперь пусть ∇rl=∇g=∂L∂srl\\\\nabla_{rl} = \\\\nabla g = \\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial s_{rl}}∇rl\\u200b=∇g=∂srl\\u200b∂L\\u200b (пришедший со следующего слоя, уже известный градиент). Тогда\\n∂L∂xij=∑r,l∂srl∂xij∇rl\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial x_{ij}} = \\\\sum_{r,l}\\\\frac{\\\\partial s_{rl}}{\\\\partial x_{ij}} \\\\nabla_{rl}\\n∂xij\\u200b∂L\\u200b=r,l∑\\u200b∂xij\\u200b∂srl\\u200b\\u200b∇rl\\u200bТак как ∂srl∂xij=0\\\\frac{\\\\partial s_{rl}}{\\\\partial x_{ij}} = 0∂xij\\u200b∂srl\\u200b\\u200b=0 при r≠ir\\\\ne ir\\ue020=i, мы можем убрать суммирование по rrr:\\n…=∑l∂sil∂xij∇il=−si1sij∇i1−…+sij(1−sij)∇ij−…−siKsij∇iK=\\\\ldots = \\\\sum_{l}\\\\frac{\\\\partial s_{il}}{\\\\partial x_{ij}} \\\\nabla_{il} = -s_{i1}s_{ij}\\\\nabla_{i1} - \\\\ldots + s_{ij}(1 - s_{ij})\\\\nabla_{ij}-\\\\ldots - s_{iK}s_{ij}\\\\nabla_{iK} =\\n…=l∑\\u200b∂xij\\u200b∂sil\\u200b\\u200b∇il\\u200b=−si1\\u200bsij\\u200b∇i1\\u200b−…+sij\\u200b(1−sij\\u200b)∇ij\\u200b−…−siK\\u200bsij\\u200b∇iK\\u200b==−sij∑tsit∇it+sij∇ij= -s_{ij}\\\\sum_t s_{it}\\\\nabla_{it} + s_{ij}\\\\nabla_{ij}\\n=−sij\\u200bt∑\\u200bsit\\u200b∇it\\u200b+sij\\u200b∇ij\\u200bТаким образом, если мы хотим продифференцировать fff в какой-то конкретной точке X0X_0X0\\u200b, то, смешивая математические обозначения с нотацией Python, мы можем записать:\\n∇X0f==−softmax(X0)⊙sum\\xa0(softmax(X0)⊙∇softmax(X0)g,\\xa0axis=1)+softmax(X0)⊙∇softmax(X0)g\\\\begin{aligned}&\\\\nabla_{X_0}f=\\\\\\\\&=-softmax(X_0)\\\\odot\\\\mathrm{sum~}(softmax(X_0)\\\\odot\\\\nabla_{softmax(X_0)}g,\\\\mathrm{~axis}=1)+\\\\\\\\&softmax(X_0)\\\\odot\\\\nabla_{softmax(X_0)}g\\\\end{aligned}\\n\\u200b∇X0\\u200b\\u200bf==−softmax(X0\\u200b)⊙sum\\xa0(softmax(X0\\u200b)⊙∇softmax(X0\\u200b)\\u200bg,\\xa0axis=1)+softmax(X0\\u200b)⊙∇softmax(X0\\u200b)\\u200bg\\u200bBackward propagation в общем виде\\nПодытожим предыдущее обсуждение, описав алгоритм error backward propagation (алгоритм обратного распространения ошибки). Допустим, у нас есть текущие значения весов W0iW^i_0W0i\\u200b и мы хотим совершить шаг SGD по мини-батчу XXX. Мы должны сделать следующее:\\n\\nСовершить forward propagation, вычислив и запомнив все промежуточные представления X=X0,X1,…,Xm=y^X = X^0, X^1, \\\\ldots, X^m = \\\\widehat{y}X=X0,X1,…,Xm=y\\u200b.\\nВычислить все градиенты с помощью backward propagation.\\nС помощью полученных градиентов совершить шаг SGD.\\n\\nПроиллюстрируем алгоритм на примере двухслойной нейронной сети со скалярным output. Для простоты опустим свободные члены в линейных слоях.\\n\\nОбучаемые параметры – матрицы UUU и WWW. Как найти градиенты по ним в точке U0,W0U_0, W_0U0\\u200b,W0\\u200b?\\n∇W0L=∇W0(12L∘h∘[W↦g(XU0)W])=\\\\nabla_{W_0}\\\\mathcal{L} = \\\\nabla_{W_0}{\\\\left({\\\\vphantom{\\\\frac12}\\\\mathcal{L}\\\\circ h\\\\circ\\\\left[W\\\\mapsto g(XU_0)W\\\\right]}\\\\right)}=\\n∇W0\\u200b\\u200bL=∇W0\\u200b\\u200b(21\\u200bL∘h∘[W↦g(XU0\\u200b)W])==g(XU0)T∇g(XU0)W0(L∘h)=g(XU0)T⏟k×N⋅[12h′(∫01g(XU0)W0)⏟N×1⊙∇h(∫01g(XU0)W0)L⏟N×1]=g(XU_0)^T\\\\nabla_{g(XU_0)W_0}(\\\\mathcal{L}\\\\circ h) = \\\\underbrace{g(XU_0)^T}_{k\\\\times N}\\\\cdot\\n\\\\left[\\\\vphantom{\\\\frac12}\\\\underbrace{h'\\\\left(\\\\vphantom{\\\\int_0^1}g(XU_0)W_0\\\\right)}_{N\\\\times 1}\\\\odot\\n\\\\underbrace{\\\\nabla_{h\\\\left(\\\\vphantom{\\\\int_0^1}g(XU_0)W_0\\\\right)}\\\\mathcal{L}}_{N\\\\times 1}\\\\right]=g(XU0\\u200b)T∇g(XU0\\u200b)W0\\u200b\\u200b(L∘h)=k×Ng(XU0\\u200b)T\\u200b\\u200b⋅\\u200b21\\u200bN×1h′(∫01\\u200bg(XU0\\u200b)W0\\u200b)\\u200b\\u200b⊙N×1∇h(∫01\\u200bg(XU0\\u200b)W0\\u200b)\\u200bL\\u200b\\u200b\\u200bИтого матрица k×1k\\\\times 1k×1, как и W0W_0W0\\u200b\\n∇U0L=∇U0(12L∘h∘[Y↦YW0]∘g∘[U↦XU])=\\\\nabla_{U_0}\\\\mathcal{L} = \\\\nabla_{U_0}\\\\left(\\\\vphantom{\\\\frac12}\\n\\\\mathcal{L}\\\\circ h\\\\circ\\\\left[Y\\\\mapsto YW_0\\\\right]\\\\circ g\\\\circ\\\\left[ U\\\\mapsto XU\\\\right]\\n\\\\right)=∇U0\\u200b\\u200bL=∇U0\\u200b\\u200b(21\\u200bL∘h∘[Y↦YW0\\u200b]∘g∘[U↦XU])==XT⋅∇XU0(12L∘h∘[Y↦YW0]∘g)==X^T\\\\cdot\\\\nabla_{XU^0}\\\\left(\\\\vphantom{\\\\frac12}\\\\mathcal{L}\\\\circ h\\\\circ [Y\\\\mapsto YW_0]\\\\circ g\\\\right) =\\n=XT⋅∇XU0\\u200b(21\\u200bL∘h∘[Y↦YW0\\u200b]∘g)==XT⋅(12g′(XU0)⊙∇g(XU0)[∈01L∘h∘[Y↦YW0])=X^T\\\\cdot\\\\left(\\\\vphantom{\\\\frac12}g'(XU_0)\\\\odot\\n\\\\nabla_{g(XU_0)}\\\\left[\\\\vphantom{\\\\in_0^1}\\\\mathcal{L}\\\\circ h\\\\circ[Y\\\\mapsto YW_0\\\\right]\\n\\\\right)=XT⋅(21\\u200bg′(XU0\\u200b)⊙∇g(XU0\\u200b)\\u200b[∈01\\u200bL∘h∘[Y↦YW0\\u200b])=…=XTD×N⋅(12g′(XU0)⏟N×K⊙[∫01(h′(∫01g(XU0)W0)⏟N×1⊙∇h(∫01g(XU0)W0)L⏟N×1)⋅WT⏟1×K]⏟N×K)=\\\\ldots = \\\\underset{D\\\\times N}{X^T}\\\\cdot\\\\left(\\\\vphantom{\\\\frac12}\\n\\\\underbrace{g'(XU_0)}_{N\\\\times K}\\\\odot\\n\\\\underbrace{\\\\left[\\\\vphantom{\\\\int_0^1}\\\\left(\\n\\\\underbrace{h'\\\\left(\\\\vphantom{\\\\int_0^1}g(XU_0)W_0\\\\right)}_{N\\\\times1}\\\\odot\\\\underbrace{\\\\nabla_{h(\\\\vphantom{\\\\int_0^1}g\\\\left(XU_0\\\\right)W_0)}\\\\mathcal{L}}_{N\\\\times 1}\\n\\\\right)\\\\cdot \\\\underbrace{W^T}_{1\\\\times K}\\\\right]}_{N\\\\times K}\\n\\\\right)=…=D×NXT\\u200b⋅\\u200b21\\u200bN×Kg′(XU0\\u200b)\\u200b\\u200b⊙N×K\\u200b∫01\\u200b\\u200bN×1h′(∫01\\u200bg(XU0\\u200b)W0\\u200b)\\u200b\\u200b⊙N×1∇h(∫01\\u200bg(XU0\\u200b)W0\\u200b)\\u200bL\\u200b\\u200b\\u200b⋅1×KWT\\u200b\\u200b\\u200b\\u200b\\u200b\\u200bИтого D×KD\\\\times KD×K, как и U0U_0U0\\u200b\\nСхематически это можно представить следующим образом:\\n\\nBackward propagation для двухслойной нейронной сети\\nЕсли вы не уследили за вычислениями в предыдущем примере, давайте более подробно разберём его чуть более конкретную версию (для g=h=σg = h = \\\\sigmag=h=σ)\\nРассмотрим двуслойную нейронную сеть для классификации. Мы уже встречали её ранее при рассмотрении линейно неразделимой выборки. Предсказания получаются следующим образом:\\ny^=σ(X1W2)=σ((σ(X0W1))W2).\\\\widehat{y} = \\\\sigma(X^1 W^2) = \\\\sigma\\\\Big(\\\\big(\\\\sigma(X^0 W^1 )\\\\big) W^2 \\\\Big).\\ny\\u200b=σ(X1W2)=σ((σ(X0W1))W2).Пусть W01W^1_0W01\\u200b и W02W^2_0W02\\u200b — текущее приближение матриц весов. Мы хотим совершить шаг по градиенту функции потерь, и для этого мы должны вычислить её градиенты по W1W^1W1 и W2W^2W2 в точке (W01,W02)(W^1_0, W^2_0)(W01\\u200b,W02\\u200b).\\nПрежде всего мы совершаем forward propagation, в ходе которого мы должны запомнить все промежуточные представления: X1=X0W01X^1 = X^0 W^1_0X1=X0W01\\u200b, X2=σ(X0W01)X^2 = \\\\sigma(X^0 W^1_0)X2=σ(X0W01\\u200b), X3=σ(X0W01)W02X^3 = \\\\sigma(X^0 W^1_0) W^2_0X3=σ(X0W01\\u200b)W02\\u200b, X4=σ(σ(X0W01)W02)=y^X^4 = \\\\sigma(\\\\sigma(X^0 W^1_0) W^2_0) = \\\\widehat{y}X4=σ(σ(X0W01\\u200b)W02\\u200b)=y\\u200b. Они понадобятся нам дальше.\\nДля полученных предсказаний вычисляется значение функции потерь:\\nl=L(y,y^)=ylog\\u2061(y^)+(1−y)log\\u2061(1−y^).l = \\\\mathcal{L}(y, \\\\widehat{y}) = y \\\\log(\\\\widehat{y}) + (1-y) \\\\log(1-\\\\widehat{y}).\\nl=L(y,y\\u200b)=ylog(y\\u200b)+(1−y)log(1−y\\u200b).Дальше мы шаг за шагом будем находить производные по переменным из всё более глубоких слоёв.\\n\\n\\nГрадиент L\\\\mathcal{L}L по предсказаниям имеет вид\\n∇y^l=yy^−1−y1−y^=y−y^y^(1−y^),    \\\\nabla_{\\\\widehat{y}}l = \\\\frac{y}{\\\\widehat{y}} - \\\\frac{1 - y}{1 - \\\\widehat{y}} = \\\\frac{y - \\\\widehat{y}}{\\\\widehat{y} (1 - \\\\widehat{y})},\\n∇y\\u200b\\u200bl=y\\u200by\\u200b−1−y\\u200b1−y\\u200b=y\\u200b(1−y\\u200b)y−y\\u200b\\u200b,где, напомним, y^=σ(X3)=σ((σ(X0W01))W02)\\\\widehat{y} = \\\\sigma(X^3) = \\\\sigma\\\\Big(\\\\big(\\\\sigma(X^0 W^1_0 )\\\\big) W^2_0 \\\\Big)y\\u200b=σ(X3)=σ((σ(X0W01\\u200b))W02\\u200b) (обратите внимание на то, что W01W^1_0W01\\u200b и W02W^2_0W02\\u200b тут именно те, из которых мы делаем градиентный шаг).\\n\\n\\nСледующий слой — поэлементное взятие σ\\\\sigmaσ. Как мы помним, при переходе через него градиент поэлементно умножается на производную σ\\\\sigmaσ, в которую подставлено предыдущее промежуточное представление:\\n∇X3l=σ′(X3)⊙∇y^l=σ(X3)(1−σ(X3))⊙y−y^y^(1−y^)=    \\\\nabla_{X^3}l = \\\\sigma'(X^3)\\\\odot\\\\nabla_{\\\\widehat{y}}l = \\\\sigma(X^3)\\\\left( 1 - \\\\sigma(X^3) \\\\right) \\\\odot \\\\frac{y - \\\\widehat{y}}{\\\\widehat{y} (1 - \\\\widehat{y})} = \\n∇X3\\u200bl=σ′(X3)⊙∇y\\u200b\\u200bl=σ(X3)(1−σ(X3))⊙y\\u200b(1−y\\u200b)y−y\\u200b\\u200b==σ(X3)(1−σ(X3))⊙y−σ(X3)σ(X3)(1−σ(X3))=    = \\\\sigma(X^3)\\\\left( 1 - \\\\sigma(X^3) \\\\right) \\\\odot \\\\frac{y - \\\\sigma(X^3)}{\\\\sigma(X^3) (1 - \\\\sigma(X^3))} = \\n=σ(X3)(1−σ(X3))⊙σ(X3)(1−σ(X3))y−σ(X3)\\u200b=∇W02l=(X2)T⋅∇X3l=(X2)T⋅(y−σ(X3))=    \\\\color{blue}{\\\\nabla_{W^2_0}l} = (X^2)^T\\\\cdot \\\\nabla_{X^3}l = (X^2)^T\\\\cdot(y - \\\\sigma(X^3)) = \\n∇W02\\u200b\\u200bl=(X2)T⋅∇X3\\u200bl=(X2)T⋅(y−σ(X3))==(σ(X0W01))T⋅(y−σ(σ(X0W01)W02))    = \\\\color{blue}{\\\\left( \\\\sigma(X^0W^1_0) \\\\right)^T \\\\cdot (y - \\\\sigma(\\\\sigma(X^0W^1_0)W^2_0))}\\n=(σ(X0W01\\u200b))T⋅(y−σ(σ(X0W01\\u200b)W02\\u200b))Аналогичным образом\\n∇X2l=∇X3l⋅(W02)T=(y−σ(X3))⋅(W02)T=    \\\\nabla_{X^2}l = \\\\nabla_{X^3}l\\\\cdot (W^2_0)^T = (y - \\\\sigma(X^3))\\\\cdot (W^2_0)^T = \\n∇X2\\u200bl=∇X3\\u200bl⋅(W02\\u200b)T=(y−σ(X3))⋅(W02\\u200b)T==(y−σ(X2W02))⋅(W02)T    = (y - \\\\sigma(X^2W_0^2))\\\\cdot (W^2_0)^T\\n=(y−σ(X2W02\\u200b))⋅(W02\\u200b)T\\n\\nСледующий слой — снова взятие σ\\\\sigmaσ.\\n∇X1l=σ′(X1)⊙∇X2l=σ(X1)(1−σ(X1))⊙((y−σ(X2W02))⋅(W02)T)=    \\\\nabla_{X^1}l = \\\\sigma'(X^1)\\\\odot\\\\nabla_{X^2}l = \\\\sigma(X^1)\\\\left( 1 - \\\\sigma(X^1) \\\\right) \\\\odot \\\\left( (y - \\\\sigma(X^2W_0^2))\\\\cdot (W^2_0)^T \\\\right) = \\n∇X1\\u200bl=σ′(X1)⊙∇X2\\u200bl=σ(X1)(1−σ(X1))⊙((y−σ(X2W02\\u200b))⋅(W02\\u200b)T)==σ(X1)(1−σ(X1))⊙((y−σ(σ(X1)W02))⋅(W02)T)    = \\\\sigma(X^1)\\\\left( 1 - \\\\sigma(X^1) \\\\right) \\\\odot\\\\left(  (y - \\\\sigma(\\\\sigma(X^1)W_0^2))\\\\cdot (W^2_0)^T \\\\right)\\n=σ(X1)(1−σ(X1))⊙((y−σ(σ(X1)W02\\u200b))⋅(W02\\u200b)T)\\n\\nНаконец, последний слой — это умножение X0X^0X0 на W01W^1_0W01\\u200b. Тут мы дифференцируем только по W1W^1W1:\\n∇W01l=(X0)T⋅∇X1l=(X0)T⋅(σ(X1)(1−σ(X1))⊙(y−σ(σ(X1)W02))⋅(W02)T)=    \\\\color{blue}{\\\\nabla_{W^1_0}l} = (X^0)^T\\\\cdot \\\\nabla_{X^1}l = (X^0)^T\\\\cdot \\\\big( \\\\sigma(X^1) \\\\left( 1 - \\\\sigma(X^1) \\\\right) \\\\odot (y - \\\\sigma(\\\\sigma(X^1)W_0^2))\\\\cdot (W^2_0)^T\\\\big) =\\n∇W01\\u200b\\u200bl=(X0)T⋅∇X1\\u200bl=(X0)T⋅(σ(X1)(1−σ(X1))⊙(y−σ(σ(X1)W02\\u200b))⋅(W02\\u200b)T)==(X0)T⋅(σ(X0W01)(1−σ(X0W01))⊙(y−σ(σ(X0W01)W02))⋅(W02)T)    = \\\\color{blue}{(X^0)^T\\\\cdot\\\\big(\\\\sigma(X^0W^1_0)\\\\left( 1 - \\\\sigma(X^0W^1_0) \\\\right) \\\\odot (y - \\\\sigma(\\\\sigma(X^0W^1_0)W_0^2))\\\\cdot (W^2_0)^T\\\\big) }\\n=(X0)T⋅(σ(X0W01\\u200b)(1−σ(X0W01\\u200b))⊙(y−σ(σ(X0W01\\u200b)W02\\u200b))⋅(W02\\u200b)T)\\n\\nИтоговые формулы для градиентов получились страшноватыми, но они были получены друг из друга итеративно с помощью очень простых операций: матричного и поэлементного умножения, в которые порой подставлялись значения заранее вычисленных промежуточных представлений.\\nАвтоматизация и autograd\\nИтак, чтобы нейросеть обучалась, достаточно для любого слоя fk:Xk−1↦Xkf^k: X^{k-1}\\\\mapsto X^kfk:Xk−1↦Xk с параметрами WkW^kWk уметь:\\n\\nпревращать ∇X0kL\\\\nabla_{X^k_0}\\\\mathcal{L}∇X0k\\u200b\\u200bL в ∇X0k−1L\\\\nabla_{X^{k-1}_0}\\\\mathcal{L}∇X0k−1\\u200b\\u200bL (градиент по выходу в градиент по входу);\\nсчитать градиент по его параметрам ∇W0kL\\\\nabla_{W^k_0}\\\\mathcal{L}∇W0k\\u200b\\u200bL.\\n\\nПри этом слою совершенно не надо знать, что происходит вокруг. То есть слой действительно может быть запрограммирован как отдельная сущность, умеющая внутри себя делать forward propagation и backward propagation, после чего слои механически, как кубики в конструкторе, собираются в большую сеть, которая сможет работать как одно целое.\\nБолее того, во многих случаях авторы библиотек для глубинного обучения уже о вас позаботились и создали средства для автоматического дифференцирования выражений (autograd). Поэтому, программируя нейросеть, вы почти всегда можете думать только о forward-проходе, прямом преобразовании данных, предоставив библиотеке дифференцировать всё самостоятельно.\\nЭто делает код нейросетей весьма понятным и выразительным (да, в реальности он тоже бывает большим и страшным, но сравните на досуге код какой-нибудь разухабистой нейросети и код градиентного бустинга на решающих деревьях и почувствуйте разницу).\\nНо это лишь начало\\nМетод обратного распространения ошибки позволяет удобно посчитать градиенты, но дальше с ними что-то надо делать, и старый добрый SGD едва ли справится с обучением современной сетки. Так что же делать? О некоторых приёмах мы расскажем в следующем параграфе.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф5.2. Первое знакомство с полносвязными нейросетямиОсновные понятия глубинного обучения. Базовые слои и\\xa0функции активацииСледующий параграф5.4. Тонкости обученияИнициализация весов. Регуляризация нейросетей. Dropout и\\xa0BatchnormЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_34.html', 'title': 'Нормализующие потоки'}, page_content=\"Нормализующие потокиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/68.1.Введение в генеративное моделирование8.2.Variational Autoencoder (VAE)8.3.Генеративно-состязательные сети (GAN)8.4.Нормализующие потокиВведениеМотивацияОпределениеРазвитие идеиOut-of-distribution detectionСравнение с другими типами генеративных моделей8.5.Диффузионные модели8.6.Языковые модели9.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Нормализующие потоки8.4. Нормализующие потокиАвторыВалерий КлюкинВведение\\nВ главе Генеративный подход к классификации мы уже познакомились с типом моделей, которые оценивают совместное распределение p(X,Y)p(X,Y)p(X,Y). Такие модели называют генеративными. Для простоты предположим, что мы имеем всего один класс, тогда задача моделирования P(X,Y)P(X,Y)P(X,Y) сводится к задаче моделирования p(X)p(X)p(X). Научившись моделировать это распределение, мы сможем:\\n\\nгенерировать объекты x∼pθdatax \\\\sim p_\\\\theta^\\\\mathcal{data}x∼pθdata\\u200b, где θ\\\\thetaθ – параметры модели;\\nоценивать вероятность встретить данный объект x\\\\mathbf{x}x среди набора наблюдаемых данных D\\\\mathcal{D}D;\\nвыучивать скрытые представления для объекта x\\\\mathbf{x}x.\\n\\nИзвестными примерами генеративных моделей являются:\\n\\nАвторегрессионные модели:\\n\\npθ(x)=∏i=1npθ(xi∣x<i),p_\\\\theta(\\\\mathbf{x}) = \\\\prod_{i=1}^n p_\\\\theta(x_i \\\\vert \\\\mathbf{x}_{<i}),\\npθ\\u200b(x)=i=1∏n\\u200bpθ\\u200b(xi\\u200b∣x<i\\u200b),x∈Rn,x \\\\in \\\\mathbb{R}^n,\\nx∈Rn,\\nВариационные автокодировщики:\\n\\npθ(x)=∫pθ(x,z)dz,p_\\\\theta(\\\\mathbf{x}) = \\\\int p_\\\\theta(\\\\mathbf{x}, \\\\mathbf{z}) d\\\\mathbf{z},\\npθ\\u200b(x)=∫pθ\\u200b(x,z)dz,Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступитьz∈Rm.\\\\mathbf{z} \\\\in \\\\mathbb{R}^m.\\nz∈Rm.Но оба эти метода не позволяют одновременно:\\n\\nполучать скрытые представления для объектов\\nточно вычислять функцию правдоподобия\\n\\nНормализующие потоки способны решить обе эти задачи.\\nМотивация\\n\\nПусть x∼px(x)\\\\mathbb{x} \\\\sim p_x(\\\\mathbb{x})x∼px\\u200b(x), где px(x)p_x(\\\\mathbb{x})px\\u200b(x) неизвестно, а z∼pz(z)=N(0,I)\\\\mathbb{z} \\\\sim p_z(\\\\mathbb{z}) = \\\\mathcal{N}(0, I)z∼pz\\u200b(z)=N(0,I). Мы хотим найти отображение fθ:Rn→Rnf_{\\\\theta} : \\\\mathbb{R}^n \\\\rightarrow \\\\mathbb{R}^nfθ\\u200b:Rn→Rn, для которого x=fθ(z)\\\\mathbb{x} = f_{\\\\theta}(\\\\mathbb{z})x=fθ\\u200b(z) и z=fθ−1(x)\\\\mathbb{z} = f_{\\\\theta}^{-1}(\\\\mathbb{x})z=fθ−1\\u200b(x).\\nОтображение fff преобразует базовую функцию плотности pzp_zpz\\u200b к более сложной pxp_xpx\\u200b. С его помощью мы можем генерировать сложный объект путем сэмплинга простого объекта z\\\\mathbb{z}z (скрытой переменной) из распределения pzp_zpz\\u200b и применения «генератора» f(z)=xf(\\\\mathbb{z}) = \\\\mathbb{x}f(z)=x. Обратное отображение f−1f^{-1}f−1 «нормализует» сложное распределение pxp_xpx\\u200b, приводя его к простому pzp_zpz\\u200b.\\nНайдя такое отображение fff, мы сможем генерировать новые объекты x\\\\mathbb{x}x, а оценить плотность px(x)p_x(\\\\mathbb{x})px\\u200b(x) поможет формула преобразования плотности случайной величины. Давайте её вспомним.\\nФормула замены переменной\\nПусть x∼px(x)\\\\mathbb{x} \\\\sim p_x(\\\\mathbb{x})x∼px\\u200b(x), z∼pz(z)\\\\mathbb{z} \\\\sim p_z(\\\\mathbb{z})z∼pz\\u200b(z), при этом отображение f:Rn→Rnf : \\\\mathbb{R}^n \\\\rightarrow \\\\mathbb{R}^nf:Rn→Rn дифференцируемо, обратимо и x=f(z)\\\\mathbb{x} = f(\\\\mathbb{z})x=f(z). Тогда:\\npx(x)=pz(f−1(x))⋅∣det(Jf−1)∣,p_x(\\\\mathbb{x}) = p_z(f^{-1}(\\\\mathbb{x})) \\\\cdot \\\\bigg| \\\\text{det} \\\\left( J_{f^{-1}} \\\\right) \\\\bigg|,\\npx\\u200b(x)=pz\\u200b(f−1(x))⋅\\u200bdet(Jf−1\\u200b)\\u200b,где det(Jf−1)\\\\text{det} \\\\left( J_{f^{-1}} \\\\right)det(Jf−1\\u200b) – якобиан отображения f−1f^{-1}f−1.\\nОпределение\\nИтак, нормализующий поток – это обратимое дифференцируемое отображение fθ:Rn→Rnf_{\\\\theta} : \\\\mathbb{R}^n \\\\rightarrow \\\\mathbb{R}^nfθ\\u200b:Rn→Rn, которое переводит исходные представления объектов в скрытые: x=fθ(z)\\\\mathbb{x} = f_{\\\\theta}(\\\\mathbb{z})x=fθ\\u200b(z) и z=fθ−1(x)\\\\mathbb{z} = f_{\\\\theta}^{-1}(\\\\mathbb{x})z=fθ−1\\u200b(x).\\nПри этом функция правдоподобия px(x)p_x(\\\\mathbb{x})px\\u200b(x) вычисляется по формуле:\\npx(x)=pz(f−1(x))⋅∣det(Jf−1)∣p_x(\\\\mathbb{x}) = p_z(f^{-1}(\\\\mathbb{x})) \\\\cdot \\\\bigg| \\\\text{det} \\\\left( J_{f^{-1}} \\\\right) \\\\bigg|\\npx\\u200b(x)=pz\\u200b(f−1(x))⋅\\u200bdet(Jf−1\\u200b)\\u200bУмея вычислять функцию правдоподобия, мы можем обучать нашу модель fθf_\\\\thetafθ\\u200b методом максимума правдоподобия (ММП):\\nlog\\u2061\\xa0px(D;θ)=∑i=1Mlog\\xa0pz(fθ−1(x))+log\\u2061\\xa0∣det(Jf−1)∣.\\\\log \\\\space p_x(\\\\mathcal{D}; \\\\theta) = \\\\sum_{i=1}^M \\\\text{log} \\\\space p_z(f^{-1}_\\\\theta(\\\\mathbb{x})) + \\\\log \\\\space \\\\bigg| \\\\text{det} \\\\left( J_{f^{-1}} \\\\right) \\\\bigg|.\\nlog\\xa0px\\u200b(D;θ)=i=1∑M\\u200blog\\xa0pz\\u200b(fθ−1\\u200b(x))+log\\xa0\\u200bdet(Jf−1\\u200b)\\u200b.где D={x(i)}i=1M\\\\mathcal{D} = \\\\{ x^{(i)}\\\\}_{i=1}^MD={x(i)}i=1M\\u200b – выборка наблюдаемых данных из распределения pxp_xpx\\u200b.\\nОбычно модель нормализующего потока составляет композицию из KKK не очень сложных отображений, чтобы она была, с одной стороны, достаточно контролируемой, а с другой – достаточно выразительной:\\nf=f1∘f2∘⋯∘fKf = f_1 \\\\circ f_2 \\\\circ \\\\dots \\\\circ f_K\\nf=f1\\u200b∘f2\\u200b∘⋯∘fK\\u200bТогда якобиан вычисляется по формуле:\\ndet\\u2061(Jf−1)=∏i=1Kdet\\u2061(Jfi−1)\\\\det \\\\left( J_{f^{-1}} \\\\right) = \\\\prod_{i=1}^{K} \\\\det \\\\left( J_{f^{-1}_i} \\\\right)\\ndet(Jf−1\\u200b)=i=1∏K\\u200bdet(Jfi−1\\u200b\\u200b)Но вычисление якобиана является очень затратной операцией. Для того, чтобы мы могли обучать модели эффективно на высокоразмерных данных (аудио, изображения), необходимо использовать такое отображение fff, подсчет якобиана которого был бы эффективен!\\n\\nВ отличе от VAE и GANов, нормализующие потоки требуют вычисления функции правдоподобия, воэтому важно уметь эффективно вычислять функцию правдоподобия. Метод максимального правдоподобия позволяет обучать нормализующие потоки стабильнее в сравнении с GAN-ами, а возможность быстро и точно вычислять значение функции правдоподобия выделяет нормализующие потоки на фоне VAE и диффузионных моделей.\\n\\nПримером такого отображения является планарный поток (Planar Flow), где отображение fff принадлежит следующему семейству функций:\\nx=fθ(z)=z+uθh(wθ⊤z+bθ),\\\\mathbb{x} = f_\\\\theta(\\\\mathbb{z}) = \\\\mathbb{z} + \\\\mathbb{u}_\\\\theta h (\\\\mathbb{w}_\\\\theta ^ \\\\top \\\\mathbb{z} + b_\\\\theta),\\nx=fθ\\u200b(z)=z+uθ\\u200bh(wθ⊤\\u200bz+bθ\\u200b),где uθ,wθ,bθ\\\\mathbb{u}_\\\\theta, \\\\mathbb{w}_\\\\theta, b_\\\\thetauθ\\u200b,wθ\\u200b,bθ\\u200b – обучаемые параметры, а hhh – гладкая нелинейная функция, например, tanh\\\\text{tanh}tanh.\\nЯкобиан такого отображения можно будет посчитать за O(n)O(n)O(n). Обозначим\\nψ(z)=h′(wθ⊤z+bθ)wθ.\\\\psi(\\\\mathbb{z}) = h'(\\\\mathbb{w}_\\\\theta^\\\\top \\\\mathbb{z} + b_\\\\theta) \\\\mathbb{w}_\\\\theta.\\nψ(z)=h′(wθ⊤\\u200bz+bθ\\u200b)wθ\\u200b.Тогда\\n∣det\\u2061(Jf)∣=∣det\\u2061(I+uθψ(z)⊤)∣=∣(1+uθ⊤ψ(z))∣.\\\\bigg| \\\\det \\\\left( J_{f} \\\\right) \\\\bigg| = \\\\big| \\\\det( \\\\mathbb{I} + \\\\mathbb{u}_\\\\theta \\\\psi(\\\\mathbb{z})^\\\\top) \\\\big| = \\\\big| (1 + \\\\mathbb{u}_\\\\theta^\\\\top \\\\psi(\\\\mathbb{z})) \\\\big|.\\n\\u200bdet(Jf\\u200b)\\u200b=\\u200bdet(I+uθ\\u200bψ(z)⊤)\\u200b=\\u200b(1+uθ⊤\\u200bψ(z))\\u200b.\\nДействие планарного нормализующего потока на нормальное и равномерное распределение (ссылка на статью).\\nРазвитие идеи\\nВ планарных потоках нам удалось быстро посчитать якобиан, потому что матрица имела специальный вид (сумма единичной и низкоранговой). Но мы знаем и другие случаи, когда определитель можно посчитать быстрее – треугольные матрицы. Их определитель равен произведению элементов на диагонали.\\nСледующие модели активно использовали этот трюк.\\nNICE: Non-linear Independent Component Estimation и RealNVP\\nАвторы модели NICE предложили использовать в качестве fθf_\\\\thetafθ\\u200b следующее семейство преобразований:\\nx=fθ(z)={x1:d=z1:dxd+1:n=zd+1:n+mθ(z1:d),\\\\mathbb{x} = f_\\\\theta(\\\\mathbb{z}) =\\n\\\\begin{cases}\\n    \\\\mathbb{x}_{1:d} = \\\\mathbb{z}_{1:d} \\\\\\\\\\n    \\\\mathbb{x}_{d+1:n} = \\\\mathbb{z}_{d+1:n} + m_\\\\theta(\\\\mathbb{z}_{1:d})\\n\\\\end{cases},\\nx=fθ\\u200b(z)={x1:d\\u200b=z1:d\\u200bxd+1:n\\u200b=zd+1:n\\u200b+mθ\\u200b(z1:d\\u200b)\\u200b,где 1<d<n1<d<n1<d<n, а mθm_\\\\thetamθ\\u200b – произвольная нейросеть с ddd входами и n−dn - dn−d выходами. Такое преобразование называют аддитивным связыванием (additive coupling).\\nОбратное преобразование вычисляется с такой же легкостью, а якобиан равен 111. То есть, px(x)=pz(f−1(x))p_x(\\\\mathbb{x}) = p_z(f^{-1}(\\\\mathbb{x}))px\\u200b(x)=pz\\u200b(f−1(x)), что является довольно сильным ограничением модели.\\nДалее, из-за того, что\\nx1:d=z1:d,\\\\mathbb{x}_{1:d} = \\\\mathbb{z}_{1:d},\\nx1:d\\u200b=z1:d\\u200b,первые ddd каналов вектора xxx совпадают с координатами нормального шума zzz, то есть моделирования этих каналов xxx не происходит. Из-за этого выразительная способность модели NICE была относительно невысокой.\\nПозже авторы NICE позже предложили использовать между слоями нормализующих потоков зафиксированные перестановки признаков/каналов x\\\\mathbb{x}x, что стало основой работы RealNVP. Использование перестановок позволяет добиться того того, чтобы все выходные каналы оказались затронуты преобразованием fθ(z)f_\\\\theta(z)fθ\\u200b(z); при этом градиент перестановки вычисляется легко.\\nx=fθ(z)={x1:d=z1:dxd+1:n=exp\\u2061(sθ(z1:d))⊙zd+1:n+mθ(z1:d),\\\\mathbb{x} = f_\\\\theta(\\\\mathbb{z}) =\\n\\\\begin{cases}\\n    \\\\mathbb{x}_{1:d} = \\\\mathbb{z}_{1:d} \\\\\\\\\\n    \\\\mathbb{x}_{d+1:n} = \\\\exp(s_\\\\theta(\\\\mathbb{z}_{1:d})) \\\\odot \\\\mathbb{z}_{d+1:n} + m_\\\\theta(\\\\mathbb{z}_{1:d})\\n\\\\end{cases},\\nx=fθ\\u200b(z)={x1:d\\u200b=z1:d\\u200bxd+1:n\\u200b=exp(sθ\\u200b(z1:d\\u200b))⊙zd+1:n\\u200b+mθ\\u200b(z1:d\\u200b)\\u200b,где ⊙\\\\odot⊙ – поэлементное умножение, а sθs_\\\\thetasθ\\u200b – нейросеть, которая может быть произвольной, но, как правило, выбирается такой же архитектуры, как и mθm_\\\\thetamθ\\u200b. Такое преобразование называют аффинным связыванием (affine coupling).\\nПолучившееся отображение тоже легко обращается, а его якобиан равен:\\ndet\\u2061(Jf−1)=exp\\u2061∑i=d+1n(sθ(z1:d))i\\\\det \\\\left( J_{f^{-1}} \\\\right) = \\\\exp \\\\sum_{i=d+1}^n ( s_\\\\theta(\\\\mathbb{z}_{1:d}) )_i\\ndet(Jf−1\\u200b)=expi=d+1∑n\\u200b(sθ\\u200b(z1:d\\u200b))i\\u200bЗаметим, что, как и в случае аддитивного связывания, значительная часть каналов остается неизменной при использовании аффинного связывания. Для того, чтобы преобразование fθ(x)f_\\\\theta(x)fθ\\u200b(x) моделировало распределение xxx во всех каналах, на разных слоях неизменными оставляют разные подмножества из ddd каналов.\\nЧтобы улучшить сходимость глубоких (K>1K > 1K>1) нормализующих потоков, авторы предложили использовать Batch Normalization. Данное преобразование тоже является обратимым, а его якобиан вычисляется крайне просто.\\nВ результате, выразительная способность модели сильно повысилась, и она стала способна выучивать сложные распределения:\\n\\nMasked Autoregressive Flows\\nСсылка на статью\\nДанный вид нормализующих потоков также обладает нижнетреугольным якобианом, но он использует другое семейство функций:\\nxi=ziexp\\u2061(fαi(x1:i−1))+fμi(x1:i−1),x_i = z_i \\\\exp( f_{\\\\alpha_i} (\\\\mathbb{x}_{1:i-1}) ) + f_{\\\\mu_i} (\\\\mathbb{x}_{1:i-1}),\\nxi\\u200b=zi\\u200bexp(fαi\\u200b\\u200b(x1:i−1\\u200b))+fμi\\u200b\\u200b(x1:i−1\\u200b),где μi(x1:i−1)\\\\mu_i(x_{1:i-1})μi\\u200b(x1:i−1\\u200b) и αi(x1:i−1)\\\\alpha_i(x_{1:i-1})αi\\u200b(x1:i−1\\u200b) – нейросети произвольной архитектуры.\\nКак видно из формулы, xix_ixi\\u200b напрямую зависит от x1:i−1x_{1:i-1}x1:i−1\\u200b. Таким образом, элементы генерируются авторегрессивно, что и дало название архитектуре.\\nЯкобиан такого преобразования вычисляется по следующей формуле:\\ndet\\u2061(Jf−1)=exp\\u2061(−∑i=1nfαi(x1:i−1))\\\\det \\\\left( J_{f^{-1}} \\\\right) = \\\\exp\\\\bigg( -\\\\sum_{i=1}^n f_{\\\\alpha_i} (\\\\mathbb{x}_{1:i-1}) \\\\bigg)\\ndet(Jf−1\\u200b)=exp(−i=1∑n\\u200bfαi\\u200b\\u200b(x1:i−1\\u200b))Таким образом, шаг генерации выглядит следующим образом:\\n\\nz∼N(0,I)z \\\\sim  \\\\mathcal{N}(0, I)z∼N(0,I)\\nx1=z1exp\\u2061(α1)+μ1x_1 = z_1  \\\\exp(\\\\alpha_1) + \\\\mu_1x1\\u200b=z1\\u200bexp(α1\\u200b)+μ1\\u200b\\nx2=z2exp\\u2061(fα2(x1))+fμ2(x1)x_2 = z_2  \\\\exp(f_{\\\\alpha_2}(x_1)) + f_{\\\\mu_2}(x_1)x2\\u200b=z2\\u200bexp(fα2\\u200b\\u200b(x1\\u200b))+fμ2\\u200b\\u200b(x1\\u200b)\\n...\\n\\nОднако вычисление скрытых переменных zzz не является авторегрессивным:\\nzi=(xi−fμi(x1:i−1))exp\\u2061(−fαi(x1:i−1))z_i = (x_i - f_{\\\\mu_i}(x_{1:i-1})) \\\\exp(-f_{\\\\alpha_i} (x_{1:i-1}))\\nzi\\u200b=(xi\\u200b−fμi\\u200b\\u200b(x1:i−1\\u200b))exp(−fαi\\u200b\\u200b(x1:i−1\\u200b))Несмотря на то, что данная разновидность нормализующих потоков кажется более мощной моделью, её трудно применить на практике к данным высокой размерности. Это происходит из-за того, что генерация нового объекта осуществляется авторегрессивно по координатам, что становится слишком затратным при обучении на высокоразмерных данных, например, на изображениях.\\nInverse Autoregressive Flows\\nСсылка на статью\\nЧтобы быстро генерировать объекты из сложного распределения, мы можем избавиться от авторегрессивности на шаге генерации, поставив в авторегрессивную зависимость не наблюдаемые, а латентные переменные:\\nzi=(xi−fμi(z1:i−1))exp\\u2061(−fαi(z1:i−1))z_i = (x_i - f_{\\\\mu_i}(z_{1:i-1})) \\\\exp(-f_{\\\\alpha_i} ({z}_{1:i-1}))\\nzi\\u200b=(xi\\u200b−fμi\\u200b\\u200b(z1:i−1\\u200b))exp(−fαi\\u200b\\u200b(z1:i−1\\u200b))Можем заметить, что проблема долгого вычисления авторегрессивных выражений никуда не уходит. Мы лишь изменяем построение модели таким образом, чтобы генерировать объекты x\\\\mathbb{x}x быстрее:\\nxi=zi⋅exp\\u2061(fαi(z1:i−1))+fμi(z1:i−1)x_i = z_i \\\\cdot \\\\exp(f_{\\\\alpha_i}(z_{1:i-1})) + f_{\\\\mu_i}(z_{1:i-1})\\nxi\\u200b=zi\\u200b⋅exp(fαi\\u200b\\u200b(z1:i−1\\u200b))+fμi\\u200b\\u200b(z1:i−1\\u200b)Но вычисление z\\\\mathbb{z}z, а значит и правдоподобия, становится долгим, и обучение занимает больше времени.\\nЗвук\\nНормализующие потоки стали наиболее актуальны в задаче генерации звука, поскольку они обладают достаточно высокой выразительностью и эффективностью, чтобы быстро генерировать аудиозаписи высокого качества. В этом контексте, модель нормализующего потока должна генерировать аудио, получая на вход описание того, что ей необходимо сгенерировать. То есть модель обуславливается на дополнительные признаки.\\nНормализующие потоки могут быть обусловлены на входные данные путем использования дополнительных входных данных в качестве переменной, от которой зависят преобразования, применяемые к данным. Обусловливающей переменной может быть любая дополнительная информация, имеющая отношение к задаче генерации, такая как текстовые описания, изображения или другие характеристики данных.\\nВ контексте генерации аудио обуславливающей переменной обычно служит mel-спектрограмма, которая позволяет отобразить интенсивность различных частот аудио-сигнала в разные моменты времени.\\n\\nНормализующий поток учится генерировать сигнал в виде waveform-а на основе спектрограммы путем обратного преобразования.\\n\\nЧтобы генерировать более длинные фрагменты звука, модель генерирует короткие звуковые кадры (фреймы) за раз, которые затем объединяются для формирования полного waveform-а.\\n\\nТеперь мы готовы узнать про применение нормализующих потоков в генерации аудио!\\nProbability Density Distillation и Parallel WaveNet\\nСсылка на статью\\nАрхитектура Inverse Autoregressive Flow (IAF) была изначально предложена для задачи генерации аудио. Она позволяет генерировать объекты крайне эффективно, но обучение методом максимального правдоподобия занимает много времени из-за авторегрессивности вычислений. Метод Probability Density Estimation позволяет решить эту проблему с помощью использования второй предобученной авторегрессивной модели в качестве учителя. IAF обучается в качестве модели-студента, минимизируя KL дивергенцию KL(pS∣∣pT),\\\\text{KL}(p_S \\\\vert\\\\vert p_T),KL(pS\\u200b∣∣pT\\u200b), где pSp_SpS\\u200b и pTp_TpT\\u200b – распределения студента и учителя соответственно. Ключевым достижением данного подхода является то, что вычисление функции потерь требует вычисления кросс-энтропии между учителем и студентом, а не правдоподобия, что позволяет максимально распараллелить все вычисления ввиду отсутствия авторегрессивности в вычислениях.\\nВместе с тем в данной работе в качестве учителя выбирается не случайная модель, а оригинальная авторегрессивная модель WaveNet, которая в 2016 году позволила достичь state-of-the-art качества генерации аудио. Эта модель является не нормализующим потоком, а обыкновенной авторегрессивной моделью, которая обучается предсказывать следующий кусочек аудио (фрейм) длиной в несколько миллисекунд.\\n\\nТаким образом, с помощью IAF и Probability Density Distillation авторам удалось ускорить генерацию более чем в 1000 раз без потери качества!\\n\\nНа картинке выше мы видим, что модель использует лингвистические признаки для генерации аудио. Эта задача является примером задачи условной генерации, где на вход модели подается спектрограмма, сгенерированная отдельной моделью по тексту, а на выход ожидается речь в аудио-формате (waveform). О том, как модель использует дополнительную информацию для обуславливания, поговорим в главе про Waveglow\\nGlow\\nИсследователи из OpenAI в 2018 году опубликовали работу Glow: Generative Flow with Invertible 1×1 Convolutions, которая значительно улучшает результаты модели RealNVP. Опишем два главых улучшения.\\nВо-первых, для перемешивания каналов Glow использует обратимые свертки с ядром 1x1 вместо фиксированной матрицы перестановок каналов в RealNVP;\\nЭто нововведение является по-настоящему красивым, так как в нем предлагается способ вычисления якобиана 2D-свертки за O(n)O(n)O(n). А именно, логарифм якобиана такой 1x1-свертки с числом каналов nnn для тензора размера h×w×nh \\\\times w \\\\times nh×w×n равен hw⋅log\\u2061∣det\\u2061(W)∣hw \\\\cdot \\\\log \\\\vert \\\\det(W) \\\\verthw⋅log∣det(W)∣, где WWW – матрица свёртки 1х1.\\nАвторы предлагают использовать следующий вариант LU-разложения для матрицы WWW:\\nW=PL(U+diag(s)),W = PL(U + \\\\text{diag}(s)),\\nW=PL(U+diag(s)),где PPP – фиксированная матрица перестановок, LLL – нижнетреугольная матрица с единицами на диагонали, UUU – верхнетреугольная матрица с нулями на диагонали, а sss – обучаемый вектор.\\nНетрудно показать, что\\nlog\\u2061∣det\\u2061(W)∣=sum(log\\u2061(s))\\\\log | \\\\det(W) | = \\\\text{sum}(\\\\log(s))\\nlog∣det(W)∣=sum(log(s))Благодаря этому авторам удалось снизить сложность вычислений якобиана с O(n3)O(n^3)O(n3) до O(n)O(n)O(n)\\nКроме того, для улучшения сходимости использовали собственно разработанный actnorm-слой (activation normalization). Поскольку нормализующие потоки требуют много вычислительных ресурсов, для обучения используются мини-батчи маленького размера, из-за чего батч-нормализация работает не очень хорошо. Авторы предлагают использовать следующий тип нормализации – actnorm:\\nxi,j′=s⊙xi,j+bx'_{i,j} = s \\\\odot x_{i,j} + b\\nxi,j′\\u200b=s⊙xi,j\\u200b+b\\nНормализуем входной тензор (промежуточное изображение) по размерности каналов;\\nИнициализируем параметры смещения bbb и разброса sss статистиками с первого батча;\\nДалее обучаем их в качестве обычных параметров.\\n\\nТаким образом, один блок нормализующего потока выглядит так:\\n\\nWaveGlow\\nСсылка на статью\\nВторым важным с практической точки зрения применением нормализующих потоков стала модель WaveGlow. Она представляет собой версию модели Glow, адаптированную для генерации речи по тексту.\\nКак мы помним, эта задача также является примером задачи условной генерации:\\nlog\\u2061\\xa0px∣c(D;θ)=∑i=1Mlog\\xa0pz∣c(fθ−1(x,c)∣c)+log\\u2061\\xa0∣det(Jf−1)∣.\\\\log \\\\space p_{x|c}(\\\\mathcal{D}; \\\\theta) = \\\\sum_{i=1}^M \\\\text{log} \\\\space p_{z|c}(f^{-1}_\\\\theta(\\\\mathbb{x}, \\\\mathbb{c}) | \\\\mathbb{c}) + \\\\log \\\\space \\\\bigg| \\\\text{det} \\\\left( J_{f^{-1}} \\\\right) \\\\bigg|.\\nlog\\xa0px∣c\\u200b(D;θ)=i=1∑M\\u200blog\\xa0pz∣c\\u200b(fθ−1\\u200b(x,c)∣c)+log\\xa0\\u200bdet(Jf−1\\u200b)\\u200b.На практике это приводит к тому, что все распределения в нашей формуле становятся условными. Таким образом, при генерации мы также сэмплируем из условного распределения z∼pz∣c(z∣c)z \\\\sim p_{z|c}(z|c)z∼pz∣c\\u200b(z∣c), а в слоях нормализующих потоков используем преобразования xi=fi(xi−1,c)x_i = f_i(x_{i-1}, c)xi\\u200b=fi\\u200b(xi−1\\u200b,c).\\nВ качестве обуславливающего фактора ccc для WaveGlow мы имеем сгенерированную по тексту mel-спектрограмму, а на выходе ожидаем получить соответствующую тексту и спектрограмме аудио-запись. Как мы видим на изображении и в формулах ниже, mel-спектрограмма используется как дополнительный признак для нейросети, генерирующей параметры афинного преобразования. В качестве модели, которая производит параметры афинного преобразования, используется похожая на WaveNet архитектура с dilated-свертками.\\nПравая часть схемы ниже более подробно показывает строение слоя affine coupling:\\n\\n(xa,xb)=split(x)(log\\u2061s,t)=WN(xa,mel-spectrogram)xb′=s⊙xa+tfcoupling−1(x)w=concat(xa,xb′)(x_a, x_b) = \\\\text{split}(x) \\\\\\\\\\n(\\\\log s, t) = WN(x_a, \\\\text{mel-spectrogram}) \\\\\\\\\\nx_b' = s \\\\odot x_a + t \\\\\\\\\\nf_{coupling}^{-1} (x)w = \\\\text{concat}(x_a, x_b')\\n(xa\\u200b,xb\\u200b)=split(x)(logs,t)=WN(xa\\u200b,mel-spectrogram)xb′\\u200b=s⊙xa\\u200b+tfcoupling−1\\u200b(x)w=concat(xa\\u200b,xb′\\u200b)Операция splitsplitsplit разделяет тензор xxx пополам на два тензора меньшей размерности xax_axa\\u200b и xbx_bxb\\u200b для их последующего участия в слое аффинного связывания (affine coupling).\\nПример генерации:\\nИсточник\\n\\nGround truthWaveNetWaveGlow\\n\\n\\n\\n\\n\\nOut-of-distribution detection\\nМожет показаться, что способность точно и эффективно вычислять функцию правдоподобия может позволить без труда обнаруживать аномалии в данных, что может пригодиться во многих приложениях. Однако в работе Kirichenko et al. на примере задачи генерации изображений было показано, что нормализующие потоки выучивают отображение картинок в латентное пространство, основываясь на локальных корреляциях пикселей и графических деталях, а не на семантическом контенте. Из-за этого правдоподобие OOD-объектов может быть выше, чем правдоподобие in-distribution сэмплов.\\n\\nОднако позже было предложено использовать ряд эвристик для того, чтобы улучшить способность к детекции аномалий за счет подсчета значения функции правдоподобия:\\n\\nИспользовать значение правдоподобия второй модели потока, обученного на отличном от исходного датасете (например, ImageNet при исходном CelebA). А затем вычислять отношенение этих двух значений для вынесения вердикта об аномальности объекта. Schirrmeister et al.\\nВ работе Serrà et al. показали, что проблема качества нормализующих потоков в задаче детекции аномалий связана с чрезмерным влиянием сложности входных данных на значение функции правдоподобия. Поэтому авторы предложили использовать в качестве поправки размер сжатого изображения с помощью одного из алгоритмов компрессии (JPEG2000/PNG).\\n\\n\\nСравнение с другими типами генеративных моделей\\nОбратимся к статье Bond-Taylor et al., в которой приводится количественный анализ всех существующих семейств генеративных моделей в задаче генерации изображений из датасета CIFAR-10.\\n\\nВ таблице выше указано, насколько представители каждого из популярных семейств генеративных моделей эффективны в следующих аспектах решения задачи:\\n\\nскорость обучения;\\nскорость генерации;\\nчисло обучаемых параметров;\\nразрешение генерируемого изображения;\\nограничение на форму якобиана;\\nвозможность вычислять правдоподобие объекта;\\nFID (Fréchet Inception Distance) тестовой выборки;\\nОтрицательный логарифм правдоподобия тестовой выборки.\\n\\nЗа расшифровкой обратимся к таблице ниже:\\n\\nПодведя итог, можно сказать, что нормализующие потоки:\\n\\nтребуют очень много времени на обучение, так как при обучении проводятся нетривиальные неоптимизированные вычисления;\\nимеют скорость генерации, сравнимую с GAN-ами;\\nменее эффективны по соотношению качество/число параметров, чем GAN-ы и диффузионные модели;\\nпозволяют быстро вычислять точное значение функции правдоподобия объекта;\\nобладают сравнительно неплохим качеством генерации, проигрывающим GAN-ам и диффузионным моделям.\\n\\nИтак, нормализующие потоки явно выделяются среди других семейств генеративных моделей своими свойствами – обратимостью и способностью вычислять правдоподобие объекта. Но если для решения задачи они не требуются, то имеет смысл попробовать другие модели – в первую очередь, GAN-ы и диффузионные модели.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф8.3. Генеративно-состязательные сети (GAN)Следующий параграф8.5. Диффузионные моделиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_63.html', 'title': 'Матричное дифференцирование'}, page_content=\"Матричное дифференцированиеЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/716.1.Матричное дифференцированиеОсновные обозначенияПростые примеры и свойства матричного дифференцированияПростые примеры вычисления производнойПримеры вычисления производных сложных функцийВторая производнаяПримеры вычисления и использования второй производной16.2.Матричная факторизация16.3.Вероятностные распределения16.4.Многомерные распределения16.5.Независимость и условные распределения вероятностей16.6.Параметрические оценки16.7.Энтропия и семейство экспоненциальных распределенийГлавная/Хендбуки/Учебник по машинному обучению/Матричное дифференцирование16.1. Матричное дифференцированиеАвторыФедотов СтаниславКак дифференцировать матрицы и\\xa0дифференцировать по\\xa0матрицам: всё, что вам не\\xa0рассказали про дифференцирование на\\xa0матанализеЛюбая задача машинного обучения — это задача оптимизации, а задачи оптимизации удобнее всего решать градиентными методами (если это возможно, конечно). Поэтому важно уметь находить производные всего, что попадается под руку. Казалось бы, в чём проблема: ведь дифференцирование — простая и понятная штука (чего не скажешь, например, об интегрировании). Зачем же как-то специально учиться дифференцировать матрицы?\\nДа в принципе-то никаких проблем: в этом параграфе вы не узнаете никаких секретных приёмов или впечатляющих теорем. Но, согласитесь, если исходная функция от вектора xxx имела вид f(x)=∣∣Ax−b∣∣2f(x) = \\\\vert\\\\vert Ax - b\\\\vert\\\\vert^2f(x)=∣∣Ax−b∣∣2 (где AAA — константная матрица, а bbb — постоянный вектор), то хотелось бы уметь и производную выражать красиво и цельно через буквы AAA, xxx и bbb, не привлекая отдельные координаты AijA_{ij}Aij\\u200b, xkx_kxk\\u200b и bsb_sbs\\u200b. Это не только эстетически приятно, но и благотворно сказывается на производительности наших вычислений: ведь матричные операции обычно очень эффективно оптимизированы в библиотеках, чего не скажешь о самописных циклах по i,j,k,si, j, k, si,j,k,s. И всё, что будет происходить дальше, преследует очень простую цель: научиться вычислять производные в удобном, векторно-матричном виде. А чтобы сделать это и не сойти с ума, мы должны ввести ясную систему обозначений, составляющую ядро техники матричного дифференцирования.\\nОсновные обозначения\\nВспомним определение производной для функции f:Rm→Rnf:\\\\mathbb{R}^m\\\\rightarrow\\\\mathbb{R}^nf:Rm→Rn. Функция f(x)f(x)f(x) дифференцируема в точке x0x_0x0\\u200b, если\\nf(x0+h)=f(x0)+[Dx0f](h)+oˉˉ(∣∣h∣∣),f(x_0 + h) = f(x_0) + \\\\color{#348FEA}{\\\\left[D_{x_0} f \\\\right]} (h) + \\\\bar{\\\\bar{o}} \\\\left(\\\\left| \\\\left| h\\\\right|\\\\right|\\\\right),\\nf(x0\\u200b+h)=f(x0\\u200b)+[Dx0\\u200b\\u200bf](h)+oˉˉ(∣∣h∣∣),где [Dx0f]\\\\color{#348FEA}{\\\\big[D_{x_0} f\\\\big]}[Dx0\\u200b\\u200bf] — дифференциал функции fff: линейное отображение из мира xxx-ов в мир значений fff. Грубо говоря, он превращает «малое приращение h=Δxh=\\\\Delta xh=Δx» в «малое приращение Δf\\\\Delta fΔf» («малые» в том смысле, что на о-малое можно плюнуть):Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nf(x0+h)−f(x0)≈[Dx0f](h)f(x_0 + h) - f(x_0)\\\\approx\\\\color{#348FEA}{\\\\left[D_{x_0} f \\\\right]} (h)\\nf(x0\\u200b+h)−f(x0\\u200b)≈[Dx0\\u200b\\u200bf](h)Отметим, что дифференциал зависит от точки x0x_0x0\\u200b, в которой он берётся: [Dx0f](h)\\\\color{#348FEA}{\\\\left[D_{\\\\color{red}{x_0}} f \\\\right]} (h)[Dx0\\u200b\\u200bf](h). Под ∣∣h∣∣\\\\vert\\\\vert h\\\\vert\\\\vert∣∣h∣∣ подразумевается норма вектора hhh, например корень из суммы квадратов координат (обычная евклидова длина).\\nДавайте рассмотрим несколько примеров и заодно разберёмся, какой вид может принимать выражение [Dx0f](h)\\\\color{#348FEA}{\\\\big[D_{x_0} f\\\\big]} (h)[Dx0\\u200b\\u200bf](h) в зависимости от формы xxx. Начнём со случаев, когда fff — скалярная функция.\\nПримеры конкретных форм [Dx0f](h)\\\\big[D_{x_0} f\\\\big] (h)[Dx0\\u200b\\u200bf](h), когда fff — скалярная функция\\nf(x)f(x)f(x) — скалярная функция, xxx — скаляр. Тогда\\n\\nf(x0+h)−f(x0)≈f′(x0)⋅hf(x_0 + h) - f(x_0) \\\\approx f'(x_0) \\\\cdot h\\nf(x0\\u200b+h)−f(x0\\u200b)≈f′(x0\\u200b)⋅h[Dx0f](h)=f′(x0)h=h⋅f′(x0)\\\\color{#348FEA}{\\\\left[D_{x_0} f\\\\right] (h)} = f'(x_0) h = h \\\\cdot f'(x_0)\\n[Dx0\\u200b\\u200bf](h)=f′(x0\\u200b)h=h⋅f′(x0\\u200b)Здесь hhh и f′(x0)f'(x_0)f′(x0\\u200b) — просто числа. В данном случае [Dx0f]\\\\color{#348FEA}{\\\\left[D_{x_0} f\\\\right]}[Dx0\\u200b\\u200bf] — это обычная линейная функция.\\n\\nf(x)f(x)f(x) — скалярная функция, xxx — вектор. Тогда\\n\\nf(x0+h)−f(x0)≈∑i∂f∂xi∣x=x0hi,f(x_0 + h) - f(x_0) \\\\approx \\\\sum\\\\limits_i \\\\left.\\\\frac{\\\\partial f}{\\\\partial x_i} \\\\right|_{x=x_0} h_i,\\nf(x0\\u200b+h)−f(x0\\u200b)≈i∑\\u200b∂xi\\u200b∂f\\u200b\\u200bx=x0\\u200b\\u200bhi\\u200b,то есть\\n[Dx0f](h)=(∇x0f)Th=⟨∇x0f,h⟩,\\t\\\\color{#348FEA}{\\\\left[D_{x_0} f\\\\right]}(h) = \\\\left(\\\\color{#FFC100}{\\\\nabla_{x_0} f}\\\\right)^T h = \\\\langle\\\\color{#FFC100}{\\\\nabla_{x_0} f}, h \\\\rangle, \\n[Dx0\\u200b\\u200bf](h)=(∇x0\\u200b\\u200bf)Th=⟨∇x0\\u200b\\u200bf,h⟩,где ⟨∙,∙⟩\\\\langle\\\\bullet, \\\\bullet\\\\rangle⟨∙,∙⟩ — операция скалярного произведения, а ∇x0f=(∂f∂x1,…,∂f∂xn)\\\\color{#FFC100}{\\\\nabla_{x_0} f} = \\\\left(\\\\frac{\\\\partial f}{\\\\partial x_1}, \\\\ldots, \\\\frac{\\\\partial f}{\\\\partial x_n}\\\\right)∇x0\\u200b\\u200bf=(∂x1\\u200b∂f\\u200b,…,∂xn\\u200b∂f\\u200b) — градиент функции fff.\\n\\nf(x)f(x)f(x) — скалярная функция, XXX — матрица. Дифференциал скалярной функции по матричному аргументу определяется следующим образом:\\n\\nf(X0+H)−f(X0)≈∑i,j\\xa0∂f∂Xij∣X=X0Hij\\tf(X_0 + H) - f(X_0) \\\\approx \\\\sum\\\\limits_{i,j}\\\\ \\\\left.\\\\frac{\\\\partial f}{\\\\partial X_{ij}} \\\\right|_{X=X_0} H_{ij}\\nf(X0\\u200b+H)−f(X0\\u200b)≈i,j∑\\u200b\\xa0∂Xij\\u200b∂f\\u200b\\u200bX=X0\\u200b\\u200bHij\\u200bМожно заметить, что это стандартное определение дифференциала функции многих переменных для случая, когда переменные — элементы матрицы XXX. Заметим также один интересный факт:\\n∑ijAijBij=trATB,\\t\\\\sum_{ij} A_{ij} B_{ij} = \\\\text{tr} A^T B,\\nij∑\\u200bAij\\u200bBij\\u200b=trATB,где AAA и BBB — произвольные матрицы одинакового размера. Объединяя оба приведённых выше факта, получаем:\\n[DX0f](H)=∑ij∂f∂Xij∣X=X0(X−X0)ij=tr\\u2009([∂f∂Xij∣X=X0]TH).\\t\\\\color{#348FEA}{\\\\left[D_{X_0} f \\\\right]} (H) \\n\\t= \\\\sum \\\\limits_{ij} \\n\\t\\t\\\\left.\\n\\t\\t\\t\\\\frac{\\\\partial f}{\\\\partial X_{ij}}\\n\\t\\t\\\\right|_{X = X_0} \\n\\t\\t\\\\left( \\n\\t\\t\\tX - X_0\\n\\t\\t\\\\right)_{ij}\\n\\t= \\\\text{tr}\\\\, \\n\\t\\t\\\\left( \\\\left[\\\\left. \\\\frac{\\\\partial f}{\\\\partial X_{ij}}\\\\right|_{X=X_0}\\\\right]^T H\\\\right).\\n[DX0\\u200b\\u200bf](H)=ij∑\\u200b∂Xij\\u200b∂f\\u200b\\u200bX=X0\\u200b\\u200b(X−X0\\u200b)ij\\u200b=tr\\u200b[∂Xij\\u200b∂f\\u200b\\u200bX=X0\\u200b\\u200b]TH\\u200b.Можно заметить, что здесь, по аналогии с примерами, где xxx — скаляр и где xxx — вектор (и f(x)f(x)f(x) — скалярная функция), получилось на самом деле скалярное произведение градиента функции fff по переменным XijX_{ij}Xij\\u200b и приращения. Этот градиент мы записали для удобства в виде матрицы с теми же размерами, что матрица XXX.\\nВ примерах выше нам дважды пришлось столкнуться с давним знакомцем из матанализа: градиентом скалярной функции (у нескалярных функций градиента не бывает). Напомним, что градиент ∇x0f\\\\color{#FFC100}{\\\\nabla_{x_0} f}∇x0\\u200b\\u200bf функции в точке x0x_0x0\\u200b состоит из частных производных этой функции по всем координатам аргумента. При этом его обычно упаковывают в ту же форму, что и сам аргумент: если xxx — вектор-строка, то и градиент записывается вектор-строкой, а если xxx — матрица, то и градиент тоже будет матрицей того же размера. Это важно, потому что для осуществления градиентного спуска мы должны уметь прибавлять градиент к точке, в которой он посчитан.\\nКак мы уже имели возможность убедиться, для градиента скалярной функции fff выполнено равенство\\n[Dx0f](x−x0)=⟨∇x0f,x−x0⟩,    \\\\left[D_{x_0} f \\\\right] (x-x_0) = \\\\langle\\\\color{#FFC100}{\\\\nabla_{x_0} f}, x-x_0\\\\rangle,\\n[Dx0\\u200b\\u200bf](x−x0\\u200b)=⟨∇x0\\u200b\\u200bf,x−x0\\u200b⟩,где скалярное произведение — это сумма попарных произведений соответствующих координат (да-да, самое обыкновенное).\\nПосмотрим теперь, как выглядит дифференцирование для функций, которые на выходе выдают не скаляр, а что-то более сложное.\\nПримеры [Dx0f](h)\\\\big[D_{x_0} f\\\\big] (h)[Dx0\\u200b\\u200bf](h), где fff — это вектор или матрица\\n\\n\\nf(x)=(f(x1)⋮f(xm))f(x) = \\\\begin{pmatrix} f(x_1)\\\\\\\\ \\\\vdots\\\\\\\\ f(x_m) \\\\end{pmatrix}\\nf(x)=\\u200bf(x1\\u200b)⋮f(xm\\u200b)\\u200b\\u200bxxx — вектор. Тогда\\nf(x0+h)−f(x0)=(f(x01+h1)−f(x01)⋮f(x0m+hm)−f(x0m))≈(f′(x01)h1⋮f′(x0m)hm)=(f′(x01)⋮f′(x0m))⊙h.f(x_0 + h) - f(x_0) = \\n\\t\\\\begin{pmatrix}\\n\\t\\tf(x_{01} + h_1) - f(x_{01})\\\\\\\\\\n\\t\\t\\\\vdots \\\\\\\\\\n\\t\\tf(x_{0m} + h_m) - f(x_{0m}) \\n\\t\\\\end{pmatrix}\\n\\\\approx \\n\\t\\\\begin{pmatrix}\\n\\t\\tf'(x_{01}) h_1\\\\\\\\\\n\\t\\t\\\\vdots  \\\\\\\\\\n\\t\\tf'(x_{0m}) h_m\\n\\t\\\\end{pmatrix} \\n=\\n\\t\\\\begin{pmatrix}\\n\\t\\tf'(x_{01}) \\\\\\\\\\n\\t\\t\\\\vdots \\\\\\\\\\n\\t\\tf'(x_{0m})\\n\\t\\\\end{pmatrix}\\n\\t\\\\odot\\n\\t\\th.\\nf(x0\\u200b+h)−f(x0\\u200b)=\\u200bf(x01\\u200b+h1\\u200b)−f(x01\\u200b)⋮f(x0m\\u200b+hm\\u200b)−f(x0m\\u200b)\\u200b\\u200b≈\\u200bf′(x01\\u200b)h1\\u200b⋮f′(x0m\\u200b)hm\\u200b\\u200b\\u200b=\\u200bf′(x01\\u200b)⋮f′(x0m\\u200b)\\u200b\\u200b⊙h.В последнем выражении происходит покомпонентное умножение:\\n[Dx0f](h)=f′(x0)⊙h=h⊙f′(x0)\\\\color{#348FEA}{\\\\big[D_{x_0} f\\\\big]} (h) = f'(x_0) \\\\odot h = h \\\\odot f'(x_0)\\n[Dx0\\u200b\\u200bf](h)=f′(x0\\u200b)⊙h=h⊙f′(x0\\u200b)\\nf(X)=XWf(X) = XWf(X)=XW, где XXX и WWW — матрицы. Тогда\\n\\nf(X0+H)−f(X0)=(X0+H)W−X0W=HW,f(X_0 + H) - f(X_0) = (X_0 + H) W - X_0 W = H W,\\nf(X0\\u200b+H)−f(X0\\u200b)=(X0\\u200b+H)W−X0\\u200bW=HW,то есть\\n[DX0f](H)=HW\\\\color{#348FEA}{\\\\big[D_{X_0} f\\\\big]} (H) = H W\\n[DX0\\u200b\\u200bf](H)=HW\\nf(W)=XWf(W) = XWf(W)=XW, где XXX и WWW — матрицы. Тогда\\n\\nf(W0+H)−f(W0)=X(W0+H)−XW0=XH,f(W_0 + H) - f(W_0) = X(W_0 + H) - XW_0 = X H,\\nf(W0\\u200b+H)−f(W0\\u200b)=X(W0\\u200b+H)−XW0\\u200b=XH,то есть\\n[DW0f](H)=XH\\\\color{#348FEA}{\\\\big[D_{W_0} f\\\\big]} (H) = X H\\n[DW0\\u200b\\u200bf](H)=XH\\nf(x)=(f1(x),…,fK(x))f(x) = (f_1(x),\\\\ldots,f_K(x))f(x)=(f1\\u200b(x),…,fK\\u200b(x)) — вектор-строка, x=(x1,…,xD)x = (x_1,\\\\ldots,x_D)x=(x1\\u200b,…,xD\\u200b) — вектор-строка. Тогда\\n\\n[Dx0f](h)=(∑j∂f1∂yj∣y=x0hj,…,∑j∂fK∂yj∣y=x0hj)=\\\\color{#348FEA}{\\\\big[D_{x_0} f\\\\big]}(h)\\n= \\\\left(\\\\sum_j \\n\\\\left. \\n\\t\\\\frac{\\\\partial f_1}{\\\\partial y_j} \\n\\t\\\\right|_{y=x_0}h_j, \\\\ldots, \\\\sum_j \\\\left. \\\\frac{\\\\partial f_K}{\\\\partial y_j} \\\\right|_{y=x_0}h_j \\\\right) =\\n[Dx0\\u200b\\u200bf](h)=(j∑\\u200b∂yj\\u200b∂f1\\u200b\\u200b\\u200by=x0\\u200b\\u200bhj\\u200b,…,j∑\\u200b∂yj\\u200b∂fK\\u200b\\u200b\\u200by=x0\\u200b\\u200bhj\\u200b)==h⋅(∂f1∂y1∣y=x0…∂fk∂y1∣y=x0⋮⋮∂f1∂yD∣y=x0…∂fk∂yD∣y=x0)=h⋅∂f∂y∣y=x0= h \\\\cdot \\n\\\\begin{pmatrix} \\n\\t\\\\left.\\n\\t\\t\\\\frac{\\\\partial f_1}{\\\\partial y_1}\\n\\t\\\\right|_{y=x_0} & \\\\ldots & \\n\\t\\\\left.\\n\\t\\t\\\\frac{\\\\partial f_k}{\\\\partial y_1}\\n\\t\\\\right|_{y=x_0} \\\\\\\\\\n\\t\\\\vdots & & \\\\vdots \\\\\\\\\\n\\t\\\\left.\\n\\t\\t\\\\frac{\\\\partial f_1}{\\\\partial y_D}\\n\\t\\\\right|_{y=x_0} & \\\\ldots & \\n\\t\\\\left.\\n\\t\\t\\\\frac{\\\\partial f_k}{\\\\partial y_D}\\n\\t\\\\right|_{y=x_0}\\\\\\\\\\n\\\\end{pmatrix}\\n= h \\\\cdot \\n\\\\left.\\n\\t\\\\frac{\\\\partial f}{\\\\partial y}\\\\right|_{y = x_0}\\n=h⋅\\u200b∂y1\\u200b∂f1\\u200b\\u200b\\u200by=x0\\u200b\\u200b⋮∂yD\\u200b∂f1\\u200b\\u200b\\u200by=x0\\u200b\\u200b\\u200b……\\u200b∂y1\\u200b∂fk\\u200b\\u200b\\u200by=x0\\u200b\\u200b⋮∂yD\\u200b∂fk\\u200b\\u200b\\u200by=x0\\u200b\\u200b\\u200b\\u200b=h⋅∂y∂f\\u200b\\u200by=x0\\u200b\\u200bМатрица, выписанная в предпоследней выкладке, — это знакомая вам из курса матанализа матрица Якоби.\\nПростые примеры и свойства матричного дифференцирования\\n\\n\\nПроизводная константы. Пусть f(x)=af(x) = af(x)=a. Тогда\\nf(x0+h)−f(x0)=0,f(x_0 + h) - f(x_0) = 0,\\nf(x0\\u200b+h)−f(x0\\u200b)=0,то есть [Dx0f]\\\\color{#348FEA}{\\\\big[D_{x_0} f\\\\big]}[Dx0\\u200b\\u200bf] — это нулевое отображение. А если fff — скалярная функция, то и ∇x0f=0.\\\\color{#FFC100}{\\\\nabla_{x_0} f} = 0.∇x0\\u200b\\u200bf=0.\\n\\n\\nПроизводная линейного отображения. Пусть f(x)f(x)f(x) — линейное отображение. Тогда\\nf(x0+h)−f(x0)=f(x0)+f(h)−f(x0)=f(h)f(x_0 + h) - f(x_0) = f(x_0) + f(h) - f(x_0) = f(h)\\nf(x0\\u200b+h)−f(x0\\u200b)=f(x0\\u200b)+f(h)−f(x0\\u200b)=f(h)Поскольку справа линейное отображение, то по определению оно и является дифференциалом [Dx0f]\\\\color{#348FEA}{\\\\big[D_{x_0} f\\\\big]}[Dx0\\u200b\\u200bf]. Мы уже видели примеры таких ситуаций выше, когда рассматривали отображения умножения на матрицу слева или справа. Если fff — (скалярная) линейная функция, то она представляется в виде ⟨a,v⟩\\\\langle a, v\\\\rangle⟨a,v⟩ для некоторого вектора aaa — он и будет градиентом fff.\\n\\n\\nЛинейность производной. Пусть f(x)=λu(x)+μv(x)f(x) = \\\\lambda u(x) + \\\\mu v(x)f(x)=λu(x)+μv(x), где λ,μ\\\\lambda, \\\\muλ,μ — скаляры, а u,vu, vu,v — некоторые отображения, тогда\\n[Dx0f]=λ[Dx0u]+μ[Dx0v]\\\\color{#348FEA}{\\\\big[D_{x_0} f\\\\big]} = \\\\lambda \\\\color{#348FEA}{\\\\big[D_{x_0} u\\\\big]} + \\\\mu \\\\color{#348FEA}{\\\\big[D_{x_0} v\\\\big]}\\n[Dx0\\u200b\\u200bf]=λ[Dx0\\u200b\\u200bu]+μ[Dx0\\u200b\\u200bv]\\n\\nПопробуйте доказать сами, прежде чем смотреть доказательство. f(x0+h)−f(x0)=(λu(x0+h)+μv(x0+h))−(λu(x0)+μv(x0))=f(x_0 + h) - f(x_0) = (\\\\lambda u(x_0 + h) + \\\\mu v(x_0 + h)) - (\\\\lambda u(x_0) + \\\\mu v(x_0)) =\\nf(x0\\u200b+h)−f(x0\\u200b)=(λu(x0\\u200b+h)+μv(x0\\u200b+h))−(λu(x0\\u200b)+μv(x0\\u200b))==λ(u(x0+h)−u(x0))+μ(v(x0+h)−v(x0))≈= \\\\lambda(u(x_0 + h) - u(x_0)) + \\\\mu(v(x_0 + h) - v(x_0)) \\\\approx \\n=λ(u(x0\\u200b+h)−u(x0\\u200b))+μ(v(x0\\u200b+h)−v(x0\\u200b))≈≈λ[Dx0u](h)+μ[Dx0v](h)\\\\approx \\\\lambda \\\\color{#348FEA}{\\\\big[D_{x_0} u\\\\big]}(h) + \\\\mu \\\\color{#348FEA}{\\\\big[D_{x_0} v\\\\big]}(h)\\n≈λ[Dx0\\u200b\\u200bu](h)+μ[Dx0\\u200b\\u200bv](h)\\n\\nПроизводная произведения. Пусть f(x)=u(x)v(x)f(x) = u(x) v(x)f(x)=u(x)v(x), где u,vu, vu,v — некоторые отображения, тогда\\n[Dx0f]=[Dx0u]⋅v(x0)+u(x0)⋅[Dx0v]\\\\color{#348FEA}{\\\\big[D_{x_0} f\\\\big]} = \\\\color{#348FEA}{\\\\big[D_{x_0} u\\\\big]} \\\\cdot v(x_0) + u(x_0) \\\\cdot \\\\color{#348FEA}{\\\\big[D_{x_0} v\\\\big]}\\n[Dx0\\u200b\\u200bf]=[Dx0\\u200b\\u200bu]⋅v(x0\\u200b)+u(x0\\u200b)⋅[Dx0\\u200b\\u200bv]\\n\\nПопробуйте доказать сами, прежде чем смотреть доказательство.Обозначим для краткости x=x0+hx = x_0 + hx=x0\\u200b+h. Тогда\\nu(x)v(x)−u(x0)v(x0)=u(x)v(x)−u(x0)v(x)+u(x0)v(x)−u(x0)v(x0)=u(x)v(x) - u(x_0)v(x_0) = u(x)v(x) - u(x_0)v(x) + u(x_0)v(x) - u(x_0)v(x_0) =\\nu(x)v(x)−u(x0\\u200b)v(x0\\u200b)=u(x)v(x)−u(x0\\u200b)v(x)+u(x0\\u200b)v(x)−u(x0\\u200b)v(x0\\u200b)=(u(x)−u(x0))v(x)+u(x0)(v(x)−v(x0))≈(u(x) - u(x_0))v(x) + u(x_0)(v(x) - v(x_0))\\\\approx \\n(u(x)−u(x0\\u200b))v(x)+u(x0\\u200b)(v(x)−v(x0\\u200b))≈≈[Dx0u](h)⋅v(x)+u(x0)⋅[Dx0v](h)\\\\approx \\\\color{#348FEA}{\\\\big[D_{x_0} u\\\\big]}(h) \\\\cdot v(x) + u(x_0)\\\\cdot \\\\color{#348FEA}{\\\\big[D_{x_0} v\\\\big]}(h)\\n≈[Dx0\\u200b\\u200bu](h)⋅v(x)+u(x0\\u200b)⋅[Dx0\\u200b\\u200bv](h)И всё бы хорошо, да в первом слагаемом v(x)v(x)v(x) вместо v(x0)v(x_0)v(x0\\u200b). Придётся разложить ещё разок:\\n[Dx0u](h)⋅v(x)≈\\\\color{#348FEA}{\\\\big[D_{x_0} u\\\\big]}(h) \\\\cdot v(x) \\\\approx \\n[Dx0\\u200b\\u200bu](h)⋅v(x)≈[Dx0u](h)⋅(v(x0)+[Dx0v](h)+o(∣∣h∣∣))=\\\\color{#348FEA}{\\\\big[D_{x_0} u\\\\big]}(h) \\\\cdot \\\\left(v(x_0) + \\\\color{#348FEA}{\\\\big[D_{x_0} v\\\\big]}(h) + o(\\\\vert\\\\vert h\\\\vert\\\\vert)\\\\right) =\\n[Dx0\\u200b\\u200bu](h)⋅(v(x0\\u200b)+[Dx0\\u200b\\u200bv](h)+o(∣∣h∣∣))=[Dx0u](h)⋅v(x0)+oˉˉ(∣∣h∣∣)\\\\color{#348FEA}{\\\\big[D_{x_0} u\\\\big]}(h) \\\\cdot v(x_0) + \\\\bar{\\\\bar{o}}\\\\left(\\\\vert\\\\vert h\\\\vert\\\\vert\\\\right)\\n[Dx0\\u200b\\u200bu](h)⋅v(x0\\u200b)+oˉˉ(∣∣h∣∣)Это же правило сработает и для скалярного произведения:\\n[Dx0⟨u,v⟩]=⟨[Dx0u],v⟩+⟨u,[Dx0v]⟩\\\\color{#348FEA}{\\\\big[D_{x_0} \\\\langle u, v\\\\rangle\\\\big]} = \\\\langle\\\\color{#348FEA}{\\\\big[D_{x_0} u\\\\big]}, v\\\\rangle + \\\\langle u, \\\\color{#348FEA}{\\\\big[D_{x_0} v\\\\big]}\\\\rangle\\n[Dx0\\u200b\\u200b⟨u,v⟩]=⟨[Dx0\\u200b\\u200bu],v⟩+⟨u,[Dx0\\u200b\\u200bv]⟩В этом нетрудно убедиться, повторив доказательство или заметив, что в доказательстве мы пользовались лишь дистрибутивностью (= билинейностью) умножения.\\n\\n\\nПроизводная сложной функции. Пусть f(x)=u(v(x))f(x) = u(v(x))f(x)=u(v(x)). Тогда\\nf(x0+h)−f(x0)=u(v(x0+h))−u(v(x0))≈f(x_0 + h) - f(x_0) = u(v(x_0 + h)) - u(v(x_0)) \\\\approx \\nf(x0\\u200b+h)−f(x0\\u200b)=u(v(x0\\u200b+h))−u(v(x0\\u200b))≈≈[Dv(x0)u](v(x0+h)−v(x0))≈[Dv(x0)u]([Dx0v](h))\\\\approx\\\\left[D_{v(x_0)} u \\\\right] (v(x_0 + h) - v(x_0)) \\\\approx \\\\left[D_{v(x_0)} u \\\\right] \\\\left( \\\\left[D_{x_0} v\\\\right] (h)\\\\right)\\n≈[Dv(x0\\u200b)\\u200bu](v(x0\\u200b+h)−v(x0\\u200b))≈[Dv(x0\\u200b)\\u200bu]([Dx0\\u200b\\u200bv](h))Здесь Dv(x0)uD_{v(x_0)} uDv(x0\\u200b)\\u200bu — дифференциал uuu в точке v(x0)v(x_0)v(x0\\u200b), а [Dv(x0)u](…)\\\\left[D_{v(x_0)} u \\\\right]\\\\left(\\\\ldots\\\\right)[Dv(x0\\u200b)\\u200bu](…) — это применение отображения [Dv(x0)u]\\\\left[D_{v(x_0)} u \\\\right][Dv(x0\\u200b)\\u200bu] к тому, что в скобках. Итого получаем:\\n[Dx0u∘v](h)=[Dv(x0)u]([Dx0v](h))\\\\left[D_{x_0} \\\\color{#5002A7}{u} \\\\circ \\\\color{#4CB9C0}{v} \\\\right](h) = \\\\color{#5002A7}{\\\\left[D_{v(x_0)} u \\\\right]} \\\\left( \\\\color{#4CB9C0}{\\\\left[D_{x_0} v\\\\right]} (h)\\\\right)\\n[Dx0\\u200b\\u200bu∘v](h)=[Dv(x0\\u200b)\\u200bu]([Dx0\\u200b\\u200bv](h))\\n\\nВажный частный случай: дифференцирование перестановочно с линейным отображением. Пусть f(x)=L(v(x))f(x) = L(v(x))f(x)=L(v(x)), где LLL — линейное отображение. Тогда [Dv(x0)L]\\\\left[D_{v(x_0)} L \\\\right][Dv(x0\\u200b)\\u200bL] совпадает с самим LLL и формула упрощается:\\n[Dx0L∘v](h)=L([Dx0v](h))\\\\left[D_{x_0} \\\\color{#5002A7}{L} \\\\circ \\\\color{#4CB9C0}{v} \\\\right](h) = \\\\color{#5002A7}{L} \\\\left( \\\\color{#4CB9C0}{\\\\left[D_{x_0} v\\\\right]} (h)\\\\right)\\n[Dx0\\u200b\\u200bL∘v](h)=L([Dx0\\u200b\\u200bv](h))\\n\\nПростые примеры вычисления производной\\n\\nВычислим дифференциал и градиент функции f(x)=⟨a,x⟩f(x) = \\\\langle a, x\\\\ranglef(x)=⟨a,x⟩, где xxx — вектор-столбец, aaa — постоянный вектор.\\n\\nПопробуйте вычислить сами, прежде чем смотреть решение.Вычислить производную можно непосредственно:\\nf(x0+h)−f(x0)=⟨a,x0+h⟩−⟨a,x0⟩=⟨a,h⟩f(x_0 + h) - f(x_0) = \\\\langle a, x_0 + h\\\\rangle - \\\\langle a, x_0\\\\rangle = \\\\langle a, h\\\\rangle\\nf(x0\\u200b+h)−f(x0\\u200b)=⟨a,x0\\u200b+h⟩−⟨a,x0\\u200b⟩=⟨a,h⟩Но можно и воспользоваться формулой дифференциала произведения:\\n[Dx0⟨a,x⟩](h)=\\\\color{#348FEA}{\\\\big[D_{x_0} \\\\langle a, x\\\\rangle\\\\big]} (h) = \\n[Dx0\\u200b\\u200b⟨a,x⟩](h)==⟨[Dx0a](h),x⟩+⟨a,[Dx0x](h)⟩=\\\\langle\\\\color{#348FEA}{\\\\big[D_{x_0} a\\\\big]}(h), x\\\\rangle + \\\\langle a, \\\\color{#348FEA}{\\\\big[D_{x_0} x\\\\big]}(h)\\\\rangle\\n=⟨[Dx0\\u200b\\u200ba](h),x⟩+⟨a,[Dx0\\u200b\\u200bx](h)⟩=⟨0,x⟩+⟨a,h⟩=⟨a,h⟩= \\\\langle 0, x\\\\rangle + \\\\langle a, h\\\\rangle = \\\\langle a, h\\\\rangle\\n=⟨0,x⟩+⟨a,h⟩=⟨a,h⟩Сразу видно, что градиент функции равен aaa.\\n\\nВычислим производную и градиент f(x)=⟨Ax,x⟩f(x) = \\\\langle Ax, x\\\\ranglef(x)=⟨Ax,x⟩, где xxx — вектор-столбец, AAA — постоянная матрица.\\n\\nПопробуйте вычислить сами, прежде чем смотреть решение. Снова воспользуемся формулой дифференциала произведения:\\n[Dx0⟨Ax,x⟩](h)=\\\\color{#348FEA}{\\\\big[D_{x_0} \\\\langle Ax, x\\\\rangle\\\\big]}(h) = \\n[Dx0\\u200b\\u200b⟨Ax,x⟩](h)==⟨[Dx0Ax](h),x0⟩+⟨Ax0,[Dx0x](h)⟩= \\\\langle\\\\color{#348FEA}{\\\\big[D_{x_0} Ax\\\\big]}(h), x_0\\\\rangle + \\\\langle Ax_0, \\\\color{#348FEA}{\\\\big[D_{x_0} x\\\\big]}(h)\\\\rangle\\n=⟨[Dx0\\u200b\\u200bAx](h),x0\\u200b⟩+⟨Ax0\\u200b,[Dx0\\u200b\\u200bx](h)⟩=⟨Ah,x0⟩+⟨Ax0,h⟩= \\\\langle Ah, x_0\\\\rangle + \\\\langle Ax_0, h\\\\rangle\\n=⟨Ah,x0\\u200b⟩+⟨Ax0\\u200b,h⟩Чтобы найти градиент, нам надо это выражение представить в виде ⟨?,h⟩\\\\langle ?, h\\\\rangle⟨?,h⟩. Для этого поменяем местами множители первого произведения и перенесём AAA в другую сторону (AAA перенесётся с транспонированием):\\n⟨ATx0,h⟩+⟨Ax0,h⟩=\\\\langle A^Tx_0, h\\\\rangle + \\\\langle Ax_0, h\\\\rangle = \\n⟨ATx0\\u200b,h⟩+⟨Ax0\\u200b,h⟩==⟨(AT+A)x0,h⟩= \\\\langle (A^T + A)x_0, h\\\\rangle\\n=⟨(AT+A)x0\\u200b,h⟩Получается, что градиент в точке x0x_0x0\\u200b равен (AT+A)x0(A^T + A)x_0(AT+A)x0\\u200b.\\n\\nВычислим производную обратной матрицы: f(X)=X−1f(X) = X^{-1}f(X)=X−1, где XXX — квадратная матрица.\\n\\nПопробуйте вычислить сами, прежде чем смотреть решение. Рассмотрим равенство I=X⋅X−1=II = X\\\\cdot X^{-1} = II=X⋅X−1=I и продифференцируем его:\\n0=[DX0(X⋅X−1)](H)=0 = \\\\color{#348FEA}{\\\\big[D_{X_0} \\\\left( X\\\\cdot X^{-1}\\\\right)\\\\big]}(H) = \\n0=[DX0\\u200b\\u200b(X⋅X−1)](H)==[DX0X](H)⋅X0−1+X0⋅[DX0X−1](H)= \\\\color{#348FEA}{\\\\big[D_{X_0} X\\\\big]}(H)\\\\cdot X_0^{-1} + X_0\\\\cdot \\\\color{#348FEA}{\\\\big[D_{X_0} X^{-1}\\\\big]}(H)\\n=[DX0\\u200b\\u200bX](H)⋅X0−1\\u200b+X0\\u200b⋅[DX0\\u200b\\u200bX−1](H)Отсюда уже легко выражается\\n[DX0X−1](H)=−X0−1⋅[DX0X](H)⋅X0−1\\\\color{#348FEA}{\\\\big[D_{X_0} X^{-1}\\\\big]}(H) = -X_0^{-1}\\\\cdot\\\\color{#348FEA}{\\\\big[D_{X_0} X\\\\big]}(H)\\\\cdot X_0^{-1}\\n[DX0\\u200b\\u200bX−1](H)=−X0−1\\u200b⋅[DX0\\u200b\\u200bX](H)⋅X0−1\\u200bОсталось подставить [DX0X](H)=H\\\\color{#348FEA}{\\\\big[D_{X_0} X\\\\big]}(H) = H[DX0\\u200b\\u200bX](H)=H, но запомните и предыдущую формулу, она нам пригодится.\\n\\nВычислим градиент определителя: f(X)=det(X)f(X) = \\\\text{det}(X)f(X)=det(X), где XXX — квадратная матрица.\\n\\nПопробуйте вычислить сами, прежде чем смотреть решение. В предыдущих примерах мы изо всех сил старались не писать матричных элементов, но сейчас, увы, придётся. Градиент функции состоит из её частных производных: ∇x0f=(∂f∂xij)i,j\\\\color{#FFC100}{\\\\nabla_{x_0} f} = \\\\left(\\\\frac{\\\\partial f}{\\\\partial{x_{ij}}}\\\\right)_{i,j}∇x0\\u200b\\u200bf=(∂xij\\u200b∂f\\u200b)i,j\\u200b. Попробуем вычислить ∂f∂xij\\\\frac{\\\\partial f}{\\\\partial{x_{ij}}}∂xij\\u200b∂f\\u200b. Для этого разложим определитель по iii-й строке:\\ndet(X)=∑kxik⋅(−1)i+kMik,\\\\text{det}(X) = \\\\sum_{k}x_{ik}\\\\cdot(-1)^{i + k}M_{ik},\\ndet(X)=k∑\\u200bxik\\u200b⋅(−1)i+kMik\\u200b,где MikM_{ik}Mik\\u200b — это определитель подматрицы, полученной из исходной выбрасыванием iii-й строки и kkk-го столбца. Теперь мы видим, что определитель линеен по переменной xijx_{ij}xij\\u200b, причём коэффициент при ней равен ⋅(−1)i+kMik\\\\cdot(-1)^{i + k}M_{ik}⋅(−1)i+kMik\\u200b. Таким образом,\\n∂f∂xij=(−1)i+kMik\\\\frac{\\\\partial f}{\\\\partial{x_{ij}}} = (-1)^{i + k}M_{ik}\\n∂xij\\u200b∂f\\u200b=(−1)i+kMik\\u200bЧтобы записать матрицу, составленную из таких определителей, покороче, вспомним, что\\nX−1=1det(X)((−1)i+jMji)i,jX^{-1} = \\\\frac1{\\\\text{det}(X)}\\\\left((-1)^{i+j}M_{\\\\color{red}{ji}}\\\\right)_{i,j}\\nX−1=det(X)1\\u200b((−1)i+jMji\\u200b)i,j\\u200bОбратите внимание на переставленные индексы iii и jjj (отмечены красным). Но всё равно похоже! Получается, что\\n∇x0f=det(X)⋅X−T,\\\\color{#FFC100}{\\\\nabla_{x_0} f} = \\\\text{det}(X)\\\\cdot X^{-T},\\n∇x0\\u200b\\u200bf=det(X)⋅X−T,где X−TX^{-T}X−T — это более короткая запись для (X−1)T(X^{-1})^T(X−1)T.\\n\\nВычислим градиент функции f(x)=∣∣Ax−b∣∣2f(x) = \\\\vert\\\\vert Ax - b\\\\vert\\\\vert^2f(x)=∣∣Ax−b∣∣2. С этой функцией мы ещё встретимся, когда будем обсуждать задачу линейной регрессии.\\n\\nПопробуйте вычислить сами, прежде чем смотреть решение. Распишем квадрат модуля в виде скалярного произведения:\\n∣∣Ax−b∣∣2=⟨Ax−b,Ax−b⟩\\\\vert\\\\vert Ax - b\\\\vert\\\\vert^2 = \\\\langle Ax - b, Ax - b\\\\rangle\\n∣∣Ax−b∣∣2=⟨Ax−b,Ax−b⟩Применим формулу дифференциала произведения и воспользуемся симметричностью скалярного произведения:\\n[Dx0⟨Ax−b,Ax−b⟩](h)=\\\\color{#348FEA}{\\\\big[D_{x_0} \\\\langle Ax - b, Ax - b\\\\rangle\\\\big]}(h) = \\n[Dx0\\u200b\\u200b⟨Ax−b,Ax−b⟩](h)=⟨[Dx0(Ax−b)](h),Ax0−b⟩+⟨Ax0−b,[Dx0(Ax−b)](h)⟩\\\\langle \\\\color{#348FEA}{\\\\big[D_{x_0} (Ax - b)\\\\big]}(h), Ax_0 - b\\\\rangle + \\\\langle Ax_0 - b, \\\\color{#348FEA}{\\\\big[D_{x_0} (Ax - b)\\\\big]}(h)\\\\rangle\\n⟨[Dx0\\u200b\\u200b(Ax−b)](h),Ax0\\u200b−b⟩+⟨Ax0\\u200b−b,[Dx0\\u200b\\u200b(Ax−b)](h)⟩=2⟨Ax0−b,[Dx0(Ax−b)](h)⟩== 2\\\\langle Ax_0 - b, \\\\color{#348FEA}{\\\\big[D_{x_0} (Ax - b)\\\\big]}(h)\\\\rangle =\\n=2⟨Ax0\\u200b−b,[Dx0\\u200b\\u200b(Ax−b)](h)⟩==2⟨Ax0−b,Ah⟩=⟨2AT(Ax0−b),h⟩= 2\\\\langle Ax_0 - b, Ah\\\\rangle = \\\\langle 2A^T(Ax_0 - b), h\\\\rangle\\n=2⟨Ax0\\u200b−b,Ah⟩=⟨2AT(Ax0\\u200b−b),h⟩Получаем, что\\n∇x0f=2AT(Ax0−b)\\\\color{#FFC100}{\\\\nabla_{x_0} f} = 2A^T(Ax_0 - b)\\n∇x0\\u200b\\u200bf=2AT(Ax0\\u200b−b)Примеры вычисления производных сложных функций\\n\\nВычислим градиент функции f(X)=log(det(X))f(X) = \\\\text{log}(\\\\text{det}(X))f(X)=log(det(X)).\\n\\nПопробуйте вычислить сами, прежде чем смотреть решение. Вспомним формулу производной сложной функции:\\n[DX0u∘v](H)=[Dv(X0)u]([DX0v](H))\\\\left[D_{X_0} u \\\\circ v \\\\right](H) = \\\\left[D_{v(X_0)} u \\\\right] \\\\left( \\\\left[D_{X_0} v\\\\right] (H)\\\\right)\\n[DX0\\u200b\\u200bu∘v](H)=[Dv(X0\\u200b)\\u200bu]([DX0\\u200b\\u200bv](H))и посмотрим, как её тут можно применить. В роли функции uuu у нас логарифм:\\nu(y)=log(u),[Dy0u](s)=1y0⋅s,u(y) = \\\\text{log}(u),\\\\quad \\\\left[D_{y_0} u\\\\right](s) = \\\\frac1y_0\\\\cdot s,\\nu(y)=log(u),[Dy0\\u200b\\u200bu](s)=y1\\u200b0\\u200b⋅s,а в роли vvv — определитель:\\nv(X)=det(X),[Dy0v](H)=⟨det(X0)⋅X0−T,H⟩,v(X) = \\\\text{det}(X),\\\\quad \\\\left[D_{y_0} v\\\\right](H) = \\\\langle \\\\text{det}(X_0)\\\\cdot X_0^{-T}, H\\\\rangle,\\nv(X)=det(X),[Dy0\\u200b\\u200bv](H)=⟨det(X0\\u200b)⋅X0−T\\u200b,H⟩,где под скалярным произведением двух матриц понимается, как обычно,\\n⟨A,B⟩=∑i,jaijbij=tr(ATB)\\\\langle A, B\\\\rangle = \\\\sum_{i,j}a_{ij}b_{ij} = \\\\text{tr}(A^TB)\\n⟨A,B⟩=i,j∑\\u200baij\\u200bbij\\u200b=tr(ATB)Подставим это всё в формулу произведения сложной функции:\\n[DX0u∘v](H)=1det(X)⋅⟨det(X)⋅X−T,H⟩=\\\\left[D_{X_0} u \\\\circ v \\\\right](H) = \\\\frac1{\\\\text{det}(X)}\\\\cdot\\\\langle \\\\text{det}(X)\\\\cdot X^{-T}, H\\\\rangle =\\n[DX0\\u200b\\u200bu∘v](H)=det(X)1\\u200b⋅⟨det(X)⋅X−T,H⟩==⟨1det(X)⋅det(X)⋅X−T,H⟩=⟨X0−T,H⟩= \\\\langle \\\\frac1{\\\\text{det}(X)}\\\\cdot\\\\text{det}(X)\\\\cdot X^{-T}, H\\\\rangle =\\n\\\\langle X_0^{-T}, H\\\\rangle=⟨det(X)1\\u200b⋅det(X)⋅X−T,H⟩=⟨X0−T\\u200b,H⟩Отсюда сразу видим, что\\n∇X0f=X0−T\\\\color{#FFC100}{\\\\nabla_{X_0} f} = X_0^{-T}\\n∇X0\\u200b\\u200bf=X0−T\\u200b\\nВычислим градиент функции f(X)=tr(AXTX)f(X) = \\\\text{tr}(AX^TX)f(X)=tr(AXTX).\\n\\nПопробуйте вычислить сами, прежде чем смотреть решение.Воспользуемся тем, что след — это линейное отображение (и значит, перестановочно с дифференцированием), а также правилом дифференцирования сложной функции:\\n[DX0f](H)=tr([DX0AXTX](H))=\\\\left[D_{X_0} f \\\\right](H) = \\\\text{tr}\\\\left(\\\\left[D_{X_0} AX^TX \\\\right](H)\\\\right) =\\n[DX0\\u200b\\u200bf](H)=tr([DX0\\u200b\\u200bAXTX](H))==tr(A⋅[DX0XT](H)⋅X0+AX0T[DX0X](H))==\\\\text{tr}\\\\left(A\\\\cdot\\\\left[D_{X_0} X^T \\\\right](H)\\\\cdot X_0 + AX_0^T\\\\left[D_{X_0} X \\\\right](H)\\\\right) =\\n=tr(A⋅[DX0\\u200b\\u200bXT](H)⋅X0\\u200b+AX0T\\u200b[DX0\\u200b\\u200bX](H))==tr(AHTX0+AX0TH)=\\\\text{tr}\\\\left(AH^TX_0 + AX_0^TH\\\\right)\\n=tr(AHTX0\\u200b+AX0T\\u200bH)Чтобы найти градиент, мы должны представить это выражение в виде ⟨?,H⟩\\\\langle ?, H\\\\rangle⟨?,H⟩, что в случае матриц переписывается, как мы уже хорошо знаем, в виде tr(?T⋅H)=tr(?⋅HT)\\\\text{tr}(?^T\\\\cdot H) = \\\\text{tr}(?\\\\cdot H^T)tr(?T⋅H)=tr(?⋅HT). Воспользуемся тем, что под знаком следа можно транспонировать и переставлять множители по циклу:\\n…=tr(AHTX0)+tr(AX0TH)=\\\\ldots=\\\\text{tr}\\\\left(AH^TX_0\\\\right) + \\\\text{tr}\\\\left(AX_0^TH\\\\right) =\\n…=tr(AHTX0\\u200b)+tr(AX0T\\u200bH)==tr(X0AHT)+tr(HTX0AT)==\\\\text{tr}\\\\left(X_0AH^T\\\\right) + \\\\text{tr}\\\\left(H^TX_0A^T\\\\right) =\\n=tr(X0\\u200bAHT)+tr(HTX0\\u200bAT)==tr(X0AHT)+tr(X0ATHT)==\\\\text{tr}\\\\left(X_0AH^T\\\\right) + \\\\text{tr}\\\\left(X_0A^TH^T\\\\right) =\\n=tr(X0\\u200bAHT)+tr(X0\\u200bATHT)==tr((X0A+X0AT)HT)=\\\\text{tr}\\\\left((X_0A + X_0A^T)H^T\\\\right)\\n=tr((X0\\u200bA+X0\\u200bAT)HT)Стало быть,\\n∇X0f=X0A+X0AT\\\\color{#FFC100}{\\\\nabla_{X_0} f} = X_0A + X_0A^T\\n∇X0\\u200b\\u200bf=X0\\u200bA+X0\\u200bAT\\nВычислим градиент функции f(X)=det(AX−1B)f(X) = \\\\text{det}\\\\left(AX^{-1}B\\\\right)f(X)=det(AX−1B).\\n\\nПодумайте, почему мы не можем расписать определитель в виде произведения определителейРасписать у нас может не получиться из-за того, что AAA и BBB могут быть не квадратными, и тогда у них нет определителей и представить исходный определитель в виде произведения невозможно.\\nВоспользуемся правилом дифференцирования сложной функции для u(Y)=det(Y)u(Y) = \\\\text{det}(Y)u(Y)=det(Y), v(X)=AX−1Bv(X) = AX^{-1}Bv(X)=AX−1B. А для этого сначала вспомним, какие дифференциалы у них самих. С функцией uuu всё просто:\\n[DY0u](S)=⟨det(Y0)Y0−T,S⟩=\\\\left[D_{Y_0} u\\\\right](S) = \\\\langle \\\\text{det}(Y_0)Y_0^{-T}, S\\\\rangle =\\n[DY0\\u200b\\u200bu](S)=⟨det(Y0\\u200b)Y0−T\\u200b,S⟩==tr(det(Y0)Y0−1S)= \\\\text{tr}\\\\left(\\\\text{det}(Y_0)Y_0^{-1}S\\\\right)\\n=tr(det(Y0\\u200b)Y0−1\\u200bS)Функция vvv сама является сложной, но, к счастью, множители AAA и BBB выносятся из-под знака дифференциала, а дифференцировать обратную матрицу мы уже умеем:\\n[DX0v](H)=−AX0−1HX0−1B\\\\left[D_{X_0} v\\\\right](H) = - AX_0^{-1}HX_0^{-1} B\\n[DX0\\u200b\\u200bv](H)=−AX0−1\\u200bHX0−1\\u200bBС учётом этого получаем:\\n[DX0f](H)=[Dv(X0)u]([DX0v](H))=\\\\left[D_{X_0} f \\\\right](H) = \\\\left[D_{v(X_0)} u \\\\right] \\\\left( \\\\left[D_{X_0} v\\\\right] (H)\\\\right) =\\n[DX0\\u200b\\u200bf](H)=[Dv(X0\\u200b)\\u200bu]([DX0\\u200b\\u200bv](H))==tr(det(AX0−1B)(AX0−1B)−1(−AX0−1HX0−1B))=\\\\text{tr}\\\\left(\\\\text{det}(AX_0^{-1}B)(AX_0^{-1}B)^{-1}\\\\left(- AX_0^{-1}HX_0^{-1} B\\\\right)\\\\right)\\n=tr(det(AX0−1\\u200bB)(AX0−1\\u200bB)−1(−AX0−1\\u200bHX0−1\\u200bB))=tr(−det(AX0−1B)(AX0−1B)−1AX0−1HX0−1B)=\\\\text{tr}\\\\left(-\\\\text{det}(AX_0^{-1}B)(AX_0^{-1}B)^{-1}AX_0^{-1}HX_0^{-1} B\\\\right)\\n=tr(−det(AX0−1\\u200bB)(AX0−1\\u200bB)−1AX0−1\\u200bHX0−1\\u200bB)Чтобы найти градиент, мы должны, как обычно, представить это выражение в виде tr(?T⋅H)\\\\text{tr}(?^T\\\\cdot H)tr(?T⋅H).\\n…=tr(−det(AX0−1B)X0−1B(AX0−1B)−1AX0−1H)\\\\ldots=\\\\text{tr}\\\\left(-\\\\text{det}(AX_0^{-1}B)X_0^{-1} B(AX_0^{-1}B)^{-1}AX_0^{-1}H\\\\right)\\n…=tr(−det(AX0−1\\u200bB)X0−1\\u200bB(AX0−1\\u200bB)−1AX0−1\\u200bH)Стало быть,\\n∇X0f=(−det(AX0−1B)X0−1B(AX0−1B)−1AX0−1)T={\\\\nabla_{X_0} f} = \\\\left(-\\\\text{det}(AX_0^{-1}B)X_0^{-1} B(AX_0^{-1}B)^{-1}AX_0^{-1}\\\\right)^T =\\n∇X0\\u200b\\u200bf=(−det(AX0−1\\u200bB)X0−1\\u200bB(AX0−1\\u200bB)−1AX0−1\\u200b)T==−det(AX0−1B)X0−TAT(AX0−1B)−TBTX0−T=-\\\\text{det}(AX_0^{-1}B)X_0^{-T} A^T(AX_0^{-1}B)^{-T}B^TX_0^{-T}\\n=−det(AX0−1\\u200bB)X0−T\\u200bAT(AX0−1\\u200bB)−TBTX0−T\\u200bВторая производная\\nРассмотрим теперь не первые два, а первые три члена ряда Тейлора:\\nf(x0+h)=f(x0)+[Dx0f](h)+12[Dx02f](h,h)+oˉˉ(∣∣h∣∣2),f(x_0 + h) = f(x_0) + \\\\color{#348FEA}{\\\\left[D_{x_0} f \\\\right]} (h) + \\\\frac12\\\\color{#4CB9C0}{\\\\left[D_{x_0}^2 f \\\\right]} (h, h) + \\\\bar{\\\\bar{o}} \\\\left(\\\\left|\\\\left| h\\\\right|\\\\right|^2\\\\right),\\nf(x0\\u200b+h)=f(x0\\u200b)+[Dx0\\u200b\\u200bf](h)+21\\u200b[Dx0\\u200b2\\u200bf](h,h)+oˉˉ(∣∣h∣∣2),где [Dx02f](h,h)\\\\color{#4CB9C0}{\\\\left[D_{x_0}^2 f \\\\right]} (h, h)[Dx0\\u200b2\\u200bf](h,h) — второй дифференциал, квадратичная форма, в которую мы объединили все члены второй степени.\\nВопрос на подумать. Докажите, что второй дифференциал является дифференциалом первого, то есть\\n[Dx0[Dx0f](h1)](h2)=[Dx02f](h1,h2)\\\\left[D_{x_0} \\\\color{#348FEA}{\\\\left[D_{x_0} f \\\\right]} (h_1) \\\\right] (h_2) = \\\\left[D_{x_0}^2 f \\\\right] (h_1, h_2)\\n[Dx0\\u200b\\u200b[Dx0\\u200b\\u200bf](h1\\u200b)](h2\\u200b)=[Dx0\\u200b2\\u200bf](h1\\u200b,h2\\u200b)Зависит ли выражение справа от порядка h1h_1h1\\u200b и h2h_2h2\\u200b?\\nЭтот факт позволяет вычислять второй дифференциал не с помощью приращений, а повторным дифференцированием производной.\\nВторая производная может оказаться полезной при реализации методов второго порядка или же для проверки того, является ли критическая точка (то есть точка, в которой градиент обращается в ноль) точкой минимума или точкой максимума. Напомним, что квадратичная форма q(h)q(h)q(h) называется положительно определённой (соответственно, отрицательно определённой), если q(h)⩾0q(h) \\\\geqslant 0q(h)⩾0 (соответственно, q(h)⩽0q(h) \\\\leqslant 0q(h)⩽0) для всех hhh, причём q(h)=0q(h) = 0q(h)=0 только при h=0h = 0h=0.\\nТеорема. Пусть функция f:Rm→Rf:\\\\mathbb{R}^m\\\\rightarrow\\\\mathbb{R}f:Rm→R имеет непрерывные частные производные второго порядка ∂2f∂xi∂xj\\\\frac{\\\\partial^2 f}{\\\\partial x_i\\\\partial x_j}∂xi\\u200b∂xj\\u200b∂2f\\u200b в окрестности точки x0x_0x0\\u200b, причём ∇x0f=0\\\\color{#FFC100}{\\\\nabla_{x_0} f} = 0∇x0\\u200b\\u200bf=0. Тогда точка x0x_0x0\\u200b является точкой минимума функции, если квадратичная форма Dx02f\\\\color{#348FEA}{D_{x_0}^2 f}Dx0\\u200b2\\u200bf положительно определена, и точкой максимума, если она отрицательно определена.\\nЕсли мы смогли записать матрицу квадратичной формы второго дифференциала, то мы можем проверить её на положительную или отрицательную определённость с помощью критерия Сильвестра.\\nПримеры вычисления и использования второй производной\\n\\nРассмотрим задачу минимизации f(x)=∣∣Ax−b∣∣2f(x) = \\\\vert\\\\vert Ax - b\\\\vert\\\\vert^2f(x)=∣∣Ax−b∣∣2 по переменной xxx, где AAA — матрица с линейно независимыми столбцами. Выше мы уже нашли градиент этой функции; он был равен ∇x0f=2AT(Ax−b)\\\\color{#FFC100}{\\\\nabla_{x_0} f} = 2A^T(Ax - b)∇x0\\u200b\\u200bf=2AT(Ax−b). Мы можем заподозрить, что минимум достигается в точке, где градиент обращается в ноль: x∗=(ATA)−1ATbx_* = (A^TA)^{-1}A^Tbx∗\\u200b=(ATA)−1ATb. Отметим, что обратная матрица существует, так как rk(ATA)=rkA\\\\text{rk}(A^TA) = \\\\text{rk}{A}rk(ATA)=rkA, а столбцы AAA по условию линейно независимы и, следовательно, rk(ATA)\\\\text{rk}(A^TA)rk(ATA) равен размеру этой матрицы. Но действительно ли эта точка является точкой минимума? Давайте оставим в стороне другие соображения (например, геометрические, о которых мы упомянем в параграфе про линейные модели) и проверим аналитически. Для этого мы должны вычислить второй дифференциал функции f(x)=∣∣Ax−b∣∣2f(x) = \\\\vert\\\\vert Ax - b\\\\vert\\\\vert^2f(x)=∣∣Ax−b∣∣2.\\n\\nПопробуйте вычислить сами, прежде чем смотреть решение.Вспомним, что\\n[Dx0∣∣Ax−b∣∣2](h1)=⟨2AT(Ax0−b),h1⟩\\\\color{#348FEA}{\\\\big[D_{x_0} \\\\vert\\\\vert Ax - b\\\\vert\\\\vert^2\\\\big]}(h_1) = \\\\langle 2A^T(Ax_0 - b), h_1\\\\rangle\\n[Dx0\\u200b\\u200b∣∣Ax−b∣∣2](h1\\u200b)=⟨2AT(Ax0\\u200b−b),h1\\u200b⟩Продифференцируем снова. Скалярное произведение — это линейная функция, поэтому можно занести дифференцирование внутрь:\\n[Dx0⟨2AT(Ax−b),h1⟩](h2)=⟨[Dx0(2ATAx−2ATb)](h2),h1⟩=\\\\color{#348FEA}{\\\\big[D_{x_0} \\\\langle 2A^T(Ax - b), h_1\\\\rangle\\\\big]}(h_2) = \\n\\\\langle \\\\color{#348FEA}{\\\\big[D_{x_0} (2A^TAx - 2A^Tb)\\\\big]}(h_2), h_1\\\\rangle =[Dx0\\u200b\\u200b⟨2AT(Ax−b),h1\\u200b⟩](h2\\u200b)=⟨[Dx0\\u200b\\u200b(2ATAx−2ATb)](h2\\u200b),h1\\u200b⟩==⟨2ATAh2,h1⟩=2h2TATAh1=\\\\langle 2A^TAh_2, h_1\\\\rangle = 2h_2^T A^TA h_1\\n=⟨2ATAh2\\u200b,h1\\u200b⟩=2h2T\\u200bATAh1\\u200bМы нашли квадратичную форму второго дифференциала; она, оказывается, не зависит от точки (впрочем, логично: исходная функция была второй степени по xxx, так что вторая производная должна быть константой). Чтобы показать, что x∗x_*x∗\\u200b действительно является точкой минимума, достаточно проверить, что эта квадратичная форма положительно определена.\\nПопробуйте сделать это сами, прежде чем смотреть решение.Хорошо знакомый с линейной алгеброй читатель сразу скажет, что матрица ATAA^TAATA положительно определена для матрицы AAA с линейно независимыми столбцами. Но всё же давайте докажем это явно. Имеем hTATAh=(Ah)TAh=∣∣Ah∣∣2⩾0h^TA^TAh = (Ah)^TAh = \\\\vert\\\\vert Ah\\\\vert\\\\vert^2 \\\\geqslant 0hTATAh=(Ah)TAh=∣∣Ah∣∣2⩾0. Это выражение равно нулю тогда и только тогда, когда Ah=0Ah = 0Ah=0. Последнее является однородной системой уравнений на hhh, ранг которой равен числу переменных, так что она имеет лишь нулевое решение h=0h = 0h=0.\\n\\nДокажем, что функция f(X)=log\\u2061det(X)f(X) = \\\\log{\\\\text{det}(X)}f(X)=logdet(X) является выпуклой вверх на множестве симметричных, положительно определённых матриц. Для этого мы должны проверить, что в любой точке квадратичная форма её дифференциала отрицательно определена. Для начала вычислим эту квадратичную форму.\\n\\nПопробуйте сделать это сами, прежде чем смотреть решение.Выше мы уже нашли дифференциал этой функции:\\n[DX0log\\u2061det(X)](H1)=⟨X0−T,H1⟩\\\\color{#348FEA}{\\\\big[D_{X_0} \\\\log{\\\\text{det}(X)}\\\\big]}(H_1) = \\\\langle X_0^{-T}, H_1\\\\rangle\\n[DX0\\u200b\\u200blogdet(X)](H1\\u200b)=⟨X0−T\\u200b,H1\\u200b⟩Продифференцируем снова:\\n[DX0⟨X−T,H1⟩](H2)=⟨[Dx0X−T](H2),h1⟩=\\\\color{#348FEA}{\\\\big[D_{X_0} \\\\langle X^{-T}, H_1\\\\rangle\\\\big]}(H_2) = \\n\\\\langle \\\\color{#348FEA}{\\\\big[D_{x_0} X^{-T}\\\\big]}(H_2), h_1\\\\rangle =[DX0\\u200b\\u200b⟨X−T,H1\\u200b⟩](H2\\u200b)=⟨[Dx0\\u200b\\u200bX−T](H2\\u200b),h1\\u200b⟩==⟨−X0−1H2X0−1,H1⟩=\\\\langle -X_0^{-1}H_2X_0^{-1}, H_1\\\\rangle\\n=⟨−X0−1\\u200bH2\\u200bX0−1\\u200b,H1\\u200b⟩Чтобы доказать требуемое в условии, мы должны проверить следующее: что для любой симметричной матрицы X0X_0X0\\u200b и для любого симметричного (чтобы не выйти из пространства симметричных матриц) приращения H≠0H\\\\ne 0H\\ue020=0 имеем\\n[DX02log\\u2061det(X)](H,H)<0\\\\color{#348FEA}{\\\\big[D^2_{X_0} \\\\log{\\\\text{det}(X)}\\\\big]}(H, H) < 0\\n[DX0\\u200b2\\u200blogdet(X)](H,H)<0Покажем это явно.\\nТак как X0X_0X0\\u200b — симметричная, положительно определённая матрица, у неё есть симметричный и положительно определённый квадратный корень: X0=X01/2⋅X01/2=X01/2⋅(X01/2)T.X_0 = X_0^{1/2}\\\\cdot X_0^{1/2} = X_0^{1/2}\\\\cdot \\\\left(X_0^{1/2}\\\\right)^T.X0\\u200b=X01/2\\u200b⋅X01/2\\u200b=X01/2\\u200b⋅(X01/2\\u200b)T. Тогда\\n⟨−X0−1HX0−1,H⟩=−tr(X01/2(X01/2)THX01/2(X01/2)THT)=\\\\langle -X_0^{-1}HX_0^{-1}, H\\\\rangle = -\\\\text{tr}\\\\left(X_0^{1/2} \\\\left(X_0^{1/2}\\\\right)^THX_0^{1/2} \\\\left(X_0^{1/2}\\\\right)^TH^T\\\\right) =\\n⟨−X0−1\\u200bHX0−1\\u200b,H⟩=−tr(X01/2\\u200b(X01/2\\u200b)THX01/2\\u200b(X01/2\\u200b)THT)=−tr((X01/2)THX01/2(X01/2)THTX01/2)=-\\\\text{tr}\\\\left(\\\\left(X_0^{1/2}\\\\right)^THX_0^{1/2} \\\\left(X_0^{1/2}\\\\right)^TH^TX_0^{1/2}\\\\right) = \\n−tr((X01/2\\u200b)THX01/2\\u200b(X01/2\\u200b)THTX01/2\\u200b)==−tr((X01/2)THX01/2[(X01/2)THX01/2]T)==-\\\\text{tr}\\\\left( \\\\left(X_0^{1/2}\\\\right)^THX_0^{1/2} \\\\left[\\\\left(X_0^{1/2}\\\\right)^THX_0^{1/2}\\\\right]^T\\\\right) =\\n=−tr((X01/2\\u200b)THX01/2\\u200b[(X01/2\\u200b)THX01/2\\u200b]T)==−∣∣(X01/2)THX01/2∣∣2,=-\\\\vert\\\\vert\\\\left(X_0^{1/2}\\\\right)^THX_0^{1/2}\\\\vert\\\\vert^2,\\n=−∣∣(X01/2\\u200b)THX01/2\\u200b∣∣2,что, конечно, меньше нуля для любой ненулевой HHH.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф15.4. Методы оптимизации в Deep LearningСледующий параграф16.2. Матричная факторизацияЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_62.html', 'title': 'Методы оптимизации в Deep Learning'}, page_content=\"Методы оптимизации в Deep LearningЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/415.1.Введение в онлайн-обучение15.2.Адаптивный FTRL15.3.Регуляризация в онлайн-обучении15.4.Методы оптимизации в Deep LearningНапоминанияСкользящее среднее в знаменателе AdaGrad. Методы RMSprop и AdamКак сломать адаптивные методы со скользящим среднимЧиним RMSprop и AdamOnline RMSpropMomentumСобираем все идеи воедино16.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Методы оптимизации в Deep Learning15.4. Методы оптимизации в Deep LearningАвторыАлексей МорозовНапоминания\\nОпределение Критической точкой гладкой функции ft(w)f_t(w)ft\\u200b(w) называется точка w∗w^*w∗, для которой\\n∇ft(w∗)=0\\\\nabla f_t(w^*) = 0\\n∇ft\\u200b(w∗)=0В выпуклой оптимизации такая точка обязательно будет точкой глобального минимума. В невыпуклой оптимизации все сильно сложнее:\\n\\nБывает много локальных минимумов\\nБывают седловые точки\\n\\nЛокальный минимум — это критическая точка w∗w^*w∗, в которой Гессиан H(w∗)=∇2ft(w∗)H(w^*) = \\\\nabla^2 f_t(w^*)H(w∗)=∇2ft\\u200b(w∗) положительно определён. Отметим, что часто в методах глобальной оптимизации рассматривается так называемая «локальная выпуклость», для которой требуется, чтобы функция ft(w)f_t(w)ft\\u200b(w) была выпуклой внутри некоторого шара радиуса ϵ\\\\epsilonϵ с центром в точке w∗w^*w∗. Критические точки, в которых гессиан не является знакоопределённым, называются седловыми.\\nПример: функция f(x1,x2)=x12−x22f(x_1, x_2) = x_1^2 - x_2^2f(x1\\u200b,x2\\u200b)=x12\\u200b−x22\\u200b имеет седловую точку {0,0}\\\\{0, 0\\\\}{0,0}. Гессиан в точке 0Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nH(0,0)=(200−2)H(0, 0) = \\\\begin{pmatrix}\\n2 & 0\\\\\\\\\\n0 & -2\\n\\\\end{pmatrix}\\nH(0,0)=(20\\u200b0−2\\u200b)Обратите внимание: во многих современных статьях про сходимость методов оптимизации первого порядка на невыпуклых функциях (пример) в качестве критерия сходимости рассматривают сходимость по норме градиента: ∣∣∇f(w)∣∣22<ϵ\\\\vert\\\\vert \\\\nabla f(w)\\\\vert\\\\vert_2^2 < \\\\epsilon∣∣∇f(w)∣∣22\\u200b<ϵ при некотором заранее фиксированном ϵ\\\\epsilonϵ.\\nВ выпуклой оптимизации этот критерий сходимости эквивалентен двум другим:\\n\\nсходимости по расстоянию до оптимума в пространстве параметров: ∣∣w−w∗∣∣22<ϵ\\\\vert\\\\vert w - w^*\\\\vert\\\\vert_2^2 < \\\\epsilon∣∣w−w∗∣∣22\\u200b<ϵ;\\nсходимости по расстоянию до оптимума по значениям функции f(w)−f(w∗)<ϵf(w) - f(w^*) < \\\\epsilonf(w)−f(w∗)<ϵ.\\n\\nВ невыпуклой оптимизации всё не так просто и поиск глобального минимума является в общем случае NP-трудной задачей. Критерий ∣∣∇f(w)∣∣22<ϵ\\\\vert\\\\vert \\\\nabla f(w)\\\\vert\\\\vert_2^2 < \\\\epsilon∣∣∇f(w)∣∣22\\u200b<ϵ даёт возможность исследовать сходимость к любой критической точке, но если речь об обучении нейронных сетях, то остается лишь надеяться, что эта критическая точка будет хорошим локальным минимумом.\\nСкользящее среднее в знаменателе AdaGrad. Методы RMSprop и Adam\\nМотивация\\nВ далекие 2012-2014е в мире было не так много опыта по построению хороших нейросетевых архитектур. «Канонические» методы оптимизации нейросетей RMSprop и Adam появлялись во времена, когда ещё не придумали основополагающих вещей вроде:\\n\\nResidual connection и Dense connection (статьи опубликованы в 2015/2016 соответственно, во всех экспериментах используется SGD, в статье и в ссылках не упоминаются методы Adam/RMSprop), плохо решались проблемы взрывов/затуханий градиентов и т.д.\\nBatch Normalization и Layer Normalization (2015/2016 соответственно)\\n\\nТакже люди не умели правильно инициализировать нейросети гигантской глубины. статьи вроде 1000+ layer fully connected и 10000+ layer CNN позже. Кстати, этот цикл статей хочется особо отметить за интересную технику анализа распространения сигнала по нейронной сети.\\nВ общем, в те времена царило архитектурное средневековье со всеми родовыми проблемами нейронных сетей:\\n\\nВзрывы градиентов;\\nЗатухания градиентов;\\nВзрывы-затухания сигнала на прямом проходе;\\nПлохие начальные инициализации, нестабильный старт обучения.\\n\\nПри попытках применять метод AdaGrad особо остро стояли проблемы 1 и 4. AdaGrad аккумулирует всю прошедшую историю 1g1:t2\\\\frac{1}{\\\\sqrt{g_{1:t}^2}}g1:t2\\u200b\\u200b1\\u200b без затухания. Если в какой-то момент возникает одна из указанных проблем, знаменатель резко возрастает и больше не выправляется.\\nЧтобы побороть проблемы 1-4, решили поработать над оптимизатором и сделать так, чтобы история в AdaGrad аккумулировалась с затуханием и метод оптимизации мог со временем забыть плохие точки. Самый популярный и простой в реализации метод — экспоненциальное скользящее среднее.\\nRMSProp\\nСамая первая и самая простая модификация метода AdaGrad — метод RMSprop — вместо суммы использует экспоненциальное скользящее среднее в знаменателе:\\nv0=0v_0 = 0\\nv0\\u200b=0vt=βvt−1+(1−β)gt2v_t = \\\\beta v_{t-1} + (1 - \\\\beta) g_t^2\\nvt\\u200b=βvt−1\\u200b+(1−β)gt2\\u200bwt+1=wt−αvtgtw_{t+1} = w_t - \\\\frac{\\\\alpha}{\\\\sqrt{v_t}}g_t\\nwt+1\\u200b=wt\\u200b−vt\\u200b\\u200bα\\u200bgt\\u200bМетоду RMSprop не было посвящено ни одной специализированной статьи, равно как и не было никаких доказательств его сходимости даже для выпуклых задач.\\nAdam\\nАвторы Adam в статье Adam: A Method For Stochastic Optimization вводят два новшества по сравнению с RMSprop. Во-первых, это Momentum. Во вторых — Bias correction term. Напомним, как работает этот метод.\\nv0=0,m0=0v_0 = 0, m_0 = 0\\nv0\\u200b=0,m0\\u200b=0mt=β1mt−1+(1−β1)gtm_t = \\\\beta_1 m_{t-1} + (1 - \\\\beta_1)g_t\\nmt\\u200b=β1\\u200bmt−1\\u200b+(1−β1\\u200b)gt\\u200bvt=β2vt−1+(1−β2)gt2v_t = \\\\beta_2 v_{t-1} + (1 - \\\\beta_2) g_t^2\\nvt\\u200b=β2\\u200bvt−1\\u200b+(1−β2\\u200b)gt2\\u200bПрименяем bias correction\\nm^t=11−β1tmt\\\\hat{m}_t = \\\\frac{1}{1 - \\\\beta_1^t}m_t\\nm^t\\u200b=1−β1t\\u200b1\\u200bmt\\u200bv^t=11−β2tv2\\\\hat{v}_t = \\\\frac{1}{1 - \\\\beta_2^t}v_2\\nv^t\\u200b=1−β2t\\u200b1\\u200bv2\\u200bwt+1=wt−αv^tm^tw_{t+1} = w_t - \\\\frac{\\\\alpha}{\\\\sqrt{\\\\hat{v}_t}}\\\\hat{m}_t\\nwt+1\\u200b=wt\\u200b−v^t\\u200b\\u200bα\\u200bm^t\\u200bСразу перепишем vtv_tvt\\u200b и mtm_tmt\\u200b в нерекурсивной форме с зависимостью только от ggg:\\nv^t=11−β2t(1−β2)∑s=1tβ2t−sgs2\\\\hat{v}_t = \\\\frac{1}{1 - \\\\beta_2^t}(1-\\\\beta_2)\\\\sum\\\\limits_{s=1}^t \\\\beta_2^{t-s} g_s^2\\nv^t\\u200b=1−β2t\\u200b1\\u200b(1−β2\\u200b)s=1∑t\\u200bβ2t−s\\u200bgs2\\u200bm^t=11−β1t(1−β1)∑s=1tβ1t−sgs\\\\hat{m}_t = \\\\frac{1}{1 - \\\\beta_1^t}(1-\\\\beta_1)\\\\sum\\\\limits_{s=1}^t \\\\beta_1^{t-s} g_s\\nm^t\\u200b=1−β1t\\u200b1\\u200b(1−β1\\u200b)s=1∑t\\u200bβ1t−s\\u200bgs\\u200bМотивация для bias correction\\nАвторы статьи пишут, что для правильной работы метода mtm_tmt\\u200b и vtv_tvt\\u200b должны быть несмещенными оценками E[g]\\\\mathbb{E}[g]E[g] и E[g2]\\\\mathbb{E}[g^2]E[g2] соответственно. Допустим, все gtg_tgt\\u200b — независимые одинаково распредёленные случайные величины. Это довольно сильное предположение, но иначе не получатся красивые формулы. Рассмотрим на примере vtv_tvt\\u200b:\\nE[vt]=E[(1−β2)∑s=1tβ2t−sgs2]=(1−β2)∑s=1tβ2t−sE[gs2]=\\\\mathbb{E}[v_t] = \\\\mathbb{E}\\\\Big[(1-\\\\beta_2)\\\\sum\\\\limits_{s=1}^t \\\\beta_2^{t-s} g_s^2\\\\Big] = (1-\\\\beta_2)\\\\sum\\\\limits_{s=1}^t \\\\beta_2^{t-s}\\\\mathbb{E}[g_s^2] =\\nE[vt\\u200b]=E[(1−β2\\u200b)s=1∑t\\u200bβ2t−s\\u200bgs2\\u200b]=(1−β2\\u200b)s=1∑t\\u200bβ2t−s\\u200bE[gs2\\u200b]==((1−β2)∑s=1tβ2t−s)E[g2]=(1−β2t)E[g2]= \\\\Big((1-\\\\beta_2)\\\\sum\\\\limits_{s=1}^t \\\\beta_2^{t-s}\\\\Big)\\\\mathbb{E}[g^2] = (1 - \\\\beta_2^t)\\\\mathbb{E}[g^2]\\n=((1−β2\\u200b)s=1∑t\\u200bβ2t−s\\u200b)E[g2]=(1−β2t\\u200b)E[g2]Отсюда очевидно, что исходные vtv_tvt\\u200b и mtm_tmt\\u200b смещены на множитель (1−β2t)(1 - \\\\beta_2^t)(1−β2t\\u200b), поэтому авторы Adam делят на него m^t\\\\hat{m}_tm^t\\u200b и v^t\\\\hat{v}_tv^t\\u200b. Так как lim\\u2061t→∞(1−β2t)=1\\\\lim\\\\limits_{t\\\\rightarrow \\\\infty} (1 - \\\\beta_2^t) = 1t→∞lim\\u200b(1−β2t\\u200b)=1 при 0≤β2<10 \\\\leq \\\\beta_2 < 10≤β2\\u200b<1, эффект смещения сильнее всего заметен в начале итерационного процесса. Например, при классическом β2=0.999\\\\beta_2 = 0.999β2\\u200b=0.999 мы получаем смещение в 0.001 раз.\\nВ начале обучения bias correction призван уменьшить слишком большие шаги оптимизатора.\\nДоказательство сходимости метода\\nВ оригинальной статье приводится теорема с доказательством сублинейного Regret. Доказательство содержало ошибку, в новой работе 2018 года было доказано, что для любого набора гиперпараметров Adam существует выпуклая задача, на которой он не сходится. Проблемы со сходимостью, впрочем, не являются специфичными для выпуклых задач: в нейронных сетях Adam тоже может вести себя странно, и об этом мы поговорим ниже в разделе «Как сломать адаптивные методы».\\nРазбирать доказательство исходной статьи мы не будем, зато обратим внимание на пару неприятных фактов о различиях между «продаваемой» частью статьи и бекендом с экспериментами и доказательствами теорем.\\nПочему Adam стали считать лучшим методом стохастической оптимизации?\\nПосле успешного введения метода Adam в эксплуатацию в нейросети его окрестили «method of choice» в задачах стохастической оптимизации. Это было на 100% обусловлено его успехом в обучении нейронных сетей с нестабильными архитектурами.\\nСтруктура статьи выглядит следующим образом:\\n\\nВыделенный в большую красивую видную рамочку алгоритм с дефолтными настройками вроде α=0.001\\\\alpha = 0.001α=0.001;\\nФормулировка теоремы в разделе про доказательства;\\nЭксперименты на нейросетях и выпуклых задачах.\\n\\nВ пункте 1 описан алгоритм, который все нынче знают, как Adam. Мало кто знает, что в доказательствах сходимости и в экспериментах на выпуклых задачах использовался немного другой алгоритм: вместо константного α\\\\alphaα авторы статьи взяли αt=αt\\\\alpha_t = \\\\frac{\\\\alpha}{\\\\sqrt{t}}αt\\u200b=t\\u200bα\\u200b. Сравним эти learning rate с AdaGrad:\\n\\n\\n\\n\\nМетод\\n\\n\\nФормулы\\n\\n\\n\\n\\nAdaGrad\\n\\n\\n1∑s=1tgs2\\\\frac{1}{\\\\sqrt{\\\\sum\\\\limits_{s=1}^t g_s^2}}s=1∑t\\u200bgs2\\u200b\\u200b1\\u200b\\n\\n\\n\\n\\nAdam\\n\\n\\n1−βtt(1−β)∑s=1tβt−sgs2\\\\frac{1 - \\\\beta^t}{\\\\sqrt{t}\\\\sqrt{(1-\\\\beta)}\\\\sqrt{\\\\sum\\\\limits_{s=1}^t \\\\beta^{t-s} g_s^2}}t\\u200b(1−β)\\u200bs=1∑t\\u200bβt−sgs2\\u200b\\u200b1−βt\\u200b\\n\\n\\n\\n\\nАвторы в экспериментах на логистической регрессии убили основное свойство Adam — неубывающие learning rate. Вспомним, как в разделе про вывод AdaGrad мы анализировали порядок убывания learning rate — он был ηt=O(1t)\\\\eta_t = O(\\\\frac{1}{\\\\sqrt{t}})ηt\\u200b=O(t\\u200b1\\u200b). Отсюда следует, что у такого Adam learning rate убывают так же, как в AdaGrad. Словом, будьте внимательны при чтении статей: смотрите не только в описание алгоритмов, но и в их реализацию.\\nНастоящий Adam, который в pytorch и tensorflow реализован без множителя 1t\\\\frac1{\\\\sqrt{t}}t\\u200b1\\u200b, в выпуклой задаче разреженной логистической регрессии обычно работает намного хуже AdaGrad. Это справедливо как для чисто линейных моделей, так и для комбинированных Wide & Deep архитектур, из-за чего в одной и той же нейросети приходится использовать разные методы оптимизации для разных параметров.\\nПромежуточный итог по Adam/RMSProp\\nТут нужно запомнить три идеи:\\n\\nMomentum\\nСкользящее среднее в learning rate\\nBias correction\\n\\nНа практике, часто почему-то рассматривают методы RMSprop и Adam как нечто отлитое в граните и не пытаются брать от них лучшее. Например, методу RMSprop обычно идет на пользу добавление bias correction от adam. Так что полезно помнить идеи, стоящие за методами оптимизации, и уметь их комбинировать.\\nКак сломать адаптивные методы со скользящим средним\\nКак и когда ломаются адаптивные методы\\nВсе диагональные адаптивные методы так или иначе используют покоординатный learning rate ηt,i=αtvt,i\\\\eta_{t,i} = \\\\frac{\\\\alpha_t}{\\\\sqrt{v_{t,i}}}ηt,i\\u200b=vt,i\\u200b\\u200bαt\\u200b\\u200b. Методы отличаются лишь формулировкой vt,iv_{t,i}vt,i\\u200b и αt\\\\alpha_tαt\\u200b:\\n\\n\\n\\n\\nМетод\\n\\n\\nРекуррентные формулы vt,iv_{t,i}vt,i\\u200b\\n\\n\\nРазвернутые формулы vt,iv_{t,i}vt,i\\u200b\\n\\n\\nαt\\\\alpha_tαt\\u200b\\n\\n\\n\\n\\nAdaGrad\\n\\n\\nvt−1,i+gt,i2v_{t-1,i} + g^2_{t,i}vt−1,i\\u200b+gt,i2\\u200b\\n\\n\\nvt,i=∑s=1tgs,i2v_{t,i} = \\\\sum\\\\limits_{s=1}^tg^2_{s,i}vt,i\\u200b=s=1∑t\\u200bgs,i2\\u200b\\n\\n\\nα\\\\alphaα\\n\\n\\n\\n\\nRMSprop\\n\\n\\nβvt−1,i+(1−β)gt,i2\\\\beta v_{t-1,i} + (1 - \\\\beta) g^2_{t,i}βvt−1,i\\u200b+(1−β)gt,i2\\u200b\\n\\n\\n(1−β)∑s=1tβt−sgs2(1-\\\\beta)\\\\sum\\\\limits_{s=1}^t \\\\beta^{t-s} g_s^2(1−β)s=1∑t\\u200bβt−sgs2\\u200b\\n\\n\\nα\\\\alphaα\\n\\n\\n\\n\\nAdam\\n\\n\\nβvt−1,i+(1−β)gt,i2\\\\beta v_{t-1,i} + (1 - \\\\beta) g^2_{t,i}βvt−1,i\\u200b+(1−β)gt,i2\\u200b\\n\\n\\n(1−β)∑s=1tβt−sgs2(1-\\\\beta)\\\\sum\\\\limits_{s=1}^t \\\\beta^{t-s} g_s^2(1−β)s=1∑t\\u200bβt−sgs2\\u200b\\n\\n\\nα1−βt\\\\alpha\\\\sqrt{1 - \\\\beta^t}α1−βt\\u200b\\n\\n\\n\\n\\nВсе эти методы имеют единый вид формул FTRL, аналогичный формулам FTRL-AdaGrad:\\nwt+1=argmin\\u2061wg1:tTw+∑s=1t∣∣w−ws∣∣σs2w_{t+1} = arg\\\\min\\\\limits_w g_{1:t}^Tw + \\\\sum\\\\limits_{s=1}^t\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{\\\\sigma_s}^2\\nwt+1\\u200b=argwmin\\u200bg1:tT\\u200bw+s=1∑t\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200bηs,i=αsvs,i\\\\eta_{s,i} = \\\\frac{\\\\alpha_s}{\\\\sqrt{v_{s,i}}}\\nηs,i\\u200b=vs,i\\u200b\\u200bαs\\u200b\\u200bσs,i=vs,iαs−vs−1,iαs−1\\\\sigma_{s,i} = \\\\frac{\\\\sqrt{v_{s,i}}}{\\\\alpha_s} - \\\\frac{\\\\sqrt{v_{s-1,i}}}{\\\\alpha_{s-1}}\\nσs,i\\u200b=αs\\u200bvs,i\\u200b\\u200b\\u200b−αs−1\\u200bvs−1,i\\u200b\\u200b\\u200bВспомним теоретические ограничения на rt(w)r_t(w)rt\\u200b(w):\\n\\nrt(w)r_t(w)rt\\u200b(w) — выпуклый;\\nrt(w)≥0r_t(w) \\\\geq 0rt\\u200b(w)≥0.\\n\\nАдаптивные методы с регуляризаторами rt(w)=∣∣w∣∣σt2r_t(w) = \\\\vert\\\\vert w\\\\vert\\\\vert_{\\\\sigma_t}^2rt\\u200b(w)=∣∣w∣∣σt\\u200b2\\u200b будут удовлетворять этим условиям, если все σs,i≥0\\\\sigma_{s,i} \\\\geq 0σs,i\\u200b≥0. В этом месте и локаются методы со скользящим средним: никто не обещал, что последовательность vtv_tvt\\u200b будет монотонно неубывать. Если же\\nvs,iαs<vs−1,iαs−1,(∗)\\\\frac{\\\\sqrt{v_{s,i}}}{\\\\alpha_s} < \\\\frac{\\\\sqrt{v_{s-1,i}}}{\\\\alpha_{s-1}},\\\\qquad(\\\\ast)\\nαs\\u200bvs,i\\u200b\\u200b\\u200b<αs−1\\u200bvs−1,i\\u200b\\u200b\\u200b,(∗)то метод может ломаться\\nОбратите внимание. Momentum в методе Adam никак не повлияет на справедливость наших рассуждений, поскольку в формулах для адаптивных learning rate он не используется. Адаптивные методы с такими learning rate сломаются и с momentum, и без него.\\nОбратите внимание. Bias correction в методе Adam уменьшает learning rate в начале обучения, заставляя метод делать меньшие шаги.\\nВсе рекуррентные формулы из таблицы можно переписать в виде\\nvt=C1vt−1+C2gt2v_{t} = C_1v_{t-1} + C_2g_t^2\\nvt\\u200b=C1\\u200bvt−1\\u200b+C2\\u200bgt2\\u200bТогда неравенство (∗)(\\\\ast)(∗) можно записать в виде\\nC1vt−1+C2gt2αt<vt−1αt−1\\\\frac{\\\\sqrt{C_1v_{t-1} + C_2g_t^2}}{\\\\alpha_t} < \\\\frac{\\\\sqrt{v_{t-1}}}{\\\\alpha_{t-1}}\\nαt\\u200bC1\\u200bvt−1\\u200b+C2\\u200bgt2\\u200b\\u200b\\u200b<αt−1\\u200bvt−1\\u200b\\u200b\\u200bgt2<αt2C1vt−1(1αt−12−C1αt2)g_t^2 < \\\\frac{\\\\alpha_t^2}{C_1}v_{t-1}\\\\left(\\\\frac{1}{\\\\alpha_{t-1}^2} - \\\\frac{C_1}{\\\\alpha_t^2}\\\\right)\\ngt2\\u200b<C1\\u200bαt2\\u200b\\u200bvt−1\\u200b(αt−12\\u200b1\\u200b−αt2\\u200bC1\\u200b\\u200b)Здесь мы можем подвести общую черту и сказать, что методы Adam и RMSprop дают σs,i<0\\\\sigma_{s,i} < 0σs,i\\u200b<0, когда gt2g_t^2gt2\\u200b становится меньше предыдущей накопленной истории с точностью до некоторой константы.\\nА когда такое бывает? Уменьшение gt2g_t^2gt2\\u200b, как правило, означает приближение к критическим точкам. Добавление квадратичных регуляризаторов с отрицательным коэффициентом приводит к тому, что метод оптимизации штрафует за близость к критическим точкам, заставляя убегать от них. Это приводит к тому, что метод не может нормально сойтись к локальным минимумам (в выпуклых задачах — просто к минимумам, что намного более критично).\\nОтметим, что по разным координатам σs,i\\\\sigma_{s,i}σs,i\\u200b могут вести себя по-разному. Таким образом, можно получить ситуацию, когда мы поощряем близость по одним координатам и штрафуем за близость по другим.\\nВывод условий поломок для конкретных методов\\nAdaGrad\\nAdaGrad невозможно сломать таким способом: для него гарантируется, что σt≥σt−1\\\\sigma_t \\\\geq \\\\sigma_{t-1}σt\\u200b≥σt−1\\u200b.\\nRMSprop\\nvt,i=βvt−1,i+(1−β)gt,i2v_{t,i} = \\\\beta v_{t-1,i} + (1-\\\\beta)g_{t,i}^2\\nvt,i\\u200b=βvt−1,i\\u200b+(1−β)gt,i2\\u200bПодставим в условие (∗)(\\\\ast)(∗), сразу сократив константный αt=α\\\\alpha_t = \\\\alphaαt\\u200b=α:\\nβvt−1,i+(1−β)gt,i2<vt−1,i\\\\beta v_{t-1,i} + (1-\\\\beta)g_{t,i}^2 < v_{t-1,i}\\nβvt−1,i\\u200b+(1−β)gt,i2\\u200b<vt−1,i\\u200bgt,i2<vt−1,ig_{t,i}^2 < v_{t-1,i}\\ngt,i2\\u200b<vt−1,i\\u200bAdam\\nЧисто технически, при выведении формул можно подумать, что Adam страдает от указанных эффектов гораздо сильнее RMSprop, но на самом деле это не так.\\nПереобозначим β2\\\\beta_2β2\\u200b из статьи про Adam как просто β\\\\betaβ для общности обозначений.\\nРаспишем неравенство (∗)(\\\\ast)(∗) для метода Adam:\\nβvt−1+(1−β)gt21−βt<vt−11−βt−1\\\\frac{\\\\beta v_{t-1} + (1 - \\\\beta) g^2_t}{1 - \\\\beta^t} < \\\\frac{v_{t-1}}{1 - \\\\beta^{t-1}}\\n1−βtβvt−1\\u200b+(1−β)gt2\\u200b\\u200b<1−βt−1vt−1\\u200b\\u200b1−β1−βtgt2<vt−1(11−βt−1−β1−βt)\\\\frac{1 - \\\\beta}{1 - \\\\beta^t}g^2_t < v_{t-1}\\\\Big(\\\\frac{1}{1 - \\\\beta^{t-1}} - \\\\frac{\\\\beta}{1 - \\\\beta^t}\\\\Big)\\n1−βt1−β\\u200bgt2\\u200b<vt−1\\u200b(1−βt−11\\u200b−1−βtβ\\u200b)1−β1−βtgt2<vt−11−βt−β+βt(1−βt−1)(1−βt)\\\\frac{1 - \\\\beta}{1 - \\\\beta^t}g^2_t < v_{t-1}\\\\frac{1 - \\\\beta^t - \\\\beta + \\\\beta^t}{(1 - \\\\beta^{t-1})(1 - \\\\beta^t)}\\n1−βt1−β\\u200bgt2\\u200b<vt−1\\u200b(1−βt−1)(1−βt)1−βt−β+βt\\u200bgt2<11−βt−1vt−1g_t^2 < \\\\frac{1}{1 - \\\\beta^{t-1}} v_{t-1}\\ngt2\\u200b<1−βt−11\\u200bvt−1\\u200bВ отличие от RMSprop, у нас появился дополнительный множитель 11−βt−1>1\\\\frac{1}{1 - \\\\beta^{t-1}} > 11−βt−11\\u200b>1. С одной стороны, можно подумать, что метод строго хуже. Однако, этот множитель сильно больше нуля только во время первых шагов оптимизации, тогда как рассматриваемая нами проблема играет роль только на поздних стадиях оптимизации при приближении к критическим точкам. А к тому моменту, этот множитель будет практически равен единице и мы получим формулы выше от RMSprop.\\nПоэтому, на самом деле, методы в одинаковой степени страдают от этих эффектов, но bias correction добавляет стабильности в начале.\\nИнтерпретации\\nИзбегание локальных минимумов или седловых точек\\nЕсли представить, что нейросеть — очень плохая и жутко невыпуклая задача, то можно рассматривать подобное поведение как «защиту» от промежуточных плохих критических точек, позволяющую нам «убегать» от них.\\nДанная интерпретация, к сожалению, имеет множество недостатков:\\n\\nНикто не обещал, что новая критическая точка будет лучше старой и что мы, прыгая таким образом, будем улучшать качество модели.\\nНе каждый локальный минимум плохой. Если текущая критическая точка — хороший локальный минимум с хорошей обобщающей способностью, то мы просто нормально не сойдемся к нему и не достигнем хорошего качества модели.\\nОбщественность уже идентифицировала такое поведение как проблему и решила ее в более поздних популярных оптимизаторах (см.раздел про AMSgrad).\\nБольшинство современных рекомендаций по обучению больших неонлайновых моделей вроде GPT или картиночных моделей содержат в себе learning rate scheduler'ы как обязательный для успеха ингредиент. Эти рекомендации нивелируют проблему отрицательных регуляризаторов.\\nВсе learning rate scheduler'ы заставляют learning rate убывать, что позволяет достигать лучших результатов, чем с помощью обычных Adam и RMSprop.\\nВ параграфе про FTL мы узнали, что градиентный метод без регуляризации отвратительно работает даже на выпуклых задачах, а если мы начнём вводить отрицательную регуляризацию, да еще и на сложных невыпуклых задачах, то все может стать еще хуже.\\n\\nВ целом, мировой опыт говорит, что полагаться на подобные интерпретации при тюнинге модели не стоит.\\nНестабильность в выпуклых задачах\\nИтак, методы RMSprop и Adam плохо работают для выпуклых задач, особенно для разреженных задач, и могут приводить к субоптимальным решениям на train. Тем не менее, есть искушение заявить, что «это такая регуляризация в классическом смысле: не слишком хорошо сходимся к оптимальной точке, не слишком сильно переобучаемся под датасет и можем лучше работать на тесте». Это искушение особенно опасно потому, что подобные эффекты действительно могут иметь место, особенно в классической (не онлайновой) постановке задачи. Любая регуляризация направлена на то, чтобы сдвинуть оптимум решения исходной некорректно поставленной задачи в надежде, что точка оптимума измененной задачи будет обладать лучшей обобщающей способностью на тесте. В частности, такой эффект может иметь ранняя остановка методов оптимизации до их сходимости к точке оптимума.\\nОднако здесь есть одно очень важное «но». Если введение регуляризации в некорректно поставленную задачу — это полностью осмысленный и контролируемый гиперпараметрами процесс, то хаотично разваливающийся вокруг точки оптимума метод оптимизации — нет. Подумайте: вдруг ваша задача фактически не является некорректно поставленной? Вдруг у вас огромный и очень репрезентативный датасет, благодаря чему оптимум на train всегда отлично работает в проде? В этом случае кривой метод оптимизации способен подпортить качество вашей модели.\\nНестабильность в разреженных задачах\\nВ задачах с разреженными параметрами ситуацию gt,i2<vt−1,ig_{t,i}^2 < v_{t-1,i}gt,i2\\u200b<vt−1,i\\u200b получить еще легче. Допустим, у нас есть некоторый параметр wiw_iwi\\u200b, который встречается в 0.1% объектов выборки. В такой ситуации между появлениями этого объекта в выборке и очередным расчетом градиентов для него проходит значительное время. За это значительное время модель дообучалась, и за счет других, менее разреженных параметров могла научиться лучше прогнозировать очередной объект с этим параметром wiw_iwi\\u200b. Тогда ∣∣gt,i∣∣2\\\\vert\\\\vert g_{t,i}\\\\vert\\\\vert_2∣∣gt,i\\u200b∣∣2\\u200b уменьшается и, следовательно, больше шансов попасть в плохую ситуацию.\\nНиже мы рассмотрим метод AMSgrad и наперёд скажем, что для оптимизации разреженных параметров Adam/RMSprop добавление AMSgrad очень часто дает прибавку в качестве.\\nЗависимость нестабильности в регуляризаторе от learning rate α\\\\alphaα\\nНа первый взгляд, парадоксальным кажется следующий факт: чем меньше learning rate, тем в бОльшую сторону может отклониться отрицательный регуляризатор:\\nσt=vt−vt−1α\\\\sigma_t = \\\\frac{\\\\sqrt{v_t} - \\\\sqrt{v_{t-1}}}{\\\\alpha}\\nσt\\u200b=αvt\\u200b\\u200b−vt−1\\u200b\\u200b\\u200bОднако в «жадных» формулах все с точностью до наоборот:\\nwt+1=wt−αvtgtw_{t+1} = w_t - \\\\frac{\\\\alpha}{\\\\sqrt{v_t}}g_t\\nwt+1\\u200b=wt\\u200b−vt\\u200b\\u200bα\\u200bgt\\u200bИз жадных формул очевидно, что уменьшение α\\\\alphaα ведет к уменьшению шага и, как следствие, увеличению стабильности алгоритма.\\nЧтобы разрешить парадокс, надо вспомнить, что в FTRL решающее значение имеет не один отдельный регуляризатор, а сумма σ0:t\\\\sigma_{0:t}σ0:t\\u200b. В начале процесса оптимизации v0=0v_0 = 0v0\\u200b=0, первый регуляризатор точно не сломается. Чем меньше learning rate, тем меньшие шаги мы делаем от начальной точки и, следовательно, тем меньше должна отличаться норма градиентов.\\nЕсли от шага к шагу норма градиента меняется не слишком сильно, то мы накопим огромную кумулятивную регуляризацию σ0:t\\\\sigma_{0:t}σ0:t\\u200b к моменту, когда регуляризатор решит отклониться в отрицательную сторону. При бОльшем learning rate мы шагаем быстрее, и точки, когда ломается регуляризатор, достигаем тоже быстрее, накопив гораздо меньшую сумму σ0:t\\\\sigma_{0:t}σ0:t\\u200b. Если теперь для очередной точки мы получили отрицательный регуляризатор, то насколько сильно он может всё поломать?\\nОкей, допустим, мы шагнули к критической точке. А насколько сильно может расколбасить одна плохая точка в регуляризаторе? Так, чтобы он перекрыл всю предыдущую сумму σ0:t\\\\sigma_{0:t}σ0:t\\u200b? Если градиенты ограничены по норме, то катастрофы, очевидно, не будет. Ограниченность градиентов по норме мы, с одной стороны, гарантировать не можем, с другой — проблемам взрыва/затухания градиентов в архитектурах уделяется столько внимания, что на практике это условие зачастую выполняется.\\nЧиним RMSprop и Adam\\nМотивация\\nВремя шло, люди учились строить хорошо обучаемые архитектуры. Стали даже появляться революционные идеи вроде ReZero (не путать с аниме) с полным отказом от batchnorm/layernorm нормализаций в глубоких сетях и с улучшением качества работы и скорости сходимости. Ситуация со стабильностью обучения нейросетей кардинально изменилась.\\nНесмотря на улучшение стабильности обучения, люди стали замечать, что при длительном процессе оптимизации Adam начинает сбоить. Авторы метода AMSgrad в статье On the Convergence of Adam and Beyond были одними из первых, кто провел почти аналогичный нашему анализ и добавили в Adam костыль, который обеспечивает выполнение условия vt⩾vt−1v_{t} \\\\geqslant v_{t-1}vt\\u200b⩾vt−1\\u200b и исключает отрицательные регуляризаторы.\\nОбратите внимание: в разделее про Learning Rate Scheduling vs AdaGrad мы поговорим о «цикличности истории» развития методов оптимизации в deep learning.\\nAMSgrad\\nАвторы статьи On the Convergence of Adam and Beyond анализируют последовательность\\nΓt+1=(vt+1αt+1−vtαt)\\\\Gamma_{t+1} = \\\\Big(\\\\frac{\\\\sqrt{v_{t+1}}}{\\\\alpha_{t+1}} - \\\\frac{\\\\sqrt{v_t}}{\\\\alpha_t} \\\\Big)\\nΓt+1\\u200b=(αt+1\\u200bvt+1\\u200b\\u200b\\u200b−αt\\u200bvt\\u200b\\u200b\\u200b)и говорят, что отрицательные значения в ней вызывают проблемы с процессом оптимизации. Их анализ в целом аналогичен приведённому выше, поэтому мы не будем его здесь дублировать.\\nАвторы статьи не стали предлагать новых схем learning rate и просто модифицировали старую: выполнение vt>=vt−1v_t >= v_{t-1}vt\\u200b>=vt−1\\u200b обеспечивается «в лоб» при помощи v^t=max\\u2061{vt,v^t−1},v^0=0\\\\hat{v}_t = \\\\max\\\\{v_t, \\\\hat{v}_{t-1}\\\\}, \\\\hat{v}_0 = 0v^t\\u200b=max{vt\\u200b,v^t−1\\u200b},v^0\\u200b=0.\\nИтоговое правило апдейта без momentum и без bias correction (оригинальный Algorithm 2 из статьи bias correction не использует):\\nvt=βvt−1+(1−β)gt2v_t = \\\\beta v_{t-1} + (1 - \\\\beta) g_t^2\\nvt\\u200b=βvt−1\\u200b+(1−β)gt2\\u200bv^t=max\\u2061{vt,v^t−1}\\\\hat{v}_t = \\\\max\\\\{v_t, \\\\hat{v}_{t-1}\\\\}\\nv^t\\u200b=max{vt\\u200b,v^t−1\\u200b}wt+1=wt−αv^tgtw_{t+1} = w_t - \\\\frac{\\\\alpha}{\\\\sqrt{\\\\hat{v}_t}}g_t\\nwt+1\\u200b=wt\\u200b−v^t\\u200b\\u200bα\\u200bgt\\u200bЕсли нужен метод с momentum, то можно просто заменить gtg_tgt\\u200b в последней формуле на mtm_tmt\\u200b\\nmt=γmt−1+(1−γ)gtm_t = \\\\gamma m_{t-1} + (1 - \\\\gamma) g_t\\nmt\\u200b=γmt−1\\u200b+(1−γ)gt\\u200bwt+1=wt−αv^tmtw_{t+1} = w_t - \\\\frac{\\\\alpha}{\\\\sqrt{\\\\hat{v}_t}}m_t\\nwt+1\\u200b=wt\\u200b−v^t\\u200b\\u200bα\\u200bmt\\u200bРеализация без дополнительной памяти\\nОригинальные формулы из статьи v^t=max\\u2061{vt,v^t−1}\\\\hat{v}_t = \\\\max\\\\{v_t, \\\\hat{v}_{t-1}\\\\}v^t\\u200b=max{vt\\u200b,v^t−1\\u200b} предполагают, что для расчета v^t\\\\hat{v}_tv^t\\u200b мы держим два параметра: vt−1v_{t-1}vt−1\\u200b и v^t−1\\\\hat{v}_{t-1}v^t−1\\u200b. RMSprop и Adam хранят только один параметр vt−1v_{t-1}vt−1\\u200b. Таким образом, включение метода требует дополнительных расходов памяти (х1.5 относительно RMSprop и x1.33 относительно Adam). Выше при разборе методов RMSprop/Adam мы сказали, что на практике AMSgrad помогает разреженным параметрам. Для разреженных моделей потребление памяти — краеугольный камень, поэтому простое включение дефолтной реализации amsgrad из статьи может быть болезненным и, к сожалению, не оправданным.\\nНа практике же эвристика вида vt=max\\u2061{vt,vt−1}v_t = \\\\max\\\\{v_t, v_{t-1}\\\\}vt\\u200b=max{vt\\u200b,vt−1\\u200b} для разреженных параметров обычно работает так же хорошо и не требует дополнительной памяти. Никаких теоретических гарантий для нее нет, но на практике она работает.\\nДобавление bias correction\\nОригинальная статья (и следующие букве оригинала стандартные реализации алгоритма, например, в PyTorch) предполагает убирание bias correction. Эксперименты на разреженных данных показывают, что убирание bias correction вредит сходимости, это полезная вещь.\\nС практической точки зрения, есть два способа реализовать bias correction в AMSgrad:\\n\\nPost-correction: v^t=11−βtmax\\u2061{vt,v^t−1}\\\\hat{v}_t = \\\\frac{1}{1 - \\\\beta^t}\\\\max\\\\{v_t, \\\\hat{v}_{t-1}\\\\}v^t\\u200b=1−βt1\\u200bmax{vt\\u200b,v^t−1\\u200b},\\nPre-correction: v^t=max\\u2061{11−βtvt,11−βt−1v^t−1}\\\\hat{v}_t = \\\\max\\\\{\\\\frac{1}{1 - \\\\beta^t}v_t, \\\\frac{1}{1 - \\\\beta^{t-1}}\\\\hat{v}_{t-1}\\\\}v^t\\u200b=max{1−βt1\\u200bvt\\u200b,1−βt−11\\u200bv^t−1\\u200b}.\\n\\nС точки зрения корректности метода AMSgrad, правильный вариант — pre-correction, так как он не ломает максимум. А вот эксперименты показывают, что добавление pre-correction ничего не даёт, а вот post-correction действительно помогает в том смысле, что AMSgrad + post-bias correction лучше, чем просто RMSProp/Adam с bias correction.\\nСоединяем эвристику + bias correction\\nИтоговые формулы можно использовать такие:\\nv^=11−βtmax\\u2061{vt,11−βt−1vt−1}\\\\hat{v} = \\\\frac{1}{1 - \\\\beta^t}\\\\max\\\\left\\\\{v_t, \\\\frac{1}{1-\\\\beta^{t-1}}v_{t-1}\\\\right\\\\}\\nv^=1−βt1\\u200bmax{vt\\u200b,1−βt−11\\u200bvt−1\\u200b}Learning Rate Scheduling\\nДругим способом улучшения сходимости методов RMSprop/Adam/SGD является learning rate scheduling (расписание learning rate, шедулер). Learning rate scheduler — это мета-алгоритм: они берёт любой стандартный метод оптимизации с константным параметром learning rate α\\\\alphaα и предписывает схему изменения αt\\\\alpha_tαt\\u200b на каждом шаге ttt, или на каждой эпохе, или на любом другом заданном периоде.\\nПоскольку мы работаем с одним параметром α\\\\alphaα, мы можем с ним делать всего две вещи: увеличивать или уменьшать. Эти два варианта имеют свои названия:\\n\\nLearning rate decay — уменьшение learning rate с течением времени с целью нивелировать осцилляцию RMSprop/Adam около критических точек.\\n(Warm)Restart — обычно резкое увеличение learning rate. Warm — потому что мы уже сошлись в какую-то хорошую точку и сбрасываем только состояние оптимизатора в ней, но не переинициализируем сами параметры. WarmRestart может заключаться не только в увеличении α\\\\alphaα, но и, например, в дополнительном сбросе состояния оптимизатора (обнуление momentum или vtv_{t}vt\\u200b), хотя автор статьи такой подход встречали достаточно редко\\n\\nСуществует огромное количество вариантов расписания, каждый со своим графиком изменения αt\\\\alpha_tαt\\u200b и со своим любовно подобранным множеством задач, на которых данный метод показывает себя лучше других. Приводить здесь их список особого смысла нет, лучше просто откройте документацию любого фреймворка и наслаждайтесь разнообразием вариантов.\\nМы же обсудим влияние learning rate decay на осцилляцию вокруг критических точек и дадим практические рекомендации по подбору расписаний.\\nВлияние learning rate decay на сходимость\\nДля выпуклых задач в разделе про схемы убывания learning rate для FTRL-методов (константный регуляризатор, 1sqrtt\\\\frac{1}{sqrt{t}}sqrtt1\\u200b и AdaGrad) мы буквально на оценках на regret видели, что это важный аспект для асимптотики сходимости.\\nВ выпуклом случае, при приближении к минимуму мы должны оптимизировать решение с куда большей точностью. Норма градиентов при приближении к минимуму тоже уменьшаются, поэтому даже с константным O(1)O(1)O(1) learning rate шаги будут становиться меньше, но — как показывают и теоретические оценки на regret, и многочисленные их валидации в статьях — этого недостаточно. Уменьшение learning rate с правильной асимптотикой уменьшения дает куда более хорошие результаты. Для глубинного обучения и оптимизации к каким-то локальным минимумам эта логика тоже применима.\\nВозвращаясь к методам Adam/RMSprop — напомним, что у них асимптотика learning rate O(1)O(1)O(1). Им в любом случае пойдет на пользу уменьшение learning rate, даже если не брать во внимание их проблемы вокруг критических точек и взять метод AMSgrad, который от этих проблем не страдает.\\nОтсюда же очевидно, что проблемы adam/rmsprop начинают стрелять гораздо меньше. Learning rate уменьшается => от критической точки мы в плохих ситуациях шагаем на гораздо меньшее расстояние => область, вокруг которой мы будем «прыгать», сужается => мы худо-бедно, но сходимся.\\nПрактические рекомендации\\nКак мы уже отмечали выше, шедулеров существует поистине фантастическое количество, гораздо больше, чем базовых оптимизаторов, к которым они применяются. Без структуризации подхода к ним работать становится сложно.\\nМы хотели бы дать вам следующие рекомендации:\\n\\nВыучите свою модель без learning rate scheduling со стандартными методами оптимизации и посмотрите, как ведёт себя loss для различных learning rate. Обязательно переберите learning rate на этом шаге.\\nНачинать внедрение расписаний рекомендуем с шедулеров, которые только уменьшают learning rate. Классические варианты — ReduceOnPlateou или linear decay. Правильный подбор learning rate и темпа его уменьшения очень важны в любой задаче стохастической оптимизации.\\nТолько после того, как вы хорошенько потюните learning rate decay, можно смотреть в сторону WarmRestart. Иногда рестарты могут помочь. Автор статьи занимается в основном рекомендательными моделями и там эту технику практически никто не применяет.\\n\\nLearning rate scheduling vs AdaGrad\\nУ методов SGD/RMSprop/Adam последовательность ηt∼O(1)\\\\eta_t \\\\sim O(1)ηt\\u200b∼O(1) не является асимптотически убывающей, и для того, чтобы это скомпенсировать, используется расписание learning rate. А вот у AdaGrad с ηt\\\\eta_tηt\\u200b и так всё в порядке.\\nДавайте восстановим хронологию событий:\\n\\nМетод AdaGrad пытаются применять к нейросетям в 2012+ годах, но тогда архитектуры были нестабильны, градиенты взрывались и навсегда портили знаменатель AdaGrad, сильно уменьшая learning rate.\\nПоявляются методы RMSprop/Adam (2013/2014) со скользящим средним в знаменателе, которые могут оправиться от взрыва градиента.\\nРазвитие архитектур нейронных сетей не стоит на месте, появляются разные виды residual connection (2015), LayerNorm/BatchNorm (2015-2016), крутые методы начальной инициализации — огромное количество способов улучшения стабильности обучения.\\nС развитием архитектур люди замечают, что RMSProp/Adam умеют застревать на одном уровне значений функции потерь, и начинают применять техники для уменьшения learning rate.\\nВ дальнейших работах метод AdaGrad часто рассматривается наравне с Adam/RSMprop и дает очень похожее, либо даже лучшее качество (см, например, статью про Shampoo). А дело в том, что архитектуры уже очень хорошо инициализируются и правильно проектируются так, чтобы не было взрывов/затуханий градиентов ни на какой стадии оптимизации.\\n\\nРазвитие методов оптимизации в deep learning сделало небольшой круг, и мы рекомендуем об этом помнить. Порой люди могут одновременно рассуждать о бесценной пользе learning rate decay (особенно с линейным убыванием как 1t\\\\frac1tt1\\u200b) и корить AdaGrad за бесконечное аккумулирование квадратов градиентов (которые убывают как 1t\\\\frac1{\\\\sqrt{t}}t\\u200b1\\u200b). Так что если у вас вдруг хорошо заработал шедулер с αt∼O(1t)\\\\alpha_t \\\\sim O(\\\\frac{1}{\\\\sqrt{t}})αt\\u200b∼O(t\\u200b1\\u200b) — возможно, обычный AdaGrad будет лучше?\\nSGD vs Adam\\nВ последнее время в литературе часто появляются заявления, что решения, полученные адаптивными методами в нейросетях, обладают худшей обобщающей способностью. Сразу хотим отметить, что большинство этих статей исследуют эти эффекты только на задачах Computer Vision на одних и тех же датасетах MNIST/CIFAR/ImageNet. В реальной жизни куда большее разнообразие постановок задач и датасетов, что сразу заставляет сомневаться в воспроизводимости этих эффектов. Рекомендация тут одна, как и всегда — досконально сами все проверяйте.\\nAdamW, SGDW\\nДанные методы предложены авторами в статье Decoupled Weight Decay Regularization, которую мы подробно разобрали в разделее про продвинутую L2L_2L2\\u200b регуляризацию. Методы AdamW и SGDW — это просто модификации методов Adam и SGD с momentum, которые используют линеаризованный decoupled L2L_2L2\\u200b.\\nАвторы статьи изучали проблему, почему в их экспериментах SGD обобщает лучше Adam (но учится дольше и требует более аккуратной настройки). Они пришли к выводу, что дело не в магии SGD, а в том, что L2L_2L2\\u200b-регуляризация у этих двух методов работает по-разному. Добавив decoupling, авторы сумели показать, что decoupled Adam обгоняет SGD.\\nЭти эффекты, повторимся, были уже рассмотрены ранее в разделее про продвинутую L2L_2L2\\u200b регуляризацию. Единственное, что мы не обсудили тогда — это momentum. В постановке Proximal Gradient Descent градиент заменяется на momentum\\nwt+1=gtTw+λ2ηtwt+12ηt∣∣w−ws∣∣22w_{t+1} = \\\\color{red}{g_t}^Tw + \\\\frac{\\\\lambda_2}{\\\\eta_t}w_t + \\\\frac{1}{2\\\\eta_t}\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_2^2\\nwt+1\\u200b=gt\\u200bTw+ηt\\u200bλ2\\u200b\\u200bwt\\u200b+2ηt\\u200b1\\u200b∣∣w−ws\\u200b∣∣22\\u200bmt=γmt−1+(1−γ)gtm_t = \\\\gamma m_{t-1} + (1 - \\\\gamma) g_t\\nmt\\u200b=γmt−1\\u200b+(1−γ)gt\\u200bwt+1=mtTw+λ2ηtwt+12ηt∣∣w−ws∣∣22w_{t+1} = \\\\color{red}{m_t}^Tw + \\\\frac{\\\\lambda_2}{\\\\eta_t}w_t + \\\\frac{1}{2\\\\eta_t}\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_2^2\\nwt+1\\u200b=mt\\u200bTw+ηt\\u200bλ2\\u200b\\u200bwt\\u200b+2ηt\\u200b1\\u200b∣∣w−ws\\u200b∣∣22\\u200bwt+1=wt−ηtmt−λ2wtw_{t+1} = w_t - \\\\eta_t m_t - \\\\lambda_2 w_t\\nwt+1\\u200b=wt\\u200b−ηt\\u200bmt\\u200b−λ2\\u200bwt\\u200bПокоординатные ηt\\\\eta_tηt\\u200b могут рассчитываться любыми методами: AdaGrad, RMSprop или Adam, не принципиально.\\nНа всякий случай напомним, что мы вывели потенциально более правильные формулы\\nwt+1=mtTw+λ22ηt∣∣w∣∣w2+12ηt∣∣w−ws∣∣22w_{t+1} = m_t^Tw + \\\\frac{\\\\lambda_2}{2\\\\eta_t}\\\\vert\\\\vert w\\\\vert\\\\vert_w^2 + \\\\frac{1}{2\\\\eta_t}\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_2^2\\nwt+1\\u200b=mtT\\u200bw+2ηt\\u200bλ2\\u200b\\u200b∣∣w∣∣w2\\u200b+2ηt\\u200b1\\u200b∣∣w−ws\\u200b∣∣22\\u200bwt+1=11+λ2(wt−ηtmt)w_{t+1} = \\\\frac{1}{1 + \\\\lambda_2}\\\\Big(w_t - \\\\eta_t m_t\\\\Big)\\nwt+1\\u200b=1+λ2\\u200b1\\u200b(wt\\u200b−ηt\\u200bmt\\u200b)Метод SGDW получается из формул выше, если убрать покоординатность ηt\\\\eta_tηt\\u200b\\nК сожалению, здесь мы не почерпнули новых идей, так как выяснили, что это просто очередная инкарнация Proximal методов оптимизации.\\nRAdam\\nЭтот метод заключается в том, чтобы стартовать с адаптивного метода Adam и в некоторый момент переключиться на SGD. «Некоторый момент» — это, интуитивно, момент стабилизации всех статистик в Adam, когда мы выжали все из ускоренного старта адаптивных методов и хотим получше сойтись к хорошему оптимуму в найденной им окрестности.\\nОтметим, что позднее переключение на SGD с неубывающими learning rate автоматически починит проблемы расходимости Adam ровно там, где они чаще всего и возникают: при хорошем приближении к локальным минимумам.\\nМы не будем здесь подробно рассматривать их анализ, вы можете сами познакомиться с ним в статье On The Variance Of The Adaptive Learning Rate And Beyond\\nOnline RMSprop\\nОсобняком стоит метод, описанный в статье Variants of RMSProp and Adagrad with Logarithmic Regret Bounds. Авторы не придумывали очередной хотфикс, а аккуратно заново выводили формулы. Также важно, что данный метод является строгим обобщением метода AdaGrad.\\nВ работе есть два нововведения:\\n\\nПереформулировка метода RMSprop так, чтобы:\\n— Осталось экспоненциальное скользящее среднее;\\n— Не было проблемы с отрицательными регуляризаторами и взрывающимися learning rate;\\n— Метод AdaGrad являлся частным случаем нового метода;\\n— Чтобы все эмпирически хорошо работало в т.ч. на глубоких моделях\\nФормулировка новых алгоритмов оптимизации SC-AdaGrad и SC-RMSprop для сильно выпуклых функций с логарифмическими гарантиями на regret. SC в названии — Strongly Convex.\\n\\nПока рассмотрим только первый пункт. Авторы вводят следующий общий метод:\\nvt=βtvt−1+(1−βt)gt2v_t = \\\\beta_t v_{t-1} + (1 - \\\\beta_t) g_t^2\\nvt\\u200b=βt\\u200bvt−1\\u200b+(1−βt\\u200b)gt2\\u200bϵt=ϵt\\\\epsilon_t = \\\\frac{\\\\epsilon}{\\\\sqrt{t}}\\nϵt\\u200b=t\\u200bϵ\\u200bαt=αt\\\\alpha_t = \\\\frac{\\\\alpha}{\\\\sqrt{t}}\\nαt\\u200b=t\\u200bα\\u200bwt+1=wt−αtvt+ϵtgtw_{t+1} = w_t - \\\\frac{\\\\alpha_t}{\\\\sqrt{v_t} + \\\\epsilon_t}g_t\\nwt+1\\u200b=wt\\u200b−vt\\u200b\\u200b+ϵt\\u200bαt\\u200b\\u200bgt\\u200bНововведение здесь в том, что вместо фиксированного β\\\\betaβ мы будем рассматривать последовательность βt\\\\beta_tβt\\u200b. Авторы доказывают сублинейный regret для любых последовательностей, удовлетворяющих\\n1−1t≤βt≤1−γt1 - \\\\frac{1}{t} \\\\leq \\\\beta_t \\\\leq 1 - \\\\frac{\\\\gamma}{t}\\n1−t1\\u200b≤βt\\u200b≤1−tγ\\u200b0<γ≤10 < \\\\gamma \\\\leq 1\\n0<γ≤1AdaGrad как частный случай\\nДокажем, что метод Adagrad — это метод OnlineRMSprop с γ=1\\\\gamma = 1γ=1. Аналогично выводам momentum в FTRL, перепишем рекуррентное выражение для vt+1v_{t+1}vt+1\\u200b:\\nvt=∑s=1t(1−βs)∏k=s+1tβkgk2v_t = \\\\sum\\\\limits_{s=1}^t(1 - \\\\beta_s)\\\\prod\\\\limits_{k=s+1}^t\\\\beta_k g_k^2\\nvt\\u200b=s=1∑t\\u200b(1−βs\\u200b)k=s+1∏t\\u200bβk\\u200bgk2\\u200bПодставив β=1−1t\\\\beta = 1 - \\\\frac{1}{t}β=1−t1\\u200b, получим\\nvt=∑s=1t(1−βs)(∏k=s+1tβk)gs2=∑s=1t1sgs2∏k=s+1tt−1t=v_t = \\\\sum\\\\limits_{s=1}^t(1 - \\\\beta_s)\\\\Big(\\\\prod\\\\limits_{k=s+1}^t\\\\beta_k\\\\Big) g_s^2 = \\\\sum\\\\limits_{s=1}^t\\\\frac{1}{s}g_s^2\\\\prod\\\\limits_{k=s+1}^t\\\\frac{t - 1}{t} =\\nvt\\u200b=s=1∑t\\u200b(1−βs\\u200b)(k=s+1∏t\\u200bβk\\u200b)gs2\\u200b=s=1∑t\\u200bs1\\u200bgs2\\u200bk=s+1∏t\\u200btt−1\\u200b==∑s=1t1sgs2∏k=s+1tk−1k=∑s=1t1sgs2st=1t∑s=1tgs2= \\\\sum\\\\limits_{s=1}^t\\\\frac{1}{s}g_s^2\\\\prod\\\\limits_{k=s+1}^t\\\\frac{k - 1}{k} = \\\\sum\\\\limits_{s=1}^t\\\\frac{1}{s}g_s^2\\\\frac{s}{t} = \\\\frac{1}{t}\\\\sum\\\\limits_{s=1}^tg_s^2\\n=s=1∑t\\u200bs1\\u200bgs2\\u200bk=s+1∏t\\u200bkk−1\\u200b=s=1∑t\\u200bs1\\u200bgs2\\u200bts\\u200b=t1\\u200bs=1∑t\\u200bgs2\\u200bДалее, подставляя это в формулу ηt=αtvt\\\\eta_t = \\\\frac{\\\\alpha_t}{\\\\sqrt{v_t}}ηt\\u200b=vt\\u200b\\u200bαt\\u200b\\u200b, получаем\\nηt=αt1t∑s=1tgs2=α∑s=1tgs2\\\\eta_t = \\\\frac{\\\\frac{\\\\alpha}{\\\\sqrt{t}}}{\\\\sqrt{\\\\frac{1}{t}\\\\sum\\\\limits_{s=1}^tg_s^2}} = \\\\frac{\\\\alpha}{\\\\sqrt{\\\\sum\\\\limits_{s=1}^tg_s^2}}\\nηt\\u200b=t1\\u200bs=1∑t\\u200bgs2\\u200b\\u200bt\\u200bα\\u200b\\u200b=s=1∑t\\u200bgs2\\u200b\\u200bα\\u200bАнализ OnlineRMSprop с γ<1\\\\gamma < 1γ<1 в стиле FTRL. Пригодность для выпуклых задач\\nДокажем, что OnlineRMSprop не может сломать регуляризаторы в regret. Для этого преобразуем неравенство\\nvtαt<vt−1αt−1\\\\frac{\\\\sqrt{v_t}}{\\\\alpha_t} < \\\\frac{\\\\sqrt{v_{t-1}}}{\\\\alpha_{t-1}}\\nαt\\u200bvt\\u200b\\u200b\\u200b<αt−1\\u200bvt−1\\u200b\\u200b\\u200btvt<(t−1)vt−1tv_t < (t-1)v_{t-1}\\ntvt\\u200b<(t−1)vt−1\\u200bt(βtvt−1+(1−βt)gt2)<(t−1)vt−1t(\\\\beta_t v_{t-1} + (1 - \\\\beta_t)g_t^2) < (t-1)v_{t-1}\\nt(βt\\u200bvt−1\\u200b+(1−βt\\u200b)gt2\\u200b)<(t−1)vt−1\\u200bt((1−γt)vt−1+γtgt2)<(t−1)vt−1t((1 - \\\\frac{\\\\gamma}{t})v_{t-1} + \\\\frac{\\\\gamma}{t}g_t^2) < (t-1)v_{t-1}\\nt((1−tγ\\u200b)vt−1\\u200b+tγ\\u200bgt2\\u200b)<(t−1)vt−1\\u200bγtgt2<vt−1(t−1−t(1−γt))\\\\frac{\\\\gamma}{t}g_t^2 < v_{t-1}(t - 1 - t(1 - \\\\frac{\\\\gamma}{t}))\\ntγ\\u200bgt2\\u200b<vt−1\\u200b(t−1−t(1−tγ\\u200b))γtgt2<vt−1(γ−1)\\\\frac{\\\\gamma}{t}g_t^2 < v_{t-1}(\\\\gamma - 1)\\ntγ\\u200bgt2\\u200b<vt−1\\u200b(γ−1)Из условия 0<γ≤10 < \\\\gamma \\\\leq 10<γ≤1 получаем, что правая часть неравенства неположительна, а левая неотрицательно. Значит, последнее неравенство невозможно, то есть все σt≥0\\\\sigma_t \\\\geq 0σt\\u200b≥0. Таким образом, регуляризаторы не сломаются, сходимость будет иметь место и данный метод можно использовать в выпуклых задачах. Строгое доказательство сходимости и оценки на Regret можно прочитать в исходной статье.\\nЭффективный learning rate\\nКак и ранее в методе AdaGrad, допустим, что ∣∣g∣∣2<R\\\\vert\\\\vert g\\\\vert\\\\vert_2 < R∣∣g∣∣2\\u200b<R. Тогда\\nηt=αt∑j=1t(1−βj)∏k=j+1tβkgj2≤αRt∑j=1t(1−βj)∏k=j+1tβk\\\\eta_t = \\\\frac{\\\\alpha}{\\\\sqrt{t}\\\\sqrt{\\\\sum\\\\limits_{j=1}^t(1-\\\\beta_j)\\\\prod\\\\limits_{k=j+1}^t\\\\beta_kg_j^2}} \\\\leq \\\\frac{\\\\alpha}{R\\\\sqrt{t}\\\\sqrt{\\\\sum\\\\limits_{j=1}^t(1-\\\\beta_j)\\\\prod\\\\limits_{k=j+1}^t\\\\beta_k}}\\nηt\\u200b=t\\u200bj=1∑t\\u200b(1−βj\\u200b)k=j+1∏t\\u200bβk\\u200bgj2\\u200b\\u200bα\\u200b≤Rt\\u200bj=1∑t\\u200b(1−βj\\u200b)k=j+1∏t\\u200bβk\\u200b\\u200bα\\u200bПри 1−1t≤βt≤1−γt1 - \\\\frac{1}{t} \\\\leq \\\\beta_t \\\\leq 1 - \\\\frac{\\\\gamma}{t}1−t1\\u200b≤βt\\u200b≤1−tγ\\u200b выполнено\\nlim\\u2061t→∞∑j=1t(1−βj)∏k=j+1tβk=1\\\\lim\\\\limits_{t\\\\rightarrow\\\\infty} \\\\sum\\\\limits_{j=1}^t(1-\\\\beta_j)\\\\prod\\\\limits_{k=j+1}^t\\\\beta_k = 1\\nt→∞lim\\u200bj=1∑t\\u200b(1−βj\\u200b)k=j+1∏t\\u200bβk\\u200b=1Докажем, что все элементы предела < 1. Из этого, в частности, будет следовать, что learning rate у OnlineRMSprop не меньше, чем learning rate в AdaGrad.\\nЕсли все все gi2=1g_i^2 = 1gi2\\u200b=1, то итерационный процесс OnlineRMSprop превращается в\\nvt=βtvt−1+(1−βt)v_t = \\\\beta_t v_{t-1} + (1 - \\\\beta_t)\\nvt\\u200b=βt\\u200bvt−1\\u200b+(1−βt\\u200b)Предположим, что vt≥1v_t \\\\geq 1vt\\u200b≥1. Тогда:\\nβtvt−1+(1−βt)≥1\\\\beta_t v_{t-1} + (1 - \\\\beta_t) \\\\geq 1\\nβt\\u200bvt−1\\u200b+(1−βt\\u200b)≥1(1−γt)vt−1+γt≥1(1 - \\\\frac{\\\\gamma}{t}) v_{t-1} + \\\\frac{\\\\gamma}{t} \\\\geq 1\\n(1−tγ\\u200b)vt−1\\u200b+tγ\\u200b≥1vt−1≥1v_{t-1} \\\\geq 1\\nvt−1\\u200b≥1По индукции разворачиваем вплоть до v0=0v_0 = 0v0\\u200b=0, получаем противоречие.\\nПолное доказательство предела оставляем читателям. Надо бы чем-нибудь снизу подпереть, что тоже к 1 сходится. Автор сдавал матан почти 10 лет назад и ему было очень неохота откапывать все эти прекрасные пределы, поэтому ответ был получен с помощью wolfram.\\nВывод: learning rate у OnlineRMSprop убывает со скоростью ηt=O(1t)\\\\eta_t = O(\\\\frac{1}{\\\\sqrt{t}})ηt\\u200b=O(t\\u200b1\\u200b). Мы исправили ошибку предыдущего RMSprop, изменив только перевзвешивание, но не асимптотику в ηt\\\\eta_tηt\\u200b. Такой RMSprop можно пробовать использовать в выпуклых задачах\\nMomentum\\nПопробуем расписать классический momentum с константным learning rate в стиле FTRL:\\nvt+1=βvt+(1−β)gtv_{t + 1} = \\\\beta v_t + (1 - \\\\beta) g_{t}\\nvt+1\\u200b=βvt\\u200b+(1−β)gt\\u200bwt+1=wt−αvt+1w_{t + 1} = w_t - \\\\alpha v_{t + 1}\\nwt+1\\u200b=wt\\u200b−αvt+1\\u200bВсё, что нам нужно сделать — это взять все рекурсивные зависимости от предыдущей итерации и «размотать» их, получив явное выражение.\\nЗависимость wt+1w_{t+1}wt+1\\u200b от wtw_twt\\u200b переписать довольно просто, мы это уже делали для обычного градиентного спуска:\\nwt+1=−α∑i=1tviw_{t+1} = -\\\\alpha\\\\sum\\\\limits_{i=1}^tv_i\\nwt+1\\u200b=−αi=1∑t\\u200bvi\\u200bТеперь надо размотать vt=(1−β)∑i=1tβt−igiv_t = (1-\\\\beta)\\\\sum\\\\limits_{i=1}^t\\\\beta^{t-i}g_ivt\\u200b=(1−β)i=1∑t\\u200bβt−igi\\u200b\\nТеперь будет чуть сложнее. Подставим это и попробуем расписать, как сумму gig_igi\\u200b с определенными коэффициентами:\\nwt+1=−α∑i=1tvi=−α∑i=1t(1−β)∑j=1iβi−jgjw_{t+1} = -\\\\alpha\\\\sum\\\\limits_{i=1}^tv_i = -\\\\alpha\\\\sum\\\\limits_{i=1}^t (1-\\\\beta)\\\\sum\\\\limits_{j=1}^i\\\\beta^{i-j}g_j\\nwt+1\\u200b=−αi=1∑t\\u200bvi\\u200b=−αi=1∑t\\u200b(1−β)j=1∑i\\u200bβi−jgj\\u200bМножитель −α(1−β)-\\\\alpha(1-\\\\beta)−α(1−β) сразу выносим за сумму и пока забываем.\\n∑i=1t∑j=1iβi−jgj=∑i=1t∑j=1tI(j≤i)βi−jgj=\\\\sum\\\\limits_{i=1}^t\\\\sum\\\\limits_{j=1}^i\\\\beta^{i-j}g_j = \\\\sum\\\\limits_{i=1}^t\\\\sum\\\\limits_{j=1}^t \\\\mathbb{I}(j\\\\leq i)\\\\beta^{i-j}g_j =\\ni=1∑t\\u200bj=1∑i\\u200bβi−jgj\\u200b=i=1∑t\\u200bj=1∑t\\u200bI(j≤i)βi−jgj\\u200b==∑j=1t∑i=1tI(j≤i)βi−jgj=∑j=1tgj∑i=1tI(j≤i)βi−j== \\\\sum\\\\limits_{j=1}^t\\\\sum\\\\limits_{i=1}^t\\\\mathbb{I}(j\\\\leq i)\\\\beta^{i-j}g_j = \\\\sum\\\\limits_{j=1}^tg_j\\\\sum\\\\limits_{i=1}^t\\\\mathbb{I}(j\\\\leq i)\\\\beta^{i-j} =\\n=j=1∑t\\u200bi=1∑t\\u200bI(j≤i)βi−jgj\\u200b=j=1∑t\\u200bgj\\u200bi=1∑t\\u200bI(j≤i)βi−j==∑j=1tgj∑i=jtβi−j= \\\\sum\\\\limits_{j=1}^tg_j\\\\sum\\\\limits_{i=j}^t\\\\beta^{i-j}\\n=j=1∑t\\u200bgj\\u200bi=j∑t\\u200bβi−jОтлично, а теперь нам нужно получить последовательность функций. В линеаризованной задаче это фактически эквивалентно получению зависимости zt+1z_{t+1}zt+1\\u200b от ztz_tzt\\u200b, где, напомним, ztz_tzt\\u200b — это сумма градиентов.\\nzt+1−zt=∑j=1t+1gj∑i=jt+1βi−j−∑j=1tgj∑i=jtβi−j=gt+1+∑j=1tgj∑i=jt+1βi−j−∑j=1tgj∑i=jtβi−j=gt+1+∑j=1tβt+1−jgj=∑j=1t+1βt+1−jgjz_{t+1} - z_t = \\\\sum\\\\limits_{j=1}^{t+1}g_j\\\\sum\\\\limits_{i=j}^{t+1}\\\\beta^{i-j} - \\\\sum\\\\limits_{j=1}^tg_j\\\\sum\\\\limits_{i=j}^t\\\\beta^{i-j} = g_{t+1} + \\\\sum\\\\limits_{j=1}^{t}g_j\\\\sum\\\\limits_{i=j}^{t+1}\\\\beta^{i-j} - \\\\sum\\\\limits_{j=1}^tg_j\\\\sum\\\\limits_{i=j}^t\\\\beta^{i-j} = g_{t+1} + \\\\sum\\\\limits_{j=1}^{t}\\\\beta^{t+1-j}g_j = \\\\sum\\\\limits_{j=1}^{t+1}\\\\beta^{t+1-j}g_j\\nzt+1\\u200b−zt\\u200b=j=1∑t+1\\u200bgj\\u200bi=j∑t+1\\u200bβi−j−j=1∑t\\u200bgj\\u200bi=j∑t\\u200bβi−j=gt+1\\u200b+j=1∑t\\u200bgj\\u200bi=j∑t+1\\u200bβi−j−j=1∑t\\u200bgj\\u200bi=j∑t\\u200bβi−j=gt+1\\u200b+j=1∑t\\u200bβt+1−jgj\\u200b=j=1∑t+1\\u200bβt+1−jgj\\u200bТеперь мы можем записать функцию, градиент которой равен zt+1−ztz_{t+1} - z_tzt+1\\u200b−zt\\u200b и онлайн-оптимизация которой эквивалентна процедуре с моментумом:\\nf^t(w)=∑j=1tβt−jfj(w)\\\\hat{f}_t(w) = \\\\sum\\\\limits_{j=1}^{t}\\\\beta^{t-j}f_j(w)\\nf^\\u200bt\\u200b(w)=j=1∑t\\u200bβt−jfj\\u200b(w)Получаем, что для онлайн-обучения мы на самом деле каждую итерацию скармливаем экспоненциально взвешенную последовательность всех предыдущих функций исходной последовательности. В принципе, нечто такое мы и ожидали увидеть. Функции ft(w)^\\\\hat{f_t(w)}ft\\u200b(w)^\\u200b, очевидно, выпуклы, так что для данной измененной последовательности функций будет сублинейный regret.\\nNesterov Momentum\\nРассмотрим классический SGD с momentum, для всех adaptive методов рассуждения аналогичны.\\nmt=γmt−1+(1−γ)gtm_t = \\\\gamma m_{t-1} + (1 - \\\\gamma)g_t\\nmt\\u200b=γmt−1\\u200b+(1−γ)gt\\u200bwt+1=wt−αmtw_{t+1} = w_t - \\\\alpha m_t\\nwt+1\\u200b=wt\\u200b−αmt\\u200bГрадиент функции gtg_tgt\\u200b посчитан в предыдущей точке wtw_twt\\u200b. Идея nesterov momentum в том, чтобы применить momentum на параметры wtw_twt\\u200b до вычисления градиента:\\ngt=∇ft(wt)g_t = \\\\nabla f_t(w_t)\\ngt\\u200b=∇ft\\u200b(wt\\u200b)g^t=∇ft(wt−mt−1)\\\\hat{g}_t = \\\\nabla f_t(w_t - m_{t-1})\\ng^\\u200bt\\u200b=∇ft\\u200b(wt\\u200b−mt−1\\u200b)У метода много всяких «интуитивных объяснений», но изначально Nesterov Momentum был выведен сугубо аналитическими методами. Увы, попытки добавлять его в стохастическую оптимизацию «в лоб» обычно улучшением качества не заканчиваются. Анализ того, почему так нельзя и делать и как можно сделать правильно, проводится в работах Katyusha: The First Direct Acceleration of Stochastic Gradient Methods и Natasha-2 (мотивация их автора Zeyuan Allen-Zhu для выбора таких наименований доподлинно неизвестна). Katuysha правильным образом использует nesterov momentum для выпуклого случая, Natasha — для невыпуклого. Данные методы используют подход SVRG для улучшения сходимости и ускорение оптимизации происходит только при приближении к точке оптимума.\\nAdan\\nДо недавнего времени громких историй успеха для nesterov momentum в глубоком обучении не было. Метод Natasha распространения не нашел. Наконец, авторы статьи Adan (2022) нашли способ правильной обработки Nesterov Momentum. Метод показал отличные результаты и обновил SOTA метрики на широком спектре задач.\\nСобираем все идеи воедино\\nАвторы данного обзора очень хотят, чтобы читатель ушел не с знанием набора наименований методов оптимизации, а с знанием набора концепций, которые тот или иной метод реализует, и при случае мог сам подстроить метод под свои нужды. Тюнинг методов оптимизации — один из главных способов улучшения качества модели на фиксированном датасете.\\n\\nAdaptive learning rate — автоматическое подстраивание метода под геометрию задачи оптимизации. Крайне важный класс методов для выпуклых/невыпуклых задач. Must-have для разреженных моделей. Методы: AdaGrad/RMSprop/Adam.\\nСкользящее среднее в adaptive learning rate представлено в методах RMSprop/Adam. Не забывайте про их плохое поведение вокруг критических точек и проблемы со сходимостью на финальных этапах оптимизации.\\nBiasCorrection: стабилизация обучения на старте для адаптивных методов со скользящим средним. Большинство экспериментов показывают, что это крайне полезная штука и стоит всегда её использовать. В том числе стоит использовать RMSprop с bias correction, если вам не нужны momentum и Adam.\\nAMSgrad: способ починить сходимость RMSprop/Adam. Не забывайте, что стандартные реализации при использовании AMSgrad отключают bias correction, а это на самом деле может навредить, а также о том, что можно реализовать AMSgrad без дополнительной памяти, и всё будет хорошо работать.\\nLearning rate decay: убывание learning rate зачастую является очень важной деталью в стохастической оптимизации. Помните, что можно брать как AdaGrad, в котором это есть из коробки со скоростью O(1t)O(\\\\frac{1}{\\\\sqrt{t}})O(t\\u200b1\\u200b) (но архитектура нейросети должна быть хорошей), так и комбинацию RMSProp/Adam + learning rate scheduler.\\nWarmRestart: эвристика, резко увеличивающая learning rate после достижения некоторой точки в процессе оптимизации. Практически всегда идет бок о бок с learning rate decay. Где-то помогает\\nПроксимальные методы для функций потерь с регуляризаторами: ProximalGD/AdamW/SGDW/FTRL-Proximal. Must-have для L1L_1L1\\u200b-регуляризаторов, без проксимальности они вообще не работают.\\nFTRL-Proximal: lazy vs greedy представление. Переписываем представление любого метода оптимизации в не-жадный вид. Позволяет по-новому взглянуть на любые регуляризаторы, особенно негладкие. Must-have для L1L_1L1\\u200b-регуляризации.\\nL1L_1L1\\u200b-регуляризация в FTRL-Proximal: Incremental/Fixed/SquareIncremental. Все три имеют разные свойства и разную область применения. Fixed является наилучшим для отбора разреженных признаков/эмбеддингов.\\nL1/2L_{1/2}L1/2\\u200b-регуляризатор для отбора эмбеддингов или автоматического подбора размерности. Можно использовать как аналог FSTR. Крайне полезный подход для разреженных нейросетей в рекомендательных системах, для которых рекомендуется использовать адаптивную схему SquareIncremental.\\nHeavy-ball Momentum: используется для ускорения процесса оптимизации. В выпуклых задачах имеет доказанные оценки на улучшение скорости сходимости, в нейросетях используется как эвристика (зачастую опциональная).\\nNesterov momentum: в выпуклом случае гораздо мощнее для batch gradient descent, чем обычный momentum, и это подверждается теоретическими гарантиями. В стохастических методах оптимизации и в онлайн обучении «в лоб» применять нельзя: для выпуклого случая подойдет Katyusha, для нейросетей — Adan.\\n\\nГлавное, что мы хотим подчеркнуть, — эти идеи друг другу не противоречат и их можно свободно комбинировать друг с другом. Например, можно собрать себе FTRL-Proximal метод с L1L_1L1\\u200b-регуляризацией, любым momentum и RMSprop learning rate с AMSgrad. Или любую другую комбинацию. Всегда можно выбрать оптимальный набор под задачу.\\nПример таблицы с общими формулами\\nЭти формулы используют все подходы выше в едином фреймворке, чтобы наглядно убедиться в том, что все можно друг с другом комбинировать.\\nGeneric FTRL-Proximal\\nwt+1=argmin\\u2061wg^1:tTw+λ1,t∣∣w∣∣1+12∣∣w∣∣λ2,t2+12∑s=1t∣∣w−ws∣∣σs2w_{t+1} = arg\\\\min\\\\limits_w \\\\hat{g}_{1:t}^Tw + \\\\lambda_{1,t}\\\\vert\\\\vert w\\\\vert\\\\vert_1 + \\\\frac{1}{2}\\\\vert\\\\vert w\\\\vert\\\\vert_{\\\\lambda_{2,t}}^2 + \\\\frac{1}{2}\\\\sum\\\\limits_{s=1}^t\\\\vert\\\\vert w - w_s\\\\vert\\\\vert_{\\\\sigma_s}^2\\nwt+1\\u200b=argwmin\\u200bg^\\u200b1:tT\\u200bw+λ1,t\\u200b∣∣w∣∣1\\u200b+21\\u200b∣∣w∣∣λ2,t\\u200b2\\u200b+21\\u200bs=1∑t\\u200b∣∣w−ws\\u200b∣∣σs\\u200b2\\u200bzt=g1:t−∑s=1tσswsz_t = g_{1:t} - \\\\sum\\\\limits_{s=1}^t\\\\sigma_s w_s\\nzt\\u200b=g1:t\\u200b−s=1∑t\\u200bσs\\u200bws\\u200bwt+1,i={0∣zt,i∣≤λ1,t−σ1:t1+λ2,tσ1:t(zt−sign(zt)λ1,t)∣zt,i∣>λ1,tw_{t+1,i} = \\\\begin{cases}\\n      0 & \\\\vert z_{t,i}\\\\vert  \\\\leq \\\\lambda_{1,t}\\\\\\\\\\n      -\\\\frac{\\\\sigma_{1:t}}{1 + \\\\lambda_{2,t}\\\\sigma_{1:t}} (z_t - sign(z_t)\\\\lambda_{1,t})& \\\\vert z_{t,i}\\\\vert  > \\\\lambda_{1,t}\\n\\\\end{cases}\\nwt+1,i\\u200b={0−1+λ2,t\\u200bσ1:t\\u200bσ1:t\\u200b\\u200b(zt\\u200b−sign(zt\\u200b)λ1,t\\u200b)\\u200b∣zt,i\\u200b∣≤λ1,t\\u200b∣zt,i\\u200b∣>λ1,t\\u200b\\u200bGeneric Mirror (Proximal) Gradient Descent\\nwt+1=argmin\\u2061wg^tTw+λ1,t∣∣w∣∣1+12∣∣w∣∣λ2,t2+12∣∣w−wt∣∣1ηt2w_{t+1} = arg\\\\min\\\\limits_w \\\\hat{g}_t^Tw + \\\\lambda_{1,t}\\\\vert\\\\vert w\\\\vert\\\\vert_1 + \\\\frac{1}{2}\\\\vert\\\\vert w\\\\vert\\\\vert_{\\\\lambda_{2,t}}^2 + \\\\frac{1}{2}\\\\vert\\\\vert w - w_t\\\\vert\\\\vert_{\\\\frac{1}{\\\\eta_t}}^2\\nwt+1\\u200b=argwmin\\u200bg^\\u200btT\\u200bw+λ1,t\\u200b∣∣w∣∣1\\u200b+21\\u200b∣∣w∣∣λ2,t\\u200b2\\u200b+21\\u200b∣∣w−wt\\u200b∣∣ηt\\u200b1\\u200b2\\u200bzt=1ηtwt−gtz_t = \\\\frac{1}{\\\\eta_t} w_t - g_t\\nzt\\u200b=ηt\\u200b1\\u200bwt\\u200b−gt\\u200bwt+1={0∣zt∣≤λ1,t−ηt1+λ2,tηt(zt−sign(zt)λ1,t)∣zt∣>λ1,tw_{t+1} = \\\\begin{cases}\\n      0 & \\\\vert z_t\\\\vert  \\\\leq \\\\lambda_{1,t}\\\\\\\\\\n      -\\\\frac{\\\\eta_t}{1 + \\\\lambda_{2,t}\\\\eta_t} (z_t - sign(z_t)\\\\lambda_{1,t})& \\\\vert z_t\\\\vert  > \\\\lambda_{1,t}\\n\\\\end{cases}\\nwt+1\\u200b={0−1+λ2,t\\u200bηt\\u200bηt\\u200b\\u200b(zt\\u200b−sign(zt\\u200b)λ1,t\\u200b)\\u200b∣zt\\u200b∣≤λ1,t\\u200b∣zt\\u200b∣>λ1,t\\u200b\\u200bСвязь:\\nσt=1ηt−1ηt−1\\\\sigma_t = \\\\frac{1}{\\\\eta_t} - \\\\frac{1}{\\\\eta_{t-1}}\\nσt\\u200b=ηt\\u200b1\\u200b−ηt−1\\u200b1\\u200b\\n\\n\\n\\nИдея\\n\\n\\nFTRL (lazy)\\n\\n\\nGradient Descent (greedy)\\n\\n\\nКомментарии\\n\\n\\n\\n\\nMomentum\\n\\n\\ng^t=mt=γmt−1+(1−γ)gt\\\\hat{g}_t = m_t = \\\\gamma m_{t-1} + (1 - \\\\gamma) g_tg^\\u200bt\\u200b=mt\\u200b=γmt−1\\u200b+(1−γ)gt\\u200b m0=0m_0 = 0m0\\u200b=0\\n\\n\\nТо же самое\\n\\n\\nНе влияет на adaptive vtv_tvt\\u200b\\n\\n\\n\\n\\nКонстантный learning rate\\n\\n\\nσ0=1α\\\\sigma_0 = \\\\frac{1}{\\\\alpha}σ0\\u200b=α1\\u200b, σt=0,t>0\\\\sigma_t = 0, t > 0σt\\u200b=0,t>0\\n\\n\\nηt=α\\\\eta_t = \\\\alphaηt\\u200b=α\\n\\n\\n\\n\\n\\nУбывающий непокоординатный learning rate\\n\\n\\nσt=1αt−1αt−1\\\\sigma_t = \\\\frac{1}{\\\\alpha_t} - \\\\frac{1}{\\\\alpha_{t-1}}σt\\u200b=αt\\u200b1\\u200b−αt−1\\u200b1\\u200b\\n\\n\\nηt=αt\\\\eta_t = \\\\alpha_tηt\\u200b=αt\\u200b\\n\\n\\nОбычно берут αt=αt=O(1t)\\\\alpha_t = \\\\frac{\\\\alpha}{\\\\sqrt{t}} = O(\\\\frac{1}{\\\\sqrt{t}})αt\\u200b=t\\u200bα\\u200b=O(t\\u200b1\\u200b)\\n\\n\\n\\n\\nGeneric Adaptive learning rate\\n\\n\\nσt=vtαt−vt−1αt−1\\\\sigma_t = \\\\frac{\\\\sqrt{v_t}}{\\\\alpha_t} - \\\\frac{\\\\sqrt{v_{t-1}}}{\\\\alpha_{t-1}}σt\\u200b=αt\\u200bvt\\u200b\\u200b\\u200b−αt−1\\u200bvt−1\\u200b\\u200b\\u200b\\n\\n\\nηt=αvt\\\\eta_t = \\\\frac{\\\\alpha}{\\\\sqrt{v_t}}ηt\\u200b=vt\\u200b\\u200bα\\u200b\\n\\n\\nvtv_tvt\\u200b и ηt\\\\eta_tηt\\u200b — векторы, везде ниже умножение ηtgt\\\\eta_tg_tηt\\u200bgt\\u200b  означает покоординатное умножение\\n\\n\\n\\n\\nGeneric Adaptive learning rate с scheduler S(α,t)S(\\\\alpha, t)S(α,t)\\n\\n\\nσt=vtS(αt,t)−vt−1S(αt−1,t)\\\\sigma_t = \\\\frac{\\\\sqrt{v_t}}{S(\\\\alpha_t, t)} - \\\\frac{\\\\sqrt{v_{t-1}}}{S(\\\\alpha_{t-1}, t)}σt\\u200b=S(αt\\u200b,t)vt\\u200b\\u200b\\u200b−S(αt−1\\u200b,t)vt−1\\u200b\\u200b\\u200b\\n\\n\\nηt=S(αt,t)vt\\\\eta_t = \\\\frac{S(\\\\alpha_t, t)}{\\\\sqrt{v_t}}ηt\\u200b=vt\\u200b\\u200bS(αt\\u200b,t)\\u200b\\n\\n\\nНапример, в Adam: α1−βt→S(α,t)1−βt\\\\alpha\\\\sqrt{1 - \\\\beta^t} \\\\rightarrow S(\\\\alpha, t)\\\\sqrt{1 - \\\\beta^t}α1−βt\\u200b→S(α,t)1−βt\\u200b\\n\\n\\n\\n\\nAdaptive learning rate: AdaGrad\\n\\n\\nvt=∑s=1tgs2v_t = \\\\sum\\\\limits_{s=1}^t g_s^2vt\\u200b=s=1∑t\\u200bgs2\\u200bvt=∑s=1tgs2v_t = \\\\sum\\\\limits_{s=1}^t g_s^2vt\\u200b=s=1∑t\\u200bgs2\\u200b\\n\\nvt=∑s=1tgs2v_t = \\\\sum\\\\limits_{s=1}^t g_s^2vt\\u200b=s=1∑t\\u200bgs2\\u200b\\n\\n\\nηt=O(1t)\\\\eta_t = O(\\\\frac{1}{\\\\sqrt{t}})ηt\\u200b=O(t\\u200b1\\u200b)\\n\\n\\n\\n\\nAdaptive learning rate: RMSprop\\n\\n\\nvt=βvt−1+(1−β)vtv_t = \\\\beta v_{t-1} + (1 - \\\\beta) v_tvt\\u200b=βvt−1\\u200b+(1−β)vt\\u200b, αt=α\\\\alpha_t = \\\\alphaαt\\u200b=α\\n\\n\\nТо же самое\\n\\n\\nηt=O(1)\\\\eta_t = O(1)ηt\\u200b=O(1), ломается у критических точек\\n\\n\\n\\n\\nAdaptive learning rate: Online RMSprop\\n\\n\\nvt=βtvt−1+(1−βt)vtv_t = \\\\beta_t v_{t-1} + (1 - \\\\beta_t) v_tvt\\u200b=βt\\u200bvt−1\\u200b+(1−βt\\u200b)vt\\u200b, αt=αt\\\\alpha_t = \\\\frac{\\\\alpha}{\\\\sqrt{t}}αt\\u200b=t\\u200bα\\u200b, βt=1−γt\\\\beta_t = 1 - \\\\frac{\\\\gamma}{t}βt\\u200b=1−tγ\\u200b\\n\\n\\nТо же самое\\n\\n\\nηt=O(1t)\\\\eta_t = O(\\\\frac{1}{\\\\sqrt{t}})ηt\\u200b=O(t\\u200b1\\u200b)\\n\\n\\n\\n\\nAdaptive learning rate: Adam\\n\\n\\nvt=βvt−1+(1−β)vtv_t = \\\\beta v_{t-1} + (1 - \\\\beta) v_tvt\\u200b=βvt−1\\u200b+(1−β)vt\\u200b, αt=α1−βt\\\\alpha_t = \\\\alpha\\\\sqrt{1 - \\\\beta^t}αt\\u200b=α1−βt\\u200b\\n\\n\\nТо же самое\\n\\n\\nηt=O(1)\\\\eta_t = O(1)ηt\\u200b=O(1), ломается у критических точек\\n\\n\\n\\n\\nAdaptive learning rate: AMSgrad\\n\\n\\nvt=βvt−1+(1−β)vtv_t = \\\\beta v_{t-1} + (1 - \\\\beta) v_tvt\\u200b=βvt−1\\u200b+(1−β)vt\\u200b, αt=α\\\\alpha_t = \\\\alphaαt\\u200b=α\\n\\n\\nТо же самое\\n\\n\\nηt=O(1)\\\\eta_t = O(1)ηt\\u200b=O(1)\\n\\n\\n\\n\\nAdaptive learning rate: RAdam\\n\\n\\nМногобукв\\n\\n\\nМногобукв\\n\\n\\nηt=O(1)\\\\eta_t = O(1)ηt\\u200b=O(1)\\n\\n\\n\\n\\nКлассическая L2L_2L2\\u200b регуляризация\\n\\n\\nλ2,t=λ2\\\\lambda_{2,t} = \\\\lambda_2λ2,t\\u200b=λ2\\u200b\\n\\n\\nλ2,t=λ2\\\\lambda_{2,t} = \\\\lambda_2λ2,t\\u200b=λ2\\u200b\\n\\n\\n\\n\\n\\nDecoupled L2L_2L2\\u200b регуляризация\\n\\n\\nλ2,t=λ2σ0:t\\\\lambda_{2,t} = \\\\frac{\\\\lambda_2}{\\\\sigma_{0:t}}λ2,t\\u200b=σ0:t\\u200bλ2\\u200b\\u200b\\n\\n\\nλ2,t=λ2ηt\\\\lambda_{2,t} = \\\\frac{\\\\lambda_2}{\\\\eta_t}λ2,t\\u200b=ηt\\u200bλ2\\u200b\\u200b\\n\\n\\n\\n\\n\\nИнкрементальная L1L_1L1\\u200b регуляризация\\n\\n\\nλ1,t=tλ1\\\\lambda_{1,t} = t\\\\lambda_1λ1,t\\u200b=tλ1\\u200b\\n\\n\\nλ1,t=λ1\\\\lambda_{1,t} = \\\\lambda_1λ1,t\\u200b=λ1\\u200b\\n\\n\\n\\n\\n\\nФиксированная L1L_1L1\\u200b регуляризация\\n\\n\\nλ1,t=λ1\\\\lambda_{1,t} = \\\\lambda_1λ1,t\\u200b=λ1\\u200b\\n\\n\\nОтсутствует\\n\\n\\n\\n\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф15.3. Регуляризация в онлайн-обученииСледующий параграф16.1. Матричное дифференцированиеКак дифференцировать матрицы и\\xa0дифференцировать по\\xa0матрицам: всё, что вам не\\xa0рассказали про дифференцирование на\\xa0матанализеЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_35.html', 'title': 'Диффузионные модели'}, page_content='Диффузионные моделиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/68.1.Введение в генеративное моделирование8.2.Variational Autoencoder (VAE)8.3.Генеративно-состязательные сети (GAN)8.4.Нормализующие потоки8.5.Диффузионные моделиВведениеБолее детальноОбучение диффузионной моделиОвервью ключевых работ на сегодняшний день8.6.Языковые модели9.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Диффузионные модели8.5. Диффузионные моделиАвторыАким КотельниковВведение\\nВ этом параграфе мы снова попробуем решить задачу генерации, когда нам дана выборка объектов из распределения x0∼q(x)x_0 \\\\sim q(x)x0\\u200b∼q(x), и хотим научиться генерировать новые объекты из распределения , которых нет в нашей выборке.\\nВероятно, вы уже знакомы с другими генеративными моделями, например VAE или GAN-ы. Здесь же мы познакомим вас с еще одним видом генеративных моделей: диффузионные модели, которые стали крайне популярны в последнее время благодаря своему высокому качеству генерации объектов из заданного распределения. В общий чертах, они работают следующим образом: берем шум из N(0,I)\\\\mathcal{N}(0, I)N(0,I) и шаг за шагом удаляем компоненты шума до тех пор, пока не получим объект x0x_0x0\\u200b из распределения, см. иллюстрацию ниже.\\n\\nБолее детально\\nДля детального понимания стоит объяснить, что такое прямой и обратный диффузионные процессы. Прямой процесс заключается в постепенном зашумлении картинки с помощью распределения qqq, а обратный, наоборот, в расшумлении с помощью распределения ppp. Их можно схематично изобразить следующим образом:Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\n\\nПрямой диффузионный процесс определяется как апостериорное распределение q(x1:T∣x0)q(x_{1:T}|x_0)q(x1:T\\u200b∣x0\\u200b). Это распределение также является Марковской цепочкой, которая постепенно добавляет гауссовский шум к объекту x0x_0x0\\u200b. На каждом шаге шум добавляется с различной магнитудой, которая определяется расписанием дисперсий {β1,...,βT}\\\\{\\\\beta_1, ... , \\\\beta_T\\\\}{β1\\u200b,...,βT\\u200b}. При правильном выборе расписания в пределе по числу шагов TTT мы должны сойтись к шуму из N(0,I)\\\\mathcal{N}(0, I)N(0,I). В качестве распределений qqq берут нормальные распределения:\\nq(xt∣xt−1):=N(xt;1−βtxt−1,βtI),\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0q(x1:T∣x0)=∏t=1Tq(xt∣xt−1)q(x_t | x_{t - 1}) := \\\\mathcal{N}(x_t; \\\\sqrt{1 - \\\\beta_t}x_{t - 1}, \\\\beta_tI), \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ q(x_{1:T}|x_0) = \\\\prod_{t = 1}^T q(x_t | x_{t - 1})\\nq(xt\\u200b∣xt−1\\u200b):=N(xt\\u200b;1−βt\\u200b\\u200bxt−1\\u200b,βt\\u200bI),\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0q(x1:T\\u200b∣x0\\u200b)=t=1∏T\\u200bq(xt\\u200b∣xt−1\\u200b)Теперь перейдем к обратному процессу и к самой диффузионной модели.\\nДиффузионная модель - это вероятностная модель с латентными переменными вида pθ(x0):=∫pθ(x0:T)dx1:Tp_\\\\theta(x_0) := \\\\int p_\\\\theta(x_{0:T}) dx_{1:T}pθ\\u200b(x0\\u200b):=∫pθ\\u200b(x0:T\\u200b)dx1:T\\u200b, где промежуточные состояния x1,...,xTx_1, ..., x_Tx1\\u200b,...,xT\\u200b соответствуют зашумленным объектам, a x0x_0x0\\u200b - объект из распределения. Совместное распределение pθ(x0:T)p_\\\\theta(x_{0:T})pθ\\u200b(x0:T\\u200b) называет обратным диффузионным процессом, который представляет собой Марковскую цепочку из гауссовских распределений pθ(xi−1∣xi)p_\\\\theta(x_{i-1}|x_{i})pθ\\u200b(xi−1\\u200b∣xi\\u200b):\\np(x0:T)=p(x0)∏t=1Tpθ(xt−1∣xt)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pθ(xT)=N(xT∣0,I)p(x_{0:T}) = p(x_0) \\\\prod_{t = 1}^Tp_{\\\\theta}(x_{t-1}|x_t) \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ p_\\\\theta(x_{T})=\\\\mathcal{N}(x_T | 0, I)\\np(x0:T\\u200b)=p(x0\\u200b)t=1∏T\\u200bpθ\\u200b(xt−1\\u200b∣xt\\u200b)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pθ\\u200b(xT\\u200b)=N(xT\\u200b∣0,I)pθ(xt−1∣xt):=N(xt−1;μθ(xt,t),Σθ(xt,t))  p_{\\\\theta}(x_{t - 1}|x_t):= \\\\mathcal{N}(x_{t - 1}; \\\\mu_{\\\\theta}(x_t, t), \\\\Sigma_{\\\\theta}(x_t, t))\\npθ\\u200b(xt−1\\u200b∣xt\\u200b):=N(xt−1\\u200b;μθ\\u200b(xt\\u200b,t),Σθ\\u200b(xt\\u200b,t))Таким образом, обратный процесс параметризуется моделью θ\\\\thetaθ, которая по зашумленному объекту xtx_txt\\u200b и шагу ttt предсказывает среднее μθ(xt,t)\\\\mu_{\\\\theta}(x_t, t)μθ\\u200b(xt\\u200b,t) и дисперсию Σθ(xt,t)\\\\Sigma_{\\\\theta}(x_t, t)Σθ\\u200b(xt\\u200b,t).\\nОбучение диффузионной модели\\nДиффузионный модели обучаются, максимизируя вариационную нижнюю оценку (ELBO) логарифма правдоподобия  log\\u2061pθ(x0)\\\\log p_{\\\\theta}(x_0)logpθ\\u200b(x0\\u200b). По тому же принципу обучаются VAE, с тем лишь отличием, что у диффузионных моделей другая форма модели с латентными переменными.  Итак, давайте выведем ELBO для диффузии:\\n−log\\u2061pθ(x0)≤−log\\u2061pθ(x0)+DKL(q(x1:T∣x0)∥pθ(x1:T∣x0))=−log\\u2061pθ(x0)+Ex1:T∼q(x1:T∣x0)[log\\u2061q(x1:T∣x0)pθ(x0)pθ(x0:T)]=−log\\u2061pθ(x0)+Eq[log\\u2061q(x1:T∣x0)pθ(x0:T)+log\\u2061pθ(x0)]=Eq[log\\u2061q(x1:T∣x0)pθ(x0:T)]Let\\xa0LVLB=Eq(x0:T)[log\\u2061q(x1:T∣x0)pθ(x0:T)]≥−Eq(x0)log\\u2061pθ(x0)\\\\begin{aligned}\\n- \\\\log p_\\\\theta(\\\\mathbf{x}_0) \\n&\\\\leq - \\\\log p_\\\\theta(\\\\mathbf{x}_0) + D_\\\\text{KL}(q(\\\\mathbf{x}_{1:T}\\\\vert\\\\mathbf{x}_0) \\\\| p_\\\\theta(\\\\mathbf{x}_{1:T}\\\\vert\\\\mathbf{x}_0) ) \\\\\\\\\\n&= -\\\\log p_\\\\theta(\\\\mathbf{x}_0) + \\\\mathbb{E}_{\\\\mathbf{x}_{1:T}\\\\sim q(\\\\mathbf{x}_{1:T} \\\\vert \\\\mathbf{x}_0)} \\\\Big[ \\\\log\\\\frac{q(\\\\mathbf{x}_{1:T}\\\\vert\\\\mathbf{x}_0) p_\\\\theta(\\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_{0:T})} \\\\Big] \\\\\\\\\\n&= -\\\\log p_\\\\theta(\\\\mathbf{x}_0) + \\\\mathbb{E}_q \\\\Big[ \\\\log\\\\frac{q(\\\\mathbf{x}_{1:T}\\\\vert\\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_{0:T})} + \\\\log p_\\\\theta(\\\\mathbf{x}_0) \\\\Big] \\\\\\\\\\n&= \\\\mathbb{E}_q \\\\Big[ \\\\log \\\\frac{q(\\\\mathbf{x}_{1:T}\\\\vert\\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_{0:T})} \\\\Big] \\\\\\\\\\n\\\\text{Let }L_\\\\text{VLB} \\n&= \\\\mathbb{E}_{q(\\\\mathbf{x}_{0:T})} \\\\Big[ \\\\log \\\\frac{q(\\\\mathbf{x}_{1:T}\\\\vert\\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_{0:T})} \\\\Big] \\\\geq - \\\\mathbb{E}_{q(\\\\mathbf{x}_0)} \\\\log p_\\\\theta(\\\\mathbf{x}_0)\\n\\\\end{aligned}\\n−logpθ\\u200b(x0\\u200b)Let\\xa0LVLB\\u200b\\u200b≤−logpθ\\u200b(x0\\u200b)+DKL\\u200b(q(x1:T\\u200b∣x0\\u200b)∥pθ\\u200b(x1:T\\u200b∣x0\\u200b))=−logpθ\\u200b(x0\\u200b)+Ex1:T\\u200b∼q(x1:T\\u200b∣x0\\u200b)\\u200b[logpθ\\u200b(x0:T\\u200b)q(x1:T\\u200b∣x0\\u200b)pθ\\u200b(x0\\u200b)\\u200b]=−logpθ\\u200b(x0\\u200b)+Eq\\u200b[logpθ\\u200b(x0:T\\u200b)q(x1:T\\u200b∣x0\\u200b)\\u200b+logpθ\\u200b(x0\\u200b)]=Eq\\u200b[logpθ\\u200b(x0:T\\u200b)q(x1:T\\u200b∣x0\\u200b)\\u200b]=Eq(x0:T\\u200b)\\u200b[logpθ\\u200b(x0:T\\u200b)q(x1:T\\u200b∣x0\\u200b)\\u200b]≥−Eq(x0\\u200b)\\u200blogpθ\\u200b(x0\\u200b)\\u200b\\nКомментарий\\n\\nЕсли вы знакомы с VAE, то вывод LVLBL_{VLB}LVLB\\u200b должен быть вам понятен, однако ниже приведен вывод с помощью неравенства Йенсена\\nLCE=−Eq(x0)log\\u2061pθ(x0)=−Eq(x0)log\\u2061(∫pθ(x0:T)dx1:T)=−Eq(x0)log\\u2061(∫q(x1:T∣x0)pθ(x0:T)q(x1:T∣x0)dx1:T)=−Eq(x0)log\\u2061(Eq(x1:T∣x0)pθ(x0:T)q(x1:T∣x0))≤−Eq(x0:T)log\\u2061pθ(x0:T)q(x1:T∣x0)=Eq(x0:T)[log\\u2061q(x1:T∣x0)pθ(x0:T)]=LVLB\\\\begin{aligned}\\nL_\\\\text{CE}\\n&= - \\\\mathbb{E}_{q(\\\\mathbf{x}_0)} \\\\log p_\\\\theta(\\\\mathbf{x}_0) \\\\\\\\\\n&= - \\\\mathbb{E}_{q(\\\\mathbf{x}_0)} \\\\log \\\\Big( \\\\int p_\\\\theta(\\\\mathbf{x}_{0:T}) d\\\\mathbf{x}_{1:T} \\\\Big) \\\\\\\\\\n&= - \\\\mathbb{E}_{q(\\\\mathbf{x}_0)} \\\\log \\\\Big( \\\\int q(\\\\mathbf{x}_{1:T} \\\\vert \\\\mathbf{x}_0) \\\\frac{p_\\\\theta(\\\\mathbf{x}_{0:T})}{q(\\\\mathbf{x}_{1:T} \\\\vert \\\\mathbf{x}_{0})} d\\\\mathbf{x}_{1:T} \\\\Big) \\\\\\\\\\n&= - \\\\mathbb{E}_{q(\\\\mathbf{x}_0)} \\\\log \\\\Big( \\\\mathbb{E}_{q(\\\\mathbf{x}_{1:T} \\\\vert \\\\mathbf{x}_0)} \\\\frac{p_\\\\theta(\\\\mathbf{x}_{0:T})}{q(\\\\mathbf{x}_{1:T} \\\\vert \\\\mathbf{x}_{0})} \\\\Big) \\\\\\\\\\n&\\\\leq - \\\\mathbb{E}_{q(\\\\mathbf{x}_{0:T})} \\\\log \\\\frac{p_\\\\theta(\\\\mathbf{x}_{0:T})}{q(\\\\mathbf{x}_{1:T} \\\\vert \\\\mathbf{x}_{0})} \\\\\\\\\\n&= \\\\mathbb{E}_{q(\\\\mathbf{x}_{0:T})}\\\\Big[\\\\log \\\\frac{q(\\\\mathbf{x}_{1:T} \\\\vert \\\\mathbf{x}_{0})}{p_\\\\theta(\\\\mathbf{x}_{0:T})} \\\\Big] = L_\\\\text{VLB}\\n\\\\end{aligned}\\nLCE\\u200b\\u200b=−Eq(x0\\u200b)\\u200blogpθ\\u200b(x0\\u200b)=−Eq(x0\\u200b)\\u200blog(∫pθ\\u200b(x0:T\\u200b)dx1:T\\u200b)=−Eq(x0\\u200b)\\u200blog(∫q(x1:T\\u200b∣x0\\u200b)q(x1:T\\u200b∣x0\\u200b)pθ\\u200b(x0:T\\u200b)\\u200bdx1:T\\u200b)=−Eq(x0\\u200b)\\u200blog(Eq(x1:T\\u200b∣x0\\u200b)\\u200bq(x1:T\\u200b∣x0\\u200b)pθ\\u200b(x0:T\\u200b)\\u200b)≤−Eq(x0:T\\u200b)\\u200blogq(x1:T\\u200b∣x0\\u200b)pθ\\u200b(x0:T\\u200b)\\u200b=Eq(x0:T\\u200b)\\u200b[logpθ\\u200b(x0:T\\u200b)q(x1:T\\u200b∣x0\\u200b)\\u200b]=LVLB\\u200b\\u200bТеперь вернемся к распределению q(xt∣xt−1)q(x_t | x_{t - 1})q(xt\\u200b∣xt−1\\u200b).  Для того чтобы получить xtx_txt\\u200b, придется итеративно получать x1,...,xt−1x_1, ..., x_{t - 1}x1\\u200b,...,xt−1\\u200b. Однако это можно сделать более эффективно благодаря нормальным распределениям. Для этого обозначим αt:=1−βt\\\\alpha_t := 1- \\\\beta_tαt\\u200b:=1−βt\\u200b и αˉt:=∏i=1tαi\\\\bar{\\\\alpha}_t:= \\\\prod_{i = 1}^t\\\\alpha_iαˉt\\u200b:=∏i=1t\\u200bαi\\u200b, тогда\\nq(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I)q(x_t | x_0) = \\\\mathcal{N}(x_t;\\\\sqrt{\\\\bar{\\\\alpha}_t} x_0, (1-\\\\bar{\\\\alpha}_t)I)\\nq(xt\\u200b∣x0\\u200b)=N(xt\\u200b;αˉt\\u200b\\u200bx0\\u200b,(1−αˉt\\u200b)I)\\nФормальный вывод этого факта\\n\\nxt=αtxt−1+1−αtzt−1;\\xa0где\\xa0zt−1,zt−2,⋯∼N(0,I)=αt(αt−1xt−2+1−αt−1zt−2)+1−αtzt−1=αtαt−1xt−2+1−αtαt−1zˉt−2;\\xa0где\\xa0zˉt−2∼N(0,I)\\xa0\\xa0(∗)=…=αˉtx0+1−αˉtzq(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I)\\\\begin{aligned}\\n\\\\mathbf{x}_t \\n&= \\\\sqrt{\\\\alpha_t}\\\\mathbf{x}_{t-1} + \\\\sqrt{1 - \\\\alpha_t}\\\\mathbf{z}_{t-1}; \\\\text{ где } \\\\mathbf{z}_{t-1}, \\\\mathbf{z}_{t-2}, \\\\dots \\\\sim \\\\mathcal{N}(\\\\mathbf{0}, \\\\mathbf{I}) \\\\\\\\\\n&= \\\\sqrt{\\\\alpha_t}(\\\\sqrt{\\\\alpha}_{t - 1}x_{t - 2} + \\\\sqrt{1 - \\\\alpha_{t - 1}}\\\\mathbf{z}_{t - 2})  + \\\\sqrt{1 - \\\\alpha_t} \\\\mathbf{z}_{t-1} \\\\\\\\\\n&= \\\\sqrt{\\\\alpha_t \\\\alpha_{t-1}} \\\\mathbf{x}_{t-2} + \\\\sqrt{1 - \\\\alpha_t \\\\alpha_{t-1}} \\\\bar{\\\\mathbf{z}}_{t-2}; \\\\text{ где } \\\\bar{\\\\mathbf{z}}_{t-2} \\\\sim \\\\mathcal{N}(\\\\mathbf{0}, \\\\mathbf{I}) \\\\ \\\\ {(*)} \\\\\\\\\\n&= \\\\dots \\\\\\\\\\n&= \\\\sqrt{\\\\bar{\\\\alpha}_t}\\\\mathbf{x}_0 + \\\\sqrt{1 - \\\\bar{\\\\alpha}_t}\\\\mathbf{z} \\\\\\\\\\nq(\\\\mathbf{x}_t \\\\vert \\\\mathbf{x}_0) &= \\\\mathcal{N}(\\\\mathbf{x}_t; \\\\sqrt{\\\\bar{\\\\alpha}_t} \\\\mathbf{x}_0, (1 - \\\\bar{\\\\alpha}_t)\\\\mathbf{I})\\n\\\\end{aligned}\\nxt\\u200bq(xt\\u200b∣x0\\u200b)\\u200b=αt\\u200b\\u200bxt−1\\u200b+1−αt\\u200b\\u200bzt−1\\u200b;\\xa0где\\xa0zt−1\\u200b,zt−2\\u200b,⋯∼N(0,I)=αt\\u200b\\u200b(α\\u200bt−1\\u200bxt−2\\u200b+1−αt−1\\u200b\\u200bzt−2\\u200b)+1−αt\\u200b\\u200bzt−1\\u200b=αt\\u200bαt−1\\u200b\\u200bxt−2\\u200b+1−αt\\u200bαt−1\\u200b\\u200bzˉt−2\\u200b;\\xa0где\\xa0zˉt−2\\u200b∼N(0,I)\\xa0\\xa0(∗)=…=αˉt\\u200b\\u200bx0\\u200b+1−αˉt\\u200b\\u200bz=N(xt\\u200b;αˉt\\u200b\\u200bx0\\u200b,(1−αˉt\\u200b)I)\\u200b(*) Пояснение ко второму переходу. У нас выходит\\nαt(1−αt−1)zt−2+1−αtzt−1=αt(1−αt−1)+(1−αt)zˉt−2=1−αtαt−1zˉt−2;\\xa0где\\xa0zt−1,zt−2,zˉt−2∼N(0,I)\\\\sqrt{\\\\alpha_t(1 - \\\\alpha_{t - 1})}z_{t - 2} + \\\\sqrt{1 - \\\\alpha_t}z_{t - 1} \\\\\\\\ = \\\\sqrt{\\\\alpha_t(1 - \\\\alpha_{t - 1}) + (1 - \\\\alpha_t)}\\\\bar{z}_{t- 2} \\\\\\\\ = \\\\sqrt{1 - \\\\alpha_t\\\\alpha_{t - 1}}\\\\bar{z}_{t - 2}; \\\\text{ где } z_{t - 1},z_{t - 2},\\\\bar{z}_{t - 2} \\\\sim \\\\mathcal{N}(0, I)\\nαt\\u200b(1−αt−1\\u200b)\\u200bzt−2\\u200b+1−αt\\u200b\\u200bzt−1\\u200b=αt\\u200b(1−αt−1\\u200b)+(1−αt\\u200b)\\u200bzˉt−2\\u200b=1−αt\\u200bαt−1\\u200b\\u200bzˉt−2\\u200b;\\xa0где\\xa0zt−1\\u200b,zt−2\\u200b,zˉt−2\\u200b∼N(0,I)Тогда LVLBL_{VLB}LVLB\\u200b может быть переписано как\\nLVLB=Eq[DKL(q(xT∣x0)∥pθ(xT))⏟LT++∑t=2TDKL(q(xt−1∣xt,x0)∥pθ(xt−1∣xt))⏟Lt−1−log\\u2061pθ(x0∣x1)⏟L0L_{VLB} = \\\\mathbb{E}_q [\\\\underbrace{D_\\\\text{KL}(q(\\\\mathbf{x}_T \\\\vert \\n\\\\mathbf{x}_0) \\\\parallel p_\\\\theta(\\\\mathbf{x}_T))}_{L_T} + + \\\\sum_{t=2}^T \\n\\\\underbrace{D_\\\\text{KL}(q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\n\\\\mathbf{x}_0) \\\\parallel p_\\\\theta(\\\\mathbf{x}_{t-1} \\n\\\\vert\\\\mathbf{x}_t))}_{L_{t-1}} \\\\underbrace{- \\\\log p_\\\\theta(\\\\mathbf{x}_0 \\n\\\\vert \\\\mathbf{x}_1)}_{L_0}\\nLVLB\\u200b=Eq\\u200b[LT\\u200bDKL\\u200b(q(xT\\u200b∣x0\\u200b)∥pθ\\u200b(xT\\u200b))\\u200b\\u200b++t=2∑T\\u200bLt−1\\u200bDKL\\u200b(q(xt−1\\u200b∣xt\\u200b,x0\\u200b)∥pθ\\u200b(xt−1\\u200b∣xt\\u200b))\\u200b\\u200bL0\\u200b−logpθ\\u200b(x0\\u200b∣x1\\u200b)\\u200b\\u200b\\nДолгий вывод\\n\\nСерым в скобках комментарий к последующему переходу.\\nLVLB=Eq(x0:T)[log\\u2061q(x1:T∣x0)pθ(x0:T)](расписываем\\xa0совместное\\xa0распределение)=Eq[log\\u2061∏t=1Tq(xt∣xt−1)pθ(xT)∏t=1Tpθ(xt−1∣xt)]\\xa0\\xa0\\xa0(берем\\xa0логарифм)=Eq[−log\\u2061pθ(xT)+∑t=1Tlog\\u2061q(xt∣xt−1)pθ(xt−1∣xt)](отщепляем\\xa0члены\\xa0суммы)=Eq[−log\\u2061pθ(xT)+∑t=2Tlog\\u2061q(xt∣xt−1)pθ(xt−1∣xt)+log\\u2061q(x1∣x0)pθ(x0∣x1)](*)=Eq[−log\\u2061pθ(xT)+∑t=2Tlog\\u2061(q(xt−1∣xt,x0)pθ(xt−1∣xt)⋅q(xt∣x0)q(xt−1∣x0))+log\\u2061q(x1∣x0)pθ(x0∣x1)](лог\\xa0произведения\\xa0раскрываем)=Eq[−log\\u2061pθ(xT)+∑t=2Tlog\\u2061q(xt−1∣xt,x0)pθ(xt−1∣xt)+∑t=2Tlog\\u2061q(xt∣x0)q(xt−1∣x0)+log\\u2061q(x1∣x0)pθ(x0∣x1)](от\\xa0второй\\xa0суммы\\xa0останется\\xa0только\\xa01ый\\xa0и\\xa0последний\\xa0член)=Eq[−log\\u2061pθ(xT)+∑t=2Tlog\\u2061q(xt−1∣xt,x0)pθ(xt−1∣xt)+log\\u2061q(xT∣x0)q(x1∣x0)+log\\u2061q(x1∣x0)pθ(x0∣x1)](комбинируем\\xa01\\xa0и\\xa03\\xa0член,\\xa03\\xa0и\\xa04\\xa0член)=Eq[log\\u2061q(xT∣x0)pθ(xT)+∑t=2Tlog\\u2061q(xt−1∣xt,x0)pθ(xt−1∣xt)−log\\u2061pθ(x0∣x1)]=Eq[DKL(q(xT∣x0)∥pθ(xT))⏟LT+∑t=2TDKL(q(xt−1∣xt,x0)∥pθ(xt−1∣xt))⏟Lt−1−log\\u2061pθ(x0∣x1)⏟L0]\\\\begin{aligned}L_\\\\text{VLB} &= \\\\mathbb{E}_{q(\\\\mathbf{x}_{0:T})} \\\\Big[ \\\\log\\\\frac{q(\\\\mathbf{x}_{1:T}\\\\vert\\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_{0:T})} \\\\Big] \\\\quad \\\\textit{\\\\color{gray}{(расписываем совместное распределение)}}\\\\\\\\&= \\\\mathbb{E}_q \\\\Big[ \\\\log\\\\frac{\\\\prod_{t=1}^T q(\\\\mathbf{x}_t\\\\vert\\\\mathbf{x}_{t-1})}{ p_\\\\theta(\\\\mathbf{x}_T) \\\\prod_{t=1}^T p_\\\\theta(\\\\mathbf{x}_{t-1} \\\\vert\\\\mathbf{x}_t) } \\\\Big] \\\\ \\\\ \\\\ \\\\textit{\\\\color{gray}{(берем логарифм)}} \\\\\\\\&= \\\\mathbb{E}_q \\\\Big[ -\\\\log p_\\\\theta(\\\\mathbf{x}_T) + \\\\sum_{t=1}^T \\\\log \\\\frac{q(\\\\mathbf{x}_t\\\\vert\\\\mathbf{x}_{t-1})}{p_\\\\theta(\\\\mathbf{x}_{t-1} \\\\vert\\\\mathbf{x}_t)} \\\\Big] \\\\textit{\\\\color{gray}{(отщепляем члены суммы)}} \\\\\\\\&= \\\\mathbb{E}_q \\\\Big[ -\\\\log p_\\\\theta(\\\\mathbf{x}_T) + \\\\sum_{t=2}^T \\\\log \\\\frac{q(\\\\mathbf{x}_t\\\\vert\\\\mathbf{x}_{t-1})}{p_\\\\theta(\\\\mathbf{x}_{t-1} \\\\vert\\\\mathbf{x}_t)} + \\\\log\\\\frac{q(\\\\mathbf{x}_1 \\\\vert \\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_0 \\\\vert \\\\mathbf{x}_1)} \\\\Big] \\\\textit{\\\\color{gray}{(*)}}\\\\\\\\&= \\\\mathbb{E}_q \\\\Big[ -\\\\log p_\\\\theta(\\\\mathbf{x}_T) + \\\\sum_{t=2}^T \\\\log \\\\Big( \\\\frac{q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_{t-1} \\\\vert\\\\mathbf{x}_t)}\\\\cdot \\\\frac{q(\\\\mathbf{x}_t \\\\vert \\\\mathbf{x}_0)}{q(\\\\mathbf{x}_{t-1}\\\\vert\\\\mathbf{x}_0)} \\\\Big) + \\\\log \\\\frac{q(\\\\mathbf{x}_1 \\\\vert \\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_0 \\\\vert \\\\mathbf{x}_1)} \\\\Big] \\\\textit{\\\\color{gray}{(лог произведения раскрываем)}}\\\\\\\\&= \\\\mathbb{E}_q \\\\Big[ -\\\\log p_\\\\theta(\\\\mathbf{x}_T) + \\\\sum_{t=2}^T \\\\log \\\\frac{q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_{t-1} \\\\vert\\\\mathbf{x}_t)} + \\\\sum_{t=2}^T \\\\log \\\\frac{q(\\\\mathbf{x}_t \\\\vert \\\\mathbf{x}_0)}{q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_0)} + \\\\log\\\\frac{q(\\\\mathbf{x}_1 \\\\vert \\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_0 \\\\vert \\\\mathbf{x}_1)} \\\\Big] \\\\textit{\\\\color{gray}{(от второй суммы останется только 1ый и последний член)}} \\\\\\\\&= \\\\mathbb{E}_q \\\\Big[ -\\\\log p_\\\\theta(\\\\mathbf{x}_T) + \\\\sum_{t=2}^T \\\\log \\\\frac{q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_{t-1} \\\\vert\\\\mathbf{x}_t)} + \\\\log\\\\frac{q(\\\\mathbf{x}_T \\\\vert \\\\mathbf{x}_0)}{q(\\\\mathbf{x}_1 \\\\vert \\\\mathbf{x}_0)} + \\\\log \\\\frac{q(\\\\mathbf{x}_1 \\\\vert \\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_0 \\\\vert \\\\mathbf{x}_1)} \\\\Big] \\\\textit{\\\\color{gray}{(комбинируем 1 и 3 член, 3 и 4 член)}}\\\\\\\\&= \\\\mathbb{E}_q \\\\Big[ \\\\log\\\\frac{q(\\\\mathbf{x}_T \\\\vert \\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_T)} + \\\\sum_{t=2}^T \\\\log \\\\frac{q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\\\mathbf{x}_0)}{p_\\\\theta(\\\\mathbf{x}_{t-1} \\\\vert\\\\mathbf{x}_t)} - \\\\log p_\\\\theta(\\\\mathbf{x}_0 \\\\vert \\\\mathbf{x}_1) \\\\Big] \\\\\\\\&= \\\\mathbb{E}_q [\\\\underbrace{D_\\\\text{KL}(q(\\\\mathbf{x}_T \\\\vert \\\\mathbf{x}_0) \\\\parallel p_\\\\theta(\\\\mathbf{x}_T))}_{L_T} + \\\\sum_{t=2}^T \\\\underbrace{D_\\\\text{KL}(q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\\\mathbf{x}_0) \\\\parallel p_\\\\theta(\\\\mathbf{x}_{t-1} \\\\vert\\\\mathbf{x}_t))}_{L_{t-1}} \\\\underbrace{- \\\\log p_\\\\theta(\\\\mathbf{x}_0 \\\\vert \\\\mathbf{x}_1)}_{L_0} ]\\\\end{aligned}\\nLVLB\\u200b\\u200b=Eq(x0:T\\u200b)\\u200b[logpθ\\u200b(x0:T\\u200b)q(x1:T\\u200b∣x0\\u200b)\\u200b](расписываем\\xa0совместное\\xa0распределение)=Eq\\u200b[logpθ\\u200b(xT\\u200b)∏t=1T\\u200bpθ\\u200b(xt−1\\u200b∣xt\\u200b)∏t=1T\\u200bq(xt\\u200b∣xt−1\\u200b)\\u200b]\\xa0\\xa0\\xa0(берем\\xa0логарифм)=Eq\\u200b[−logpθ\\u200b(xT\\u200b)+t=1∑T\\u200blogpθ\\u200b(xt−1\\u200b∣xt\\u200b)q(xt\\u200b∣xt−1\\u200b)\\u200b](отщепляем\\xa0члены\\xa0суммы)=Eq\\u200b[−logpθ\\u200b(xT\\u200b)+t=2∑T\\u200blogpθ\\u200b(xt−1\\u200b∣xt\\u200b)q(xt\\u200b∣xt−1\\u200b)\\u200b+logpθ\\u200b(x0\\u200b∣x1\\u200b)q(x1\\u200b∣x0\\u200b)\\u200b](*)=Eq\\u200b[−logpθ\\u200b(xT\\u200b)+t=2∑T\\u200blog(pθ\\u200b(xt−1\\u200b∣xt\\u200b)q(xt−1\\u200b∣xt\\u200b,x0\\u200b)\\u200b⋅q(xt−1\\u200b∣x0\\u200b)q(xt\\u200b∣x0\\u200b)\\u200b)+logpθ\\u200b(x0\\u200b∣x1\\u200b)q(x1\\u200b∣x0\\u200b)\\u200b](лог\\xa0произведения\\xa0раскрываем)=Eq\\u200b[−logpθ\\u200b(xT\\u200b)+t=2∑T\\u200blogpθ\\u200b(xt−1\\u200b∣xt\\u200b)q(xt−1\\u200b∣xt\\u200b,x0\\u200b)\\u200b+t=2∑T\\u200blogq(xt−1\\u200b∣x0\\u200b)q(xt\\u200b∣x0\\u200b)\\u200b+logpθ\\u200b(x0\\u200b∣x1\\u200b)q(x1\\u200b∣x0\\u200b)\\u200b](от\\xa0второй\\xa0суммы\\xa0останется\\xa0только\\xa01ый\\xa0и\\xa0последний\\xa0член)=Eq\\u200b[−logpθ\\u200b(xT\\u200b)+t=2∑T\\u200blogpθ\\u200b(xt−1\\u200b∣xt\\u200b)q(xt−1\\u200b∣xt\\u200b,x0\\u200b)\\u200b+logq(x1\\u200b∣x0\\u200b)q(xT\\u200b∣x0\\u200b)\\u200b+logpθ\\u200b(x0\\u200b∣x1\\u200b)q(x1\\u200b∣x0\\u200b)\\u200b](комбинируем\\xa01\\xa0и\\xa03\\xa0член,\\xa03\\xa0и\\xa04\\xa0член)=Eq\\u200b[logpθ\\u200b(xT\\u200b)q(xT\\u200b∣x0\\u200b)\\u200b+t=2∑T\\u200blogpθ\\u200b(xt−1\\u200b∣xt\\u200b)q(xt−1\\u200b∣xt\\u200b,x0\\u200b)\\u200b−logpθ\\u200b(x0\\u200b∣x1\\u200b)]=Eq\\u200b[LT\\u200bDKL\\u200b(q(xT\\u200b∣x0\\u200b)∥pθ\\u200b(xT\\u200b))\\u200b\\u200b+t=2∑T\\u200bLt−1\\u200bDKL\\u200b(q(xt−1\\u200b∣xt\\u200b,x0\\u200b)∥pθ\\u200b(xt−1\\u200b∣xt\\u200b))\\u200b\\u200bL0\\u200b−logpθ\\u200b(x0\\u200b∣x1\\u200b)\\u200b\\u200b]\\u200bПояснение (*). Пользуемся тем, что у нас Марковский процесс, и теоремой Байеса:\\nq(xt∣xt−1)=q(xt∣xt−1,x0)=q(xt−1∣xt,x0)q(xt∣x0)q(xt−1∣x0)q(x_t | x_{t - 1}) = q(x_t|x_{t - 1}, x_0) = \\\\frac{q(x_{t - 1}| x_t, x_0)q(x_t | x_0)}{q(x_{t - 1}|x_0)}\\nq(xt\\u200b∣xt−1\\u200b)=q(xt\\u200b∣xt−1\\u200b,x0\\u200b)=q(xt−1\\u200b∣x0\\u200b)q(xt−1\\u200b∣xt\\u200b,x0\\u200b)q(xt\\u200b∣x0\\u200b)\\u200bТаким образом во время обучения, на каждой итерации параллельно оптимизируются случайные член LtL_tLt\\u200b с помощью градиентного спуск (сэмлируем t∼U{1,...,T}t \\\\sim U\\\\{1,...,T\\\\}t∼U{1,...,T}). Поскольку все распределения нормальные, то KL между ними можно выписать в явной форме (см. ниже).\\n\\nФормула KL между двумя нормальными\\n\\nKL(N1\\xa0∣∣\\xa0N2)=12(Tr(Σ2−1Σ1)+(μ2−μ1)TΣ2−1(μ2−μ1)++ln\\u2061det(Σ2)det(Σ1)−d)\\\\begin{split}        KL(\\\\mathcal{N}_1 \\\\ || \\\\ \\\\mathcal{N}_2) = \\\\frac{1}{2}\\\\bigg(Tr(\\\\Sigma^{-1}_2 \\\\Sigma_1) + (\\\\mu_2 - \\\\mu_1)^T \\\\Sigma^{-1}_2 (\\\\mu_2 - \\\\mu_1) + \\\\\\\\[1.5ex]        + \\\\ln \\\\frac{det(\\\\Sigma_2)}{det(\\\\Sigma_1)} - d \\\\bigg)    \\\\end{split}\\nKL(N1\\u200b\\xa0∣∣\\xa0N2\\u200b)=21\\u200b(Tr(Σ2−1\\u200bΣ1\\u200b)+(μ2\\u200b−μ1\\u200b)TΣ2−1\\u200b(μ2\\u200b−μ1\\u200b)++lndet(Σ1\\u200b)det(Σ2\\u200b)\\u200b−d)\\u200bЕсли Σ1=σ1I,\\xa0Σ2=σ2I\\\\Sigma_1 = \\\\sigma_1I, \\\\ \\\\Sigma_2 = \\\\sigma_2IΣ1\\u200b=σ1\\u200bI,\\xa0Σ2\\u200b=σ2\\u200bI\\nKL(N1\\xa0∣∣\\xa0N2)=12(dσ1σ2+1σ2∥μ2−μ1∥2++ln\\u2061σ2σ1)\\\\begin{split}        KL(\\\\mathcal{N}_1 \\\\ || \\\\ \\\\mathcal{N}_2) = \\\\frac{1}{2}\\\\bigg(\\\\frac{d\\\\sigma_1}{\\\\sigma_2} + \\\\frac{1}{\\\\sigma_2}\\\\|\\\\mu_2 - \\\\mu_1\\\\|^2 + \\\\\\\\[1.5ex]        + \\\\ln \\\\frac{\\\\sigma_2}{\\\\sigma_1}\\\\bigg)    \\\\end{split}\\nKL(N1\\u200b\\xa0∣∣\\xa0N2\\u200b)=21\\u200b(σ2\\u200bdσ1\\u200b\\u200b+σ2\\u200b1\\u200b∥μ2\\u200b−μ1\\u200b∥2++lnσ1\\u200bσ2\\u200b\\u200b)\\u200bОсталось только выписать q(xt−1∣xt,x0)q(x_{t - 1}| x_t, x_0)q(xt−1\\u200b∣xt\\u200b,x0\\u200b) . Мы знаем, поскольку у нас все распределения нормальные, то и q(xt−1∣xt,x0)q(x_{t - 1}| x_t, x_0)q(xt−1\\u200b∣xt\\u200b,x0\\u200b)  будет нормальным.\\nОбозначим\\nq(xt−1∣xt,x0)=N(xt−1;μ~(xt,x0),β~tI)q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\\\mathbf{x}_0) = \\\\mathcal{N}(\\\\mathbf{x}_{t-1}; \\\\color{blue}{\\\\tilde{\\\\boldsymbol{\\\\mu}}}(\\\\mathbf{x}_t, \\\\mathbf{x}_0), \\\\color{red}{\\\\tilde{\\\\beta}_t} \\\\mathbf{I}) \\nq(xt−1\\u200b∣xt\\u200b,x0\\u200b)=N(xt−1\\u200b;μ~\\u200b(xt\\u200b,x0\\u200b),β~\\u200bt\\u200bI)\\nВывод q(xt−1∣xt,x0)q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\\\mathbf{x}_0)q(xt−1\\u200b∣xt\\u200b,x0\\u200b)\\n\\nПрименим формулу Байеса и распишем. Тут мы просто пытаемся понять, как будут выглядеть среднее и дисперсия, выделяя квадратичную форму в показателе экспоненты\\nq(xt−1∣xt,x0)=q(xt∣xt−1,x0)q(xt−1∣x0)q(xt∣x0)∝q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\\\mathbf{x}_0) = q(\\\\mathbf{x}_t \\\\vert \\\\mathbf{x}_{t-1}, \\\\mathbf{x}_0) \\\\frac{ q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_0) }{ q(\\\\mathbf{x}_t \\\\vert \\\\mathbf{x}_0) } \\\\propto \\nq(xt−1\\u200b∣xt\\u200b,x0\\u200b)=q(xt\\u200b∣xt−1\\u200b,x0\\u200b)q(xt\\u200b∣x0\\u200b)q(xt−1\\u200b∣x0\\u200b)\\u200b∝∝exp\\u2061(−12((xt−αtxt−1)2βt+(xt−1−αˉt−1x0)21−αˉt−1−(xt−αˉtx0)21−αˉt))  \\\\propto \\\\exp \\\\Big(-\\\\frac{1}{2} \\\\big(\\\\frac{(\\\\mathbf{x}_t - \\\\sqrt{\\\\alpha_t} \\\\mathbf{x}_{t-1})^2}{\\\\beta_t} + \\\\frac{(\\\\mathbf{x}_{t-1} - \\\\sqrt{\\\\bar{\\\\alpha}_{t-1}} \\\\mathbf{x}_0)^2}{1-\\\\bar{\\\\alpha}_{t-1}} - \\\\frac{(\\\\mathbf{x}_t - \\\\sqrt{\\\\bar{\\\\alpha}_t} \\\\mathbf{x}_0)^2}{1-\\\\bar{\\\\alpha}_t} \\\\big) \\\\Big) \\n∝exp(−21\\u200b(βt\\u200b(xt\\u200b−αt\\u200b\\u200bxt−1\\u200b)2\\u200b+1−αˉt−1\\u200b(xt−1\\u200b−αˉt−1\\u200b\\u200bx0\\u200b)2\\u200b−1−αˉt\\u200b(xt\\u200b−αˉt\\u200b\\u200bx0\\u200b)2\\u200b))=exp\\u2061(−12(xt2−2αtxtxt−1+αtxt−12βt+xt−12−2αˉt−1x0xt−1+αˉt−1x021−αˉt−1−(xt−αˉtx0)21−αˉt))=  = \\\\exp \\\\Big(-\\\\frac{1}{2} \\\\big(\\\\frac{\\\\mathbf{x}_t^2 - 2\\\\sqrt{\\\\alpha_t} \\\\mathbf{x}_t \\\\color{blue}{\\\\mathbf{x}_{t-1}} \\\\color{black}{+ \\\\alpha_t} \\\\color{red}{\\\\mathbf{x}_{t-1}^2} }{\\\\beta_t} + \\\\frac{ \\\\color{red}{\\\\mathbf{x}_{t-1}^2} \\\\color{black}{- 2 \\\\sqrt{\\\\bar{\\\\alpha}_{t-1}} \\\\mathbf{x}_0} \\\\color{blue}{\\\\mathbf{x}_{t-1}} \\\\color{black}{+ \\\\bar{\\\\alpha}_{t-1} \\\\mathbf{x}_0^2} }{1-\\\\bar{\\\\alpha}_{t-1}} - \\\\frac{(\\\\mathbf{x}_t - \\\\sqrt{\\\\bar{\\\\alpha}_t} \\\\mathbf{x}_0)^2}{1-\\\\bar{\\\\alpha}_t} \\\\big) \\\\Big)=  \\n=exp(−21\\u200b(βt\\u200bxt2\\u200b−2αt\\u200b\\u200bxt\\u200bxt−1\\u200b+αt\\u200bxt−12\\u200b\\u200b+1−αˉt−1\\u200bxt−12\\u200b−2αˉt−1\\u200b\\u200bx0\\u200bxt−1\\u200b+αˉt−1\\u200bx02\\u200b\\u200b−1−αˉt\\u200b(xt\\u200b−αˉt\\u200b\\u200bx0\\u200b)2\\u200b))==exp\\u2061(−12((αtβt+11−αˉt−1)xt−12−(2αtβtxt+2αˉt−11−αˉt−1x0)xt−1+C(xt,x0)))  = \\\\exp\\\\Big( -\\\\frac{1}{2} \\\\big( \\\\color{red}{(\\\\frac{\\\\alpha_t}{\\\\beta_t} + \\\\frac{1}{1 - \\\\bar{\\\\alpha}_{t-1}})} \\\\mathbf{x}_{t-1}^2 - \\\\color{blue}{(\\\\frac{2\\\\sqrt{\\\\alpha_t}}{\\\\beta_t} \\\\mathbf{x}_t+\\\\frac{2\\\\sqrt{\\\\bar{\\\\alpha}_{t-1}}}{1 - \\\\bar{\\\\alpha}_{t-1}} \\\\mathbf{x}_0)} \\\\mathbf{x}_{t-1} \\\\color{black}{ + C(\\\\mathbf{x}_t, \\\\mathbf{x}_0) \\\\big) \\\\Big)}\\n=exp(−21\\u200b((βt\\u200bαt\\u200b\\u200b+1−αˉt−1\\u200b1\\u200b)xt−12\\u200b−(βt\\u200b2αt\\u200b\\u200b\\u200bxt\\u200b+1−αˉt−1\\u200b2αˉt−1\\u200b\\u200b\\u200bx0\\u200b)xt−1\\u200b+C(xt\\u200b,x0\\u200b)))Далее перепишем красные и синие выражения в более красивой форме\\nβ~t=1/(αtβt+11−αˉt−1)=1/(αt−αˉt+βtβt(1−αˉt−1))=1−αˉt−11−αˉt⋅βt\\\\color{red}{\\\\tilde{\\\\beta}_t} = 1/(\\\\frac{\\\\alpha_t}{\\\\beta_t} + \\\\frac{1}{1 - \\\\bar{\\\\alpha}_{t-1}}) = 1/(\\\\frac{\\\\alpha_t - \\\\bar{\\\\alpha}_t + \\\\beta_t}{\\\\beta_t(1 - \\\\bar{\\\\alpha}_{t-1})}) = \\\\boxed{\\\\frac{1 - \\\\bar{\\\\alpha}_{t-1}}{1 - \\\\bar{\\\\alpha}_t} \\\\cdot \\\\beta_t} \\nβ~\\u200bt\\u200b=1/(βt\\u200bαt\\u200b\\u200b+1−αˉt−1\\u200b1\\u200b)=1/(βt\\u200b(1−αˉt−1\\u200b)αt\\u200b−αˉt\\u200b+βt\\u200b\\u200b)=1−αˉt\\u200b1−αˉt−1\\u200b\\u200b⋅βt\\u200b\\u200bμ~t(xt,x0)=(αtβtxt+αˉt−11−αˉt−1x0)/(αtβt+11−αˉt−1)\\\\color{blue}{\\\\tilde{\\\\boldsymbol{\\\\mu}}_t (\\\\mathbf{x}_t, \\\\mathbf{x}_0)} = (\\\\frac{\\\\sqrt{\\\\alpha_t}}{\\\\beta_t} \\\\mathbf{x}_t + \\\\frac{\\\\sqrt{\\\\bar{\\\\alpha}_{t-1} }}{1 - \\\\bar{\\\\alpha}_{t-1}} \\\\mathbf{x}_0)/(\\\\frac{\\\\alpha_t}{\\\\beta_t} + \\\\frac{1}{1 - \\\\bar{\\\\alpha}_{t-1}}) \\nμ~\\u200bt\\u200b(xt\\u200b,x0\\u200b)=(βt\\u200bαt\\u200b\\u200b\\u200bxt\\u200b+1−αˉt−1\\u200bαˉt−1\\u200b\\u200b\\u200bx0\\u200b)/(βt\\u200bαt\\u200b\\u200b+1−αˉt−1\\u200b1\\u200b)=(αtβtxt+αˉt−11−αˉt−1x0)1−αˉt−11−αˉt⋅βt  = (\\\\frac{\\\\sqrt{\\\\alpha_t}}{\\\\beta_t} \\\\mathbf{x}_t + \\\\frac{\\\\sqrt{\\\\bar{\\\\alpha}_{t-1} }}{1 - \\\\bar{\\\\alpha}_{t-1}} \\\\mathbf{x}_0)\\\\frac{1 - \\\\bar{\\\\alpha}_{t-1}}{1 - \\\\bar{\\\\alpha}_t} \\\\cdot \\\\beta_t \\n=(βt\\u200bαt\\u200b\\u200b\\u200bxt\\u200b+1−αˉt−1\\u200bαˉt−1\\u200b\\u200b\\u200bx0\\u200b)1−αˉt\\u200b1−αˉt−1\\u200b\\u200b⋅βt\\u200b=αt(1−αˉt−1)1−αˉtxt+αˉt−1βt1−αˉtx0  = \\\\boxed{\\\\frac{\\\\sqrt{\\\\alpha_t}(1 - \\\\bar{\\\\alpha}_{t-1})}{1 - \\\\bar{\\\\alpha}_t} \\\\mathbf{x}_t + \\\\frac{\\\\sqrt{\\\\bar{\\\\alpha}_{t-1}}\\\\beta_t}{1 - \\\\bar{\\\\alpha}_t} \\\\mathbf{x}_0}\\n=1−αˉt\\u200bαt\\u200b\\u200b(1−αˉt−1\\u200b)\\u200bxt\\u200b+1−αˉt\\u200bαˉt−1\\u200b\\u200bβt\\u200b\\u200bx0\\u200b\\u200bДругой лосс. Предсказываем шум\\nВ прошлой подсекции наша модель предсказывала среднее и дисперсию нормального распределения. Давайте зафиксируем Σθ(xt,t)=σt2I\\\\Sigma_{\\\\theta}(x_t, t) = \\\\sigma^2_tIΣθ\\u200b(xt\\u200b,t)=σt2\\u200bI. Обычно берут σt2=βt\\\\sigma^2_t = \\\\beta_tσt2\\u200b=βt\\u200b или σt2=β~t=1−αˉt−11−αˉtβt.\\\\sigma^2_t = \\\\tilde{\\\\beta}_t = \\\\frac{1 - \\\\bar{\\\\alpha}_{t - 1}}{1 - \\\\bar{\\\\alpha}_t}\\\\beta_t.σt2\\u200b=β~\\u200bt\\u200b=1−αˉt\\u200b1−αˉt−1\\u200b\\u200bβt\\u200b.  Тогда Lt−1L_{t - 1}Lt−1\\u200b из предыдущей секции можно переписать как\\nLt=Eq[12σt2∥μθ(xt,x0)−μ~t(xt,x0)∥2]+const(θ)L_{t} = \\\\mathbb{E}_q\\\\bigg[\\\\frac{1}{2\\\\sigma^2_t}\\\\|\\\\mu_{\\\\theta}(x_t, x_0) - \\\\tilde{\\\\mu}_t(x_t, x_0)\\\\|^2\\\\bigg] + const(\\\\theta)\\nLt\\u200b=Eq\\u200b[2σt2\\u200b1\\u200b∥μθ\\u200b(xt\\u200b,x0\\u200b)−μ~\\u200bt\\u200b(xt\\u200b,x0\\u200b)∥2]+const(θ)Это первый момент, как меняется функционал, если мы не хотим предсказывать Σθ(xt,t)\\\\Sigma_{\\\\theta}(x_t, t)Σθ\\u200b(xt\\u200b,t), а фиксируем её.\\nТеперь вспомним, что  q(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I)q(x_t | x_0) = \\\\mathcal{N}(x_t;\\\\sqrt{\\\\bar{\\\\alpha}_t} x_0, (1-\\\\bar{\\\\alpha}_t)I)q(xt\\u200b∣x0\\u200b)=N(xt\\u200b;αˉt\\u200b\\u200bx0\\u200b,(1−αˉt\\u200b)I), но благодаря тому, что у нас гауссовское распределение, это можно переписать в виде\\nxt(x0,ϵ)=αˉtx0+1−αˉtϵ,\\xa0\\xa0\\xa0ϵ∼N(0,I)x_t(x_0, \\\\epsilon) = \\\\sqrt{\\\\bar{\\\\alpha}_t} x_0 +  \\\\sqrt{1-\\\\bar{\\\\alpha}_t}\\\\epsilon, \\\\ \\\\ \\\\ \\\\epsilon \\\\sim \\\\mathcal{N}(0, I)\\nxt\\u200b(x0\\u200b,ϵ)=αˉt\\u200b\\u200bx0\\u200b+1−αˉt\\u200b\\u200bϵ,\\xa0\\xa0\\xa0ϵ∼N(0,I)Выразим отсюда x0x_0x0\\u200b и получим, что x0=1αˉt(xt−1−αˉtϵ)x_0 = \\\\frac{1}{\\\\sqrt{\\\\bar{\\\\alpha}_t}}(x_t - \\\\sqrt{1 - \\\\bar{\\\\alpha}_t}\\\\epsilon)x0\\u200b=αˉt\\u200b\\u200b1\\u200b(xt\\u200b−1−αˉt\\u200b\\u200bϵ), тогда подставим это выражение в  формулу для μ~(xt,x0)\\\\tilde{\\\\boldsymbol{\\\\mu}}(\\\\mathbf{x}_t, \\\\mathbf{x}_0)μ~\\u200b(xt\\u200b,x0\\u200b) (из подсекции «Вывод q(xt−1∣xt,x0)q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\\\mathbf{x}_0)q(xt−1\\u200b∣xt\\u200b,x0\\u200b)») и получим\\nμ~(xt,x0)=1αt(xt−βt1−αˉtϵ)\\\\tilde{\\\\boldsymbol{\\\\mu}}(\\\\mathbf{x}_t, \\\\mathbf{x}_0) = \\\\frac{1}{\\\\sqrt{\\\\alpha_t}}(x_t - \\\\frac{\\\\beta_t}{\\\\sqrt{1 - \\\\bar{\\\\alpha}_t}}\\\\epsilon) \\nμ~\\u200b(xt\\u200b,x0\\u200b)=αt\\u200b\\u200b1\\u200b(xt\\u200b−1−αˉt\\u200b\\u200bβt\\u200b\\u200bϵ)Теперь скажем, что наша модель будет предсказывать ϵ\\\\epsilonϵ. И просто будем «подставлять» его в выражение для μ~\\\\tilde{\\\\mu}μ~\\u200b выше. Обозначим предсказание модели как ϵθ(xt,t)\\\\epsilon_{\\\\theta}(x_t, t)ϵθ\\u200b(xt\\u200b,t) — предсказанный шум ϵ\\\\epsilonϵ. Тогда лосс LtL_tLt\\u200b превратиться в\\nLt=Ex0,z[βt22αt(1−αˉt)σt2∥ϵ−ϵθ(αˉtx0+(1−αˉt)ϵ,t)∥2]L_t = \\\\mathbb{E}_{x_0, z}\\\\bigg[ \\\\frac{\\\\beta^2_t}{2\\\\alpha_t(1 - \\\\bar{\\\\alpha}_t )\\\\sigma^2_t}\\\\|\\\\epsilon - \\\\epsilon_{\\\\theta}(\\\\sqrt{\\\\bar{\\\\alpha}_t} x_0 +  (1-\\\\bar{\\\\alpha}_t)\\\\epsilon, t)\\\\|^2\\\\bigg]\\nLt\\u200b=Ex0\\u200b,z\\u200b[2αt\\u200b(1−αˉt\\u200b)σt2\\u200bβt2\\u200b\\u200b∥ϵ−ϵθ\\u200b(αˉt\\u200b\\u200bx0\\u200b+(1−αˉt\\u200b)ϵ,t)∥2]Тем не менее лосс можно еще больше упростить и просто обучать с помощью MSE на ϵ\\\\epsilonϵ.\\nLtsimple=Ex0,ϵ,t[∥ϵ−ϵθ(αˉtx0+(1−αˉt)ϵ,t)∥2]L^{simple}_t = \\\\mathbb{E}_{x_0, \\\\epsilon, t}\\\\bigg[ \\\\|\\\\epsilon - \\\\epsilon_{\\\\theta}(\\\\sqrt{\\\\bar{\\\\alpha}_t} x_0 +  (1-\\\\bar{\\\\alpha}_t)\\\\epsilon, t)\\\\|^2\\\\bigg]\\nLtsimple\\u200b=Ex0\\u200b,ϵ,t\\u200b[∥ϵ−ϵθ\\u200b(αˉt\\u200b\\u200bx0\\u200b+(1−αˉt\\u200b)ϵ,t)∥2]Итак, алгоритмы обучения и сэмплирования выглядят вот так (на картинке  z:=ϵz:= \\\\epsilonz:=ϵ).\\n\\n\\n\\nАлгоритм обучения и сэмплирования диффузионной модели (Изображение взято из: Ho et al. 2020)\\n\\n\\nСтоит отметить, что важным недостатком диффузионных моделей является низкая скорость сэмплирования. Согласно Song et al. 2020: «Требуется 20 часов на генерацию 50 тысяч картинок размера 32х32, используя DDPM, и меньше минуты, используя GAN» (Nvidia 2080 Ti GPU). Тем не менее, в данном направлении был достигнут значительный прогресс и в целом проблема медленного сэмплирования была частично решена: Jiaming Song et al. (2021), Kong & Ping (2021), Bond-Taylor et al. (2021)\\nДавайте зафиксируем, какие функции потерь можно использовать. Для всех них справедлив тот факт, что мы сэмплируем шаг равномерно во время обучение t∼U{1,...,T})t \\\\sim U\\\\{1,...,T\\\\})t∼U{1,...,T}) и оптимизируем соответствующий LtL_{t}Lt\\u200b.\\n\\nОптимизируя член из суммы LVLBL_{VLB}LVLB\\u200b. Это KL дивергенция между двумя нормальными распределениями\\n\\nLVLB=EqDKL(q(xT∣x0)∥pθ(xT))⏟LT++∑t=2TDKL(q(xt−1∣xt,x0)∥pθ(xt−1∣xt))⏟Lt−1−log\\u2061pθ(x0∣x1)⏟L0L_{VLB} = \\\\mathbb{E}_q \\\\underbrace{D_\\\\text{KL}(q(\\\\mathbf{x}_T \\\\vert \\n\\\\mathbf{x}_0) \\\\parallel p_\\\\theta(\\\\mathbf{x}_T))}_{L_T} + + \\\\sum_{t=2}^T \\n\\\\underbrace{D_\\\\text{KL}(q(\\\\mathbf{x}_{t-1} \\\\vert \\\\mathbf{x}_t, \\n\\\\mathbf{x}_0) \\\\parallel p_\\\\theta(\\\\mathbf{x}_{t-1} \\n\\\\vert\\\\mathbf{x}_t))}_{L_{t-1}} \\\\underbrace{- \\\\log p_\\\\theta(\\\\mathbf{x}_0 \\n\\\\vert \\\\mathbf{x}_1)}_{L_0}\\nLVLB\\u200b=Eq\\u200bLT\\u200bDKL\\u200b(q(xT\\u200b∣x0\\u200b)∥pθ\\u200b(xT\\u200b))\\u200b\\u200b++t=2∑T\\u200bLt−1\\u200bDKL\\u200b(q(xt−1\\u200b∣xt\\u200b,x0\\u200b)∥pθ\\u200b(xt−1\\u200b∣xt\\u200b))\\u200b\\u200bL0\\u200b−logpθ\\u200b(x0\\u200b∣x1\\u200b)\\u200b\\u200b\\nПри фиксированной дисперсии Σθ\\\\Sigma_{\\\\theta}Σθ\\u200b можно оптимизировать взвешенную MSE между средними нормальных распределений\\n\\nLt=Eq[12σt2∥μθ(xt,x0)−μ~t(xt,x0)∥2]+const(θ)L_{t} = \\\\mathbb{E}_q\\\\bigg[\\\\frac{1}{2\\\\sigma^2_t}\\\\|\\\\mu_{\\\\theta}(x_t, x_0) - \\\\tilde{\\\\mu}_t(x_t, x_0)\\\\|^2\\\\bigg] + const(\\\\theta)\\nLt\\u200b=Eq\\u200b[2σt2\\u200b1\\u200b∥μθ\\u200b(xt\\u200b,x0\\u200b)−μ~\\u200bt\\u200b(xt\\u200b,x0\\u200b)∥2]+const(θ)\\nПри фиксированной дисперсии и при предсказании шума с помощью взвешенной MSE. Или просто MSE. LtsimpleL^{simple}_tLtsimple\\u200b является самым популярным вариантом, который на практике дает лучшие результаты.\\n\\nLt=Ex0,z[βt22αt(1−αˉt)σt2∥ϵ−ϵθ(αˉtx0+(1−αˉt)ϵ,t)∥2]Ltsimple=Ex0,z[∥ϵ−ϵθ(αˉtx0+(1−αˉt)ϵ,t)∥2]L_t = \\\\mathbb{E}_{x_0, z}\\\\bigg[ \\\\frac{\\\\beta^2_t}{2\\\\alpha_t(1 - \\\\bar{\\\\alpha}_t )\\\\sigma^2_t}\\\\|\\\\epsilon - \\\\epsilon_{\\\\theta}(\\\\sqrt{\\\\bar{\\\\alpha}_t} x_0 +  (1-\\\\bar{\\\\alpha}_t)\\\\epsilon, t)\\\\|^2\\\\bigg] \\\\\\\\\\nL^{simple}_t = \\\\mathbb{E}_{x_0, z}\\\\bigg[ \\\\|\\\\epsilon - \\\\epsilon_{\\\\theta}(\\\\sqrt{\\\\bar{\\\\alpha}_t} x_0 +  (1-\\\\bar{\\\\alpha}_t)\\\\epsilon, t)\\\\|^2\\\\bigg]\\nLt\\u200b=Ex0\\u200b,z\\u200b[2αt\\u200b(1−αˉt\\u200b)σt2\\u200bβt2\\u200b\\u200b∥ϵ−ϵθ\\u200b(αˉt\\u200b\\u200bx0\\u200b+(1−αˉt\\u200b)ϵ,t)∥2]Ltsimple\\u200b=Ex0\\u200b,z\\u200b[∥ϵ−ϵθ\\u200b(αˉt\\u200b\\u200bx0\\u200b+(1−αˉt\\u200b)ϵ,t)∥2]Выбор расписания βt\\\\beta_tβt\\u200b\\nРасписание является гиперпараметром, основными требованиями на который являются невозрастание (β1≤...≤βT)(\\\\beta_1 \\\\leq ... \\\\leq \\\\beta_T)(β1\\u200b≤...≤βT\\u200b) и чтобы прямой процесс сходился к N(0,I)\\\\mathcal{N}(0, I)N(0,I) в пределе по TTT. Второе может гарантироваться тем, что αˉt→0\\\\bar{\\\\alpha}_t \\\\to 0αˉt\\u200b→0. Вспомним,\\nq(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I)q(x_t | x_0) = \\\\mathcal{N}(x_t;\\\\sqrt{\\\\bar{\\\\alpha}_t} x_0, (1-\\\\bar{\\\\alpha}_t)I)\\nq(xt\\u200b∣x0\\u200b)=N(xt\\u200b;αˉt\\u200b\\u200bx0\\u200b,(1−αˉt\\u200b)I)Однако на практике оно также проверяется, чтобы DKL(q(xT∣x0)∣∣N(0,I))D_{KL}(q(x_T | x_0) || \\\\mathcal{N}(0, I))DKL\\u200b(q(xT\\u200b∣x0\\u200b)∣∣N(0,I)) было близко к 0.\\nТакже стоит упомянуть, что обычно берут T=1000T = 1000T=1000. Но также важно помнить про требования выше, ведь расписание шума непосредственно зависит от TTT.\\nЧаще всего используют линейное расписание, где β1=10−4,\\xa0βT=0.02\\\\beta_1 = 10^{-4}, \\\\ \\\\beta_T = 0.02β1\\u200b=10−4,\\xa0βT\\u200b=0.02.  У данных констант нет никакой мотивации, кроме той, которая описана выше. Они были предложены в Ho et al. (2020).\\nВ Nichol & Dhariwal (2021) было предложено косинусное расписание, которое помогло диффузионным моделям достичь лучшего NLL (negative loglikelihood):\\nβt=clip(1−αˉtαˉt−1,0.999)αˉt=f(t)f(0)where\\xa0f(t)=cos\\u2061(t/T+s1+s⋅π2)\\\\beta_t = \\\\text{clip}(1-\\\\frac{\\\\bar{\\\\alpha}_t}{\\\\bar{\\\\alpha}_{t-1}}, 0.999) \\\\quad\\\\bar{\\\\alpha}_t = \\\\frac{f(t)}{f(0)}\\\\quad\\\\text{where }f(t)=\\\\cos\\\\Big(\\\\frac{t/T+s}{1+s}\\\\cdot\\\\frac{\\\\pi}{2}\\\\Big)\\nβt\\u200b=clip(1−αˉt−1\\u200bαˉt\\u200b\\u200b,0.999)αˉt\\u200b=f(0)f(t)\\u200bwhere\\xa0f(t)=cos(1+st/T+s\\u200b⋅2π\\u200b)Авторы обнаружили, что линейное расписание плохо работает на картинках 64х64 и меньше. А именно, последнии шаги прямого прохода были шумными и малоинформатиыными (просто зашумляем шум еще больше):\\n\\n\\n\\n    Пример зашумления картинки для линейного (сверху) и косинусного (снизу) расписания.\\n  \\n\\nТакже они обнаружили, что если обучать модель с линейным расписанием только на 80% первых шагов, то модель не становится сильно хуже, что подтверждает неиформативность последних шагов. Далее, они подобрали расписание так, чтобы αˉt\\\\sqrt{\\\\bar{\\\\alpha}_t}αˉt\\u200b\\u200b убывало линейно на большей части отрезка (от 0 до TTT) и почти не менялось рядом с 0 и TTT. Разницу в αˉt\\\\sqrt{\\\\bar{\\\\alpha}}_tαˉ\\u200bt\\u200b для разных расписаний можно увидеть на картинке ниже:\\n\\n\\n\\nИзображение взято из Nichol & Dhariwal, 2021\\n\\n\\n\\nДетали\\n\\nТакже они  ограничивают βt\\\\beta_tβt\\u200b числом 0.999, чтобы в конце процесса не было проблем с численной устойчивостью. Коэффициент sss используется, чтобы βt\\\\beta_tβt\\u200b не были слишком малы рядом с нулем. Он равен 0.008. Такое число было выбрано так, чтобы «β0\\\\sqrt{\\\\beta_0}β0\\u200b\\u200b была немного меньше, чем размер бина одного пикселя, то есть 1/127.51/127.51/127.5»\\nClassifier guidance\\nВ Nichol & Dhariwal (2021) был предложен метод условной генерации, который повышает качество генерируемых картинок, при этом уменьшая их разнообразие. Для этого предобучается «шумный» классификатор на зашумленных картинках, то есть pϕ(y∣xt)p_{\\\\phi}(y |x_t)pϕ\\u200b(y∣xt\\u200b). Затем он используется во время сэмплирования, корректируя предсказанное среднее на ∇xlog\\u2061pϕ(y∣xt)\\\\nabla_x \\\\log p_{\\\\phi}(y|x_t)∇x\\u200blogpϕ\\u200b(y∣xt\\u200b). В Nichol & Dhariwal (2021) (Секция 4.1) показывают, что данная добавка позволяет превратить распределение pθ(xi−1∣xi)p_\\\\theta(x_{i-1}|x_{i})pθ\\u200b(xi−1\\u200b∣xi\\u200b) в pθ(xi−1∣xi,y)p_\\\\theta(x_{i-1}|x_{i}, y)pθ\\u200b(xi−1\\u200b∣xi\\u200b,y). Важно, что исходная диффузионная модель никак не меняется, что делает трюк еще более привлекательным.  Алгоритм сэмплирования можно видеть на картинке ниже. Коэффициент sss отвечает за силу guidance.\\n\\nМотивация\\nУ генеративной модели GAN есть способ, который позволяет «балансировать» между разнообразием картинок и их качеством — truncation trick. Он заключается в сэмплировании латентного вектора truncated normal distibution. Данный трюк был хорошо описан и исследован в статье про BigGAN.  Поэтому в диффузионных моделях тоже хотелось бы иметь метод, который позволяет балансировать между качеством и разнообразием. Авторы предложили classifier guidance, сравнили его с truncation trick и показали, что их метод строго лучше.\\n\\n\\n\\n\\nИзображение взято из Nichol & Dhariwal, 2021\\n\\n\\nClassifier-free guidance\\nHo & Salimans (2021) предложили метод, в котором guidance достигается без использования дополнительной модели, поскольку это достаточно затратно. Для этого они обучали условную модель ϵθ(xt∣y)\\\\epsilon_{\\\\theta}(x_t | y)ϵθ\\u200b(xt\\u200b∣y), у которой во время обучения реальная метка yyy заменялась с какой-то фиксированной вероятностью (10%) на пустую метку (y=∅y=\\\\emptysety=∅). Это по сути позволяет нам обучать безусловную модель ϵθ(xt)\\\\epsilon_{\\\\theta}(x_t)ϵθ\\u200b(xt\\u200b) одновременно с условной ϵθ(xt∣y)\\\\epsilon_{\\\\theta}(x_t | y)ϵθ\\u200b(xt\\u200b∣y)Тогда во время сэмплирования делаем так, чтобы предсказание немного менялось в сторону ϵθ(xt∣y)\\\\epsilon_{\\\\theta}(x_t | y)ϵθ\\u200b(xt\\u200b∣y), а именно:\\nϵ^θ(xt∣y)=ϵθ(xt∣∅)+s⋅(ϵθ(xt∣y)−ϵθ(xt∣∅))\\\\hat{\\\\epsilon}_{\\\\theta}(x_t | y) = \\\\epsilon_{\\\\theta}(x_t | \\\\emptyset) + s\\\\cdot(\\\\epsilon_{\\\\theta}(x_t | y) - \\\\epsilon_{\\\\theta}(x_t | \\\\emptyset))\\nϵ^θ\\u200b(xt\\u200b∣y)=ϵθ\\u200b(xt\\u200b∣∅)+s⋅(ϵθ\\u200b(xt\\u200b∣y)−ϵθ\\u200b(xt\\u200b∣∅))Мотивация этой формулы следовала из формулы Байеса:\\np(y∣xt)∝p(xt∣y)p(xt)\\u2005\\u200a⟹\\u2005\\u200alog\\u2061p(y∣xt)∝log\\u2061p(xt∣y)−log\\u2061p(xt)\\u2005\\u200a⟹\\u2005\\u200a∇xtlog\\u2061p(y∣xt)∝∇xtlog\\u2061p(xt∣y)−∇xtlog\\u2061p(xt)\\u2005\\u200a⟹\\u2005\\u200a∇xtlog\\u2061p(y∣xt)∝ϵ(xt∣y)−ϵ(xt)p(y | x_t) \\\\propto \\\\frac{p(x_t | y) }{p(x_t)} \\\\\\\\\\n\\\\implies \\\\log p(y | x_t) \\\\propto \\\\log p(x_t | y) - \\\\log p(x_t) \\\\\\\\\\n\\\\implies \\\\nabla_{x_t} \\\\log p(y | x_t) \\\\propto \\\\nabla_{x_t} \\\\log p(x_t | y) - \\\\nabla_{x_t} \\\\log p(x_t) \\\\\\\\\\n\\\\implies \\\\nabla_{x_t} \\\\log p(y | x_t) \\\\propto \\\\epsilon(x_t | y) - \\\\epsilon(x_t)\\np(y∣xt\\u200b)∝p(xt\\u200b)p(xt\\u200b∣y)\\u200b⟹logp(y∣xt\\u200b)∝logp(xt\\u200b∣y)−logp(xt\\u200b)⟹∇xt\\u200b\\u200blogp(y∣xt\\u200b)∝∇xt\\u200b\\u200blogp(xt\\u200b∣y)−∇xt\\u200b\\u200blogp(xt\\u200b)⟹∇xt\\u200b\\u200blogp(y∣xt\\u200b)∝ϵ(xt\\u200b∣y)−ϵ(xt\\u200b)Тогда мы можем просто подставить ∇xtlog\\u2061p(y∣xt)\\\\nabla_{x_t} \\\\log p(y | x_t)∇xt\\u200b\\u200blogp(y∣xt\\u200b) в формулу для classifier guidance из предыдущей подсекции и получить желаемое равенство с точностью до коэффициента sss.\\nОвервью ключевых работ на сегодняшний день\\n\\nJonathan Ho et al. «Denoising diffusion probabilistic models.» arxiv Preprint arxiv:2006.11239 (2020)\\n\\nОсновная работа, в которой диффузионные модели (Denoising Diffusion Probabilistic Models, DDPMs) были применены для генерации картинок. Параграф в основном построен на ней.\\n\\nJiaming Song et al. «Denoising diffusion implicit models.» arxiv Preprint arxiv:2010.02502 (2020)\\n\\nОдна из первых попыток ускорить генерацию объектов. Идея следущая: давайте изменим прямой диффузионный процесс так, чтобы используя предобученную DDPM, приближать новый обратный процесс за меньшее число шагов.\\nЧтобы не обучать новую модель, нам нужен прямой диффузионный процесс, у которого будет такая же (суррогатная) функция потерь, а обратный процесс все еще останется Марковским. Оказалось, что существует целое семейство не-Марковских прямых процессов, удовлетворяющих этим требования. Это семейство имеет следующий вид:\\nqσ(x1:T∣x0):=qσ(xT∣x0)∏t=2Tqσ(xt−1∣xt,x0),q_\\\\sigma (x_{1:T}|x_0):= q_\\\\sigma (x_{T}|x_0)\\\\prod_{t=2}^{T} q_\\\\sigma (x_{t-1}|x_t,x_0), \\nqσ\\u200b(x1:T\\u200b∣x0\\u200b):=qσ\\u200b(xT\\u200b∣x0\\u200b)t=2∏T\\u200bqσ\\u200b(xt−1\\u200b∣xt\\u200b,x0\\u200b),где qσ(xT∣x0)=N(αtx0,(1−αt)I)q_\\\\sigma (x_{T}|x_0)= \\\\mathcal{N}(\\\\sqrt{\\\\alpha_t}x_0, (1 - \\\\alpha_t)I)qσ\\u200b(xT\\u200b∣x0\\u200b)=N(αt\\u200b\\u200bx0\\u200b,(1−αt\\u200b)I) и для всех t>1,t>1,t>1,\\nqσ(xt−1∣xt,x0)=N(αt−1x0+1−αt−1−σt2⋅xt−αtx01−αt,σt2I)q_\\\\sigma (x_{t-1}|x_t,x_0)= \\\\mathcal{N}(\\\\sqrt{\\\\alpha_{t-1}}x_0+\\\\sqrt{1-\\\\alpha_{t-1}-\\\\sigma_t^2 }\\\\cdot\\\\frac{x_t-\\\\sqrt{\\\\alpha_{t}}x_0}{\\\\sqrt{1-\\\\alpha_t}},\\\\sigma_t^2 I)\\nqσ\\u200b(xt−1\\u200b∣xt\\u200b,x0\\u200b)=N(αt−1\\u200b\\u200bx0\\u200b+1−αt−1\\u200b−σt2\\u200b\\u200b⋅1−αt\\u200b\\u200bxt\\u200b−αt\\u200b\\u200bx0\\u200b\\u200b,σt2\\u200bI)Среднее было выбрано так, чтобы qσ(xt∣x0)=N(αtx0,(1−αt)I)q_{\\\\sigma}(x_t | x_0) = \\\\mathcal{N}(\\\\sqrt{\\\\alpha_t}x_0, (1 - \\\\alpha_t)I)qσ\\u200b(xt\\u200b∣x0\\u200b)=N(αt\\u200b\\u200bx0\\u200b,(1−αt\\u200b)I) для всех ttt. (см. Лемму 1 в Приложении B к статье). То есть важно лишь то, чтобы маргинальное распределение qσ(xt∣x0)q_{\\\\sigma}(x_t | x_0)qσ\\u200b(xt\\u200b∣x0\\u200b) не менялось по сравнению с обычным Марковским случаем. Прямой процесс может быть получен с помощью теоремы Байеса:\\nqσ(xt∣xt−1,x0)=qσ(xt−1∣xt,x0)qσ(xt∣x0)qσ(xt−1∣x0)q_\\\\sigma (x_t|x_{t-1},x_0)=\\\\frac{q_\\\\sigma (x_{t-1}|x_t,x_0)q_\\\\sigma (x_t|x_0)}{q_\\\\sigma (x_{t-1}|x_0)}\\nqσ\\u200b(xt\\u200b∣xt−1\\u200b,x0\\u200b)=qσ\\u200b(xt−1\\u200b∣x0\\u200b)qσ\\u200b(xt−1\\u200b∣xt\\u200b,x0\\u200b)qσ\\u200b(xt\\u200b∣x0\\u200b)\\u200bТут σ\\\\sigmaσ контролирует степень стохастичности прямого процесса. Можно заметить, что в отличии от исходного диффузионного процесса, предложенный прямой процесс больше не является Марковским, так как каждый xtx_txt\\u200b теперь зависит и от xt−1x_{t-1}xt−1\\u200b и от x0x_0x0\\u200b. Схематично, это можно изобразить как на картинке справа. (Слева исходный диффузионный процесс для сравнения)\\n\\n\\nЗаметка\\nАвторы обращают внимание, что функция потерь в DDPM зависит от q(xt∣x0)q(x_t|x_0)q(xt\\u200b∣x0\\u200b), а не от q(x1:xT∣x0)q(x_1{:}x_T | x_0)q(x1\\u200b:xT\\u200b∣x0\\u200b) напрямую. Это означает, что нам нужно выбрать любой другой прямой диффузионный процесс, у которого q(xt∣x0)q(x_t|x_0)q(xt\\u200b∣x0\\u200b) остались те же.\\n\\nДалее, мы можем переписать обратный процесс в данном виде:\\nxt−1=αt−1\\u2005\\u200axt−1−αtϵθ(t)(xt)αt⏟\"predicted\\u2005x0\"+1−αt−1−σt2⋅ϵθ(t)(xt)⏟\"direction\\u2005\\u200apointing\\u2005\\u200ato\\u2005\\u200axt\"+σtϵt⏟random\\u2005noisex_{t-1}=\\\\sqrt{\\\\alpha_{t-1}}\\\\;\\\\underbrace{\\\\frac{x_t-\\\\sqrt{1-\\\\alpha_{t}}\\\\epsilon_\\\\theta^{(t)}(x_t)}{\\\\sqrt{\\\\alpha_t}}}_{\"predicted\\\\:x_0\"}+\\\\underbrace{\\\\sqrt{1-\\\\alpha_{t-1}-\\\\sigma_t^2 }\\\\cdot \\\\epsilon_\\\\theta^{(t)}(x_t)}_{\"direction\\\\;pointing\\\\;to\\\\;x_t\"}+\\\\underbrace{\\\\sigma_t\\\\epsilon_t}_{random\\\\:noise}\\nxt−1\\u200b=αt−1\\u200b\\u200b\"predictedx0\\u200b\"αt\\u200b\\u200bxt\\u200b−1−αt\\u200b\\u200bϵθ(t)\\u200b(xt\\u200b)\\u200b\\u200b\\u200b+\"directionpointingtoxt\\u200b\"1−αt−1\\u200b−σt2\\u200b\\u200b⋅ϵθ(t)\\u200b(xt\\u200b)\\u200b\\u200b+randomnoiseσt\\u200bϵt\\u200b\\u200b\\u200bЗаметим, что при σt=(1−αt−1)(1−αt)1−αt/αt−1\\\\sigma_t = \\\\sqrt{(1 - \\\\alpha_{t - 1})(1 - \\\\alpha_t)}\\\\sqrt{1 - \\\\alpha_t / \\\\alpha_{t - 1}}σt\\u200b=(1−αt−1\\u200b)(1−αt\\u200b)\\u200b1−αt\\u200b/αt−1\\u200b\\u200b прямой процесс становится марковским, а обратный как у DDPM (обычное сэмплирование, описанное в основной секции). При σt=0\\\\sigma_t = 0σt\\u200b=0 процесс сэмплирования становится детерминистичным (данный способ и называется DDIM). Ускорение сэмплирования достигается засчет использования лишь какого-то подмножества шагов (0≤τ1≤...≤τS≤T,\\xa0\\xa0\\xa0S<T0 \\\\leq \\\\tau_1 \\\\leq ... \\\\leq \\\\tau_S \\\\leq T, \\\\ \\\\ \\\\ S < T0≤τ1\\u200b≤...≤τS\\u200b≤T,\\xa0\\xa0\\xa0S<T). Также одним из плюсов детерминистичного сэмплирования является возможность делать семантическую интерполяцию в латентном пространстве (как у GANов).\\n\\nAlex Nichol & Prafulla Dhariwal. «Improved denoising diffusion probabilistic models» arxiv Preprint arxiv:2102.09672 (2021)\\n\\nУлучшение DDPM, в котором был предложен новое расписание шума, что улучшило NLL. Также был изучен вариант, в котором дисперсия Σθ(xt,t)\\\\Sigma_{\\\\theta}(x_t, t)Σθ\\u200b(xt\\u200b,t) предсказывается моделью.\\n\\nPrafula Dhariwal & Alex Nichol. «Diffusion Models Beat GANs on Image Synthesis.» arxiv Preprint arxiv:2105.05233 (2021).\\n\\nСтатья, в которой показывается, что DDPM могут генерировать более качественные картинки по сравнению с GANами. Также был предложен метод conditional сэмплирования. Для этого предобучается классификатор на зашумленных сэмплах, а во время сэмплирования среднее нормального распределения «корректируется» на градиент классификатора.\\n\\nJacob Austin et al. «Structured Denoising Diffusion Models in Discrete State-Spaces».arXiv:2107.03006 (2021)\\n\\nДиффузионные модели на дискретных данных (например, текст). Вместо нормальных распределений используются категориальные. Также была обобщена мультиномиальная диффузия с помощью «матриц перехода», которые задают способ зашумления дискретных данных.\\nБолее подробно: у нас есть xt∈{1,...,K}x_t \\\\in \\\\{1, ..., K\\\\}xt\\u200b∈{1,...,K} — дискретная величина на всех шагах диффузии, тогда для каждого шага ttt определена матрица прямого перехода QtQ_tQt\\u200b такая, что [Qt]ij=q(xt=j∣xt−1=i)[Q_t]_{ij} = q(x_t = j| x_{t - 1} = i)[Qt\\u200b]ij\\u200b=q(xt\\u200b=j∣xt−1\\u200b=i). То есть строки матрицы суммируются в единицу. Тогда если обозначить через xt∈RK\\\\mathbf{x}_t \\\\in \\\\mathbb{R}^Kxt\\u200b∈RK one-hot-закодированную версию xtx_txt\\u200b, то прямой процесс можно описать через категориальные распределения:\\nq(xt∣xt−1)=Cat(xt;p=xt−1Qt)q(\\\\mathbf{x}_t | \\\\mathbf{x}_{t -1}) = Cat(\\\\mathbf{x}_t; \\\\mathbf{p} = \\\\mathbf{x}_{t - 1}Q_t)\\nq(xt\\u200b∣xt−1\\u200b)=Cat(xt\\u200b;p=xt−1\\u200bQt\\u200b)Как и в нормальных распределениях, можем выписать\\nq(xt∣x0)=Cat(xt;p=x0Qˉt),\\xa0\\xa0\\xa0где\\xa0\\xa0\\xa0Qˉt=Q1Q2...Qtq(\\\\mathbf{x}_t | \\\\mathbf{x}_{0}) = Cat(\\\\mathbf{x}_t; \\\\mathbf{p} = \\\\mathbf{x}_{0}\\\\bar{Q}_t), \\\\ \\\\ \\\\ где \\\\ \\\\ \\\\ \\\\bar{Q}_t=Q_1Q_2...Q_t  \\nq(xt\\u200b∣x0\\u200b)=Cat(xt\\u200b;p=x0\\u200bQˉ\\u200bt\\u200b),\\xa0\\xa0\\xa0где\\xa0\\xa0\\xa0Qˉ\\u200bt\\u200b=Q1\\u200bQ2\\u200b...Qt\\u200bq(xt−1∣xt,x0)=Cat(xt−1;p=xtQtT⊙x0Qˉt−1x0QˉtxtT)q(\\\\mathbf{x}_{t - 1}| \\\\mathbf{x}_t, \\\\mathbf{x}_0) = Cat(\\\\mathbf{x}_{t - 1}; \\\\mathbf{p} = \\\\frac{\\\\mathbf{x}_{t}{Q}^T_t \\\\odot \\\\mathbf{x}_0\\\\bar{Q}_{t-1}}{\\\\mathbf{x}_0\\\\bar{Q}_t\\\\mathbf{x}^T_t})\\nq(xt−1\\u200b∣xt\\u200b,x0\\u200b)=Cat(xt−1\\u200b;p=x0\\u200bQˉ\\u200bt\\u200bxtT\\u200bxt\\u200bQtT\\u200b⊙x0\\u200bQˉ\\u200bt−1\\u200b\\u200b)Поскольку тут нет такой хорошей параметризации через ϵ\\\\epsilonϵ, как у нормальных распределений, то единственный способ обучать — с помощью KL дивергенции  (членамиLVLBL_{VLB}LVLB\\u200b).\\nОстается только понять, как выбирать QtQ_tQt\\u200b. Помимо того, чтобы сумма в каждой строчке была один, требуется, чтобы Qˉt\\\\bar{Q}_tQˉ\\u200bt\\u200b сходилось (при t→∞t \\\\to \\\\inftyt→∞) к равномерному распределению в каждой строчке (аналог нормального шума). За конкретными примерами стоит обратиться к статье.\\n\\nСерия работ про text-conditional diffusions: GLIDE, ImaGen, DALLE-2\\n\\nОпишем работу метода GLIDE. Стоит задача генерировать картинки по заданному текстовому описанию. Для этого используется classifier-free guided diffusion model или CLIP. Это два разных варианта модели, которые авторы сравнивают. В первом случае модель обуславливается на эмбеддинги текста, которые были получены из обучаемого трансформера. Во втором случае guidance осуществляется за счет ∇xt⟨f(xt),g(c)⟩\\\\nabla_{x_t} \\\\langle f(x_t), g(c) \\\\rangle∇xt\\u200b\\u200b⟨f(xt\\u200b),g(c)⟩ (это по сути градиент лосса метода CLIP) . Тут fff — это картиночный энкодер (на зашумленных картинках), а ggg — это  энкодер текстового входа. В целом, авторы получили, что classifier-free guidance генерирует более качественные картинки.\\n\\nSong et al. «Score-Based Generative Modeling through Stochastic Differential Equations»\\n\\nСпособ описать диффузионные модели через стохастические дифференциальные уравнения.\\n\\nWhat are Diffusion Models?. Прекрасный блог от Lilian Weng (OpenAI).\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф8.4. Нормализующие потокиСледующий параграф8.6. Языковые моделиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_23.html', 'title': 'Тонкости обучения'}, page_content=\"Тонкости обученияЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/45.1.Нейронные сети5.2.Первое знакомство с полносвязными нейросетями5.3.Метод обратного распространения ошибки5.4.Тонкости обученияИнициализируем правильноМетоды оптимизации в нейронных сетяхРегуляризация нейронных сетей6.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Тонкости обучения5.4. Тонкости обученияАвторыНейчев РадославИнициализация весов. Регуляризация нейросетей. Dropout и\\xa0BatchnormЕсли открыть случайную научную статью по глубинному обучению и попробовать воспроизвести её результаты, можно запросто потерпеть крах, и даже код на github, если он есть, может не помочь.\\nА дело в том, что обучение сложной модели — это сложная инженерная задача, в которой успеху сопутствует огромное число разных хаков, и изменение какого-нибудь безобидного параметра может очень сильно повлиять на результат.\\nВ этом параграфе мы познакомим вас с некоторыми из таких приёмов.\\nИнициализируем правильно\\nКак вы уже успели заметить, нейронные сети — достаточно сложные модели, чувствительные к изменениям архитектуры, гиперпараметров, распределения данных и другим вещам.\\nПоэтому значительную роль играет начальная инициализация весов вашей сети. Стоит отметить, что здесь речь идет именно о начальной инициализации параметров сети, вопрос дообучения (и использования предобученных сетей в качестве backbone) в данном параграфе рассматриваться не будет.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nНейронные сети включают в себя различные преобразования, и инициализация по-хорошему также должна зависеть от типа используемого преобразования. На практике вопрос часто остается без внимания, так как в большинстве современных фреймворков уже реализованы методы инициализации, зависящие от используемой функции активации и гиперпараметров слоя, и пользователь может не задумываться об этом. Но всё же важно понимать, какие соображения привели к появлению тех или иных стратегий инициализации.\\nДавайте разберём несколько методов инициализации и обсудим их свойства.\\nНаивный подход №0: инициализация нулем/константой\\nКазалось бы, инициализация параметров слоя нулями — это достаточно просто и лаконично. Но инициализация нулём (как и любой другой константой) ведёт к катастрофе! Вот пример того, что может получиться:\\n\\nСтоит, впрочем, отметить, что из-за численных ошибок значения параметров могут всё-таки сдвинуться с мёртвой точки, и тогда нейросеть что-нибудь выучит:\\n\\nМатематическая иллюстрация того, почему плохо инициализировать нулямиНам понадобится сеть с хотя бы двумя слоями (иначе ничего не получится).\\nРассмотрим несколько последовательных скрытых представлений:\\n\\nX1X^1X1;\\nX2=X1WX^2 = X^1WX2=X1W, где W=0W = 0W=0 — матрица весов, которую инициализировали нулями;\\nX3=h(X2)X^3 = h(X^2)X3=h(X2), где hhh — поэлементная нелинейность;\\nX4=X3UX^4 = X^3UX4=X3U, где U=0U = 0U=0 — ещё одна инициализированная нулями матрица весов.\\n\\nПроследим, что происходит во время forward pass.\\nX2=X1⋅W=0X^2 = X^1\\\\cdot W = 0\\nX2=X1⋅W=0Получилась снова матрица с одинаковыми столбцами, и это сохраняется дальше:\\nX3=h(0)X^3 = h(0)\\nX3=h(0)X4=X3U=0X^4 = X^3U = 0\\nX4=X3U=0Теперь рассмотрим обратный проход. Допустим, мы вычислили градиент ∇X4L\\\\nabla_{X^4}\\\\mathcal{L}∇X4\\u200bL. Тогда\\n∇UL=(X3)T∇X4L=0,\\\\nabla_{U}\\\\mathcal{L} = (X^3)^T\\\\nabla_{X^4}\\\\mathcal{L} = 0,\\n∇U\\u200bL=(X3)T∇X4\\u200bL=0,то есть матрица UUU никак не обновится. Далее,\\n∇X3L=∇X4LUT=0,\\\\nabla_{X^3}\\\\mathcal{L} = \\\\nabla_{X^4}\\\\mathcal{L}U^T = 0,\\n∇X3\\u200bL=∇X4\\u200bLUT=0,∇X2L=∇X3L⊙h′(X2)=0,\\\\nabla_{X^2}\\\\mathcal{L} = \\\\nabla_{X^3}\\\\mathcal{L}\\\\odot h'(X^2) = 0,\\n∇X2\\u200bL=∇X3\\u200bL⊙h′(X2)=0,∇WL=(X1)T∇X2L=0,\\\\nabla_W\\\\mathcal{L} = (X^1)^T\\\\nabla_{X^2}\\\\mathcal{L} = 0,\\n∇W\\u200bL=(X1)T∇X2\\u200bL=0,то есть веса WWW тоже не обновятся. Таким образом, обучение происходить не будет.\\nА что же будет с однослойной нейросетью? Вспомним, что веса логистической регрессии (нейросети с одним-единственным, последним слоем) можно инициализировать чем угодно, в том числе нулями: функция потерь выпукла, поэтому градиентный спуск сходится из любой точки (по модулю численных эффектов).\\nЗдесь также стоит привести цитату из замечательной Deep Learning book (страница 301):\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1Perhaps the only property known with complete certainty is that the initial parameters need to “break symmetry” between different units. If two hidden units with the same activation function are connected to the same inputs, then these units must have different initial parameters. If they have the same initial parameters, then a deterministic learning algorithm applied to a deterministic cost and model will constantly update both of these units in the same way.\\n\\n\\nЭвристический подход №1: инициализация случайными числами\\nЕсли константная инициализация не подходит, можно инициализировать нейросеть случайными числами. Допустим, веса пришли из распределения с нулевым средним и дисперсией σ2\\\\sigma^2σ2, например, из нормального распределения N(0,σ2)\\\\mathcal{N}(0, \\\\sigma^2)N(0,σ2).\\nПусть теперь на вход линейному слою с весами w\\\\mathbf{w}w размерности ninn_{\\\\text{in}}nin\\u200b пришел вектор x\\\\mathbf{x}x аналогичной размерности.\\nЗамечание. Можем считать, что мы рассматриваем лишь одну компоненту следующего промежуточного представления z\\\\mathbf{z}z.\\nВсе компоненты x\\\\mathbf{x}x распределены одинаковым образом и обладают нулевым средним. Тогда дисперсия их произведения yyy имеет вид:\\nVar(y)=Var(wTx)=∑i=1n[E[xi]2Var(wi)+E[wi]2Var(xi)+Var(wi)Var(xi)]\\\\text{Var}(y) = \\\\text{Var}(\\\\mathbf{w}^T\\\\mathbf{x}) = \\\\sum_{i=1}^n [ \\\\mathbb{E}[x_i]^2 \\\\text{Var}(w_i) + \\\\mathbb{E}[w_i]^2 \\\\text{Var}(x_i) + \\\\text{Var}(w_i)\\\\text{Var}(x_i)]\\nVar(y)=Var(wTx)=i=1∑n\\u200b[E[xi\\u200b]2Var(wi\\u200b)+E[wi\\u200b]2Var(xi\\u200b)+Var(wi\\u200b)Var(xi\\u200b)]Первое и второе слагаемые равны нулю так как математические ожидание и весов, и значений x\\\\mathbf{x}x равны нулю.\\nЗамечание. Стоит заметить, что это будет верно и для промежуточных слоев в случае использования симметричной относительно нуля функции активации, например, tanh.\\nПоскольку все веса пришли из одного распределения, можно выразить дисперсию результата следующим образом:\\nVar(wTx)=ninVar(w)Var(x),\\\\text{Var}(\\\\mathbf{w}^T\\\\mathbf{x}) = n_{\\\\text{in}} \\\\text{Var}(w)\\\\text{Var}(x),\\nVar(wTx)=nin\\u200bVar(w)Var(x),где Var(x)\\\\text{Var}(x)Var(x) — это дисперсия любой компоненты x\\\\mathbf{x}x (как было оговорено ранее, они распределены одинаково), а Var(w)=σ2\\\\text{Var}(w) = \\\\sigma^2Var(w)=σ2 — дисперсия компоненты w\\\\mathbf{w}w.\\nСледовательно, дисперсия результата линейно зависит от дисперсии входных данных с коэффициентом ninVar(w)n_{\\\\text{in}} \\\\text{Var}(w)nin\\u200bVar(w).\\nУвеличение дисперсии промежуточных представлений с каждым новым преобразованием (слоем) может вызвать численные ошибки или насыщение функций активации (таких как tanh и sigmoid), что не лучшим образом скажется на обучении сети.\\nСнижение дисперсии может привести к почти нулевым промежуточным представлениям (плюс «линейному» поведению tanh и sigmoid в непосредственной близости от нуля), что тоже негативно повлияет на результаты обучения.\\nПоэтому для начальной инициализации весов имеет смысл использовать распределение, дисперсия которого позволила бы сохранить дисперсию результата. Например, ∀i,wi∼N(0,1nin),\\\\forall i, w_i \\\\sim \\\\mathbb{N}(0, \\\\frac{1}{n_{\\\\text{in}}}),∀i,wi\\u200b∼N(0,nin\\u200b1\\u200b), или же в общем случае\\n∀i,Var(wi)=1nin\\\\forall i, \\\\text{Var}(w_i) = \\\\frac{1}{n_\\\\text{in}}\\n∀i,Var(wi\\u200b)=nin\\u200b1\\u200bДанный подход часто упоминается как calibrated random numbers initialization.\\nПодход №2: Xavier & Normalized Xavier initialization\\nЕсли обратиться к предыдущему подходу, можно обнаружить, что все выкладки верны как для «прямого» прохода (forward propagation), так и для обратного (backward propagation). Дисперсия градиента при этом меняется в noutVar(w)n_{\\\\text{out}} \\\\text{Var}(w)nout\\u200bVar(w) раз, где noutn_{\\\\text{out}}nout\\u200b — размерность следующего за x\\\\mathbf{x}x промежуточного представления.\\nИ если мы хотим, чтобы сохранялись дисперсии и промежуточных представлений, и градиентов, у нас возникают сразу два ограничения:\\n∀i,Var(wi)=1nin\\\\forall i, \\\\text{Var}(w_i) = \\\\frac{1}{n_\\\\text{in}}\\n∀i,Var(wi\\u200b)=nin\\u200b1\\u200bи\\n∀i,Var(wi)=1nout.\\\\forall i, \\\\text{Var}(w_i) = \\\\frac{1}{n_\\\\text{out}}.\\n∀i,Var(wi\\u200b)=nout\\u200b1\\u200b.Легко заметить, что оба этих ограничения могут быть выполнены только в случае, когда размерность пространства не меняется при отображении, что случается далеко не всегда.\\nВ работе Understanding the difficulty of training deep feedforward neural networks за авторством Xavier Glorot и Yoshua Bengio в качестве компромисса предлагается использовать параметры из распределения с дисперсией\\n∀i,Var(wi)=2nin+nout.\\\\forall i, \\\\text{Var}(w_i) = \\\\frac{2}{n_\\\\text{in} + n_\\\\text{out}}.\\n∀i,Var(wi\\u200b)=nin\\u200b+nout\\u200b2\\u200b.Подробный вывод данного результата можно найти в оригинальной статье в формулах 2-12.\\nОбратите внимание: эта инициализация хорошо подходит именно для tanh, так как в выводе явно учитывается симметричность функции активации относительно нуля.\\nВ случае использования равномерного распределения UUU для инициализации весов с учетом описанных выше ограничений мы получим normalized Xavier initialization:\\n∀i,wi∼U[−6nin+nout,6nin+nout].\\\\forall i, w_i \\\\sim U\\\\left[-\\\\frac{\\\\sqrt{6}}{\\\\sqrt{n_\\\\text{in} + n_\\\\text{out}}}, \\\\frac{\\\\sqrt{6}}{\\\\sqrt{n_\\\\text{in} + n_\\\\text{out}}}\\\\right].\\n∀i,wi\\u200b∼U[−nin\\u200b+nout\\u200b\\u200b6\\u200b\\u200b,nin\\u200b+nout\\u200b\\u200b6\\u200b\\u200b].Замечание. Здесь используется тот факт, что дисперсия непрерывного равномерного распределения Var[U[a,b]]=112(b−a)2\\\\text{Var}[U[a, b]] = \\\\frac{1}{12} (b-a)^2Var[U[a,b]]=121\\u200b(b−a)2.\\nСравнение подобной инициализации для поведения промежуточных представлений (сверху) и градиентов (снизу) проиллюстрированы ниже (иллюстрации из оригинальной статьи):\\n\\n\\nПодход №3: Kaiming initialization\\nВы могли обратить внимание, что Xavier initialization во многом опиралась на поведение функции активации tanh. Данный тип инициализации и впрямь лучше подходит для нее, но само использование гиперболического тангенса приводит к некоторым сложностям (например, к затуханию градиентов).\\nВ 2015 году в работе Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification за авторством Kaiming He, Xiangyu Zhang, Shaoqing Ren и Jian Sun были рассмотрены особые свойства функции активации ReLU, в частности, существенно смещенная относительно нуля область значений.\\nПусть представление на входе было получено после применения данной функции активации к предыдущему представлению zprev\\\\mathbf{z}_\\\\text{prev}zprev\\u200b:\\nx=ReLU(zprev),\\\\mathbf{x} = \\\\text{ReLU}(\\\\mathbf{z}_\\\\text{prev}),\\nx=ReLU(zprev\\u200b),где  zprev\\\\mathbf{z}_\\\\text{prev}zprev\\u200b, в свою очередь, — это выход предыдущего линейного слоя с нулевым средним для каждой компоненты весов, то есть, в частности, E(zprev)=0\\\\mathbb{E}(z_\\\\text{prev}) = 0E(zprev\\u200b)=0\\nВ таком случае дисперсия выхода следующего линейного слоя примет вид:\\nVar(wTx)=∑i=1n[E[xi]2Var(wi)+E[wi]2Var(xi)+Var(wi)Var(xi)]=\\\\text{Var}(\\\\mathbf{w}^T\\\\mathbf{x}) = \\\\sum_{i=1}^n [ \\\\mathbb{E}[x_i]^2 \\\\text{Var}(w_i) + \\\\mathbb{E}[w_i]^2 \\\\text{Var}(x_i) + \\\\text{Var}(w_i)\\\\text{Var}(x_i)]=\\nVar(wTx)=i=1∑n\\u200b[E[xi\\u200b]2Var(wi\\u200b)+E[wi\\u200b]2Var(xi\\u200b)+Var(wi\\u200b)Var(xi\\u200b)]==(E[xi]2+V(xi))V(wi).=\\\\big(\\\\mathbb{E}[x_i]^2 + \\\\mathbb{V}(x_i) \\\\big)\\\\mathbb{V}(w_i).\\n=(E[xi\\u200b]2+V(xi\\u200b))V(wi\\u200b).В данном случае первый член не может быть проигнорирован, так как ReLU имеет ассиметричную область значений, а значит, распределения xix_ixi\\u200b будут смещёнными.\\nС учетом того, что Var(x)=E[xi2]−E[xi]2\\\\text{Var}(x) = \\\\mathbb{E}[x_i^2] - \\\\mathbb{E}[x_i]^2Var(x)=E[xi2\\u200b]−E[xi\\u200b]2, выражение выше примет итоговый вид:\\nVar(wTx)=Var(wTx)=ninVar(wi)E(xi2).\\\\text{Var}(\\\\mathbf{w}^T\\\\mathbf{x}) = \\\\text{Var}(\\\\mathbf{w}^T\\\\mathbf{x}) = n_\\\\text{in} \\\\text{Var}(w_i) \\\\mathbb{E}(x_i^2).\\nVar(wTx)=Var(wTx)=nin\\u200bVar(wi\\u200b)E(xi2\\u200b).С учётом поведения ReLU и того, что E(zprev)=0\\\\mathbb{E}(z_\\\\text{prev}) = 0E(zprev\\u200b)=0, можно сказать, что\\nE(xi2)=12Var(zprev),\\\\mathbb{E}(x_i^2) = \\\\frac{1}{2}\\\\text{Var}(z_\\\\text{prev}),\\nE(xi2\\u200b)=21\\u200bVar(zprev\\u200b),то есть\\nVar(wTx)=12ninVar(wi)Var(zprev).\\\\text{Var}(\\\\mathbf{w}^T\\\\mathbf{x}) = \\\\frac{1}{2}n_\\\\text{in} \\\\text{Var}(w_i) \\\\text{Var}(z_\\\\text{prev}).\\nVar(wTx)=21\\u200bnin\\u200bVar(wi\\u200b)Var(zprev\\u200b).Получается, что использование ReLU приводит к необходимости инициализировать веса из распределения, чья дисперсия удовлетворяет следующему ограничению:\\n∀i,12ninVar(wi)=1.\\\\forall i, \\\\frac{1}{2}n_\\\\text{in}\\\\text{Var}(w_i) = 1.\\n∀i,21\\u200bnin\\u200bVar(wi\\u200b)=1.Например, подходит нормальное распределение N(0,2nin)\\\\mathbb{N}(0, \\\\frac{2}{n_\\\\text{in}})N(0,nin\\u200b2\\u200b).\\nДанный способ инициализации (и его сравнение с Xavier initialization) проиллюстрирован ниже:\\n\\n\\n\\nИсточник\\n\\n\\nПромежуточные выводы\\nРассмотренные способы инициализации используют достаточно много предположений, но все-таки они работают и позволяют нейронным сетям в некоторых случаях значительно быстрее сходиться.\\nПонимание принципов работы даже таких небольших механизмов – ключ к глубокому освоению области глубокого обучения 😃\\nМетоды оптимизации в нейронных сетях\\nТак как мы договорились, что нейросети представляют собой параметризованные дифференцируемые функции и для каждого параметра мы можем посчитать градиент, то, так же как и линейные модели, их можно настраивать с помощью градиентных методов.\\nВ параграфе про линейные модели мы под этим подразумевали обычно стохастический градиентный спуск на батчах, и это совершенно подходящий способ и для нейросетей тоже. Но существует множество модификаций и эвристик, позволяющих ускорить его сходимость, познакомиться с которыми вы можете в специальном параграфе, посвящённом методам оптимизации.\\nРегуляризация нейронных сетей\\nСмысл термина регуляризация (англ. regularization) гораздо шире привычного вам прибавления L1L_1L1\\u200b- или L2L_2L2\\u200b-нормы вектора весов к функции потерь. Фактически он объединяет большое количество техник для борьбы с переобучением и для получения более подходящего решения с точки зрения эксперта.\\nКаждая из них позволяет навязать модели определённые свойства, пусть даже и ценой некоторого снижения качества предсказания на обучающей выборке. Например, уже знакомая читателю L1L_1L1\\u200b- или L2L_2L2\\u200b-регуляризация в задаче линейной регрессии (регуляризация Тихонова) позволяет исключить наименее значимые признаки (для линейной модели) или же получить устойчивое (хоть и смещённое) решение в случае мультиколлинеарных признаков.\\nВ нейронных сетях техники регуляризации можно разделить на три обширные группы:\\n\\nсвязанные с изменением функции потерь;\\nсвязанные с изменением структуры сети;\\nсвязанные с изменением данных.\\n\\nРассмотрим каждую из них подробнее.\\nРегуляризация через функцию потерь\\nИзменение функции потерь — классический способ получить решение, удовлетворяющее определённым условиям. В глубинном обучении часто используется техника Weight Decay, очень близкая к регуляризации Тихонова. Она представляет собой аналогичный штраф за высокие значения весов нейронной сети с коэффициентом регуляризации λ\\\\lambdaλ:\\nLwith\\xa0regularization=Loriginal+λ∣∣W∣∣2L_\\\\text{with regularization} = L_\\\\text{original} + \\\\lambda ||\\\\mathbf{W}||_2\\nLwith\\xa0regularization\\u200b=Loriginal\\u200b+λ∣∣W∣∣2\\u200bДанная техника регуляризации была совмещена с методом градиентной оптимизации Adam, в результате чего был получен метод AdamW (описанный в параграфе параграфе про методы оптимизации).\\nТакже достаточно часто в качестве регуляризационного члена встречается энтропия распределения, предсказанного нейронной сетью.\\nПредставьте, что вы рекомендуете пользователю товары по истории его взаимодействия с сервисом, семплируя товары для показа в соответствии с распределением предсказанной релевантности. Вам может быть важно, чтобы рекомендации не были фиксированными (менялись при обновлении страницы), ведь это повысит вероятность того, что пользователь найдёт что-то интересное, а вы узнаете о нём что-нибудь новое.\\nВ такой ситуации при обучении модели вы можете потребовать, чтобы распределение предсказаний не сходилось к вырожденному, и в качестве дополнительной штрафной функции может выступать энтропия этого распределения. Энтропия дифференцируема, как и сами предсказанные величины, и может быть использована в качестве регуляризационного члена. Для задачи классификации он будет выглядеть следующим образом:\\np^=f(x;θ),\\\\widehat{\\\\mathbf{p}} = f(x; \\\\mathbf{\\\\theta}),\\np\\u200b=f(x;θ),Lwith\\xa0regularization=Loriginal−λ\\u2005\\u200a∑kp^klog\\u2061p^k,L_\\\\text{with regularization} = L_\\\\text{original} - \\\\lambda \\\\; \\\\sum_k \\\\widehat{p}_k \\\\log \\\\widehat{p}_k,\\nLwith\\xa0regularization\\u200b=Loriginal\\u200b−λk∑\\u200bp\\u200bk\\u200blogp\\u200bk\\u200b,где λ\\\\lambdaλ — коэффициент регуляризации, p^\\\\widehat{\\\\mathbf{p}}p\\u200b — предсказанные вероятности.\\nТем самым эксперт привносит своё знание непосредственно в процесс обучения модели в подходящей математической форме: «предсказания должны быть разнообразными» —> «распределение не должно быть вырожденным» —> «энтропия не должна быть слишком низкой».\\nРегуляризация через ограничение структуры модели\\nВнесение подходящих преобразований в структуру сети также может быть хорошим способом добиться желаемых результатов. Огромное влияние на развитие нейронных сетей оказали техники dropout (2014) и batch normalization (2015), позволившие сделать нейронные сети более устойчивыми к переобучению и многократно ускорить их сходимость соответственно.\\nDropout\\nОбратимся к простым полносвязным (FC/Dense) сетям из нескольких слоёв. Каждый из слоёв порождает новое признаковое описание xkx^{k}xk объекта xinx^{\\\\text{in}}xin, который пришёл на вход:\\nxk=fk(xk−1).x^{k} = f_k(x^{k-1}).\\nxk=fk\\u200b(xk−1).Но как можно гарантировать, что модель будет эффективно использовать все доступные параметры, а не переобучится под использование лишь небольшого их подмножества, поделив для себя внутреннее представление на сигнал и шум?\\nxoverfittedk=[signal,noise]x^{k}_\\\\text{overfitted} = [\\\\text{signal}, \\\\text{noise}]\\nxoverfittedk\\u200b=[signal,noise]Для этого можно было бы случайным образом «выключать» доступ к некоторым координатам внутренних представлений на этапе обучения. Тогда при выключении «полезных» координат произойдёт резкое изменение предсказаний модели, что приведёт к увеличению ошибки, а полученные градиенты этой ошибки укажут, как её исправить с использованием (и изменением) других координат. Сравнение тока информации по исходной модели и по модели с «выключенными» координатами внутренних представлений можно проиллюстрировать с помощью классической картинки:\\n\\nОбратите внимание: «выключать» можно как оригинальные признаки, так и признаки, возникающие на любом другом уровне представления объектов. С точки зрения (k+1)(k+1)(k+1)-го слоя нейронной сети данные приходят откуда-то извне: при k=0k = 0k=0 — из реального мира, а при k>1k > 1k>1 — с предыдущих слоёв.\\nТехнически это осуществляется следующим образом: некоторые координаты внутреннего представления домножаются на ноль. То есть добавляется ещё одно преобразование, которое представляет собой домножение выхода предыдущего слоя на маску из нулей и единиц.\\nxk+1=11−pxk⊙mask,x^{k+1} = \\\\frac1{1-p} x^{k} \\\\odot \\\\text{mask},\\nxk+1=1−p1\\u200bxk⊙mask,maski∼Bernoulli(1−p).\\\\text{mask}_{i} \\\\sim \\\\text{Bernoulli}(1 - p).\\nmaski\\u200b∼Bernoulli(1−p).где (1−p)(1-p)(1−p) (вероятность обнуления координаты) — это гиперпараметр слоя. Отметим, что во многих фреймворках для глубинного обучения в качестве параметра слоя указывается именно вероятность обнуления, а не выживания. Данная маска участвует и при подсчёте градиентов:\\n∇xkL=11−p∇xk+1L⊙mask\\\\nabla_{x^{k}}{\\\\mathcal{L}} = \\\\frac1{1-p}\\\\nabla_{x^{k+1}}{\\\\mathcal{L}} \\\\odot \\\\text{mask}\\n∇xk\\u200bL=1−p1\\u200b∇xk+1\\u200bL⊙maskКак правило, маска генерируется независимо на каждом шаге градиентного спуска. Важно отметить, что на этапе предсказания dropout ничего не меняет, то есть xk+1=xkx^{k+1} = x^kxk+1=xk.\\nМножитель 11−p\\\\frac1{1-p}1−p1\\u200b нужен для того, чтобы распределение xk+1x^{k+1}xk+1 на этапе предсказания совпадало с распределением на этапе обучения. В самом деле, если даже математическое ожидание xkx^kxk было равно нулю, выборочная дисперсия xk⊙maskx^k\\\\odot\\\\text{mask}xk⊙mask ниже, чем у xkx^kxk: ведь часть значений обнулилась.\\nНа этапе предсказания dropout «выключается»: внутренние представления используются как есть, без умножения на маску. А чтобы слой знал, обучается он сейчас или предсказывает, в нейросетевых библиотеках в классе слоя обычно реализовано переключение между этими режимами (например, булев флаг training в pytorch-модулях).\\nПримечаниеПолезно провести аналогию с другим алгоритмом, использующим техники ансамблирования и метод случайных подпространств: речь о случайном лесе (Random Forest). При обучении сети на каждом шаге обучается лишь некоторая подсеть (некоторый подграф вычислений из исходного графа). При переходе в режим inference (то есть применения к реальным данным с целью получения результата, а не обучения) активируются сразу все подсети, и их результаты усредняются. Таким образом, сеть с dropout можно рассматривать как ансамбль из экспоненциально большого числа сетей меньшего размера (подробнее можно прочитать здесь). Это приводит к получению более устойчивой оценки значений целевой переменной.\\nВ этом свойстве кроется и главное коварство dropout (как и большинства других техник регуляризации): благодаря получению более устойчивой оценки целевой переменной путём усреднения множества подсетей, эффективная обобщающая способность итоговой сети снижается! В самом деле, пусть при обучении каждый раз модели была доступна лишь половина параметров. В таком случае итоговая модель представляет собой усреднение множества более слабых моделей, в которых вдвое меньше параметров. Её предсказания будут более устойчивы к шуму, но при этом она неспособна выучить столь сложные зависимости, как сеть аналогичной структуры, но без dropout. То есть за более устойчивые предсказания (и получение менее переобученной модели) приходится расплачиваться и меньшей обобщающей способностью.\\nСтоит отметить, что dropout может применяться и к входным данным (то есть слой dropout может стоять первым в сети), и это может приводить к получению более качественных результатов. Например, если в данных множество мультикоррелирующих признаков или присутствует шум, наличие dropout позволит избежать обусловливания модели на лишь их подмножество и позволит учитывать их все. Так, подобный подход может быть использован, если данные представляют собой сильно разреженные векторы высокой размерности (скажем, сведения об интересе пользователя к тем или иным товарам).\\nBatch normalization\\nПоявление техники batch normalization привело к значительному ускорению обучения нейронных сетей. В данном параграфе мы рассмотрим лишь основные принципы работы batch normalization.\\nДискуссия о свойствах и причинах эффективности batch normalization всё ещё ведётся, рекомендуем обратить внимание на статью с NeurIPS 2018. Нам, впрочем, кажется, что, несмотря на активную критику в его адрес, полезно знать и предложенное авторами подхода объяснение необходимости batch normalization.\\nПредложенная авторами мотивацияОбратимся к механизму обратного распространения ошибки. Пусть мы находимся на этапе обновления параметров WkW^{k}Wk некоторого kkk-го слоя:\\nxk=f(xk−1,Wk),x^k = f(x^{k-1}, W^{k}),\\nxk=f(xk−1,Wk),где fff — некоторая функция, которая вычисляется на данном слое. В общем случае WkW^{k}Wk и xk−1x^{k-1}xk−1 не обязательно взаимодействуют линейным образом; функция fff может быть и нелинейной. В ходе error backward propagation мы вычисляем градиент:\\n∇WkL=g(xk−1,xk,xk+1,…;Wk),\\\\nabla_{W^{k}}\\\\mathcal{L} = g(x^{k-1}, x^k, x^{k+1},\\\\ldots;W^k),\\n∇Wk\\u200bL=g(xk−1,xk,xk+1,…;Wk),где ggg — некоторая функция, в которой будут участвовать представления со слоёв, начиная с (k−1)(k-1)(k−1)-го (вычисленные в ходе forward pass и запомненные).\\nНовое значение параметров примет вид:\\nWnewk=Wk−α∇WkL=Wk−αg(xk−1,xk,…)W^k_{\\\\text{new}} = W^{k} - \\\\alpha \\\\nabla_{W^{k}} \\\\mathcal{L} = W^{k} - \\\\alpha g(x^{k-1},x^k,\\\\ldots)\\nWnewk\\u200b=Wk−α∇Wk\\u200bL=Wk−αg(xk−1,xk,…)После обновления параметров WkW^{k}Wk мы перейдём к обновлению параметров предыдущего слоя Wk−1W^{k-1}Wk−1 и обновим их аналогичным образом.\\nВажно. Это приведёт к изменению представления, которое пришло на вход kkk-му слою, которое мы не учитываем:\\nxnewk=f(xk−1,Wnewk)=f(xk−1,Wk+ϕ),x^k_{\\\\text{new}} = f(x^{k-1}, W^{k}_{\\\\text{new}}) = f(x^{k-1}, W^{k} + \\\\phi),\\nxnewk\\u200b=f(xk−1,Wnewk\\u200b)=f(xk−1,Wk+ϕ),где ϕ\\\\phiϕ — разница между предыдущими и новыми параметрами WkW^{k}Wk.\\nТо есть параметры (k−1)(k-1)(k−1)-го слоя будут обновлены исходя из предположения, что данные приходят из некоторого распределения на xkx^kxk, которое параметризовалось Wk−1W^{k-1}Wk−1, но теперь параметры изменились и данные могут обладать иными свойствами. Например, может существенно измениться среднее или дисперсия, что может привести, например, к попаданию на «хвосты» функции активации и затуханию градиента.\\nДо появления batch normalization с этой проблемой боролись достаточно просто: использовали небольшие значения шага обучения (learning rate) α\\\\alphaα. Благодаря этому изменения были не слишком большими и можно было предположить, что и распределение внутренних представлений поменялось незначительно.\\nИспользование batch normalization гарантирует, что каждая компонента представления на выходе будет иметь контролируемое среднее и дисперсию. Достигается это следующим образом:\\n\\nСперва идёт собственно слой batch normalization, на котором текущий батч приводится к нулевому среднему и единичной дисперсии:\\n\\nXk+1=Xk−μσ2+ε,X^{k+1} = \\\\frac{X^k - \\\\mu}{\\\\sqrt{\\\\sigma}^2 + \\\\varepsilon},\\nXk+1=σ\\u200b2+εXk−μ\\u200b,где μ\\\\muμ и σ2\\\\sigma^2σ2 — среднее и дисперсия признаков по обрабатываемому батчу, а ε\\\\varepsilonε — гиперпараметр слоя, небольшое положительное число, добавляемое для улучшения численной устойчивости. Отметим, что μ\\\\muμ и σ\\\\sigmaσ, будучи функциями от XkX^kXk, тоже участвуют в вычислении градиентов. В ходе предсказания (или, как ещё говорят, инференса, от английского inference) используются фиксированные значения μ∗\\\\mu_{\\\\ast}μ∗\\u200b и σ∗2\\\\sigma_{\\\\ast}^2σ∗2\\u200b, которые были получены в ходе обучения как скользящее среднее всех μ\\\\muμ и σ2\\\\sigma^2σ2. Более точно: на каждой итерации forward pass мы вычисляем\\nμ∗=μ∗λ+μ(1−λ) \\\\mu_{\\\\ast} = \\\\mu_{\\\\ast} \\\\lambda + \\\\mu (1 - \\\\lambda)\\nμ∗\\u200b=μ∗\\u200bλ+μ(1−λ)σ∗2=σ∗2λ+σ2(1−λ),\\\\sigma^2_{\\\\ast} = \\\\sigma^2_{\\\\ast} \\\\lambda + \\\\sigma^2 (1 - \\\\lambda),\\nσ∗2\\u200b=σ∗2\\u200bλ+σ2(1−λ),где λ\\\\lambdaλ также является гиперпараметром слоя.\\n\\nДалее идёт слой channelwise scaling, который позволяет выучить оптимальное шкалирование для всех признаков Xk+2X^{k+2}Xk+2:\\n\\nXk+2=βXk+1+γ,X^{k+2} = \\\\beta X^{k+1} + \\\\gamma,\\nXk+2=βXk+1+γ,где β\\\\betaβ и γ\\\\gammaγ — обучаемые параметры, позволяющие настраивать в ходе обучения оптимальные значения матожидания и дисперсии выходного слоя Xk+2X^{k+2}Xk+2.\\nНиже приведён алгоритм из оригинальноq статьи  2015 года за авторством Сергея Иоффе и Кристиана Сегеди:\\n\\nПричина популярности batch normalization заключается в значительном ускорении обучения нейронных сетей и в улучшении их сходимости в целом. Рассмотрим график из оригинальной статьи:\\n\\nКак видно на иллюстрации выше, использование batch normalization позволило ускорить обучение в несколько раз и даже добиться лучших результатов, чем SotA-подход 2014 года Inception (структура которого была приведена на одной из иллюстраций в начале этого параграфа).\\nЗначительное ускорение достигается в том числе благодаря использованию более высокого learning rate: благодаря нормировке связь между слоями не нарушается столь сильно.\\nСтоит заметить, что причины столь эффективной работы batch normalization до сих пор являются поводом для дискуссий и строгого теоретического объяснения эффекта от batch normalization ещё нет. Несмотря на это, он перевернул область глубинного обучения и вошёл в стандартный инструментарий при обучении нейронных сетей.\\nПримечание: стоит заметить, что в настоящее время существуют и другие способы нормировать промежуточные представления: instance normalization, layer normalization и так далее.\\nВ завершение рекомендуем ознакомиться со статьёй  о работе метода обратного распространения ошибки в слое batch normalization.\\nРегуляризация через изменение данных\\nВнесение изменений в данные (аугментация данных) также является популярной техникой регуляризации. Рассмотрим её на примере.\\nПусть перед нами фотография самолёта. Добавим мелкодисперсный шум к изображению. Мы всё ещё сможем увидеть на фотографии самолёт, но, с точки зрения модели машинного обучения (в данном случае — нейронной сети), полученное изображение является новым объектом!\\nПовернём изображение самолёта на 10 градусов по часовой стрелке. В нашем распоряжении ещё одно изображение с известной целевой меткой (например, меткой класса «самолёт»), в котором присутствует поворот. Таким образом, внесение новых данных позволяет дать модели понять, какие преобразования над данными являются допустимыми, и она уже будет более устойчивой к наличию небольшого шума в данных или к поворотам (к которым чувствительна операция свёртки).\\nВдобавок аугментации позволяют значительно увеличить объём обучающей выборки. Особую популярность аугментации приобрели в области компьютерного зрения. В качестве примера приведём отличную библиотеку, позволяющую производить аугментацию изображений.\\nСтоит обратить внимание, что используемые аугментации должны быть адекватны решаемой задаче. Инвертирование цветов на фотографии, внесение значительного количества шумов или переворот изображения могут привести и к негативным результатам (по сути, просто сделают выборку более зашумлённой или даже заставят сеть учиться на данных, которые она никогда не встретит в реальности), так как обобщающая способность сети ограниченна. Можно сказать, что аугментированные данные должны принадлежать к той же генеральной совокупности, что и оригинальный датасет.\\nИтак, эксперт может привнести своё понимание задачи и на уровне аугментации данных: если данное преобразование является допустимым (то есть преобразованный объект мог бы попасть в обучающую выборку и самостоятельно — как фотография с другого устройства или запись речи другого человека с опечаткой), то модель должна быть устойчива к данным с подобными преобразованиями.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф5.3. Метод обратного распространения ошибкиКак эффективно посчитать градиенты по\\xa0весам нейронной сетиСледующий параграф6.1. Свёрточные нейросетиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\"),\n",
       " Document(metadata={'source': 'data/page_5.html', 'title': 'Линейные модели'}, page_content='Линейные моделиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/52.1.Линейные моделиПочему модели линейные?Линейная регрессия и метод наименьших квадратов (МНК)РегуляризацияДругие лоссыЛинейная классификацияМногоклассовая классификацияМасштабируемость линейных моделейПодытожим2.2.Метрические методы2.3.Решающие деревья2.4.Ансамбли в машинном обучении2.5.Градиентный бустинг3.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Линейные модели2.1. Линейные моделиАвторыСиницин ФилиппСоколов ЕвгенийЛинейные модели от\\xa0линейной до\\xa0логистической регрессии. Регуляризация, работа с\\xa0категориальными признаками, многоклассовая классификацияМы начнем с самых простых и понятных моделей машинного обучения: линейных. В этом параграфе мы разберёмся, что это такое, почему они работают и в каких случаях их стоит использовать. Так как это первый класс моделей, с которым вы столкнётесь, мы постараемся подробно проговорить все важные моменты. Заодно объясним, как работает машинное обучение, на сравнительно простых примерах.\\nПочему модели линейные?\\nПредставьте, что у вас есть множество объектов X\\\\mathbb{X}X, а вы хотели бы каждому объекту сопоставить какое-то значение. К примеру, у вас есть набор операций по банковской карте, а вы бы хотели, понять, какие из этих операций сделали мошенники. Если вы разделите все операции на два класса и нулём обозначите законные действия, а единицей мошеннические, то у вас получится простейшая задача классификации. Представьте другую ситуацию: у вас есть данные геологоразведки, по которым вы хотели бы оценить перспективы разных месторождений. В данном случае по набору геологических данных ваша модель будет, к примеру, оценивать потенциальную годовую доходность шахты. Это пример задачи регрессии. Числа, которым мы хотим сопоставить объекты из нашего множества иногда называют таргетами (от английского target).\\nТаким образом, задачи классификации и регрессии можно сформулировать как поиск отображения из множества объектов X\\\\mathbb{X}X в множество возможных таргетов.\\nМатематически задачи можно описать так:\\n\\nклассификация: X→{0,1,…,K}\\\\mathbb{X}  \\\\to \\\\{0,1,\\\\ldots,K\\\\}X→{0,1,…,K}, где 0,…,K0, \\\\ldots, K0,…,K – номера классов,\\nрегрессия: X→R\\\\mathbb{X} \\\\to \\\\mathbb{R}X→R.\\n\\nОчевидно, что просто сопоставить какие-то объекты каким-то числам — дело довольно бессмысленное. Мы же хотим быстро обнаруживать мошенников или принимать решение, где строить шахту. Значит нам нужен какой-то критерий качества. Мы бы хотели найти такое отображение, которое лучше всего приближает истинное соответствие между объектами и таргетами. Что значит «лучше всего» – вопрос сложный. Мы к нему будем много раз возвращаться. Однако, есть более простой вопрос: среди каких отображений мы будем искать самое лучшее? Возможных отображений может быть много, но мы можем упростить себе задачу и договориться, что хотим искать решение только в каком-то заранее заданном параметризированном семействе функций. Весь этот параграф будет посвящен самому простому такому семейству — линейным функциям видаВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\ny=w1x1+…+wDxD+w0,y = w_1 x_1 + \\\\ldots + w_D x_D + w_0,\\ny=w1\\u200bx1\\u200b+…+wD\\u200bxD\\u200b+w0\\u200b,где yyy – целевая переменная (таргет), (x1,…,xD)(x_1, \\\\ldots, x_D)(x1\\u200b,…,xD\\u200b) – вектор, соответствующий объекту выборки (вектор признаков), а w1,…,wD,w0w_1, \\\\ldots, w_D, w_0w1\\u200b,…,wD\\u200b,w0\\u200b – параметры модели. Признаки ещё называют фичами (от английского features). Вектор w=(w1,…,wD)w = (w_1,\\\\ldots,w_D)w=(w1\\u200b,…,wD\\u200b) часто называют вектором весов, так как на предсказание модели можно смотреть как на взвешенную сумму признаков объекта, а число w0w_0w0\\u200b – свободным коэффициентом, или сдвигом (bias). Более компактно линейную модель можно записать в виде\\ny=⟨x,w⟩+w0y = \\\\langle x, w\\\\rangle + w_0\\ny=⟨x,w⟩+w0\\u200bТеперь, когда мы выбрали семейство функций, в котором будем искать решение, задача стала существенно проще. Мы теперь ищем не какое-то абстрактное отображение, а конкретный вектор (w0,w1,…,wD)∈RD+1(w_0,w_1,\\\\ldots,w_D)\\\\in\\\\mathbb{R}^{D+1}(w0\\u200b,w1\\u200b,…,wD\\u200b)∈RD+1.\\nЗамечание. Чтобы применять линейную модель, нужно, чтобы каждый объект уже был представлен вектором численных признаков x1,…,xDx_1,\\\\ldots,x_Dx1\\u200b,…,xD\\u200b. Конечно, просто текст или граф в линейную модель не положить, придётся сначала придумать для него численные фичи. Модель называют линейной, если она является линейной по этим численным признакам.\\nРазберёмся, как будет работать такая модель в случае, если D=1D = 1D=1. То есть у наших объектов есть ровно один численный признак, по которому они отличаются. Теперь наша линейная модель будет выглядеть совсем просто: y=w1x1+w0y = w_1 x_1 + w_0y=w1\\u200bx1\\u200b+w0\\u200b. Для задачи регрессии мы теперь пытаемся приблизить значение игрек какой-то линейной функцией от переменной икс. А что будет значить линейность для задачи классификации? Давайте вспомним про пример с поиском мошеннических транзакций по картам. Допустим, нам известна ровно одна численная переменная — объём транзакции. Для бинарной классификации транзакций на законные и потенциально мошеннические мы будем искать так называемое разделяющее правило: там, где значение функции положительно, мы будем предсказывать один класс, где отрицательно – другой. В нашем примере простейшим правилом будет какое-то пороговое значение объёма транзакций, после которого есть смысл пометить транзакцию как подозрительную.\\n\\nВ случае более высоких размерностей вместо прямой будет гиперплоскость с аналогичным смыслом.\\nВопрос на подумать. Если вы посмотрите содержание учебника, то не найдёте в нём ни «полиномиальных» моделей, ни каких-нибудь «логарифмических», хотя, казалось бы, зависимости бывают довольно сложными. Почему так?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Линейные зависимости не так просты, как кажется. Пусть мы решаем задачу регрессии. Если мы подозреваем, что целевая переменная yyy не выражается через x1,x2x_1, x_2x1\\u200b,x2\\u200b как линейная функция, а зависит ещё от логарифма x1x_1x1\\u200b и ещё как-нибудь от того, разные ли знаки у признаков, то мы можем ввести дополнительные слагаемые в нашу линейную зависимость, просто объявим эти слагаемые новыми переменными и добавив перед ними соответствующие регрессионные коэффициенты\\ny≈w1x1+w2x2+w3log\\u2061x1+w4sgn(x1x2)+w0,y \\\\approx w_1 x_1 + w_2 x_2 + w_3\\\\log{x_1} + w_4\\\\text{sgn}(x_1x_2) + w_0,\\ny≈w1\\u200bx1\\u200b+w2\\u200bx2\\u200b+w3\\u200blogx1\\u200b+w4\\u200bsgn(x1\\u200bx2\\u200b)+w0\\u200b,и в итоге из двумерной нелинейной задачи мы получили четырёхмерную линейную регрессию.\\nВопрос на подумать. А как быть, если одна из фичей является категориальной, то есть принимает значения из (обычно конечного числа) значений, не являющихся числами? Например, это может быть время года, уровень образования, марка машины и так далее. Как правило, с такими значениями невозможно производить арифметические операции или же результаты их применения не имеют смысла.\\nОтвет (не открывайте сразу; сначала подумайте сами!)В линейную модель можно подать только численные признаки, так что категориальную фичу придётся как-то закодировать. Рассмотрим для примера вот такой датасет\\n\\nЗдесь два категориальных признака – pet_type и color. Первый принимает четыре различных значения, второй – пять.\\nСамый простой способ – использовать one-hot кодирование (one-hot encoding). Пусть исходный признак мог принимать MMM значений c1,…,cMc_1,\\\\ldots, c_Mc1\\u200b,…,cM\\u200b. Давайте заменим категориальный признак на MMM признаков, которые принимают значения 000 и 111: iii-й будет отвечать на вопрос «принимает ли признак значение cic_ici\\u200b?». Иными словами, вместо ячейки со значением cic_ici\\u200b у объекта появляется строка нулей и единиц, в которой единица стоит только на iii-м месте.\\nВ нашем примере получится вот такая табличка:\\n\\nМожно было бы на этом остановиться, но добавленные признаки обладают одним неприятным свойством: в каждом из них ровно одна единица, так что сумма соответствующих столбцов равна столбцу из единиц. А это уже плохо. Представьте, что у нас есть линейная модель\\ny∼w1x1+…+wD−1xd−1+wc1xc1+…+wcMxcM+w0y \\\\sim w_1x_1 + \\\\ldots + w_{D-1}x_{d-1} + w_{c_1}x_{c_1} + \\\\ldots + w_{c_M}x_{c_M} + w_0\\ny∼w1\\u200bx1\\u200b+…+wD−1\\u200bxd−1\\u200b+wc1\\u200b\\u200bxc1\\u200b\\u200b+…+wcM\\u200b\\u200bxcM\\u200b\\u200b+w0\\u200bПреобразуем немного правую часть:\\ny∼w1x1+…+wD−1xd−1+(wc1−wcM)⏟=:wc1′xc1+…+(wcM−1−wcM)⏟=:wCM−1′xcM−1+wcM(xc1+…+xcM)⏟=1+w0=y\\\\sim w_1x_1 + \\\\ldots + w_{D-1}x_{d-1} + \\\\underbrace{(w_{c_1} - w_{c_M})}_{=:w\\'_{c_1}}x_{c_1} + \\\\ldots + \\\\underbrace{(w_{c_{M-1}} - w_{c_M})}_{=:w\\'_{C_{M-1}}}x_{c_{M-1}} + w_{c_M}\\\\underbrace{(x_{c_1} + \\\\ldots + x_{c_M})}_{=1} + w_0 =\\ny∼w1\\u200bx1\\u200b+…+wD−1\\u200bxd−1\\u200b+=:wc1\\u200b′\\u200b(wc1\\u200b\\u200b−wcM\\u200b\\u200b)\\u200b\\u200bxc1\\u200b\\u200b+…+=:wCM−1\\u200b′\\u200b(wcM−1\\u200b\\u200b−wcM\\u200b\\u200b)\\u200b\\u200bxcM−1\\u200b\\u200b+wcM\\u200b\\u200b=1(xc1\\u200b\\u200b+…+xcM\\u200b\\u200b)\\u200b\\u200b+w0\\u200b==w1x1+…+wD−1xd−1+wc1′xc1+…+wcM−1′xcM−1+(wcM+w0)⏟=w0′= w_1x_1 + \\\\ldots + w_{D-1}x_{d-1} + w\\'_{c_1}x_{c_1} + \\\\ldots + w\\'_{c_{M-1}}x_{c_{M-1}} + \\\\underbrace{(w_{c_M} + w_0)}_{=w\\'_{0}}\\n=w1\\u200bx1\\u200b+…+wD−1\\u200bxd−1\\u200b+wc1\\u200b′\\u200bxc1\\u200b\\u200b+…+wcM−1\\u200b′\\u200bxcM−1\\u200b\\u200b+=w0′\\u200b(wcM\\u200b\\u200b+w0\\u200b)\\u200b\\u200bКак видим, от одного из новых признаков можно избавиться, не меняя модель. Более того, это стоит сделать, потому что наличие «лишних» признаков ведёт к переобучению или вовсе ломает модель – подробнее об этом мы поговорим в разделе про регуляризацию. Поэтому при использовании one-hot-encoding обычно выкидывают признак, соответствующий одному из значений. Например, в нашем примере итоговая матрица объекты-признаки будет иметь вид:\\n\\nКонечно, one-hot кодирование – это самый наивный способ работы с категориальными признаками, и для более сложных фичей или фичей с большим количеством значений оно плохо подходит. С рядом более продвинутых техник вы познакомитесь в разделе про обучение представлений.\\nПомимо простоты, у линейных моделей есть несколько других достоинств. К примеру, мы можем достаточно легко судить, как влияют на результат те или иные признаки. Скажем, если вес wiw_iwi\\u200b положителен, то с ростом iii-го признака таргет в случае регрессии будет увеличиваться, а в случае классификации наш выбор будет сдвигаться в пользу одного из классов. Значение весов тоже имеет прозрачную интерпретацию: чем вес wiw_iwi\\u200b больше, тем «важнее» iii-й признак для итогового предсказания. То есть, если вы построили линейную модель, вы неплохо можете объяснить заказчику те или иные её результаты. Это качество моделей называют интерпретируемостью. Оно особенно ценится в индустриальных задачах, цена ошибки в которых высока. Если от работы вашей модели может зависеть жизнь человека, то очень важно понимать, как модель принимает те или иные решения и какими принципами руководствуется. При этом не все методы машинного обучения хорошо интерпретируемы, к примеру, поведение искусственных нейронных сетей или градиентного бустинга интерпретировать довольно сложно.\\nВ то же время слепо доверять весам линейных моделей тоже не стоит по целому ряду причин:\\n\\nЛинейные модели всё-таки довольно узкий класс функций, они неплохо работают для небольших датасетов и простых задач. Однако, если вы решаете линейной моделью более сложную задачу, то вам, скорее всего, придётся выдумывать дополнительные признаки, являющиеся сложными функциями от исходных. Поиск таких дополнительных признаков называется feature engineering, технически он устроен примерно так, как мы описали в вопросе про \"полиномиальные модели\". Вот только поиском таких искусственных фичей можно сильно увлечься, так что осмысленность интерпретации будет сильно зависеть от здравого смысла эксперта, строившего модель.\\nЕсли между признаками есть приближённая линейная зависимость, коэффициенты в линейной модели могут совершенно потерять физический смысл (об этой проблеме и о том, как с ней бороться, мы поговорим дальше, когда будем обсуждать регуляризацию).\\nОсобенно осторожно стоит верить в утверждения вида «этот коэффициент маленький, значит, этот признак не важен». Во-первых, всё зависит от масштаба признака: вдруг коэффициент мал, чтобы скомпенсировать его. Во-вторых, зависимость действительно может быть слабой, но кто знает, в какой ситуации она окажется важна. Такие решения принимаются на основе данных, например, путём проверки статистического критерия (об этом мы коротко упомянем в разделе про вероятностные модели).\\nКонкретные значения весов могут меняться в зависимости от обучающей выборки, хотя с ростом её размера они будут потихоньку сходиться к весам «наилучшей» линейной модели, которую можно было бы построить по всем-всем-всем данным на свете.\\n\\nОбсудив немного общие свойства линейных моделей, перейдём к тому, как их всё-таки обучать. Сначала разберёмся с регрессией, а затем настанет черёд классификации.\\nЛинейная регрессия и метод наименьших квадратов (МНК)\\nМы начнём с использования линейных моделей для решения задачи регрессии. Простейшим примером постановки задачи линейной регрессии является метод наименьших квадратов (Ordinary least squares).\\nПусть у нас задан датасет (X,y)(X, y)(X,y), где y=(yi)i=1N∈RNy=(y_i)_{i=1}^N \\\\in \\\\mathbb{R}^Ny=(yi\\u200b)i=1N\\u200b∈RN – вектор значений целевой переменной, а X=(xi)i=1N∈RN×D,xi∈RDX=(x_i)_{i = 1}^N \\\\in \\\\mathbb{R}^{N \\\\times D}, x_i \\\\in \\\\mathbb{R}^DX=(xi\\u200b)i=1N\\u200b∈RN×D,xi\\u200b∈RD – матрица объекты-признаки, в которой iii-я строка – это вектор признаков iii-го объекта выборки. Мы хотим моделировать зависимость yiy_iyi\\u200b от xix_ixi\\u200b как линейную функцию со свободным членом. Общий вид такой функции из RD\\\\mathbb{R}^DRD в R\\\\mathbb{R}R выглядит следующим образом:\\nfw(xi)=⟨w,xi⟩+w0\\\\color{#348FEA}{f_w(x_i) = \\\\langle w, x_i \\\\rangle + w_0}\\nfw\\u200b(xi\\u200b)=⟨w,xi\\u200b⟩+w0\\u200bСвободный член w0w_0w0\\u200b часто опускают, потому что такого же результата можно добиться, добавив ко всем xix_ixi\\u200b признак, тождественно равный единице; тогда роль свободного члена будет играть соответствующий ему вес:\\n(xi1…xiD)⋅(w1⋮wD)+w0=(1xi1…xiD)⋅(w0w1⋮wD)\\\\begin{pmatrix}x_{i1} & \\\\ldots & x_{iD} \\\\end{pmatrix}\\\\cdot\\\\begin{pmatrix}w_1\\\\\\\\ \\\\vdots \\\\\\\\ w_D\\\\end{pmatrix} + w_0 =\\n\\\\begin{pmatrix}1 &  x_{i1} & \\\\ldots & x_{iD} \\\\end{pmatrix}\\\\cdot\\\\begin{pmatrix}w_0 \\\\\\\\ w_1\\\\\\\\ \\\\vdots \\\\\\\\ w_D \\\\end{pmatrix}\\n(xi1\\u200b\\u200b…\\u200bxiD\\u200b\\u200b)⋅\\u200bw1\\u200b⋮wD\\u200b\\u200b\\u200b+w0\\u200b=(1\\u200bxi1\\u200b\\u200b…\\u200bxiD\\u200b\\u200b)⋅\\u200bw0\\u200bw1\\u200b⋮wD\\u200b\\u200b\\u200bПоскольку это сильно упрощает запись, в дальнейшем мы будем считать, что это уже сделано и зависимость имеет вид просто fw(xi)=⟨w,xi⟩f_w(x_i) = \\\\langle w, x_i \\\\ranglefw\\u200b(xi\\u200b)=⟨w,xi\\u200b⟩.\\nСведение к задаче оптимизации\\nМы хотим, чтобы на нашем датасете (то есть на парах (xi,yi)(x_i, y_i)(xi\\u200b,yi\\u200b) из обучающей выборки) функция fwf_wfw\\u200b как можно лучше приближала нашу зависимость.\\n\\nДля того, чтобы чётко сформулировать задачу, нам осталось только одно: на математическом языке выразить желание «приблизить fw(x)f_w(x)fw\\u200b(x) к yyy». Говоря простым языком, мы должны научиться измерять качество модели и минимизировать её ошибку, как-то меняя обучаемые параметры. В нашем примере обучаемые параметры — это веса www. Функция, оценивающая то, как часто модель ошибается, традиционно называется функцией потерь, функционалом качества или просто лоссом (loss function). Важно, чтобы её было легко оптимизировать: скажем, гладкая функция потерь – это хорошо, а кусочно постоянная – просто ужасно.\\nФункции потерь бывают разными. От их выбора зависит то, насколько задачу в дальнейшем легко решать, и то, в каком смысле у нас получится приблизить предсказание модели к целевым значениям. Интуитивно понятно, что для нашей текущей задачи нам нужно взять вектор yyy и вектор предсказаний модели и как-то сравнить, насколько они похожи. Так как эти вектора «живут» в одном векторном пространстве, расстояние между ними вполне может быть функцией потерь. Более того, положительная непрерывная функция от этого расстояния тоже подойдёт в качестве функции потерь. При этом способов задать расстояние между векторами тоже довольно много. От всего этого разнообразия глаза разбегаются, но мы обязательно поговорим про это позже. Сейчас давайте в качестве лосса возьмём квадрат L2L^2L2-нормы вектора разницы предсказаний модели и yyy. Во-первых, как мы увидим дальше, так задачу будет нетрудно решить, а во-вторых, у этого лосса есть ещё несколько дополнительных свойств:\\n\\n\\nL2L^2L2-норма разницы – это евклидово расстояние ∥y−fw(x)∥2\\\\|y - f_w(x)\\\\|_2∥y−fw\\u200b(x)∥2\\u200b между вектором таргетов и вектором ответов модели, то есть мы их приближаем в смысле самого простого и понятного «расстояния».\\n\\n\\nКак мы увидим в разделе про вероятностные модели, с точки зрения статистики это соответствует гипотезе о том, что наши данные состоят из линейного «сигнала» и нормально распределенного «шума».\\n\\n\\nТак вот, наша функция потерь выглядит так:\\nL(f,X,y)=∥y−f(X)∥22=L(f, X, y) = \\\\|y - f(X)\\\\|_2^2 =\\nL(f,X,y)=∥y−f(X)∥22\\u200b==∥y−Xw∥22=∑i=1N(yi−⟨xi,w⟩)2= \\\\|y - Xw\\\\|_2^2 = \\\\sum_{i=1}^N(y_i - \\\\langle x_i, w \\\\rangle)^2\\n=∥y−Xw∥22\\u200b=i=1∑N\\u200b(yi\\u200b−⟨xi\\u200b,w⟩)2Такой функционал ошибки не очень хорош для сравнения поведения моделей на выборках разного размера. Представьте, что вы хотите понять, насколько качество модели на тестовой выборке из 250025002500 объектов хуже, чем на обучающей из 500050005000 объектов. Вы измерили L2L^2L2-норму ошибки и получили в одном случае 300300300, а в другом 500500500. Эти числа не очень интерпретируемы. Гораздо лучше посмотреть на среднеквадратичное отклонение\\nL(f,X,y)=1N∑i=1N(yi−⟨xi,w⟩)2L(f, X, y) = \\\\frac1N\\\\sum_{i=1}^N(y_i - \\\\langle x_i, w \\\\rangle)^2\\nL(f,X,y)=N1\\u200bi=1∑N\\u200b(yi\\u200b−⟨xi\\u200b,w⟩)2По этой метрике на тестовой выборке получаем 0,120,120,12, а на обучающей 0,10,10,1.\\nФункция потерь 1N∑i=1N(yi−⟨xi,w⟩)2\\\\frac1N\\\\sum_{i=1}^N(y_i - \\\\langle x_i, w \\\\rangle)^2N1\\u200b∑i=1N\\u200b(yi\\u200b−⟨xi\\u200b,w⟩)2 называется Mean Squared Error, MSE или среднеквадратическим отклонением. Разница с L2L^2L2-нормой чисто косметическая, на алгоритм решения задачи она не влияет:\\nMSE(f,X,y)=1N∥y−Xw∥22\\\\color{#348FEA}{\\\\text{MSE}(f, X, y) =  \\\\frac{1}{N}\\\\|y - X w\\\\|_2^2}\\nMSE(f,X,y)=N1\\u200b∥y−Xw∥22\\u200bВ самом широком смысле, функции работают с объектами множеств: берут какой-то входящий объект из одного множества и выдают на выходе соответствующий ему объект из другого. Если мы имеем дело с отображением, которое на вход принимает функции, а на выходе выдаёт число, то такое отображение называют функционалом. Если вы посмотрите на нашу функцию потерь, то увидите, что это именно функционал. Для каждой конкретной линейной функции, которую задают веса wiw_iwi\\u200b, мы получаем число, которое оценивает, насколько точно эта функция приближает наши значения yyy. Чем меньше это число, тем точнее наше решение, значит для того, чтобы найти лучшую модель, этот функционал нам надо минимизировать по www:\\n∥y−Xw∥22⟶min\\u2061w\\\\color{#348FEA}{\\\\|y - Xw\\\\|_2^2 \\\\longrightarrow \\\\min_w}\\n∥y−Xw∥22\\u200b⟶wmin\\u200bЭту задачу можно решать разными способами. В этом параграфе мы сначала решим эту задачу аналитически, а потом приближенно. Сравнение двух этих решений позволит нам проиллюстрировать преимущества того подхода, которому посвящена эта книга. На наш взгляд, это самый простой способ \"на пальцах\" показать суть машинного обучения.\\nМНК: точный аналитический метод\\nТочку минимума можно найти разными способами. Если вам интересно аналитическое решение, вы можете найти его в параграфе про матричные дифференцирования (раздел «Примеры вычисления производных сложных функций»). Здесь же мы воспользуемся геометрическим подходом.\\nПусть x(1),…,x(D)x^{(1)},\\\\ldots,x^{(D)}x(1),…,x(D) – столбцы матрицы XXX, то есть столбцы признаков. Тогда\\nXw=w1x(1)+…+wDx(D),Xw = w_1x^{(1)}+\\\\ldots+w_Dx^{(D)},\\nXw=w1\\u200bx(1)+…+wD\\u200bx(D),и задачу регрессии можно сформулировать следующим образом: найти линейную комбинацию столбцов x(1),…,x(D)x^{(1)},\\\\ldots,x^{(D)}x(1),…,x(D), которая наилучшим способом приближает столбец yyy по евклидовой норме – то есть найти проекцию вектора yyy на подпространство, образованное векторами x(1),…,x(D)x^{(1)},\\\\ldots,x^{(D)}x(1),…,x(D).\\nРазложим y=y∥+y⊥y = y_{\\\\parallel} + y_{\\\\perp}y=y∥\\u200b+y⊥\\u200b, где y∥=Xwy_{\\\\parallel} = Xwy∥\\u200b=Xw – та самая проекция, а y⊥y_{\\\\perp}y⊥\\u200b – ортогональная составляющая, то есть y⊥=y−Xw⊥x(1),…,x(D)y_{\\\\perp} = y - Xw\\\\perp x^{(1)},\\\\ldots,x^{(D)}y⊥\\u200b=y−Xw⊥x(1),…,x(D). Как это можно выразить в матричном виде? Оказывается, очень просто:\\nXT(y−Xw)=0X^T(y - Xw) = 0\\nXT(y−Xw)=0В самом деле, каждый элемент столбца XT(y−Xw)X^T(y - Xw)XT(y−Xw) – это скалярное произведение строки XTX^TXT (=столбца XXX = одного из x(i)x^{(i)}x(i)) на y−Xwy - Xwy−Xw. Из уравнения XT(y−Xw)=0X^T(y - Xw) = 0XT(y−Xw)=0 уже очень легко выразить www:\\nw=(XTX)−1XTyw = (X^TX)^{-1}X^Ty\\nw=(XTX)−1XTyВопрос на подумать Для вычисления w∗w_{\\\\ast}w∗\\u200b нам приходится обращать (квадратную) матрицу XTXX^TXXTX, что возможно, только если она невырождена. Что это значит с точки зрения анализа данных? Почему мы верим, что это выполняется во всех разумных ситуациях?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Как известно из линейной алгебры, для вещественной матрицы XXX ранги матриц XXX и XTXX^TXXTX совпадают. Матрица XTXX^TXXTX невырождена тогда и только тогда, когда её ранг равен числу её столбцов, что равно числу столбцов матрицы XXX. Иными словами, формула регрессии поломается, только если столбцы матрицы XXX линейно зависимы. Столбцы матрицы XXX – это признаки. А если наши признаки линейно зависимы, то, наверное, что-то идёт не так и мы должны выкинуть часть из них, чтобы остались только линейно независимые.\\nДругое дело, что зачастую признаки могут быть приближённо линейно зависимы, особенно если их много. Тогда матрица XTXX^TXXTX будет близка к вырожденной, и это, как мы дальше увидим, будет вести к разным, в том числе вычислительным проблемам.\\nВычислительная сложность аналитического решения — O(D2N+D3)O(D^2N + D^3)O(D2N+D3), где NNN — длина выборки, DDD — число признаков у одного объекта. Слагаемое ND2ND^2ND2 отвечает за сложность перемножения матриц XTX^TXT и XXX, а слагаемое D3D^3D3 — за сложность обращения их произведения. Перемножать матрицы (XTX)−1(X^TX)^{-1}(XTX)−1 и XTX^TXT не стоит. Гораздо лучше сначала умножить yyy на XTX^TXT, а затем полученный вектор на (XTX)−1(X^TX)^{-1}(XTX)−1: так будет быстрее и, кроме того, не нужно будет хранить матрицу (XTX)−1XT(X^TX)^{-1}X^T(XTX)−1XT.\\nВычисление можно ускорить, используя продвинутые алгоритмы перемножения матриц или итерационные методы поиска обратной матрицы.\\nПроблемы «точного» решения\\nЗаметим, что для получения ответа нам нужно обратить матрицу XTXX^TXXTX. Это создает множество проблем:\\n\\nОсновная проблема в обращении матрицы — это то, что вычислительно обращать большие матрицы дело сложное, а мы бы хотели работать с датасетами, в которых у нас могут быть миллионы точек,\\nМатрица XTXX^TXXTX, хотя почти всегда обратима в разумных задачах машинного обучения, зачастую плохо обусловлена. Особенно если признаков много, между ними может появляться приближённая линейная зависимость, которую мы можем упустить на этапе формулировки задачи. В подобных случаях погрешность нахождения www будет зависеть от квадрата числа обусловленности матрицы XXX, что очень плохо. Это делает полученное таким образом решение численно неустойчивым: малые возмущения yyy могут приводить к катастрофическим изменениям www.\\n\\nПара слов про число обусловленности.Пожертвовав математической строгостью, мы можем считать, что число обусловленности матрицы XXX – это корень из отношения наибольшего и наименьшего из собственных чисел матрицы XTXX^TXXTX. Грубо говоря, оно показывает, насколько разного масштаба бывают собственные значения XTXX^TXXTX. Если рассмотреть L2L^2L2-норму ошибки предсказания, как функцию от www, то её линии уровня будут эллипсоидами, форма которых определяется квадратичной формой с матрицей XTXX^TXXTX (проверьте это!). Таким образом, число обусловленности говорит о том, насколько вытянутыми являются эти эллипсоиды.\\nПодробнееДанные проблемы не являются поводом выбросить решение на помойку. Существует как минимум два способа улучшить его численные свойства, однако если вы не знаете про сингулярное разложение, то лучше вернитесь сюда, когда узнаете.\\n\\nПостроим QRQRQR-разложение матрицы XXX. Напомним, что это разложение, в котором матрица QQQ ортогональна по столбцам (то есть её столбцы ортогональны и имеют длину 1; в частности, QTQ=EQ^TQ=EQTQ=E), а RRR квадратная и верхнетреугольная. Подставив его в формулу, получим\\n\\nw=((QR)TQR)−1(QR)Ty=(RTQTQ⏟=ER)−1RTQTy=R−1R−TRTQTy=R−1QTyw = ((QR)^TQR)^{-1}(QR)^T y = (R^T\\\\underbrace{Q^TQ}_{=E}R)^{-1}R^TQ^Ty = R^{-1}R^{-T}R^TQ^Ty = R^{-1}Q^Ty\\nw=((QR)TQR)−1(QR)Ty=(RT=EQTQ\\u200b\\u200bR)−1RTQTy=R−1R−TRTQTy=R−1QTyОтметим, что написать (RTR)−1=R−1R−T(R^TR)^{-1} = R^{-1}R^{-T}(RTR)−1=R−1R−T мы имеем право благодаря тому, что RRR квадратная. Полученная формула намного проще, обращение верхнетреугольной матрицы (=решение системы с верхнетреугольной левой частью) производится быстро и хорошо, погрешность вычисления www будет зависеть просто от числа обусловленности матрицы XXX, а поскольку нахождение QRQRQR-разложения является достаточно стабильной операцией, мы получаем решение с более хорошими, чем у исходной формулы, численными свойствами.\\n\\nТакже можно использовать псевдообратную матрицу, построенную с помощью сингулярного разложения, о котором подробно написано в разделе про матричные разложения. А именно, пусть\\n\\nA=Udiag(σ1,…,σr)⏟=ΣVTA = U\\\\underbrace{\\\\mathrm{diag}(\\\\sigma_1,\\\\ldots,\\\\sigma_r)}_{=\\\\Sigma}V^T\\nA=U=Σdiag(σ1\\u200b,…,σr\\u200b)\\u200b\\u200bVT– это усечённое сингулярное разложение, где rrr – это ранг AAA. В таком случае диагональная матрица посередине является квадратной, UUU и VVV ортогональны по столбцам: UTU=EU^TU = EUTU=E, VTV=EV^TV = EVTV=E. Тогда\\nw=(VΣUTU⏟=EΣVT)−1VΣUTyw = (V\\\\Sigma \\\\underbrace{U^TU}_{=E}\\\\Sigma V^T)^{-1}V\\\\Sigma U^Ty\\nw=(VΣ=EUTU\\u200b\\u200bΣVT)−1VΣUTyЗаметим, что VΣ−2VT⋅VΣ2VT=E=VΣ2VT⋅VΣ−2VTV\\\\Sigma^{-2}V^T\\\\cdot V\\\\Sigma^2V^T = E = V\\\\Sigma^2V^T\\\\cdot V\\\\Sigma^{-2}V^TVΣ−2VT⋅VΣ2VT=E=VΣ2VT⋅VΣ−2VT, так что (VΣ2VT)−1=VΣ−2VT(V\\\\Sigma^2 V^T)^{-1} = V\\\\Sigma^{-2}V^T(VΣ2VT)−1=VΣ−2VT, откуда\\nw=VΣ−2VTV⏟=EΣUTy=VΣ−1Uyw = V\\\\Sigma^{-2}\\\\underbrace{V^TV}_{=E}\\\\Sigma U^Ty = V\\\\Sigma^{-1}Uy\\nw=VΣ−2=EVTV\\u200b\\u200bΣUTy=VΣ−1UyХорошие численные свойства сингулярного разложения позволяют утверждать, что и это решение ведёт себя довольно неплохо.\\nТем не менее, вычисление всё равно остаётся довольно долгим и будет по-прежнему страдать (хоть и не так сильно) в случае плохой обусловленности матрицы XXX.\\nПолностью вылечить проблемы мы не сможем, но никто и не обязывает нас останавливаться на «точном» решении (которое всё равно никогда не будет вполне точным). Поэтому ниже мы познакомим вас с совершенно другим методом.\\nМНК: приближенный численный метод\\nМинимизируемый функционал является гладким и выпуклым, а это значит, что можно эффективно искать точку его минимума с помощью итеративных градиентных методов. Более подробно вы можете прочитать о них в разделе про методы оптимизации, а здесь мы лишь коротко расскажем об одном самом базовом подходе.\\nКак известно, градиент функции в точке направлен в сторону её наискорейшего роста, а антиградиент (противоположный градиенту вектор) в сторону наискорейшего убывания. То есть имея какое-то приближение оптимального значения параметра www, мы можем его улучшить, посчитав градиент функции потерь в точке и немного сдвинув вектор весов в направлении антиградиента:\\nwj↦wj−αddwjL(fw,X,y)w_j \\\\mapsto w_j - \\\\alpha \\\\frac{d}{d{w_j}} L(f_w, X, y)\\nwj\\u200b↦wj\\u200b−αdwj\\u200bd\\u200bL(fw\\u200b,X,y)где α\\\\alphaα – это параметр алгоритма («темп обучения»), который контролирует величину шага в направлении антиградиента. Описанный алгоритм называется градиентным спуском.\\nПосмотрим, как будет выглядеть градиентный спуск для функции потерь L(fw,X,y)=1N∣∣Xw−y∣∣2L(f_w, X, y) = \\\\frac1N\\\\vert\\\\vert Xw - y\\\\vert\\\\vert^2L(fw\\u200b,X,y)=N1\\u200b∣∣Xw−y∣∣2. Градиент квадрата евклидовой нормы мы уже считали; соответственно,\\n∇wL=2NXT(Xw−y)\\\\nabla_wL = \\\\frac2{N} X^T (Xw - y)\\n∇w\\u200bL=N2\\u200bXT(Xw−y)Следовательно, стартовав из какого-то начального приближения, мы можем итеративно уменьшать значение функции, пока не сойдёмся (по крайней мере в теории) к минимуму (вообще говоря, локальному, но в данном случае глобальному).\\nАлгоритм градиентного спуска\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1w = random_normal()             # можно пробовать и другие виды инициализации\\n2repeat S times:                 # другой вариант: while abs(err) > tolerance\\n3   f = X.dot(w)                 # посчитать предсказание\\n4   err = f - y                  # посчитать ошибку\\n5   grad = 2 * X.T.dot(err) / N  # посчитать градиент\\n6   w -= alpha * grad            # обновить веса\\n\\n\\nС теоретическими результатами о скорости и гарантиях сходимости градиентного спуска вы можете познакомиться в параграфе про методы оптимизации. Мы позволим себе лишь несколько общих замечаний:\\n\\nПоскольку задача выпуклая, выбор начальной точки влияет на скорость сходимости, но не настолько сильно, чтобы на практике нельзя было стартовать всегда из нуля или из любой другой приятной вам точки;\\nЧисло обусловленности матрицы XXX существенно влияет на скорость сходимости градиентного спуска: чем более вытянуты эллипсоиды уровня функции потерь, тем хуже;\\nТемп обучения α\\\\alphaα тоже сильно влияет на поведение градиентного спуска; вообще говоря, он является гиперпараметром алгоритма, и его, возможно, придётся подбирать отдельно. Другими гиперпараметрами являются максимальное число итераций SSS и/или порог tolerance.\\n\\nИллюстрация.Рассмотрим три задачи регрессии, для которых матрица XXX имеет соответственно маленькое, среднее и большое числа обусловленности. Будем строить для них модели вида y=w1x1+w2x2y=w_1x_1 + w_2x_2y=w1\\u200bx1\\u200b+w2\\u200bx2\\u200b. Раскрасим плоскость (w1,w2)(w_1, w_2)(w1\\u200b,w2\\u200b) в соответствии со значениями ∥Xtrainw−ytrain∥2\\\\|X_{\\\\text{train}}w - y_{\\\\text{train}}\\\\|^2∥Xtrain\\u200bw−ytrain\\u200b∥2. Тёмная область содержит минимум этой функции – оптимальное значение w∗w_{\\\\ast}w∗\\u200b. Также запустим из двух точек градиентный спуск с разными значениями темпа обучения α\\\\alphaα и посмотрим, что получится:\\n\\nЗаголовки графиков (\"Round\", \"Elliptic\", \"Stripe-like\") относятся к форме линий уровня потерь (чем более они вытянуты, тем хуже обусловлена задача и тем хуже может вести себя градиентный спуск).\\nИтог: при неудачном выборе α\\\\alphaα алгоритм не сходится или идёт вразнос, а для плохо обусловленной задачи он сходится абы куда.\\nВычислительная сложность градиентного спуска – O(NDS)O(NDS)O(NDS), где, как и выше, NNN – длина выборки, DDD – число признаков у одного объекта. Сравните с оценкой O(D2N+D3)O(D^2N+D^3)O(D2N+D3) для «наивного» вычисления аналитического решения.\\nСложность по памяти – O(ND)O(ND)O(ND) на хранение выборки. В памяти мы держим и выборку, и градиент, но в большинстве реалистичных сценариев доминирует выборка.\\nСтохастический градиентный спуск\\nНа каждом шаге градиентного спуска нам требуется выполнить потенциально дорогую операцию вычисления градиента по всей выборке (сложность O(ND)O(ND)O(ND)). Возникает идея заменить градиент его оценкой на подвыборке (в английской литературе такую подвыборку обычно именуют batch или mini-batch; в русской разговорной терминологии тоже часто встречается слово батч или мини-батч).\\nА именно, если функция потерь имеет вид суммы по отдельным парам объект-таргет\\nL(w,X,y)=1N∑i=1NL(w,xi,yi),L(w, X, y) = \\\\frac1N\\\\sum_{i=1}^NL(w, x_i, y_i),\\nL(w,X,y)=N1\\u200bi=1∑N\\u200bL(w,xi\\u200b,yi\\u200b),а градиент, соответственно, записывается в виде\\n∇wL(w,X,y)=1N∑i=1N∇wL(w,xi,yi),\\\\nabla_wL(w, X, y) = \\\\frac1N\\\\sum_{i=1}^N\\\\nabla_wL(w, x_i, y_i),\\n∇w\\u200bL(w,X,y)=N1\\u200bi=1∑N\\u200b∇w\\u200bL(w,xi\\u200b,yi\\u200b),то предлагается брать оценку\\n∇wL(w,X,y)≈1B∑t=1B∇wL(w,xit,yit)\\\\nabla_wL(w, X, y) \\\\approx \\\\frac1B\\\\sum_{t=1}^B\\\\nabla_wL(w, x_{i_t}, y_{i_t})\\n∇w\\u200bL(w,X,y)≈B1\\u200bt=1∑B\\u200b∇w\\u200bL(w,xit\\u200b\\u200b,yit\\u200b\\u200b)для некоторого подмножества этих пар (xit,yit)t=1B(x_{i_t}, y_{i_t})_{t=1}^B(xit\\u200b\\u200b,yit\\u200b\\u200b)t=1B\\u200b. Обратите внимание на множители 1N\\\\frac1NN1\\u200b и 1B\\\\frac1BB1\\u200b перед суммами. Почему они нужны? Полный градиент ∇wL(w,X,y)\\\\nabla_wL(w, X, y)∇w\\u200bL(w,X,y) можно воспринимать как среднее градиентов по всем объектам, то есть как оценку матожидания E∇wL(w,x,y)\\\\mathbb{E}\\\\nabla_wL(w, x, y)E∇w\\u200bL(w,x,y); тогда, конечно, оценка матожидания по меньшей подвыборке тоже будет иметь вид среднего градиентов по объектам этой подвыборки.\\nКак делить выборку на батчи? Ясно, что можно было бы случайным образом сэмплировать их из полного датасета, но даже если использовать быстрый алгоритм вроде резервуарного сэмплирования, сложность этой операции не самая оптимальная. Поэтому используют линейный проход по выборке (которую перед этим лучше всё-таки случайным образом перемешать). Давайте введём ещё один параметр нашего алгоритма: размер батча, который мы обозначим BBB. Теперь на BBB очередных примерах вычислим градиент и обновим веса модели. При этом вместо количества шагов алгоритма обычно задают количество эпох EEE. Это ещё один гиперпараметр. Одна эпоха – это один полный проход нашего сэмплера по выборке. Заметим, что если выборка очень большая, а модель компактная, то даже первый проход бывает можно не заканчивать.\\nАлгоритм:\\n\\n\\n\\n        Скопировать код\\n      \\n\\n1 w = normal(0, 1)\\n2 repeat E times:\\n3   for i = B, i <= n, i += B\\n4      X_batch = X[i-B : i]\\n5      y_batch = y[i-B : i]\\n6      f = X_batch.dot(w)                 # посчитать предсказание\\n7      err = f - y_batch                  # посчитать ошибку\\n8      grad = 2 * X_batch.T.dot(err) / B  # посчитать градиент\\n9      w -= alpha * grad\\n10\\n\\n\\nСложность по времени – O(NDE)O(NDE)O(NDE). На первый взгляд, она такая же, как и у обычного градиентного спуска, но заметим, что мы сделали в N/BN / BN/B раз больше шагов, то есть веса модели претерпели намного больше обновлений.\\nСложность по памяти можно довести до O(BD)O(BD)O(BD): ведь теперь всю выборку не надо держать в памяти, а достаточно загружать лишь текущий батч (а остальная выборка может лежать на диске, что удобно, так как в реальности задачи, в которых выборка целиком не влезает в оперативную память, встречаются сплошь и рядом). Заметим, впрочем, что при этом лучше бы BBB взять побольше: ведь чтение с диска – намного более затратная по времени операция, чем чтение из оперативной памяти.\\nВ целом, разницу между алгоритмами можно представлять как-то так:\\n\\nШаги стохастического градиентного спуска заметно более шумные, но считать их получается значительно быстрее. В итоге они тоже сходятся к оптимальному значению из-за того, что матожидание оценки градиента на батче равно самому градиенту. По крайней мере, сходимость можно получить при хорошо подобранных коэффициентах темпа обучения в случае выпуклого функционала качества. Подробнее мы об этом поговорим в параграфе про оптимизацию. Для сложных моделей и лоссов стохастический градиентный спуск может сходиться плохо или застревать в локальных минимумах, поэтому придумано множество его улучшений. О некоторых из них также рассказано в параграфе про оптимизацию.\\nСуществует определённая терминологическая путаница, иногда стохастическим градиентным спуском называют версию алгоритма, в которой размер батча равен единице (то есть максимально шумная и быстрая версия алгоритма), а версии с бОльшим размером батча называют batch gradient descent. В книгах, которые, возможно, старше вас, такая процедура иногда ещё называется incremental gradient descent. Это не очень принципиально, но вы будьте готовы, если что.\\nВопрос на подумать. Вообще говоря, если объём данных не слишком велик и позволяет это сделать, объекты лучше случайным образом перемешивать перед тем, как подавать их в алгоритм стохастического градиентного спуска. Как вам кажется, почему?\\nТакже можно использовать различные стратегии отбора объектов. Например, чаще брать объекты, на которых ошибка больше. Какие ещё стратегии вы могли бы придумать?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Легко представить себе ситуацию, в которой объекты как-нибудь неудачно упорядочены, скажем, по возрастанию таргета. Тогда модель будет попеременно то запоминать, что все таргеты маленькие, то – что все таргеты большие. Это может и не повлиять на качество итоговой модели, но может привести и к довольно печальным последствиям. И вообще, чем более разнообразные батчи модель увидит в процессе обучения, тем лучше.\\nСтратегий можно придумать много. Например, не брать объекты, на которых ошибка слишком большая (возможно, это выбросы – зачем на них учиться), или вообще не брать те, на которых ошибка достаточно мала (они «ничему не учат»). Рекомендуем, впрочем, прибегать к этим эвристикам, только если вы понимаете, зачем они вам нужны и почему есть надежда, что они помогут.\\nНеградиентные методы\\nПосле прочтения этой главы у вас может сложиться ощущение, что приближённые способы решения ML задач и градиентные методы – это одно и тоже, но вы будете правы в этом только на 98%. В принципе, существуют и другие способы численно решать эти задачи, но в общем случае они работают гораздо хуже, чем градиентный спуск, и не обладают таким хорошим теоретическим обоснованием. Мы не будем рассказывать про них подробно, но можете на досуге почитать, скажем, про Stepwise regression, Orthogonal matching pursuit или LARS. У LARS, кстати, есть довольно интересное свойство: он может эффективно работать на выборках, в которых число признаков больше числа примеров. С алгоритмом LARS вы можете познакомиться в параграфе про оптимизацию.\\nРегуляризация\\nВсегда ли решение задачи регрессии единственно? Вообще говоря, нет. Так, если в выборке два признака будут линейно зависимы (и следовательно, ранг матрицы будет меньше DDD), то гарантировано найдётся такой вектор весов ν\\\\nuν что ⟨ν,xi⟩=0\\xa0\\xa0∀xi\\\\langle\\\\nu, x_i\\\\rangle = 0\\\\ \\\\ \\\\forall x_i⟨ν,xi\\u200b⟩=0\\xa0\\xa0∀xi\\u200b. В этом случае, если какой-то www является решением оптимизационной задачи, то и w+ανw + \\\\alpha \\\\nuw+αν тоже является решением для любого α\\\\alphaα. То есть решение не только не обязано быть уникальным, так ещё может быть сколь угодно большим по модулю. Это создаёт вычислительные трудности. Малые погрешности признаков сильно возрастают при предсказании ответа, а в градиентном спуске накапливается погрешность из-за операций со слишком большими числами.\\nКонечно, в жизни редко бывает так, что признаки строго линейно зависимы, а вот быть приближённо линейно зависимыми они вполне могут быть. Такая ситуация называется мультиколлинеарностью. В этом случае у нас, всё равно, возникают проблемы, близкие к описанным выше. Дело в том, что Xν∼0X\\\\nu\\\\sim 0Xν∼0 для вектора ν\\\\nuν, состоящего из коэффициентов приближённой линейной зависимости, и, соответственно, XTXν≈0X^TX\\\\nu\\\\approx 0XTXν≈0, то есть матрица XTXX^TXXTX снова будет близка к вырожденной. Как и любая симметричная матрица, она диагонализуется в некотором ортонормированном базисе, и некоторые из собственных значений λi\\\\lambda_iλi\\u200b близки к нулю. Если вектор XTyX^TyXTy в выражении (XTX)−1XTy(X^TX)^{-1}X^Ty(XTX)−1XTy будет близким к соответствующему собственному вектору, то он будет умножаться на 1/λi1 /{\\\\lambda_i}1/λi\\u200b, что опять же приведёт к появлению у www очень больших по модулю компонент (при этом www ещё и будет вычислен с большой погрешностью из-за деления на маленькое число). И, конечно же, все ошибки и весь шум, которые имелись в матрице XXX, при вычислении y∼Xwy\\\\sim Xwy∼Xw будут умножаться на эти большие и неточные числа и возрастать во много-много раз, что приведёт к проблемам, от которых нас не спасёт никакое сингулярное разложение.\\nВажно ещё отметить, что в случае, когда несколько признаков линейно зависимы, веса wiw_iwi\\u200b при них теряют физический смысл. Может даже оказаться, что вес признака, с ростом которого таргет, казалось бы, должен увеличиваться, станет отрицательным. Это делает модель не только неточной, но и принципиально не интерпретируемой. Вообще, неадекватность знаков или величины весов – хорошее указание на мультиколлинеарность.\\nДля того, чтобы справиться с этой проблемой, задачу обычно регуляризуют, то есть добавляют к ней дополнительное ограничение на вектор весов. Это ограничение можно, как и исходный лосс, задавать по-разному, но, как правило, ничего сложнее, чем L1L^1L1- и L2L^2L2-нормы, не требуется.\\nВместо исходной задачи теперь предлагается решить такую:\\nmin\\u2061wL(f,X,y)=min\\u2061w(∥Xw−y∥22+λ∥w∥kk)\\\\color{#348FEA}{\\\\min_w L(f, X, y) = \\\\min_w(\\\\|X w - y\\\\|_2^2 + \\\\lambda \\\\|w\\\\|^k_k )}\\nwmin\\u200bL(f,X,y)=wmin\\u200b(∥Xw−y∥22\\u200b+λ∥w∥kk\\u200b)λ\\\\lambdaλ – это очередной параметр, а ∥w∥kˆk\\\\|w\\\\|\\\\^k_k∥w∥kˆk\\u200b – это один из двух вариантов:\\n∥w∥22=w12+…+wD2\\\\color{#348FEA}{\\\\|w\\\\|^2_2 = w^2_1 + \\\\ldots + w^2_D}\\n∥w∥22\\u200b=w12\\u200b+…+wD2\\u200bили\\n∥w∥11=∣w1∣+…+∣wD∣\\\\color{#348FEA}{\\\\|w\\\\|_1^1 = \\\\vert w_1 \\\\vert + \\\\ldots + \\\\vert w_D \\\\vert}\\n∥w∥11\\u200b=∣w1\\u200b∣+…+∣wD\\u200b∣Добавка λ∥w∥kk\\\\lambda\\\\|w\\\\|^k_kλ∥w∥kk\\u200b называется регуляризационным членом или регуляризатором, а число λ\\\\lambdaλ – коэффициентом регуляризации.\\nКоэффициент λ\\\\lambdaλ является гиперпараметром модели и достаточно сильно влияет на качество итогового решения. Его подбирают по логарифмической шкале (скажем, от 1e-2 до 1e+2), используя для сравнения моделей с разными значениями λ\\\\lambdaλ дополнительную валидационную выборку. При этом качество модели с подобранным коэффициентом регуляризации уже проверяют на тестовой выборке, чтобы исключить переобучение. Более подробно о том, как нужно подбирать гиперпараметры, вы можете почитать в соответствующем параграфе.\\nОтдельно надо договориться о том, что вес w0w_0w0\\u200b, соответствующий отступу от начала координат (то есть признаку из всех единичек), мы регуляризовать не будем, потому что это не имеет смысла: если даже все значения yyy равномерно велики, это не должно портить качество обучения. Обычно это не отображают в формулах, но если придираться к деталям, то стоило бы написать сумму по всем весам, кроме w0w_0w0\\u200b:\\n∥w∥22=∑j=1Dwj2,\\\\|w\\\\|^2_2 = \\\\sum_{\\\\color{red}{j=1}}^{D}w_j^2,\\n∥w∥22\\u200b=j=1∑D\\u200bwj2\\u200b,∥w∥1=∑j=1D∣wj∣\\\\|w\\\\|_1 = \\\\sum_{\\\\color{red}{j=1}}^{D} \\\\vert w_j \\\\vert\\n∥w∥1\\u200b=j=1∑D\\u200b∣wj\\u200b∣В случае L2L^2L2-регуляризации решение задачи изменяется не очень сильно. Например, продифференцировав новый лосс по www, легко получить, что «точное» решение имеет вид:\\nw=(XTX+λI)−1XTyw = (X^TX + \\\\lambda I)^{-1}X^Ty\\nw=(XTX+λI)−1XTyОтметим, что за этой формулой стоит и понятная численная интуиция: раз матрица XTXX^TXXTX близка к вырожденной, то обращать её сродни самоубийству. Мы лучше слегка исказим её добавкой λI\\\\lambda IλI, которая увеличит все собственные значения на λ\\\\lambdaλ, отодвинув их от нуля. Да, аналитическое решение перестаёт быть «точным», но за счёт снижения численных проблем мы получим более качественное решение, чем при использовании «точной» формулы.\\nВ свою очередь, градиент функции потерь\\nL(fw,X,y)=∥Xw−y∥2+λ∥w∥2L(f_w, X, y) = \\\\|Xw - y\\\\|^2 + \\\\lambda\\\\|w\\\\|^2\\nL(fw\\u200b,X,y)=∥Xw−y∥2+λ∥w∥2по весам теперь выглядит так:\\n∇wL(fw,X,y)=2XT(Xw−y)+2λw\\\\nabla_wL(f_w, X, y) = 2X^T(Xw - y) + 2\\\\lambda w\\n∇w\\u200bL(fw\\u200b,X,y)=2XT(Xw−y)+2λwПодставив этот градиент в алгоритм стохастического градиентного спуска, мы получаем обновлённую версию приближенного алгоритма, отличающуюся от старой только наличием дополнительного слагаемого.\\nВопрос на подумать. Рассмотрим стохастический градиентный спуск для L2L^2L2-регуляризованной линейной регрессии с батчами размера 111. Выберите правильный вариант шага SGD:\\n(а) wi↦wi−2α(⟨w,xj⟩−yj)xji−2αλNwi,i=1,…,Dw_i\\\\mapsto w_i - 2\\\\alpha(\\\\langle w, x_j\\\\rangle - y_j)x_{ji} - \\\\frac{2\\\\alpha\\\\lambda}N w_i,\\\\quad i=1,\\\\ldots,Dwi\\u200b↦wi\\u200b−2α(⟨w,xj\\u200b⟩−yj\\u200b)xji\\u200b−N2αλ\\u200bwi\\u200b,i=1,…,D;\\n(б) wi↦wi−2α(⟨w,xj⟩−yj)xji−2αλwi,i=1,…,Dw_i\\\\mapsto w_i - 2\\\\alpha(\\\\langle w, x_j\\\\rangle - y_j)x_{ji} - 2\\\\alpha\\\\lambda w_i,\\\\quad i=1,\\\\ldots,Dwi\\u200b↦wi\\u200b−2α(⟨w,xj\\u200b⟩−yj\\u200b)xji\\u200b−2αλwi\\u200b,i=1,…,D;\\n(в) wi↦wi−2α(⟨w,xj⟩−yj)xji−2λNwi,i=1,…Dw_i\\\\mapsto w_i - 2\\\\alpha(\\\\langle w, x_j\\\\rangle - y_j)x_{ji} - 2\\\\lambda N w_i,\\\\quad i=1,\\\\ldots Dwi\\u200b↦wi\\u200b−2α(⟨w,xj\\u200b⟩−yj\\u200b)xji\\u200b−2λNwi\\u200b,i=1,…D.\\nОтвет (не открывайте сразу; сначала подумайте сами!)Не регуляризованная функция потерь имеет вид L(X,y,w)=1N∑i=1NL(xi,yi,w)\\\\mathcal{L}(X, y, w) = \\\\frac1N\\\\sum_{i=1}^N\\\\mathcal{L}(x_i, y_i, w)L(X,y,w)=N1\\u200b∑i=1N\\u200bL(xi\\u200b,yi\\u200b,w), и её можно воспринимать, как оценку по выборке (xi,yi)i=1N(x_i, y_i)_{i=1}^N(xi\\u200b,yi\\u200b)i=1N\\u200b идеальной функции потерь\\nL(w)=Ex,yL(x,y,w)\\\\mathcal{L}(w) = \\\\mathbb{E}_{x, y}\\\\mathcal{L}(x, y, w)\\nL(w)=Ex,y\\u200bL(x,y,w)Регуляризационный член не зависит от выборки и добавляется отдельно:\\nLreg(w)=Ex,yL(x,y,w)+λ∥w∥2\\\\mathcal{L}_{\\\\text{reg}}(w) = \\\\mathbb{E}_{x, y}\\\\mathcal{L}(x, y, w) + \\\\lambda\\\\|w\\\\|^2\\nLreg\\u200b(w)=Ex,y\\u200bL(x,y,w)+λ∥w∥2Соответственно, идеальный градиент регуляризованной функции потерь имеет вид\\n∇wLreg(w)=Ex,y∇wL(x,y,w)+2λw,\\\\nabla_w\\\\mathcal{L}_{\\\\text{reg}}(w) = \\\\mathbb{E}_{x, y}\\\\nabla_w\\\\mathcal{L}(x, y, w) + 2\\\\lambda w,\\n∇w\\u200bLreg\\u200b(w)=Ex,y\\u200b∇w\\u200bL(x,y,w)+2λw,Градиент по батчу – это тоже оценка градиента идеальной функции потерь, только не на выборке (X,y)(X, y)(X,y), а на батче (xti,yti)i=1B(x_{t_i}, y_{t_i})_{i=1}^B(xti\\u200b\\u200b,yti\\u200b\\u200b)i=1B\\u200b размера BBB. Он будет выглядеть так:\\n∇wLreg(w)=1B∑i=1B∇wL(xti,yti,w)+2λw.\\\\nabla_w\\\\mathcal{L}_{\\\\text{reg}}(w) = \\\\frac1B\\\\sum_{i=1}^B\\\\nabla_w\\\\mathcal{L}(x_{t_i}, y_{t_i}, w) + 2\\\\lambda w.\\n∇w\\u200bLreg\\u200b(w)=B1\\u200bi=1∑B\\u200b∇w\\u200bL(xti\\u200b\\u200b,yti\\u200b\\u200b,w)+2λw.Как видите, коэффициентов, связанных с числом объектов в батче или в исходной выборке, во втором слагаемом нет. Так что верным является второй вариант. Кстати, обратите внимание, что в третьем ещё и нет коэффициента α\\\\alphaα перед производной регуляризационного слагаемого, это тоже ошибка.\\nВопрос на подумать. Распишите процедуру стохастического градиентного спуска для L1L^1L1-регуляризованной линейной регрессии. Как вам кажется, почему никого не волнует, что функция потерь, строго говоря, не дифференцируема?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Распишем для случая батча размера 1:\\nwi↦wi−α(⟨w,xj⟩−yj)xji−λNsign(wi),i=1,…,Dw_i\\\\mapsto w_i - \\\\alpha(\\\\langle w, x_j\\\\rangle - y_j)x_{ji} - \\\\frac{\\\\lambda}N \\\\text{sign}(w_i),\\\\quad i=1,\\\\ldots,D\\nwi\\u200b↦wi\\u200b−α(⟨w,xj\\u200b⟩−yj\\u200b)xji\\u200b−Nλ\\u200bsign(wi\\u200b),i=1,…,DФункция потерь не дифференцируема лишь в одной точке. Так как в машинном обучении чаще всего мы имеем дело с данными вероятностного характера, это не влечёт каких-то особых проблем. Дело в том, что попадание прямо в ноль очень маловероятно из-за численных погрешностей в данных, так что мы можем просто доопределить производную в одной точке, а если даже пару раз попадём в неё за время обучения, это не приведёт к каким-то значительным изменениям результатов.\\nОтметим, что L1L^1L1- и L2L^2L2-регуляризацию можно определять для любой функции потерь L(w,X,y)L(w, X, y)L(w,X,y) (и не только в задаче регрессии, а и, например, в задаче классификации тоже). Новая функция потерь будет соответственно равна\\nL~(w,X,y)=L(w,X,y)+λ∥w∥1\\\\widetilde{L}(w, X, y) = L(w, X, y) + \\\\lambda\\\\|w\\\\|_1\\nL(w,X,y)=L(w,X,y)+λ∥w∥1\\u200bили\\nL~(w,X,y)=L(w,X,y)+λ∥w∥22\\\\widetilde{L}(w, X, y) = L(w, X, y) + \\\\lambda\\\\|w\\\\|_2^2\\nL(w,X,y)=L(w,X,y)+λ∥w∥22\\u200bРазреживание весов в L1L^1L1-регуляризации\\nL2L^2L2-регуляризация работает прекрасно и используется в большинстве случаев, но есть одна полезная особенность L1L^1L1-регуляризации: её применение приводит к тому, что у признаков, которые не оказывают большого влияния на ответ, вес в результате оптимизации получается равным 000. Это позволяет удобным образом удалять признаки, слабо влияющие на таргет. Кроме того, это даёт возможность автоматически избавляться от признаков, которые участвуют в соотношениях приближённой линейной зависимости, соответственно, спасает от проблем, связанных с мультиколлинеарностью, о которых мы писали выше.\\nНе очень строгим, но довольно интуитивным образом это можно объяснить так:\\n\\nВ точке оптимума линии уровня регуляризационного члена касаются линий уровня основного лосса, потому что, во-первых, и те, и другие выпуклые, а во-вторых, если они пересекаются трансверсально, то существует более оптимальная точка:\\n\\n\\n\\nЛинии уровня L1L^1L1-нормы – это NNN-мерные октаэдры. Точки их касания с линиями уровня лосса, скорее всего, лежат на грани размерности, меньшей N−1N-1N−1, то есть как раз в области, где часть координат равна нулю:\\n\\n\\nЗаметим, что данное построение говорит о том, как выглядит оптимальное решение задачи, но ничего не говорит о способе, которым это решение можно найти. На самом деле, найти такой оптимум непросто: у L1L^1L1 меры довольно плохая производная. Однако, способы есть. Можете на досуге прочитать, например, вот эту статью о том, как работало предсказание CTR в google в 2012 году. Там этой теме посвящается довольно много места. Кроме того, рекомендуем посмотреть про проксимальные методы в разделе этой книги про оптимизацию в ML.\\nЗаметим также, что вообще-то оптимизация любой нормы Lx,\\xa00<x≤1L_x, \\\\ 0  < x \\\\leq 1Lx\\u200b,\\xa00<x≤1, приведёт к появлению разреженных векторов весов, просто если c L1L^1L1 ещё хоть как-то можно работать, то с остальными всё будет ещё сложнее.\\nДругие лоссы\\nСтохастический градиентный спуск можно очевидным образом обобщить для решения задачи линейной регрессии с любой другой функцией потерь, не только квадратичной: ведь всё, что нам нужно от неё, – это чтобы у функции потерь был градиент. На практике это делают редко, но тем не менее рассмотрим ещё пару вариантов.\\nMAE\\nMean absolute error, абсолютная ошибка, появляется при замене L2L^2L2 нормы в MSE на L1L^1L1:\\nMAE(y,y^)=1N∑i=1N∣yi−y^i∣\\\\color{#348FEA}{MAE(y, \\\\widehat{y}) = \\\\frac1N\\\\sum_{i=1}^N \\\\vert y_i - \\\\widehat{y}_i\\\\vert}\\nMAE(y,y\\u200b)=N1\\u200bi=1∑N\\u200b∣yi\\u200b−y\\u200bi\\u200b∣Можно заметить, что в MAE по сравнению с MSE существенно меньший вклад в ошибку будут вносить примеры, сильно удалённые от ответов модели. Дело тут в том, что в MAE мы считаем модуль расстояния, а не квадрат, соответственно, вклад больших ошибок в MSE получается существенно больше. Такая функция потерь уместна в случаях, когда вы пытаетесь обучить регрессию на данных с большим количеством выбросов в таргете.\\nИначе на эту разницу можно посмотреть так: MSE приближает матожидание условного распределения y∣xy \\\\mid xy∣x, а MAE – медиану.\\nMAPE\\nMean absolute percentage error, относительная ошибка.\\nMAPE(y,y^)=1N∑i=1N∣y^i−yiyi∣MAPE(y, \\\\widehat{y}) = \\\\frac1N\\\\sum_{i=1}^N \\\\left|\\\\frac{\\\\widehat{y}_i-y_i}{y_i}\\\\right|\\nMAPE(y,y\\u200b)=N1\\u200bi=1∑N\\u200b\\u200byi\\u200by\\u200bi\\u200b−yi\\u200b\\u200b\\u200bЧасто используется в задачах прогнозирования (например, погоды, загруженности дорог, кассовых сборов фильмов, цен), когда ответы могут быть различными по порядку величины, и при этом мы бы хотели верно угадать порядок, то есть мы не хотим штрафовать модель за предсказание 2000 вместо 1000 в разы сильней, чем за предсказание 2 вместо 1.\\nВопрос на подумать. Кроме описанных выше в задаче линейной регрессии можно использовать и другие функции потерь, например, Huber loss:\\nL(f,X,y)=∑i=1Nhδ(yi−⟨wi,x⟩),\\xa0где\\xa0hδ(z)={12z2,∣z∣⩽δδ(∣z∣−12δ),∣z∣>δ\\\\mathcal{L}(f, X, y)=\\\\sum_{i=1}^N h_\\\\delta\\\\left(y_i-\\\\left\\\\langle w_i, x\\\\right\\\\rangle\\\\right), \\\\text { где } h_\\\\delta(z)=\\\\left\\\\{\\\\begin{array}{l}\\n\\\\frac{1}{2} z^2,|z| \\\\leqslant \\\\delta \\\\\\\\\\n\\\\delta\\\\left(|z|-\\\\frac{1}{2} \\\\delta\\\\right),|z|>\\\\delta\\n\\\\end{array}\\\\right.\\nL(f,X,y)=i=1∑N\\u200bhδ\\u200b(yi\\u200b−⟨wi\\u200b,x⟩),\\xa0где\\xa0hδ\\u200b(z)={21\\u200bz2,∣z∣⩽δδ(∣z∣−21\\u200bδ),∣z∣>δ\\u200bЧисло δ\\\\deltaδ является гиперпараметром. Сложная формула при ∣z∣>δ\\\\vert z\\\\vert > \\\\delta∣z∣>δ нужна, чтобы функция hδ(z)h_{\\\\delta}(z)hδ\\u200b(z) была непрерывной. Попробуйте объяснить, зачем может быть нужна такая функция потерь.\\nОтвет (не открывайте сразу; сначала подумайте сами!)Часто требования формулируют в духе «функция потерь должна слабее штрафовать то-то и сильней штрафовать вот это». Например, L2L^2L2-регуляризованный лосс штрафует за большие по модулю веса. В данном случае можно заметить, что при небольших значениях ошибки берётся просто MSE, а при больших мы начинаем штрафовать нашу модель менее сурово. Например, это может быть полезно для того, чтобы выбросы не так сильно влияли на результат обучения.\\nЛинейная классификация\\nТеперь давайте поговорим про задачу классификации. Для начала будем говорить про бинарную классификацию на два класса. Обобщить эту задачу до задачи классификации на KKK классов не составит большого труда. Пусть теперь наши таргеты yyy кодируют принадлежность к положительному или отрицательному классу, то есть принадлежность множеству {−1,1}\\\\{-1,1\\\\}{−1,1} (в этом параграфе договоримся именно так обозначать классы, хотя в жизни вам будут нередко встречаться и метки {0,1}\\\\{0,1\\\\}{0,1}), а xxx – по-прежнему векторы из RD\\\\mathbb{R}^DRD. Мы хотим обучить линейную модель так, чтобы плоскость, которую она задаёт, как можно лучше отделяла объекты одного класса от другого.\\n\\nВ идеальной ситуации найдётся плоскость, которая разделит классы: положительный окажется с одной стороны от неё, а отрицательный с другой. Выборка, для которой это возможно, называется линейно разделимой. Увы, в реальной жизни такое встречается крайне редко.\\nКак обучить линейную модель классификации, нам ещё предстоит понять, но уже ясно, что итоговое предсказание можно будет вычислить по формуле\\ny=sign⟨w,xi⟩y = \\\\text{sign} \\\\langle w, x_i\\\\rangle\\ny=sign⟨w,xi\\u200b⟩Почему бы не решать, как задачу регрессии?Мы можем попробовать предсказывать числа −1-1−1 и 111, минимизируя для этого, например, MSE с последующим взятием знака, но ничего хорошего не получится. Во-первых, регрессия почти не штрафует за ошибки на объектах, которые лежат близко к разделяющей плоскости, но не с той стороны. Во вторых, ошибкой будет считаться предсказание, например, 555 вместо 111, хотя нам-то на самом деле не важно, какой у числа модуль, лишь бы знак был правильным. Если визуализировать такое решение, то проблемы тоже вполне заметны:\\n\\nНам нужна прямая, которая разделяет эти точки, а не проходит через них!\\nСконструируем теперь функционал ошибки так, чтобы он вышеперечисленными проблемами не обладал. Мы хотим минимизировать число ошибок классификатора, то есть\\n∑iI[yi≠sign⟨w,xi⟩]⟶min\\u2061w\\\\sum_i \\\\mathbb{I}[y_i \\\\neq sign \\\\langle w, x_i\\\\rangle]\\\\longrightarrow \\\\min_w\\ni∑\\u200bI[yi\\u200b\\ue020=sign⟨w,xi\\u200b⟩]⟶wmin\\u200bДомножим обе части на yiy_iyi\\u200b и немного упростим\\n∑iI[yi⟨w,xi⟩<0]⟶min\\u2061w\\\\sum_i \\\\mathbb{I}[y_i \\\\langle w, x_i\\\\rangle < 0]\\\\longrightarrow \\\\min_w\\ni∑\\u200bI[yi\\u200b⟨w,xi\\u200b⟩<0]⟶wmin\\u200bВеличина M=yi⟨w,xi⟩M = y_i \\\\langle w, x_i\\\\rangleM=yi\\u200b⟨w,xi\\u200b⟩ называется отступом (margin) классификатора. Такая фунция потерь называется misclassification loss. Легко видеть, что\\n\\n\\nотступ положителен, когда sign(yi)=sign(⟨w,xi⟩)sign(y_i) = sign(\\\\langle w, x_i\\\\rangle)sign(yi\\u200b)=sign(⟨w,xi\\u200b⟩), то есть класс угадан верно; при этом чем больше отступ, тем больше расстояние от xix_ixi\\u200b до разделяющей гиперплоскости, то есть «уверенность классификатора»;\\n\\n\\nотступ отрицателен, когда sign(yi)≠sign(⟨w,xi⟩)sign(y_i) \\\\ne sign(\\\\langle w, x_i\\\\rangle)sign(yi\\u200b)\\ue020=sign(⟨w,xi\\u200b⟩), то есть класс угадан неверно; при этом чем больше по модулю отступ, тем более сокрушительно ошибается классификатор.\\n\\n\\nОт каждого из отступов мы вычисляем функцию\\nF(M)=I[M<0]={1,\\xa0M<0,0,\\xa0M⩾0F(M) = \\\\mathbb{I}[M < 0] = \\\\begin{cases}1,\\\\ M < 0,\\\\\\\\ 0,\\\\ M\\\\geqslant 0\\\\end{cases}\\nF(M)=I[M<0]={1,\\xa0M<0,0,\\xa0M⩾0\\u200bОна кусочно-постоянная, и из-за этого всю сумму невозможно оптимизировать градиентными методами: ведь её производная равна нулю во всех точках, где она существует. Но мы можем мажорировать её какой-нибудь более гладкой функцией, и тогда задачу можно будет решить. Функции можно использовать разные, у них свои достоинства и недостатки, давайте рассмотрим несколько примеров:\\n\\nВопрос на подумать. Допустим, мы как-то обучили классификатор, и подавляющее большинство отступов оказались отрицательными. Правда ли нас постигла катастрофа?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Наверное, мы что-то сделали не так, но ситуацию можно локально выправить, если предсказывать классы, противоположные тем, которые выдаёт наша модель.\\nВопрос на подумать. Предположим, что у нас есть два классификатора с примерно одинаковыми и достаточно приемлемыми значениями интересующей нас метрики. При этом одна почти всегда выдаёт предсказания с большими по модулю отступами, а вторая – с относительно маленькими. Верно ли, что первая модель лучше, чем вторая?\\nОтвет (не открывайте сразу; сначала подумайте сами!)На первый взгляд кажется, что первая модель действительно лучше: ведь она предсказывает «увереннее», но на самом деле всё не так однозначно: во многих случаях модель, которая умеет «честно признать, что не очень уверена в ответе», может быть предпочтительней модели, которая врёт с той же непотопляемой уверенностью, что и говорит правду. В некоторых случаях лучше может оказаться модель, которая, по сути, просто отказывается от классификации на каких-то объектах.\\nОшибка перцептрона\\nРеализуем простейшую идею: давайте считать отступы только на неправильно классифицированных объектах и учитывать их не бинарно, а линейно, пропорционально их размеру. Получается такая функция:\\nF(M)=max\\u2061(0,−M)F(M) = \\\\max(0, -M)\\nF(M)=max(0,−M)Давайте запишем такой лосс с L2L^2L2-регуляризацией:\\nL(w,x,y)=λ∣∣w∣∣22+∑imax\\u2061(0,−yi⟨w,xi⟩)L(w, x, y) = \\\\lambda\\\\vert\\\\vert w\\\\vert\\\\vert^2_2 + \\\\sum_i \\\\max(0, -y_i \\\\langle w, x_i\\\\rangle)\\nL(w,x,y)=λ∣∣w∣∣22\\u200b+i∑\\u200bmax(0,−yi\\u200b⟨w,xi\\u200b⟩)Найдём градиент:\\n∇wL(w,x,y)=2λw+∑i{0,yi⟨w,xi⟩>0−yixi,yi⟨w,xi⟩≤0\\\\nabla_w L(w, x, y) = 2 \\\\lambda w + \\\\sum_i\\n\\\\begin{cases}\\n0,            & y_i \\\\langle w, x_i \\\\rangle > 0 \\\\\\\\\\n- y_i x_i,  & y_i \\\\langle w, x_i \\\\rangle \\\\leq 0\\n\\\\end{cases}\\n∇w\\u200bL(w,x,y)=2λw+i∑\\u200b{0,−yi\\u200bxi\\u200b,\\u200byi\\u200b⟨w,xi\\u200b⟩>0yi\\u200b⟨w,xi\\u200b⟩≤0\\u200bИмея аналитическую формулу для градиента, мы теперь можем так же, как и раньше, применить стохастический градиентный спуск, и задача будет решена.\\nДанная функция потерь впервые была предложена для перцептрона Розенблатта, первой вычислительной модели нейросети, которая в итоге привела к появлению глубокого обучения.\\nОна решает задачу линейной классификации, но у неё есть одна особенность: её решение не единственно и сильно зависит от начальных параметров. Например, все изображённые ниже классификаторы имеют одинаковый нулевой лосс:\\n\\nHinge loss, SVM\\nДля таких случаев, как на картинке выше, возникает логичное желание не только найти разделяющую прямую, но и постараться провести её на одинаковом удалении от обоих классов, то есть максимизировать минимальный отступ:\\n\\nЭто можно сделать, слегка поменяв функцию ошибки, а именно положив её равной:\\nF(M)=max\\u2061(0,1−M)F(M) = \\\\max(0, 1-M)\\nF(M)=max(0,1−M)L(w,x,y)=λ∥∣w∥∣22+∑imax\\u2061(0,1−yi⟨w,xi⟩)L(w, x, y) = \\\\lambda\\\\||w\\\\||^2_2 + \\\\sum_i \\\\max(0, 1-y_i \\\\langle w, x_i\\\\rangle)\\nL(w,x,y)=λ∥∣w∥∣22\\u200b+i∑\\u200bmax(0,1−yi\\u200b⟨w,xi\\u200b⟩)∇wL(w,x,y)=2λw+∑i{0,1−yi⟨w,xi⟩≤0−yixi,1−yi⟨w,xi⟩>0\\\\nabla_w L(w, x, y) = 2 \\\\lambda w + \\\\sum_i\\n        \\\\begin{cases}\\n            0,           & 1 - y_i \\\\langle w, x_i \\\\rangle \\\\leq 0 \\\\\\\\\\n            - y_i x_i,   & 1 - y_i \\\\langle w, x_i \\\\rangle > 0\\n        \\\\end{cases}\\n∇w\\u200bL(w,x,y)=2λw+i∑\\u200b{0,−yi\\u200bxi\\u200b,\\u200b1−yi\\u200b⟨w,xi\\u200b⟩≤01−yi\\u200b⟨w,xi\\u200b⟩>0\\u200bПочему же добавленная единичка приводит к желаемому результату?\\nИнтуитивно это можно объяснить так: объекты, которые проклассифицированы правильно, но не очень \"уверенно\" (то есть 0≤yi⟨w,xi⟩<10 \\\\leq y_i \\\\langle w, x_i\\\\rangle < 10≤yi\\u200b⟨w,xi\\u200b⟩<1), продолжают вносить свой вклад в градиент и пытаются \"отодвинуть\" от себя разделяющую плоскость как можно дальше.\\nК данному выводу можно прийти и чуть более строго; для этого надо совершенно по-другому взглянуть на выражение, которое мы минимизируем. Поможет вот эта картинка:\\n\\nЕсли мы максимизируем минимальный отступ, то надо максимизировать 2∥w∥2\\\\frac{2}{\\\\|w\\\\|_2}∥w∥2\\u200b2\\u200b, то есть ширину полосы при условии того, что большинство объектов лежат с правильной стороны, что эквивалентно решению нашей исходной задачи:\\nλ∥w∥22+∑imax\\u2061(0,1−yi⟨w,xi⟩)⟶min\\u2061w\\\\lambda\\\\|w\\\\|^2_2 + \\\\sum_i \\\\max(0, 1-y_i \\\\langle w, x_i\\\\rangle) \\\\longrightarrow\\\\min\\\\limits_{w}\\nλ∥w∥22\\u200b+i∑\\u200bmax(0,1−yi\\u200b⟨w,xi\\u200b⟩)⟶wmin\\u200bОтметим, что первое слагаемое у нас обратно пропорционально ширине полосы, но мы и максимизацию заменили на минимизацию, так что тут всё в порядке. Второе слагаемое – это штраф за то, что некоторые объекты неправильно расположены относительно разделительной полосы. В конце концов, никто нам не обещал, что классы наши линейно разделимы и можно провести оптимальную плоскость вообще без ошибок.\\nИтоговое положение плоскости задаётся всего несколькими обучающими примерами. Это ближайшие к плоскости правильно классифицированные объекты, которые называют опорными векторами или support vectors. Весь метод, соответственно, зовётся методом опорных векторов, или support vector machine, или сокращённо SVM. Начиная с шестидесятых годов это был сильнейший из известных методов машинного обучения. В девяностые его сменили методы, основанные на деревьях решений, которые, в свою очередь, недавно передали «пальму первенства» нейросетям.\\nПочему же SVM был столь популярен? Из-за небольшого количества параметров и доказуемой оптимальности. Сейчас для нас нормально выбирать специальный алгоритм под задачу и подбирать оптимальные гиперпараметры для этого алгоритма перебором, а когда-то трава была зеленее, а компьютеры медленнее, и такой роскоши у людей не было. Поэтому им нужны были модели, которые гарантированно неплохо работали бы в любой ситуации. Такой моделью и был SVM.\\nДругие замечательные свойства SVM: существование уникального решения и доказуемо минимальная склонность к переобучению среди всех популярных классов линейных классификаторов. Кроме того, несложная модификация алгоритма, ядровый SVM, позволяет проводить нелинейные разделяющие поверхности.\\nСтрогий вывод постановки задачи SVM можно прочитать тут или в лекции К.В. Воронцова.\\nЛогистическая регрессия\\nВ этом параграфе мы будем обозначать классы нулём и единицей.\\nЕщё один интересный метод появляется из желания посмотреть на классификацию как на задачу предсказания вероятностей. Хороший пример – предсказание кликов в интернете (например, в рекламе и поиске). Наличие клика в обучающем логе не означает, что, если повторить полностью условия эксперимента, пользователь обязательно кликнет по объекту опять. Скорее у объектов есть какая-то \"кликабельность\", то есть истинная вероятность клика по данному объекту. Клик на каждом обучающем примере является реализацией этой случайной величины, и мы считаем, что в пределе в каждой точке отношение положительных и отрицательных примеров должно сходиться к этой вероятности.\\nПроблема состоит в том, что вероятность, по определению, величина от 0 до 1, а простого способа обучить линейную модель так, чтобы это ограничение соблюдалось, нет. Из этой ситуации можно выйти так: научить линейную модель правильно предсказывать какой-то объект, связанный с вероятностью, но с диапазоном значений (−∞,∞)(-\\\\infty,\\\\infty)(−∞,∞), и преобразовать ответы модели в вероятность. Таким объектом является logit или log odds – логарифм отношения вероятности положительного события к отрицательному log\\u2061(p1−p)\\\\log\\\\left(\\\\frac{p}{1-p}\\\\right)log(1−pp\\u200b).\\nЕсли ответом нашей модели является log\\u2061(p1−p)\\\\log\\\\left(\\\\frac{p}{1-p}\\\\right)log(1−pp\\u200b), то искомую вероятность посчитать не трудно:\\n⟨w,xi⟩=log\\u2061(p1−p)\\\\langle w, x_i\\\\rangle = \\\\log\\\\left(\\\\frac{p}{1-p}\\\\right)\\n⟨w,xi\\u200b⟩=log(1−pp\\u200b)e⟨w,xi⟩=p1−pe^{\\\\langle w, x_i\\\\rangle} = \\\\frac{p}{1-p}\\ne⟨w,xi\\u200b⟩=1−pp\\u200bp=11+e−⟨w,xi⟩p=\\\\frac{1}{1 + e^{-\\\\langle w, x_i\\\\rangle}}\\np=1+e−⟨w,xi\\u200b⟩1\\u200bФункция в правой части называется сигмоидой и обозначается\\nσ(z)=11+e−z\\\\color{#348FEA}{\\\\sigma(z) = \\\\frac1{1 + e^{-z}}}\\nσ(z)=1+e−z1\\u200bТаким образом, p=σ(⟨w,xi⟩)p = \\\\sigma(\\\\langle w, x_i\\\\rangle)p=σ(⟨w,xi\\u200b⟩)\\nКак теперь научиться оптимизировать www так, чтобы модель как можно лучше предсказывала логиты? Нужно применить метод максимума правдоподобия для распределения Бернулли. Это самое простое распределение, которое возникает, к примеру, при бросках монетки, которая орлом выпадает с вероятностью ppp. У нас только событием будет не орёл, а то, что пользователь кликнул на объект с такой вероятностью. Если хотите больше подробностей, почитайте про распределение Бернулли в теоретическом минимуме.\\nПравдоподобие позволяет понять, насколько вероятно получить данные значения таргета yyy при данных XXX и весах www. Оно имеет вид\\np(y∣X,w)=∏ip(yi∣xi,w)p(y\\\\mid X, w) =\\\\prod_i p(y_i\\\\mid x_i, w)\\np(y∣X,w)=i∏\\u200bp(yi\\u200b∣xi\\u200b,w)и для распределения Бернулли его можно выписать следующим образом:\\np(y∣X,w)=∏ipiyi(1−pi)1−yip(y\\\\mid X, w) =\\\\prod_i p_i^{y_i} (1-p_i)^{1-y_i}\\np(y∣X,w)=i∏\\u200bpiyi\\u200b\\u200b(1−pi\\u200b)1−yi\\u200bгде pip_ipi\\u200b – это вероятность, посчитанная из ответов модели. Оптимизировать произведение неудобно, хочется иметь дело с суммой, так что мы перейдём к логарифмическому правдоподобию и подставим формулу для вероятности, которую мы получили выше:\\nℓ(w,X,y)=∑i(yilog\\u2061(pi)+(1−yi)log\\u2061(1−pi))=\\\\ell(w, X, y) = \\\\sum_i \\\\big( y_i \\\\log(p_i) + (1-y_i)\\\\log(1-p_i) \\\\big) =\\nℓ(w,X,y)=i∑\\u200b(yi\\u200blog(pi\\u200b)+(1−yi\\u200b)log(1−pi\\u200b))==∑i(yilog\\u2061(σ(⟨w,xi⟩))+(1−yi)log\\u2061(1−σ(⟨w,xi⟩)))=\\\\sum_i \\\\big( y_i \\\\log(\\\\sigma(\\\\langle w, x_i \\\\rangle)) + (1-y_i)\\\\log(1 - \\\\sigma(\\\\langle w, x_i \\\\rangle)) \\\\big)\\n=i∑\\u200b(yi\\u200blog(σ(⟨w,xi\\u200b⟩))+(1−yi\\u200b)log(1−σ(⟨w,xi\\u200b⟩)))Если заметить, что\\nσ(−z)=11+ez=e−ze−z+1=1−σ(z),\\\\sigma(-z) = \\\\frac{1}{1 + e^z} = \\\\frac{e^{-z}}{e^{-z} + 1} = 1 - \\\\sigma(z),\\nσ(−z)=1+ez1\\u200b=e−z+1e−z\\u200b=1−σ(z),то выражение можно переписать проще:\\nℓ(w,X,y)=∑i(yilog\\u2061(σ(⟨w,xi⟩))+(1−yi)log\\u2061(σ(−⟨w,xi⟩)))\\\\ell(w, X, y)=\\\\sum_i \\\\big( y_i \\\\log(\\\\sigma(\\\\langle w, x_i \\\\rangle)) + (1 - y_i) \\\\log(\\\\sigma(-\\\\langle w, x_i \\\\rangle)) \\\\big)\\nℓ(w,X,y)=i∑\\u200b(yi\\u200blog(σ(⟨w,xi\\u200b⟩))+(1−yi\\u200b)log(σ(−⟨w,xi\\u200b⟩)))Нас интересует www, для которого правдоподобие максимально. Чтобы получить функцию потерь, которую мы будем минимизировать, умножим его на минус один:\\nL(w,X,y)=−∑i(yilog\\u2061(σ(⟨w,xi⟩))+(1−yi)log\\u2061(σ(−⟨w,xi⟩)))\\\\color{#348FEA}{L(w, X, y) = -\\\\sum_i \\\\big( y_i \\\\log(\\\\sigma(\\\\langle w, x_i \\\\rangle)) + (1 - y_i) \\\\log(\\\\sigma(-\\\\langle w, x_i \\\\rangle)) \\\\big)}\\nL(w,X,y)=−i∑\\u200b(yi\\u200blog(σ(⟨w,xi\\u200b⟩))+(1−yi\\u200b)log(σ(−⟨w,xi\\u200b⟩)))В отличие от линейной регрессии, для логистической нет явной формулы решения. Деваться некуда, будем использовать градиентный спуск. К счастью, градиент устроен очень просто:\\n∇wL(y,X,w)=−∑ixi(yi−σ(⟨w,xi⟩))\\\\nabla_w L(y, X, w) = -\\\\sum_i x_i \\\\big( y_i - \\\\sigma(\\\\langle w, x_i \\\\rangle) \\\\big)\\n∇w\\u200bL(y,X,w)=−i∑\\u200bxi\\u200b(yi\\u200b−σ(⟨w,xi\\u200b⟩))Вывод формулы градиентаНам окажется полезным ещё одно свойство сигмоиды:\\ndlog\\u2061σ(z)dz=(log\\u2061(11+e−z))′=e−z1+e−z=σ(−z)\\\\frac{d \\\\log \\\\sigma(z)}{d z} = \\\\left( \\\\log \\\\left( \\\\frac{1}{1 + e^{-z}} \\\\right)  \\\\right)\\' = \\\\frac{e^{-z}}{1 + e^{-z}} = \\\\sigma(-z) \\\\\\\\\\ndzdlogσ(z)\\u200b=(log(1+e−z1\\u200b))′=1+e−ze−z\\u200b=σ(−z)dlog\\u2061σ(−z)dz=−σ(z)\\\\frac{d \\\\log \\\\sigma(-z)}{d z} =  -\\\\sigma(z)\\ndzdlogσ(−z)\\u200b=−σ(z)Отсюда:\\n∇wlog\\u2061σ(⟨w,xi⟩)=σ(−⟨w,xi⟩)xi\\\\nabla_w \\\\log \\\\sigma(\\\\langle w, x_i \\\\rangle) =  \\\\sigma(-\\\\langle w, x_i \\\\rangle) x_i \\\\\\\\\\n∇w\\u200blogσ(⟨w,xi\\u200b⟩)=σ(−⟨w,xi\\u200b⟩)xi\\u200b∇wlog\\u2061σ(−⟨w,xi⟩)=−σ(⟨w,xi⟩)xi\\\\nabla_w \\\\log \\\\sigma(-\\\\langle w, x_i \\\\rangle) =  -\\\\sigma(\\\\langle w, x_i \\\\rangle) x_i\\n∇w\\u200blogσ(−⟨w,xi\\u200b⟩)=−σ(⟨w,xi\\u200b⟩)xi\\u200bи градиент оказывается равным\\n∇wL(y,X,w)=−∑i(yixiσ(−⟨w,xi⟩)−(1−yi)xiσ(⟨w,xi⟩))=\\\\nabla_w L(y, X, w) = -\\\\sum_i \\\\big( y_i x_i \\\\sigma(-\\\\langle w, x_i \\\\rangle) - (1 - y_i) x_i \\\\sigma(\\\\langle w, x_i \\\\rangle) \\\\big) = \\\\\\\\\\n∇w\\u200bL(y,X,w)=−i∑\\u200b(yi\\u200bxi\\u200bσ(−⟨w,xi\\u200b⟩)−(1−yi\\u200b)xi\\u200bσ(⟨w,xi\\u200b⟩))==−∑i(yixi(1−σ(⟨w,xi⟩))−(1−yi)xiσ(⟨w,xi⟩))== -\\\\sum_i \\\\big( y_i x_i (1 - \\\\sigma(\\\\langle w, x_i \\\\rangle)) - (1 - y_i) x_i \\\\sigma(\\\\langle w, x_i \\\\rangle)\\\\big) = \\\\\\\\\\n=−i∑\\u200b(yi\\u200bxi\\u200b(1−σ(⟨w,xi\\u200b⟩))−(1−yi\\u200b)xi\\u200bσ(⟨w,xi\\u200b⟩))==−∑i(yixi−yixiσ(⟨w,xi⟩)−xiσ(⟨w,xi⟩)+yixiσ(⟨w,xi⟩))== -\\\\sum_i \\\\big( y_i x_i - y_i x_i \\\\sigma(\\\\langle w, x_i \\\\rangle) - x_i \\\\sigma(\\\\langle w, x_i \\\\rangle) + y_i x_i \\\\sigma(\\\\langle w, x_i \\\\rangle) \\\\big) = \\\\\\\\\\n=−i∑\\u200b(yi\\u200bxi\\u200b−yi\\u200bxi\\u200bσ(⟨w,xi\\u200b⟩)−xi\\u200bσ(⟨w,xi\\u200b⟩)+yi\\u200bxi\\u200bσ(⟨w,xi\\u200b⟩))==−∑i(yixi−xiσ(⟨w,xi⟩))= -\\\\sum_i \\\\big( y_i x_i - x_i \\\\sigma(\\\\langle w, x_i \\\\rangle) \\\\big)\\n=−i∑\\u200b(yi\\u200bxi\\u200b−xi\\u200bσ(⟨w,xi\\u200b⟩))Предсказание модели будет вычисляться, как мы договаривались, следующим образом:\\np=σ(⟨w,xi⟩)p=\\\\sigma(\\\\langle w, x_i\\\\rangle)\\np=σ(⟨w,xi\\u200b⟩)Это вероятность положительного класса, а как от неё перейти к предсказанию самого класса? В других методах нам достаточно было посчитать знак предсказания, но теперь все наши предсказания положительные и находятся в диапазоне от 0 до 1. Что же делать? Интуитивным и не совсем (и даже совсем не) правильным является ответ «взять порог 0.5». Более корректным будет подобрать этот порог отдельно, для уже построенной регрессии минимизируя нужную вам метрику на отложенной тестовой выборке. Например, сделать так, чтобы доля положительных и отрицательных классов примерно совпадала с реальной.\\nОтдельно заметим, что метод называется логистической регрессией, а не логистической классификацией именно потому, что предсказываем мы не классы, а вещественные числа – логиты.\\nВопрос на подумать. Проверьте, что, если метки классов – это ±1\\\\pm1±1, а не 000 и 111, то функцию потерь для логистической регрессии можно записать в более компактном виде:\\nL(w,X,y)=∑i=1Nlog\\u2061(1+e−yi⟨w,xi⟩)\\\\mathcal{L}(w, X, y) = \\\\sum_{i=1}^N\\\\log(1 + e^{-y_i\\\\langle w, x_i\\\\rangle})\\nL(w,X,y)=i=1∑N\\u200blog(1+e−yi\\u200b⟨w,xi\\u200b⟩)Вопрос на подумать. Правда ли разделяющая поверхность модели логистической регрессии является гиперплоскостью?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Разделяющая поверхность отделяет множество точек, которым мы присваиваем класс 000 (или −1-1−1), и множество точек, которым мы присваиваем класс 111. Представляется логичным провести отсечку по какому-либо значению предсказанной вероятности. Однако, выбор этого значения — дело не очевидное. Как мы увидим в параграфе про калибровку классификаторов, это может быть не настоящая вероятность. Допустим, мы решили провести границу по значению 12\\\\frac1221\\u200b. Тогда разделяющая поверхность как раз задаётся равенством p=12p = \\\\frac12p=21\\u200b, что равносильно ⟨w,x⟩=0\\\\langle w, x\\\\rangle = 0⟨w,x⟩=0. А это гиперплоскость.\\nВопрос на подумать. Допустим, что матрица объекты-признаки XXX имеет полный ранг по столбцам (то есть все её столбцы линейно независимы). Верно ли, что решение задачи восстановления логистической регрессии единственно?\\nОтвет (не открывайте сразу; сначала подумайте сами!)В этот раз хорошего геометрического доказательства, как было для линейной регрессии, пожалуй, нет; нам придётся честно посчитать вторую производную и доказать, что она является положительно определённой. Сделаем это для случая, когда метки классов – это ±1\\\\pm1±1. Формулы так получатся немного попроще. Напомним, что в этом случае\\nL(w,X,y)=∑i=1Nlog\\u2061(1+e−yi⟨w,xi⟩)L(w, X, y) = \\\\sum_{i=1}^N\\\\log(1 + e^{-y_i\\\\langle w, x_i\\\\rangle})\\nL(w,X,y)=i=1∑N\\u200blog(1+e−yi\\u200b⟨w,xi\\u200b⟩)Следовательно,\\n∂∂wjL(w,X,y)=∑i=1Nyixije−yi⟨w,xi⟩1+e−yi⟨w,xi⟩=∑i=1Nyixij(1−11+e−yi⟨w,xi⟩)\\\\frac{\\\\partial}{\\\\partial w_{j}}L(w, X, y) = \\\\sum_{i=1}^N\\\\frac{y_ix_{ij}e^{-y_i\\\\langle w, x_i\\\\rangle}}{1 + e^{-y_i\\\\langle w, x_i\\\\rangle}} = \\\\sum_{i=1}^Ny_ix_{ij}\\\\left(1 - \\\\frac1{1 + e^{-y_i\\\\langle w, x_i\\\\rangle}}\\\\right)\\n∂wj\\u200b∂\\u200bL(w,X,y)=i=1∑N\\u200b1+e−yi\\u200b⟨w,xi\\u200b⟩yi\\u200bxij\\u200be−yi\\u200b⟨w,xi\\u200b⟩\\u200b=i=1∑N\\u200byi\\u200bxij\\u200b(1−1+e−yi\\u200b⟨w,xi\\u200b⟩1\\u200b)∂2L∂wj∂wk(w,X,y)=∑i=1Nyi2xijxike−yi⟨w,xi⟩(1+e−yi⟨w,xi⟩)2=\\\\frac{\\\\partial^2L}{\\\\partial w_j\\\\partial w_k}(w, X, y) = \\\\sum_{i=1}^Ny^2_ix_{ij}x_{ik}\\\\frac{e^{-y_i\\\\langle w, x_i\\\\rangle}}{(1 + e^{-y_i\\\\langle w, x_i\\\\rangle})^2} =\\n∂wj\\u200b∂wk\\u200b∂2L\\u200b(w,X,y)=i=1∑N\\u200byi2\\u200bxij\\u200bxik\\u200b(1+e−yi\\u200b⟨w,xi\\u200b⟩)2e−yi\\u200b⟨w,xi\\u200b⟩\\u200b==∑i=1Nyi2xijxikσ(yi⟨w,xi⟩)(1−σ(yi⟨w,xi⟩))= \\\\sum_{i=1}^Ny^2_ix_{ij}x_{ik}\\\\sigma(y_i\\\\langle w, x_i\\\\rangle)(1 - \\\\sigma(y_i\\\\langle w, x_i\\\\rangle))\\n=i=1∑N\\u200byi2\\u200bxij\\u200bxik\\u200bσ(yi\\u200b⟨w,xi\\u200b⟩)(1−σ(yi\\u200b⟨w,xi\\u200b⟩))Теперь заметим, что yi2=1y_i^2 = 1yi2\\u200b=1 и что, если обозначить через DDD диагональную матрицу с элементами σ(yi⟨w,xi⟩)(1−σ(yi⟨w,xi⟩))\\\\sigma(y_i\\\\langle w, x_i\\\\rangle)(1 - \\\\sigma(y_i\\\\langle w, x_i\\\\rangle))σ(yi\\u200b⟨w,xi\\u200b⟩)(1−σ(yi\\u200b⟨w,xi\\u200b⟩)) на диагонали, матрицу вторых производных можно представить в виде:\\n∇2L=(∂2L∂wj∂wk)=XTDX\\\\nabla^2L = \\\\left(\\\\frac{\\\\partial^2\\\\mathcal{L}}{\\\\partial w_j\\\\partial w_k}\\\\right) = X^TDX\\n∇2L=(∂wj\\u200b∂wk\\u200b∂2L\\u200b)=XTDXТак как 0<σ(yi⟨w,xi⟩)<10 < \\\\sigma(y_i\\\\langle w, x_i\\\\rangle) < 10<σ(yi\\u200b⟨w,xi\\u200b⟩)<1, у матрицы DDD на диагонали стоят положительные числа, из которых можно извлечь квадратные корни, представив DDD в виде D=D1/2D1/2D = D^{1/2}D^{1/2}D=D1/2D1/2. В свою очередь, матрица XXX имеет полный ранг по столбцам. Стало быть, для любого вектора приращения u≠0u\\\\ne 0u\\ue020=0 имеем\\nuTXTDXu=uTXT(D1/2)TD1/2Xu=∣D1/2Xu∣2>0u^TX^TDXu = u^TX^T(D^{1/2})^TD^{1/2}Xu = \\\\vert D^{1/2}Xu \\\\vert^2 > 0\\nuTXTDXu=uTXT(D1/2)TD1/2Xu=∣D1/2Xu∣2>0Таким образом, функция LLL выпукла вниз как функция от www, и, соответственно, точка её экстремума непременно будет точкой минимума.\\nА теперь – почему это не совсем правда. Дело в том, что, говоря «точка её экстремума непременно будет точкой минимума», мы уже подразумеваем существование этой самой точки экстремума. Только вот существует этот экстремум не всегда. Можно показать, что для линейно разделимой выборки функция потерь логистической регрессии не ограничена снизу, и, соответственно, никакого экстремума нет. Доказательство мы оставляем читателю.\\nВопрос на подумать. На картинке ниже представлены результаты работы на одном и том же датасете трёх моделей логистической регрессии с разными коэффициентами L2L^2L2-регуляризации:\\n\\nНаверху показаны предсказанные вероятности положительного класса, внизу – вид разделяющей поверхности.\\nКак вам кажется, какие картинки соответствуют самому большому коэффициенту регуляризации, а какие – самому маленькому? Почему?\\nОтвет (не открывайте сразу; сначала подумайте сами!)Коэффициент регуляризации максимален у левой модели. На это нас могут натолкнуть два соображения. Во-первых, разделяющая прямая проведена достаточно странно, то есть можно заподозрить, что регуляризационный член в лосс-функции перевесил функцию потерь исходной задачи. Во-вторых, модель предсказывает довольно близкие к 12\\\\frac1221\\u200b вероятности – это значит, что значения ⟨w,x⟩\\\\langle w, x\\\\rangle⟨w,x⟩ близки к нулю, то есть сам вектор www близок к нулевому. Это также свидетельствует о том, что регуляризационный член играет слишком важную роль при оптимизации.\\nНаименьший коэффициент регуляризации у правой модели. Её предсказания достаточно «уверенные» (цвета на верхнем графике сочные, то есть вероятности быстро приближаются к 000 или 111). Это может свидетельствовать о том, что числа ⟨w,x⟩\\\\langle w, x\\\\rangle⟨w,x⟩ достаточно велики по модулю, то есть ∣∣w∣∣\\\\vert\\\\vert w \\\\vert\\\\vert∣∣w∣∣ достаточно велик.\\nМногоклассовая классификация\\nВ этом разделе мы будем следовать изложению из лекций Евгения Соколова.\\nПусть каждый объект нашей выборки относится к одному из KKK классов: Y={1,…,K}\\\\mathbb{Y} = \\\\{1, \\\\ldots, K\\\\}Y={1,…,K}. Чтобы предсказывать эти классы с помощью линейных моделей, нам придётся свести задачу многоклассовой классификации к набору бинарных, которые мы уже хорошо умеем решать. Мы разберём два самых популярных способа это сделать – one-vs-all и all-vs-all, а проиллюстрировать их нам поможет вот такой игрушечный датасет\\n\\nОдин против всех (one-versus-all)\\nОбучим KKK линейных классификаторов b1(x),…,bK(x)b_1(x), \\\\ldots, b_K(x)b1\\u200b(x),…,bK\\u200b(x), выдающих оценки принадлежности классам 1,…,K1, \\\\ldots, K1,…,K соответственно. В случае с линейными моделями эти классификаторы будут иметь вид\\nbk(x)=sgn(⟨wk,x⟩+w0k)b_k(x) = \\\\text{sgn}\\\\left(\\\\langle w_k, x \\\\rangle + w_{0k}\\\\right)\\nbk\\u200b(x)=sgn(⟨wk\\u200b,x⟩+w0k\\u200b)Классификатор с номером kkk будем обучать по выборке (xi,2I[yi=k]−1)i=1N\\\\left(x_i, 2\\\\mathbb{I}[y_i = k] - 1\\\\right)_{i = 1}^{N}(xi\\u200b,2I[yi\\u200b=k]−1)i=1N\\u200b; иными словами, мы учим классификатор отличать kkk-й класс от всех остальных.\\nЛогично, чтобы итоговый классификатор выдавал класс, соответствующий самому уверенному из бинарных алгоритмов. Уверенность можно в каком-то смысле измерить с помощью значений линейных функций:\\na(x)=argmaxk(⟨wk,x⟩+w0k)a(x) = \\\\text{argmax}_k \\\\left(\\\\langle w_k, x \\\\rangle + w_{0k}\\\\right)\\na(x)=argmaxk\\u200b(⟨wk\\u200b,x⟩+w0k\\u200b)Давайте посмотрим, что даст этот подход применительно к нашему датасету. Обучим три линейных модели, отличающих один класс от остальных:\\n\\nТеперь сравним значения линейных функций\\n\\nи для каждой точки выберем тот класс, которому соответствует большее значение, то есть самый «уверенный» классификатор:\\n\\nХочется сказать, что самый маленький класс «обидели».\\nПроблема данного подхода заключается в том, что каждый из классификаторов b1(x),…,bK(x)b_1(x), \\\\dots, b_K(x)b1\\u200b(x),…,bK\\u200b(x) обучается на своей выборке, и значения линейных функций ⟨wk,x⟩+w0k\\\\langle w_k, x \\\\rangle + w_{0k}⟨wk\\u200b,x⟩+w0k\\u200b или, проще говоря, \"выходы\" классификаторов могут иметь разные масштабы. Из-за этого сравнивать их будет неправильно. Нормировать вектора весов, чтобы они выдавали ответы в одной и той же шкале, не всегда может быть разумным решением: так, в случае с SVM веса перестанут являться решением задачи, поскольку нормировка изменит норму весов.\\nВсе против всех (all-versus-all)\\nОбучим CK2C_K^2CK2\\u200b классификаторов aij(x)a_{ij}(x)aij\\u200b(x), i,j=1,…,Ki, j = 1, \\\\dots, Ki,j=1,…,K, i≠ji \\\\neq ji\\ue020=j. Например, в случае с линейными моделями эти модели будут иметь вид\\nbij(x)=sgn(⟨wij,x⟩+w0,ij)b_{ij}(x) = \\\\text{sgn}\\\\left( \\\\langle w_{ij}, x \\\\rangle + w_{0,ij} \\\\right)\\nbij\\u200b(x)=sgn(⟨wij\\u200b,x⟩+w0,ij\\u200b)Классификатор aij(x)a_{ij}(x)aij\\u200b(x) будем настраивать по подвыборке Xij⊂XX_{ij} \\\\subset XXij\\u200b⊂X, содержащей только объекты классов iii и jjj. Соответственно, классификатор aij(x)a_{ij}(x)aij\\u200b(x) будет выдавать для любого объекта либо класс iii, либо класс jjj. Проиллюстрируем это для нашей выборки:\\n\\nЧтобы классифицировать новый объект, подадим его на вход каждого из построенных бинарных классификаторов. Каждый из них проголосует за свой класс; в качестве ответа выберем тот класс, за который наберется больше всего голосов:\\na(x)=argmaxk∑i=1K∑j≠iI[aij(x)=k]a(x) = \\\\text{argmax}_k\\\\sum_{i = 1}^{K} \\\\sum_{j \\\\neq i}\\\\mathbb{I}[a_{ij}(x) = k]\\na(x)=argmaxk\\u200bi=1∑K\\u200bj\\ue020=i∑\\u200bI[aij\\u200b(x)=k]Для нашего датасета получается следующая картинка:\\n\\nОбратите внимание на серый треугольник на стыке областей. Это точки, для которых голоса разделились (в данном случае каждый классификатор выдал какой-то свой класс, то есть у каждого класса было по одному голосу). Для этих точек нет явного способа выдать обоснованное предсказание.\\nМногоклассовая логистическая регрессия\\nНекоторые методы бинарной классификации можно напрямую обобщить на случай многих классов. Выясним, как это можно проделать с логистической регрессией.\\nВ логистической регрессии для двух классов мы строили линейную модель\\nb(x)=⟨w,x⟩+w0,b(x) = \\\\langle w, x \\\\rangle + w_0,\\nb(x)=⟨w,x⟩+w0\\u200b,а затем переводили её прогноз в вероятность с помощью сигмоидной функции σ(z)=11+exp\\u2061(−z)\\\\sigma(z) = \\\\frac{1}{1 + \\\\exp(-z)}σ(z)=1+exp(−z)1\\u200b. Допустим, что мы теперь решаем многоклассовую задачу и построили KKK линейных моделей\\nbk(x)=⟨wk,x⟩+w0k,b_k(x) = \\\\langle w_k, x \\\\rangle + w_{0k},\\nbk\\u200b(x)=⟨wk\\u200b,x⟩+w0k\\u200b,каждая из которых даёт оценку принадлежности объекта одному из классов. Как преобразовать вектор оценок (b1(x),…,bK(x))(b_1(x), \\\\ldots, b_K(x))(b1\\u200b(x),…,bK\\u200b(x)) в вероятности? Для этого можно воспользоваться оператором softmax(z1,…,zK)\\\\text{softmax}(z_1, \\\\ldots, z_K)softmax(z1\\u200b,…,zK\\u200b), который производит «нормировку» вектора:\\nsoftmax(z1,…,zK)=(exp\\u2061(z1)∑k=1Kexp\\u2061(zk),…,exp\\u2061(zK)∑k=1Kexp\\u2061(zk)).\\\\text{softmax}(z_1, \\\\ldots, z_K) = \\\\left(\\\\frac{\\\\exp(z_1)}{\\\\sum_{k = 1}^{K} \\\\exp(z_k)},\\n\\\\dots, \\\\frac{\\\\exp(z_K)}{\\\\sum_{k = 1}^{K} \\\\exp(z_k)}\\\\right).\\nsoftmax(z1\\u200b,…,zK\\u200b)=(∑k=1K\\u200bexp(zk\\u200b)exp(z1\\u200b)\\u200b,…,∑k=1K\\u200bexp(zk\\u200b)exp(zK\\u200b)\\u200b).В этом случае вероятность kkk-го класса будет выражаться как\\nP(y=k∣x,w)=exp\\u2061(⟨wk,x⟩+w0k)∑j=1Kexp\\u2061(⟨wj,x⟩+w0j).P(y = k \\\\vert x, w) = \\\\frac{\\n\\\\exp{(\\\\langle w_k, x \\\\rangle + w_{0k})}}{ \\\\sum_{j = 1}^{K} \\\\exp{(\\\\langle w_j, x \\\\rangle + w_{0j})}}.\\nP(y=k∣x,w)=∑j=1K\\u200bexp(⟨wj\\u200b,x⟩+w0j\\u200b)exp(⟨wk\\u200b,x⟩+w0k\\u200b)\\u200b.Обучать эти веса предлагается с помощью метода максимального правдоподобия: так же, как и в случае с двухклассовой логистической регрессией:\\n∑i=1Nlog\\u2061P(y=yi∣xi,w)→max\\u2061w1,…,wK\\\\sum_{i = 1}^{N} \\\\log P(y = y_i \\\\vert x_i, w) \\\\to \\\\max_{w_1, \\\\dots, w_K}\\ni=1∑N\\u200blogP(y=yi\\u200b∣xi\\u200b,w)→w1\\u200b,…,wK\\u200bmax\\u200bМасштабируемость линейных моделей\\nМы уже обсуждали, что SGD позволяет обучению хорошо масштабироваться по числу объектов, так как мы можем не загружать их целиком в оперативную память. А что делать, если признаков очень много, или мы не знаем заранее, сколько их будет? Такое может быть актуально, например, в следующих ситуациях:\\n\\nКлассификация текстов: мы можем представить текст в формате «мешка слов», то есть неупорядоченного набора слов, встретившихся в данном тексте, и обучить на нём, например, определение тональности отзыва в интернете. Наличие каждого слова из языка в тексте у нас будет кодироваться отдельной фичой. Тогда размерность каждого элемента обучающей выборки будет порядка нескольких сотен тысяч.\\nВ задаче предсказания кликов по рекламе можно получить выборку любой размерности, например, так: в качестве фичи закодируем индикатор того, что пользователь X побывал на веб-странице Y. Суммарная размерность тогда будет порядка 109⋅107=101610^9 \\\\cdot 10^7 = 10^{16}109⋅107=1016. Кроме того, всё время появляются новые пользователи и веб-страницы, так что на этапе применения нас ждут сюрпризы.\\n\\nЕсть несколько хаков, которые позволяют бороться с такими проблемами:\\n\\nНесмотря на то, что полная размерность объекта в выборке огромна, количество ненулевых элементов в нём невелико. Значит, можно использовать разреженное кодирование, то есть вместо плотного вектора хранить словарь, в котором будут перечислены индексы и значения ненулевых элементов вектора.\\nДаже хранить все веса не обязательно! Можно хранить их в хэш-таблице и вычислять индекс по формуле hash(feature) %  tablesize. Хэш может вычисляться прямо от слова или id пользователя. Таким образом, несколько фичей будут иметь общий вес, который тем не менее обучится оптимальным образом. Такой подход называется hashing trick. Ясно, что сжатие вектора весов приводит к потерям в качестве, но, как правило, ценой совсем небольших потерь можно сжать этот вектор на много порядков.\\n\\nПримером открытой библиотеки, в которой реализованы эти возможности, является vowpal wabbit.\\nParameter server\\nЕсли при решении задачи ставки столь высоки, что мы не можем разменивать качество на сжатие вектора весов, а признаков всё-таки очень много, то задачу можно решать распределённо, храня все признаки в шардированной хеш-таблице\\n\\nКружки здесь означают отдельные сервера. Жёлтые загружают данные, а серые хранят части модели. Для обучения жёлтый кружок запрашивает у серого нужные ему для предсказания веса, считает градиент и отправляет его обратно, где тот потом применяется. Схема обладает бесконечной масштабируемостью, но задач, где это оправдано, не очень много.\\nПодытожим\\nНа линейную модель можно смотреть как на однослойную нейросеть, поэтому многие методы, которые были изначально разработаны для них, сейчас переиспользуются в задачах глубокого обучения, а базовые подходы к регрессии, классификации и оптимизации вообще выглядят абсолютно так же. Так что несмотря на то, что в целом линейные модели на сегодня применяются редко, то, из чего они состоят и как строятся, знать очень и очень полезно.\\nНадеемся также, что главным итогом прочтения этого параграфа для вас будет осознание того, что решение любой ML-задачи состоит из выбора функции потерь, параметризованного класса моделей и способа оптимизации. В следующих параграфах мы познакомимся с другими моделями и оптимизаторами, но эти базовые принципы не изменятся.\\n\\nТеперь предлагаем вам потренировать изученный материал на практике. Скачайте ноутбук с лабораторной работой. В нём вы найдете описания заданий и дополнительные материалы. Задания из лабораторной прикреплены к этому параграфу в виде задач в системе Яндекс Контест. Чтобы проверить себя, отправляйте решения по соответствующим задачам в систему. Успехов в практике!\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф1.3. Машинное обучениеЧто такое машинное обучение и каким оно бывает. Основные понятия машинного обучения: признаки, таргеты, метрики, переобучениеСледующий параграф2.2. Метрические методыАлгоритмы KNN. Быстрый поиск ближайших соседейЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_58.html', 'title': 'Сходимость SGD'}, page_content='Сходимость SGDЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/414.1.Оптимизация в ML14.2.Проксимальные методы14.3.Методы второго порядка14.4.Сходимость SGDДоказательство сходимостиМетоды редукции дисперсии15.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Сходимость SGD14.4. Сходимость SGDАвторыГорбунов Эдуард Почему он\\xa0всё-таки сходитсяСтохастический Градиентный Спуск (SGD) имеет достаточно простую запись:\\nxk+1=xk−αkgk.x_{k+1} = x_k - \\\\alpha_k g_k.\\nxk+1\\u200b=xk\\u200b−αk\\u200bgk\\u200b.Здесь gkg_kgk\\u200b — это некоторая аппроксимация градиента целевой функции ∇f(xk)\\\\nabla f(x_k)∇f(xk\\u200b) в точке xkx_kxk\\u200b, называемая стохастическим градиентом (или просто стох. градиентом), αk>0\\\\alpha_k > 0αk\\u200b>0 — это размер шага (stepsize, learning rate) на итерации kkk. Для простоты мы будем считать, что αk=α>0\\\\alpha_k = \\\\alpha > 0αk\\u200b=α>0 для всех k≥0k \\\\geq 0k≥0. Обычно предполагается, что стох. градиент является несмещённой оценкой ∇f(xk)\\\\nabla f(x_k)∇f(xk\\u200b) при фиксированном xkx_kxk\\u200b: E(gk∣xk)=∇f(xk)\\\\mathbb{E}\\\\left(g_k \\\\mid x_k\\\\right) = \\\\nabla f(x_k)E(gk\\u200b∣xk\\u200b)=∇f(xk\\u200b).\\nДоказательство сходимости\\nЗададимся следующим вопросом: с какой скоростью и в каком смысле SGD сходится к решению и сходится ли? Во-первых, как и во многих работах по стохастической оптимизации, нас будет интересовать сходимость метода в среднем, т.е. оценки на E(14∣∣xk−x∗∣∣2)\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_\\\\ast\\\\vert\\\\vert^2\\\\right)E(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2) или E(f(xk)−f(x∗))\\\\mathbb{E}\\\\left(f(x_k) - f(x_\\\\ast)\\\\right)E(f(xk\\u200b)−f(x∗\\u200b)), где x∗x_\\\\astx∗\\u200b — решение задачи (для простоты будем считать, что оно единственное).\\nВо-вторых, чтобы SGD сходился в указанном смысле, необходимо ввести дополнительные предположения. Действительно, например, если дисперсия стох. градиента не ограничена E(14∣∣gk−∇f(xk)∣∣2∣xk)=∞\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k - \\\\nabla f(x_k)\\\\vert\\\\vert^2 \\\\mid x_k\\\\right) = \\\\inftyE(41\\u200b∣∣gk\\u200b−∇f(xk\\u200b)∣∣2∣xk\\u200b)=∞, то E(14∣∣xk−x∗∣∣2)=∞\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_\\\\ast\\\\vert\\\\vert^2\\\\right) = \\\\inftyE(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2)=∞ и никаких разумных гарантий доказать не удаётся. Поэтому дополнительно к несмещённости часто предполагается, что дисперсия равномерно ограничена: предположим, что существует такое число σ≥0\\\\sigma \\\\ge 0σ≥0, что для всех k≥0k \\\\ge 0k≥0 выполненоВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nE(14∣∣gk−∇f(xk)∣∣2∣xk)≤σ2.\\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k - \\\\nabla f(x_k)\\\\vert\\\\vert^2\\\\right| x_k\\\\right) \\\\leq \\\\sigma^2.\\nE(41\\u200b∣∣gk\\u200b−∇f(xk\\u200b)∣∣2\\u200bxk\\u200b)≤σ2.Данное предположение выполнено, например, для задачи логистической регрессии (поскольку в данной задаче норма градиентов слагаемых ограничена), но в то же время является весьма обременительным. Его можно заменить на более реалистичные предположения, что мы немного затронем далее. Однако при данном предположении анализ SGD является очень простым и полезным для дальнейших обобщений и рассуждений.\\nДля простоты везде далее мы будем считать, что функция fff является LLL-гладкой и μ\\\\muμ-сильно выпуклой, т.е. для всех x,y∈Rdx, y \\\\in \\\\mathbb{R}^dx,y∈Rd выполнены неравенства\\n∣∣∇f(x)−∇f(y)∣∣≤L∣∣x−y∣∣,    \\\\vert\\\\vert\\\\nabla f(x) - \\\\nabla f(y)\\\\vert\\\\vert \\\\leq L\\\\vert\\\\vert x-y\\\\vert\\\\vert,\\n∣∣∇f(x)−∇f(y)∣∣≤L∣∣x−y∣∣,f(y)≥f(x)+⟨∇f(x),y−x⟩+μ2∣∣y−x∣∣2.    f(y) \\\\geq f(x) + \\\\langle\\\\nabla f(x), y- x \\\\rangle + \\\\frac{\\\\mu}{2}\\\\vert\\\\vert y - x\\\\vert\\\\vert^2.\\nf(y)≥f(x)+⟨∇f(x),y−x⟩+2μ\\u200b∣∣y−x∣∣2.Теорема. Предположим, что fff является LLL-гладкой и μ\\\\muμ-сильно выпуклой, стох. градиент gkg_kgk\\u200b имеет ограниченную дисперсию, и размер шага удовлетворяет 0<α≤1/L0 < \\\\alpha \\\\leq 1/L0<α≤1/L. Тогда для всех k≥0k \\\\geq 0k≥0 выполняется неравенство\\nE(14∣∣xk−x∗∣∣2)≤(1−αμ)k∣∣x0−x∗∣∣2+ασ2μ.        \\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2\\\\right) \\\\leq (1 - \\\\alpha\\\\mu)^k\\\\vert\\\\vert x_0 - x_{\\\\ast}\\\\vert\\\\vert^2 + \\\\frac{\\\\alpha\\\\sigma^2}{\\\\mu}.\\nE(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2)≤(1−αμ)k∣∣x0\\u200b−x∗\\u200b∣∣2+μασ2\\u200b.Доказательство. Используя выражение для xk+1x_{k+1}xk+1\\u200b, мы выводим\\n∣∣xk+1−x∗∣∣2=∣∣xk−x∗−αgk∣∣2=∣∣xk−x∗∣∣2−2α⟨xk−x∗,gk⟩+α2∣∣gk∣∣2\\\\vert\\\\vert x_{k+1} - x_\\\\ast \\\\vert\\\\vert^2 = \\\\vert\\\\vert x_k - x_{\\\\ast} - \\\\alpha g_k\\\\vert\\\\vert^2\\\\\\\\\\n= \\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 - 2\\\\alpha \\\\langle x_k - x_{\\\\ast}, g_k \\\\rangle + \\\\alpha^2 \\\\vert\\\\vert g_k\\\\vert\\\\vert^2\\n∣∣xk+1\\u200b−x∗\\u200b∣∣2=∣∣xk\\u200b−x∗\\u200b−αgk\\u200b∣∣2=∣∣xk\\u200b−x∗\\u200b∣∣2−2α⟨xk\\u200b−x∗\\u200b,gk\\u200b⟩+α2∣∣gk\\u200b∣∣2Далее мы берём условное матожидание E(⋅∣xk)\\\\mathbb{E}\\\\left(\\\\cdot\\\\mid x_k\\\\right)E(⋅∣xk\\u200b) от левой и правой частей и получаем:\\nE(14∣∣xk+1−x∗∣∣2∣xk)=∣∣xk−x∗∣∣2−2αE(⟨xk−x∗,gk⟩∣xk)+α2E(14∣∣gk∣∣2∣xk)=∣∣xk−x∗∣∣2−2α⟨xk−x∗,E(gk∣xk)⟩+α2E(14∣∣gk∣∣2∣xk)=∣∣xk−x∗∣∣2−2α⟨xk−x∗,∇f(xk)⟩+α2E(14∣∣gk∣∣2∣xk).    \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2\\\\right| x_k\\\\right) = \\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 - 2\\\\alpha\\\\mathbb{E}\\\\left( \\\\langle x_k - x_{\\\\ast}, g_k \\\\rangle \\\\mid x_k\\\\right) + \\\\alpha^2 \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k\\\\vert\\\\vert^2\\\\right| x_k\\\\right)\\\\\\\\\\n    = \\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 - 2\\\\alpha\\\\left\\\\langle x_k - x_{\\\\ast}, \\\\mathbb{E}\\\\left(g_k \\\\mid x_k\\\\right) \\\\right\\\\rangle + \\\\alpha^2 \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k\\\\vert\\\\vert^2\\\\right| x_k\\\\right)\\\\\\\\\\n    = \\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 - 2\\\\alpha\\\\langle x_k - x_{\\\\ast}, \\\\nabla f(x_k)\\\\rangle + \\\\alpha^2 \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k\\\\vert\\\\vert^2\\\\right| x_k\\\\right).\\nE(41\\u200b∣∣xk+1\\u200b−x∗\\u200b∣∣2\\u200bxk\\u200b)=∣∣xk\\u200b−x∗\\u200b∣∣2−2αE(⟨xk\\u200b−x∗\\u200b,gk\\u200b⟩∣xk\\u200b)+α2E(41\\u200b∣∣gk\\u200b∣∣2\\u200bxk\\u200b)=∣∣xk\\u200b−x∗\\u200b∣∣2−2α⟨xk\\u200b−x∗\\u200b,E(gk\\u200b∣xk\\u200b)⟩+α2E(41\\u200b∣∣gk\\u200b∣∣2\\u200bxk\\u200b)=∣∣xk\\u200b−x∗\\u200b∣∣2−2α⟨xk\\u200b−x∗\\u200b,∇f(xk\\u200b)⟩+α2E(41\\u200b∣∣gk\\u200b∣∣2\\u200bxk\\u200b).Следующий шаг в доказательстве состоит в оценке второго момента E(14∣∣gk∣∣2∣xk)\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k\\\\vert\\\\vert^2 \\\\mid x_k\\\\right)E(41\\u200b∣∣gk\\u200b∣∣2∣xk\\u200b). Используя предположение об ограниченности дисперсии стох. градиента, мы выводим:\\nE(14∣∣gk∣∣2∣xk)=E(14∣∣∇f(xk)+gk−∇f(xk)∣∣2∣xk)=∣∣∇f(xk)∣∣2+E(⟨∇f(xk),gk−∇f(xk)⟩∣xk)+E(14∣∣gk−∇f(xk)∣∣2∣xk)=∣∣∇f(xk)∣∣2+⟨∇f(xk),E(gk−∇f(xk)∣xk)⟩+E(14∣∣gk−∇f(xk)∣∣2∣xk)=∣∣∇f(xk)∣∣2+E(14∣∣gk−∇f(xk)∣∣2∣xk)≤∣∣∇f(xk)∣∣2+σ2    \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k\\\\vert\\\\vert^2\\\\right| x_k\\\\right) = \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert\\\\nabla f(x_k) + g_k - \\\\nabla f(x_k)\\\\vert\\\\vert^2\\\\right| x_k\\\\right)\\\\\\\\\\n    = \\\\vert\\\\vert\\\\nabla f(x_k)\\\\vert\\\\vert^2 + \\\\mathbb{E}\\\\left(\\\\langle \\\\nabla f(x_k), g_k - \\\\nabla f(x_k) \\\\rangle \\\\mid x_k\\\\right) + \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k - \\\\nabla f(x_k)\\\\vert\\\\vert^2 \\\\right| x_k\\\\right)\\\\\\\\\\n    = \\\\vert\\\\vert\\\\nabla f(x_k)\\\\vert\\\\vert^2 + \\\\left\\\\langle \\\\nabla f(x_k), \\\\mathbb{E}\\\\left(g_k - \\\\nabla f(x_k) \\\\mid x_k\\\\right)\\\\right\\\\rangle  + \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k - \\\\nabla f(x_k)\\\\vert\\\\vert^2 \\\\right| x_k\\\\right)\\\\\\\\\\n    = \\\\vert\\\\vert\\\\nabla f(x_k)\\\\vert\\\\vert^2 + \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k - \\\\nabla f(x_k)\\\\vert\\\\vert^2 \\\\right| x_k\\\\right)\\\\\\\\\\n    \\\\leq \\\\vert\\\\vert\\\\nabla f(x_k)\\\\vert\\\\vert^2 + \\\\sigma^2\\nE(41\\u200b∣∣gk\\u200b∣∣2\\u200bxk\\u200b)=E(41\\u200b∣∣∇f(xk\\u200b)+gk\\u200b−∇f(xk\\u200b)∣∣2\\u200bxk\\u200b)=∣∣∇f(xk\\u200b)∣∣2+E(⟨∇f(xk\\u200b),gk\\u200b−∇f(xk\\u200b)⟩∣xk\\u200b)+E(41\\u200b∣∣gk\\u200b−∇f(xk\\u200b)∣∣2\\u200bxk\\u200b)=∣∣∇f(xk\\u200b)∣∣2+⟨∇f(xk\\u200b),E(gk\\u200b−∇f(xk\\u200b)∣xk\\u200b)⟩+E(41\\u200b∣∣gk\\u200b−∇f(xk\\u200b)∣∣2\\u200bxk\\u200b)=∣∣∇f(xk\\u200b)∣∣2+E(41\\u200b∣∣gk\\u200b−∇f(xk\\u200b)∣∣2\\u200bxk\\u200b)≤∣∣∇f(xk\\u200b)∣∣2+σ2Чтобы оценить сверху ∣∣∇f(xk)∣∣2\\\\vert\\\\vert\\\\nabla f(x_k)\\\\vert\\\\vert^2∣∣∇f(xk\\u200b)∣∣2, мы используем следующий факт, справедливый для любой выпуклой LLL-гладкой функции fff (см. книгу Ю. Е. Нестерова \"Методы выпуклой оптимизации\", 2010):\\n∣∣∇f(x)−∇f(y)∣∣2≤2L(f(x)−f(y)−⟨∇f(y),x−y⟩).    \\\\vert\\\\vert\\\\nabla f(x) - \\\\nabla f(y)\\\\vert\\\\vert^2 \\\\leq 2L\\\\left(f(x) - f(y) - \\\\langle \\\\nabla f(y), x- y \\\\rangle\\\\right).\\n∣∣∇f(x)−∇f(y)∣∣2≤2L(f(x)−f(y)−⟨∇f(y),x−y⟩).Беря в этом неравенстве x=xkx = x_kx=xk\\u200b, y=x∗y = x_\\\\asty=x∗\\u200b и используя ∇f(x∗)=0\\\\nabla f(x_\\\\ast) = 0∇f(x∗\\u200b)=0, получаем\\nE(14∣∣gk∣∣2∣xk)≤2L(f(xk)−f(x∗))+σ2.    \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k\\\\vert\\\\vert^2\\\\right| x_k\\\\right) \\\\leq 2L\\\\left(f(x_k) - f(x_\\\\ast)\\\\right) + \\\\sigma^2.\\nE(41\\u200b∣∣gk\\u200b∣∣2\\u200bxk\\u200b)≤2L(f(xk\\u200b)−f(x∗\\u200b))+σ2.Далее мы подставляем эту оценку в выражение для E(∣∣xk+1−x∗∣∣2∣xk)\\\\mathbb{E}\\\\left(\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2 \\\\mid x_k\\\\right)E(∣∣xk+1\\u200b−x∗\\u200b∣∣2∣xk\\u200b):\\nE(14∣∣xk+1−x∗∣∣2∣xk)≤∣∣xk−x∗∣∣2−2α⟨xk−x∗,∇f(xk)⟩+2α2L(f(xk)−f(x∗))+α2σ2.    \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2\\\\right| x_k\\\\right) \\\\leq \\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 - 2\\\\alpha\\\\langle x_k - x_{\\\\ast}, \\\\nabla f(x_k)\\\\rangle + 2\\\\alpha^2L\\\\left(f(x_k) - f(x_\\\\ast)\\\\right) + \\\\alpha^2\\\\sigma^2.\\nE(41\\u200b∣∣xk+1\\u200b−x∗\\u200b∣∣2\\u200bxk\\u200b)≤∣∣xk\\u200b−x∗\\u200b∣∣2−2α⟨xk\\u200b−x∗\\u200b,∇f(xk\\u200b)⟩+2α2L(f(xk\\u200b)−f(x∗\\u200b))+α2σ2.Остаётся оценить скалярное произведение в правой части неравенства. Это можно сделать, воспользовавшись сильной выпуклостью функции fff: из\\nf(x∗)≥f(xk)+⟨∇f(xk),x∗−xk⟩+μ2∣∣x∗−xk∣∣2    f(x_\\\\ast) \\\\geq f(x_k) + \\\\langle\\\\nabla f(x_k), x_\\\\ast - x_k \\\\rangle + \\\\frac{\\\\mu}{2}\\\\vert\\\\vert x_\\\\ast - x_k\\\\vert\\\\vert^2\\nf(x∗\\u200b)≥f(xk\\u200b)+⟨∇f(xk\\u200b),x∗\\u200b−xk\\u200b⟩+2μ\\u200b∣∣x∗\\u200b−xk\\u200b∣∣2следует\\n⟨∇f(xk),xk−x∗⟩≥f(xk)−f(x∗)+μ2∣∣xk−x∗∣∣2.    \\\\langle\\\\nabla f(x_k), x_k - x_\\\\ast \\\\rangle \\\\geq f(x_k) - f(x_\\\\ast) + \\\\frac{\\\\mu}{2}\\\\vert\\\\vert x_k - x_\\\\ast\\\\vert\\\\vert^2.\\n⟨∇f(xk\\u200b),xk\\u200b−x∗\\u200b⟩≥f(xk\\u200b)−f(x∗\\u200b)+2μ\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2.Используя это неравенство в выведенной ранее верхней оценке на E(∣∣xk+1−x∗∣∣2∣xk)\\\\mathbb{E}\\\\left(\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2\\\\mid x_k\\\\right)E(∣∣xk+1\\u200b−x∗\\u200b∣∣2∣xk\\u200b), мы приходим к следующему неравенству:\\nE(14∣∣xk+1−x∗∣∣2∣xk)≤(1−αμ)∣∣xk−x∗∣∣2−2α(1−αL)(f(xk)−f(x∗))+α2σ2≤(1−αμ)∣∣xk−x∗∣∣2+α2σ2,    \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2\\\\right| x_k\\\\right) \\\\leq (1-\\\\alpha\\\\mu)\\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 -2\\\\alpha(1 - \\\\alpha L)\\\\left(f(x_k) - f(x_\\\\ast)\\\\right) + \\\\alpha^2\\\\sigma^2\\\\\\\\\\n    \\\\leq (1-\\\\alpha\\\\mu)\\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 + \\\\alpha^2\\\\sigma^2,\\nE(41\\u200b∣∣xk+1\\u200b−x∗\\u200b∣∣2\\u200bxk\\u200b)≤(1−αμ)∣∣xk\\u200b−x∗\\u200b∣∣2−2α(1−αL)(f(xk\\u200b)−f(x∗\\u200b))+α2σ2≤(1−αμ)∣∣xk\\u200b−x∗\\u200b∣∣2+α2σ2,где в последнем неравенстве мы воспользовались неотрицательностью 2α(1−αL)(f(xk)−f(x∗))2\\\\alpha(1 - \\\\alpha L)\\\\left(f(x_k) - f(x_\\\\ast)\\\\right)2α(1−αL)(f(xk\\u200b)−f(x∗\\u200b)), что следует из 0<α≤1/L0 < \\\\alpha \\\\leq 1/L0<α≤1/L и f(xk)≥f(x∗)f(x_k) \\\\geq f(x_\\\\ast)f(xk\\u200b)≥f(x∗\\u200b). Чтобы получить результат, заявленный в теореме, нужно взять полное мат. ожидание от левой и правой частей полученного неравенства (воспользовавшись при этом крайне полезным свойством условного мат. ожидания — tower property: E(E(⋅∣xk))=E(⋅)\\\\mathbb{E}\\\\left(\\\\mathbb{E}\\\\left(\\\\cdot\\\\mid x^k\\\\right)\\\\right) = \\\\mathbb{E}\\\\left(\\\\cdot\\\\right)E(E(⋅∣xk))=E(⋅))\\nE(14∣∣xk+1−x∗∣∣2)≤(1−αμ)E(14∣∣xk−x∗∣∣2)+α2σ2,    \\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2\\\\right) \\\\leq (1-\\\\alpha\\\\mu)\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2\\\\right) + \\\\alpha^2\\\\sigma^2,\\nE(41\\u200b∣∣xk+1\\u200b−x∗\\u200b∣∣2)≤(1−αμ)E(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2)+α2σ2,а затем, применяя это неравенство для E(14∣∣xk−x∗∣∣2)\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2\\\\right)E(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2), E(14∣∣xk−1−x∗∣∣2)\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_{k-1} - x_{\\\\ast}\\\\vert\\\\vert^2\\\\right)E(41\\u200b∣∣xk−1\\u200b−x∗\\u200b∣∣2), …\\\\ldots… , E(14∣∣x1−x∗∣∣2)\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_1 - x_{\\\\ast}\\\\vert\\\\vert^2\\\\right)E(41\\u200b∣∣x1\\u200b−x∗\\u200b∣∣2), получим\\nE(14∣∣xk+1−x∗∣∣2)≤(1−αμ)k+1∣∣x0−x∗∣∣2+α2σ2∑t=0k(1−αμ)t≤(1−αμ)k+1∣∣x0−x∗∣∣2+α2σ2∑t=0∞(1−αμ)t=(1−αμ)k+1∣∣x0−x∗∣∣2+ασ2μ,\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2\\\\right) \\\\leq (1-\\\\alpha\\\\mu)^{k+1}\\\\vert\\\\vert x_0 - x_{\\\\ast}\\\\vert\\\\vert^2 + \\\\alpha^2\\\\sigma^2\\\\sum\\\\limits_{t=0}^k (1-\\\\alpha\\\\mu)^t\\\\\\\\\\n\\\\leq (1-\\\\alpha\\\\mu)^{k+1}\\\\vert\\\\vert x_0 - x_{\\\\ast}\\\\vert\\\\vert^2 + \\\\alpha^2\\\\sigma^2\\\\sum\\\\limits_{t=0}^\\\\infty (1-\\\\alpha\\\\mu)^t\\\\\\\\\\n= (1 - \\\\alpha\\\\mu)^{k+1}\\\\vert\\\\vert x_0 - x_{\\\\ast}\\\\vert\\\\vert^2 + \\\\frac{\\\\alpha\\\\sigma^2}{\\\\mu},\\nE(41\\u200b∣∣xk+1\\u200b−x∗\\u200b∣∣2)≤(1−αμ)k+1∣∣x0\\u200b−x∗\\u200b∣∣2+α2σ2t=0∑k\\u200b(1−αμ)t≤(1−αμ)k+1∣∣x0\\u200b−x∗\\u200b∣∣2+α2σ2t=0∑∞\\u200b(1−αμ)t=(1−αμ)k+1∣∣x0\\u200b−x∗\\u200b∣∣2+μασ2\\u200b,что и требовалось доказать.\\nДанный результат утверждает, что SGD с потоянным шагом сходится линейно к окрестности решения, радиус которой пропорционален ασμ\\\\tfrac{\\\\sqrt{\\\\alpha} \\\\sigma}{\\\\sqrt{\\\\mu}}μ\\u200bα\\u200bσ\\u200b. Отметим, что чем больше размер шага α\\\\alphaα, тем быстрее SGD достигает некоторой окрестности решения, в которой продолжает осциллировать. Однако чем больше размер шага, тем больше эта окрестность. Соответственно, чтобы найти более точное решение, необходимо уменьшать размер шага в SGD. Этот феномен хорошо проиллюстрирован здесь.\\nТеорема выше доказана при достаточно обременительных предположениях: мы предположили, что функция является сильно выпуклой, LLL-гладкой и стох. градиент имеет равномерно ограниченную дисперсию. В практически интересных задачах данные условия (в данном виде) выполняются крайне редко. Тем не менее, выводы, которые мы сделали из доказанной теоремы, справедливы для многих задач, не удовлетворяющих введённым предположениям (во многом потому, что указанные свойства важны лишь на некотором компакте вокруг решения задачи, что в свою очередь не так и обременительно).\\nБолее того, если мы сделаем немного другое предположение о стохастических градиентах, то сможем покрыть некоторые случаи, когда дисперсия не является равномерно ограниченной на всём пространстве. Предположим теперь, что gk=∇fξk(xk)g_k = \\\\nabla f_{\\\\xi_k}(x_k)gk\\u200b=∇fξk\\u200b\\u200b(xk\\u200b), где ξk\\\\xi_kξk\\u200b просэмплировано из некоторого распределения D\\\\cal DD независимо от предыдущих итераций, f(x)=Eξ∼D(fξ(x))f(x) = \\\\mathbb{E}_{\\\\xi\\\\sim \\\\cal D}\\\\left(f_{\\\\xi}(x)\\\\right)f(x)=Eξ∼D\\u200b(fξ\\u200b(x)) и fξ(x)f_{\\\\xi}(x)fξ\\u200b(x) является выпуклой и LξL_{\\\\xi}Lξ\\u200b-гладкой для всех ξ\\\\xiξ (данное предположение тоже можно ослабить, но для простоты изложения остановимся именно на такой формулировке). Будем называть данные условия предположением о выпуклых гладких стохастчиеских реализациях. Они выполнены, например, для задач линейно регрессии и логистической регрессии.\\nВ таком случае, для точек, сгенерированных SGD, справедливо, что SGD с потоянным шагом сходится линейно к окрестности решения, радиус которой пропорционален ασμ\\\\tfrac{\\\\sqrt{\\\\alpha} \\\\sigma}{\\\\sqrt{\\\\mu}}μ\\u200bα\\u200bσ\\u200b. Отметим, что чем больше размер шага α\\\\alphaα, тем быстрее SGD достигает некоторой окрестности решения, в которой продолжает осциллировать. Однако чем больше размер шага, тем больше эта окрестность. Соответственно, чтобы найти более точное решение, необходимо уменьшать размер шага в SGD. Этот феномен хорошо проиллюстрирован здесь.\\nТеорема. Предположим, что fff является LLL-гладкой и μ\\\\muμ-сильно выпуклой, стохастчиеские реализации являются выпуклыми и гладкими, и размер шага удовлетворяет 0<α≤1/2Lmax\\u20610 < \\\\alpha \\\\leq 1/2L_{\\\\max}0<α≤1/2Lmax\\u200b, где Lmax\\u2061=max\\u2061ξ∼DLξL_{\\\\max} = \\\\max_{\\\\xi\\\\sim \\\\cal D} L_{\\\\xi}Lmax\\u200b=maxξ∼D\\u200bLξ\\u200b. Тогда для всех k≥0k \\\\geq 0k≥0 выполняется неравенство\\nE(14∣∣xk−x∗∣∣2)≤(1−αμ)k∣∣x0−x∗∣∣2+2ασ∗2μ,    \\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2\\\\right) \\\\leq (1 - \\\\alpha\\\\mu)^k\\\\vert\\\\vert x_0 - x_{\\\\ast}\\\\vert\\\\vert^2 + \\\\frac{2\\\\alpha\\\\sigma_\\\\ast^2}{\\\\mu},\\nE(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2)≤(1−αμ)k∣∣x0\\u200b−x∗\\u200b∣∣2+μ2ασ∗2\\u200b\\u200b,где σ∗2=Eξ∼D∣∣∇fξ(x∗)∣∣2\\\\sigma_\\\\ast^2 = \\\\mathbb{E}_{\\\\xi\\\\sim \\\\cal D}\\\\vert\\\\vert\\\\nabla f_{\\\\xi}(x_\\\\ast)\\\\vert\\\\vert^2σ∗2\\u200b=Eξ∼D\\u200b∣∣∇fξ\\u200b(x∗\\u200b)∣∣2.\\nДоказательство. Аналогично предыдущей доказательству предыдущей теоремы, получаем\\nE(14∣∣xk+1−x∗∣∣2∣xk)=∣∣xk−x∗∣∣2−2α⟨xk−x∗,∇f(xk)⟩+α2E(14∣∣gk∣∣2∣xk).    \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2\\\\right| x_k\\\\right) = \\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 - 2\\\\alpha\\\\langle x_k - x_{\\\\ast}, \\\\nabla f(x_k)\\\\rangle + \\\\alpha^2 \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k\\\\vert\\\\vert^2\\\\right| x_k\\\\right).\\nE(41\\u200b∣∣xk+1\\u200b−x∗\\u200b∣∣2\\u200bxk\\u200b)=∣∣xk\\u200b−x∗\\u200b∣∣2−2α⟨xk\\u200b−x∗\\u200b,∇f(xk\\u200b)⟩+α2E(41\\u200b∣∣gk\\u200b∣∣2\\u200bxk\\u200b).Поскольку fξ(x)f_{\\\\xi}(x)fξ\\u200b(x) является выпуклой и LξL_\\\\xiLξ\\u200b-гладкой, имеем (см. книгу Ю. Е. Нестерова \"Методы выпуклой оптимизации\", 2010):\\n∣∣∇fξ(x)−∇fξ(y)∣∣2≤2Lξ(fξ(x)−fξ(y)−⟨∇fξ(y),x−y⟩).    \\\\vert\\\\vert\\\\nabla f_{\\\\xi}(x) - \\\\nabla f_{\\\\xi}(y)\\\\vert\\\\vert^2 \\\\leq 2L_{\\\\xi}\\\\left(f_{\\\\xi}(x) - f_{\\\\xi}(y) - \\\\langle \\\\nabla f_{\\\\xi}(y), x - y \\\\rangle\\\\right).\\n∣∣∇fξ\\u200b(x)−∇fξ\\u200b(y)∣∣2≤2Lξ\\u200b(fξ\\u200b(x)−fξ\\u200b(y)−⟨∇fξ\\u200b(y),x−y⟩).Применяя это неравенство для x=xkx = x_kx=xk\\u200b, y=x∗y = x_{\\\\ast}y=x∗\\u200b, получаем\\nE(14∣∣gk∣∣2∣xk)=Eξk∼D(∣∣∇fξk(xk)−∇fξk(x∗)+∇fξk(x∗)∣∣2)≤2Eξk∼D(∣∣∇fξk(xk)−∇fξk(x∗)∣∣2)+2Eξk∼D(∣∣∇fξk(x∗)∣∣2)≤Eξk∼D(4Lξk(fξk(xk)−fξk(x∗)−⟨∇fξk(x∗),xk−x∗⟩))+2Eξ∼D(∣∣∇fξk(x∗)∣∣2)≤4Lmax\\u2061(f(xk)−f(x∗)−⟨∇f(x∗),xk−x∗⟩)+2σ∗2=4Lmax\\u2061(f(xk)−f(x∗))+2σ∗2,    \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k\\\\vert\\\\vert^2\\\\right| x_k\\\\right) = \\\\mathbb{E}_{\\\\xi_k \\\\sim \\\\mathcal{D}}\\\\left(\\\\vert\\\\vert\\\\nabla f_{\\\\xi_k}(x_k) - \\\\nabla f_{\\\\xi_k}(x_\\\\ast) + \\\\nabla f_{\\\\xi_k}(x_\\\\ast)\\\\vert\\\\vert^2\\\\right)\\\\\\\\\\n    \\\\leq 2\\\\mathbb{E}_{\\\\xi_k \\\\sim \\\\mathcal{D}}\\\\left(\\\\vert\\\\vert\\\\nabla f_{\\\\xi_k}(x_k) - \\\\nabla f_{\\\\xi_k}(x_\\\\ast)\\\\vert\\\\vert^2\\\\right) + 2\\\\mathbb{E}_{\\\\xi_k \\\\sim \\\\mathcal{D}}\\\\left(\\\\vert\\\\vert\\\\nabla f_{\\\\xi_k}(x_\\\\ast)\\\\vert\\\\vert^2\\\\right)\\\\\\\\\\n    \\\\leq \\\\mathbb{E}_{\\\\xi_k \\\\sim \\\\mathcal{D}}\\\\left(4L_{\\\\xi_k}\\\\left(f_{\\\\xi_k}(x_k) - f_{\\\\xi_k}(x_\\\\ast) - \\\\langle \\\\nabla f_{\\\\xi_k}(x_\\\\ast), x_k - x_\\\\ast \\\\rangle\\\\right)\\\\right) + 2\\\\mathbb{E}_{\\\\xi \\\\sim \\\\mathcal{D}}\\\\left(\\\\vert\\\\vert\\\\nabla f_{\\\\xi_k}(x_\\\\ast)\\\\vert\\\\vert^2\\\\right)\\\\\\\\\\n    \\\\leq 4L_{\\\\max} \\\\left(f(x_k) - f(x_\\\\ast) - \\\\langle \\\\nabla f(x_\\\\ast), x_k - x_\\\\ast\\\\rangle\\\\right) + 2\\\\sigma_{\\\\ast}^2 \\\\\\\\\\n    = 4L_{\\\\max} \\\\left(f(x_k) - f(x_\\\\ast)\\\\right) + 2\\\\sigma_{\\\\ast}^2,\\nE(41\\u200b∣∣gk\\u200b∣∣2\\u200bxk\\u200b)=Eξk\\u200b∼D\\u200b(∣∣∇fξk\\u200b\\u200b(xk\\u200b)−∇fξk\\u200b\\u200b(x∗\\u200b)+∇fξk\\u200b\\u200b(x∗\\u200b)∣∣2)≤2Eξk\\u200b∼D\\u200b(∣∣∇fξk\\u200b\\u200b(xk\\u200b)−∇fξk\\u200b\\u200b(x∗\\u200b)∣∣2)+2Eξk\\u200b∼D\\u200b(∣∣∇fξk\\u200b\\u200b(x∗\\u200b)∣∣2)≤Eξk\\u200b∼D\\u200b(4Lξk\\u200b\\u200b(fξk\\u200b\\u200b(xk\\u200b)−fξk\\u200b\\u200b(x∗\\u200b)−⟨∇fξk\\u200b\\u200b(x∗\\u200b),xk\\u200b−x∗\\u200b⟩))+2Eξ∼D\\u200b(∣∣∇fξk\\u200b\\u200b(x∗\\u200b)∣∣2)≤4Lmax\\u200b(f(xk\\u200b)−f(x∗\\u200b)−⟨∇f(x∗\\u200b),xk\\u200b−x∗\\u200b⟩)+2σ∗2\\u200b=4Lmax\\u200b(f(xk\\u200b)−f(x∗\\u200b))+2σ∗2\\u200b,где во втором переходе мы воспользовались стандартным фактом: ∣∣a+b∣∣2≤∣∣a∣∣2+∣∣b∣∣2\\\\vert\\\\vert a+b\\\\vert\\\\vert^2 \\\\leq \\\\vert\\\\vert a\\\\vert\\\\vert^2 + \\\\vert\\\\vert b\\\\vert\\\\vert^2∣∣a+b∣∣2≤∣∣a∣∣2+∣∣b∣∣2 для любых a,b∈Rna, b \\\\in \\\\mathbb{R}^na,b∈Rn. Подставим полученное неравенство в выражение для E(∣∣xk+1−x∗∣∣2∣xk)\\\\mathbb{E}\\\\left(\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2 \\\\mid x_k\\\\right)E(∣∣xk+1\\u200b−x∗\\u200b∣∣2∣xk\\u200b), доказанное ранее:\\nE(14∣∣xk+1−x∗∣∣2∣xk)=∣∣xk−x∗∣∣2−2α⟨xk−x∗,∇f(xk)⟩+4Lmax\\u2061α2(f(xk)−f(x∗))+2α2σ∗2.    \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2\\\\right| x_k\\\\right) = \\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 - 2\\\\alpha\\\\langle x_k - x_{\\\\ast}, \\\\nabla f(x_k)\\\\rangle + 4L_{\\\\max}\\\\alpha^2 \\\\left(f(x_k) - f(x_\\\\ast)\\\\right) + 2\\\\alpha^2\\\\sigma_{\\\\ast}^2.\\nE(41\\u200b∣∣xk+1\\u200b−x∗\\u200b∣∣2\\u200bxk\\u200b)=∣∣xk\\u200b−x∗\\u200b∣∣2−2α⟨xk\\u200b−x∗\\u200b,∇f(xk\\u200b)⟩+4Lmax\\u200bα2(f(xk\\u200b)−f(x∗\\u200b))+2α2σ∗2\\u200b.Остаётся оценить скалярное произведение в правой части неравенства. Это можно сделать, воспользовавшись сильной выпуклостью функции fff: из\\nf(x∗)≥f(xk)+⟨∇f(xk),x∗−xk⟩+μ2∣∣x∗−xk∣∣2    f(x_\\\\ast) \\\\geq f(x_k) + \\\\langle\\\\nabla f(x_k), x_\\\\ast - x_k \\\\rangle + \\\\frac{\\\\mu}{2}\\\\vert\\\\vert x_\\\\ast - x_k\\\\vert\\\\vert^2\\nf(x∗\\u200b)≥f(xk\\u200b)+⟨∇f(xk\\u200b),x∗\\u200b−xk\\u200b⟩+2μ\\u200b∣∣x∗\\u200b−xk\\u200b∣∣2следует\\n⟨∇f(xk),xk−x∗⟩≥f(xk)−f(x∗)+μ2∣∣xk−x∗∣∣2.    \\\\langle\\\\nabla f(x_k), x_k - x_\\\\ast \\\\rangle \\\\geq f(x_k) - f(x_\\\\ast) + \\\\frac{\\\\mu}{2}\\\\vert\\\\vert x_k - x_\\\\ast\\\\vert\\\\vert^2.\\n⟨∇f(xk\\u200b),xk\\u200b−x∗\\u200b⟩≥f(xk\\u200b)−f(x∗\\u200b)+2μ\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2.Используя это неравенство в выведенной ранее верхней оценке на E(∣∣xk+1−x∗∣∣2∣xk)\\\\mathbb{E}\\\\left(\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2 \\\\mid x_k\\\\right)E(∣∣xk+1\\u200b−x∗\\u200b∣∣2∣xk\\u200b), мы приходим к следующему неравенству:\\nE(14∣∣xk+1−x∗∣∣2∣xk)≤(1−αμ)∣∣xk−x∗∣∣2−2α(1−2αLmax\\u2061)(f(xk)−f(x∗))+2α2σ2≤(1−αμ)∣∣xk−x∗∣∣2+2α2σ∗2,    \\\\mathbb{E}\\\\left(\\\\left.\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_{k+1} - x_\\\\ast\\\\vert\\\\vert^2\\\\right| x_k\\\\right) \\\\leq (1-\\\\alpha\\\\mu)\\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 -2\\\\alpha(1 - 2\\\\alpha L_{\\\\max})\\\\left(f(x_k) - f(x_\\\\ast)\\\\right) + 2\\\\alpha^2\\\\sigma^2\\\\\\\\\\n    \\\\leq (1-\\\\alpha\\\\mu)\\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2 + 2\\\\alpha^2\\\\sigma_{\\\\ast}^2,\\nE(41\\u200b∣∣xk+1\\u200b−x∗\\u200b∣∣2\\u200bxk\\u200b)≤(1−αμ)∣∣xk\\u200b−x∗\\u200b∣∣2−2α(1−2αLmax\\u200b)(f(xk\\u200b)−f(x∗\\u200b))+2α2σ2≤(1−αμ)∣∣xk\\u200b−x∗\\u200b∣∣2+2α2σ∗2\\u200b,где в последнем неравенстве мы воспользовались неотрицательностью 2α(1−2αLmax\\u2061)(f(xk)−f(x∗))2\\\\alpha(1 - 2\\\\alpha L_{\\\\max})\\\\left(f(x_k) - f(x_\\\\ast)\\\\right)2α(1−2αLmax\\u200b)(f(xk\\u200b)−f(x∗\\u200b)), что следует из 0<α≤1/2Lmax\\u20610 < \\\\alpha \\\\leq 1/2L_{\\\\max}0<α≤1/2Lmax\\u200b и f(xk)≥f(x∗)f(x_k) \\\\geq f(x_\\\\ast)f(xk\\u200b)≥f(x∗\\u200b). Действуя по аналогии с доказательством предыдущей теоремы, получаем требуемый результат.\\nВыводы, которые можно сделать из данной теоремы, очень похожи на те, что мы уже сделали из прошлой теоремы. Главные отличия заключаются в том, что Lmax\\u2061L_{\\\\max}Lmax\\u200b может быть гораздо больше LLL, т.е. максимальный допустимый размер шага α\\\\alphaα в данной теореме может быть гораздо меньше, чем в предыдущей. Однако размер окрестности теперь зависит от дисперсии стох. градиента в решении σ∗2\\\\sigma_\\\\ast^2σ∗2\\u200b, что может быть значительно меньше σ2\\\\sigma^2σ2.\\nРассмотрим важный частный случай — задачи минимизации суммы функций:\\nmin\\u2061x∈Rd{f(x)=1n∑i=1nfi(x)}.\\\\min\\\\limits_{x \\\\in \\\\mathbb{R}^d}\\\\left\\\\{f(x) = \\\\frac{1}{n}\\\\sum\\\\limits_{i=1}^n f_i(x)\\\\right\\\\}.\\nx∈Rdmin\\u200b{f(x)=n1\\u200bi=1∑n\\u200bfi\\u200b(x)}.Обычно fi(x)f_i(x)fi\\u200b(x) имеет смысл функции потерь на iii-м объекте датасета. Предположим, что fi(x)f_i(x)fi\\u200b(x) — выпуклая и LiL_iLi\\u200b-гладкая функция. Тогда выполняется предположение о выпуклых гладких стохастчиеских реализациях: действительно, достаточно задать ξ\\\\xiξ как случайное число из {1,2,…,n}\\\\{1,2,\\\\ldots,n\\\\}{1,2,…,n}, имеющее равномерное распределение. Тогда справедлив результат предыдущей теоремы с Lmax\\u2061=max\\u2061i∈1,…,n)LiL_{\\\\max} = \\\\max_{i\\\\in 1,\\\\ldots,n)}L_iLmax\\u200b=maxi∈1,…,n)\\u200bLi\\u200b и σ∗2=1n∑i=1n∣∣∇fi(x∗)∣∣2\\\\sigma_\\\\ast^2 = \\\\tfrac{1}{n}\\\\sum_{i=1}^n \\\\vert\\\\vert\\\\nabla f_i(x_\\\\ast)\\\\vert\\\\vert^2σ∗2\\u200b=n1\\u200b∑i=1n\\u200b∣∣∇fi\\u200b(x∗\\u200b)∣∣2.\\nДля любого K≥0K \\\\ge 0K≥0 можно выбрать шаг в SGD следующим образом:\\nесли\\xa0K≤2Lmax\\u2061μ,γk=12Lmax\\u2061,если\\xa0K>2Lmax\\u2061μ\\xa0и\\xa0k<k0,γk=12Lmax\\u2061,если\\xa0K>2Lmax\\u2061μ\\xa0и\\xa0k≥k0,γk=14Lmax\\u2061+μ(k−k0),    \\\\text{если } K \\\\leq \\\\frac{2L_{\\\\max}}{\\\\mu},  \\\\gamma_k = \\\\frac{1}{2L_{\\\\max}},\\\\\\\\\\n    \\\\text{если } K > \\\\frac{2L_{\\\\max}}{\\\\mu} \\\\text{ и } k < k_0,  \\\\gamma_k = \\\\frac{1}{2L_{\\\\max}},\\\\\\\\\\n    \\\\text{если } K > \\\\frac{2L_{\\\\max}}{\\\\mu} \\\\text{ и } k \\\\geq k_0,  \\\\gamma_k = \\\\frac{1}{4L_{\\\\max} + \\\\mu(k-k_0)},\\nесли\\xa0K≤μ2Lmax\\u200b\\u200b,γk\\u200b=2Lmax\\u200b1\\u200b,если\\xa0K>μ2Lmax\\u200b\\u200b\\xa0и\\xa0k<k0\\u200b,γk\\u200b=2Lmax\\u200b1\\u200b,если\\xa0K>μ2Lmax\\u200b\\u200b\\xa0и\\xa0k≥k0\\u200b,γk\\u200b=4Lmax\\u200b+μ(k−k0\\u200b)1\\u200b,где k0=⌈K/2⌉k_0 = \\\\lceil K/2 \\\\rceilk0\\u200b=⌈K/2⌉. Тогда из доказанного выше результата следует (см. Лемму 3 из статьи С. Стиха), что после KKK итераций\\nE(14∣∣xK−x∗∣∣2)=O(Lmax\\u2061∣∣x0−x∗∣∣2μexp\\u2061(−μLmax\\u2061K)+σ∗2μ2K).    \\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_K - x_\\\\ast\\\\vert\\\\vert^2\\\\right) = \\\\cal O\\\\left(\\\\frac{L_{\\\\max} \\\\vert\\\\vert x_0 - x_\\\\ast\\\\vert\\\\vert^2}{\\\\mu}\\\\exp\\\\left(-\\\\frac{\\\\mu}{L_{\\\\max}}K\\\\right) + \\\\frac{\\\\sigma_{\\\\ast}^2}{\\\\mu^2 K}\\\\right).\\nE(41\\u200b∣∣xK\\u200b−x∗\\u200b∣∣2)=O(μLmax\\u200b∣∣x0\\u200b−x∗\\u200b∣∣2\\u200bexp(−Lmax\\u200bμ\\u200bK)+μ2Kσ∗2\\u200b\\u200b).Таким образом, чтобы гарантировать E(14∣∣xK−x∗∣∣2)≤ε\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_K - x_\\\\ast\\\\vert\\\\vert^2\\\\right) \\\\leq \\\\varepsilonE(41\\u200b∣∣xK\\u200b−x∗\\u200b∣∣2)≤ε, SGD требуется\\nO(Lmax\\u2061μlog\\u2061(Lmax\\u2061∣∣x0−x∗∣∣2με)+σ∗2μ2ε)  \\\\cal O\\\\left(\\\\frac{L_{\\\\max} }{\\\\mu}\\\\log\\\\left(\\\\frac{L_{\\\\max} \\\\vert\\\\vert x_0 - x_\\\\ast\\\\vert\\\\vert^2}{\\\\mu\\\\varepsilon}\\\\right) + \\\\frac{\\\\sigma_{\\\\ast}^2}{\\\\mu^2 \\\\varepsilon}\\\\right)\\nO(μLmax\\u200b\\u200blog(μεLmax\\u200b∣∣x0\\u200b−x∗\\u200b∣∣2\\u200b)+μ2εσ∗2\\u200b\\u200b)итераций/подсчётов градиентов слагаемых. Чтобы гарантировать то же самое, градиентному спуску (GD) необходимо сделать\\nO(nLμlog\\u2061(∣∣x0−x∗∣∣2ε))  \\\\cal O\\\\left(n\\\\frac{L}{\\\\mu}\\\\log\\\\left(\\\\frac{\\\\vert\\\\vert x_0 - x_\\\\ast\\\\vert\\\\vert^2}{\\\\varepsilon}\\\\right)\\\\right)\\nO(nμL\\u200blog(ε∣∣x0\\u200b−x∗\\u200b∣∣2\\u200b))подсчётов градиентов слагаемых, поскольку каждая итерация GD требует nnn подсчётов градиентов слагаемых (нужно вычислять полный градиент ∇f(x)=1n∑i=1n∇fi(x)\\\\nabla f(x) = \\\\tfrac{1}{n}\\\\sum_{i=1}^n \\\\nabla f_i(x)∇f(x)=n1\\u200b∑i=1n\\u200b∇fi\\u200b(x)). Можно показать, что L≤Lmax\\u2061≤nLL \\\\leq L_{\\\\max} \\\\leq nLL≤Lmax\\u200b≤nL, поэтому в худшем случае полученная оценка для SGD заведомо хуже, чем для GD. Однако в случае, когда Lmax\\u2061=O(L)L_{\\\\max} = \\\\cal O(L)Lmax\\u200b=O(L), однозначного вывода сделать нельзя: при большом ε\\\\varepsilonε может доминировать первое слагаемое в оценке сложности SGD, поэтому в таком случае SGD будет доказуемо быстрее, чем GD (если пренебречь логарифмическими множителями).\\nИными словами, чтобы достичь не очень большой точности решения, выгоднее использовать SGD, чем GD. В ряде ситуаций небольшой точности вполне достаточно, но так происходит не всегда. Поэтому возникает ествественный вопрос: можно ли так модифицировать SGD, чтобы полученный метод сходился линейно асимптотически к точному решению (а не к окрестности как SGD), но при этом стоимость его итераций была сопоставима со стоимостью итераций SGD? Оказывается, что да и соответствующие методы называются методами редукции дисперсии.\\nМетоды редукции дисперсии\\nПеред тем, как мы начнём говорить о методах редукции дисперсии, хотелось бы раскрыть подробнее причину того, что SGD не сходится линейно асимптотически к точному решению. Мы рассмотрели анализ SGD в двух предположениях, и в обоих случаях нам требовалось вывести некоторую верхнюю оценку на второй момент стох. градиента, т.е. на E(∣∣gk∣∣2∣xk)\\\\mathbb{E}\\\\left(\\\\vert\\\\vert g_k\\\\vert\\\\vert^2 \\\\mid x_k\\\\right)E(∣∣gk\\u200b∣∣2∣xk\\u200b). В обоих случаях эта оценка содержала некоторый константный член (σ2\\\\sigma^2σ2 или 2σ∗22\\\\sigma_\\\\ast^22σ∗2\\u200b — зависит от рассматриваемого предположения), который потом возникал и в финальной оценке на E(14∣∣xk−x∗∣∣2)\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_\\\\ast\\\\vert\\\\vert^2\\\\right)E(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2), препятствуя тем самым линейно сходимости метода. Конечно, это рассуждение существенно опирается на конкретный способ анализа метода, а потому не является строгим объяснением, почему SGD не сходится линейно.\\nОднако важно отметить, что оценка на E(∣∣gk∣∣2∣xk)\\\\mathbb{E}\\\\left(\\\\vert\\\\vert g_k\\\\vert\\\\vert^2 \\\\mid x_k\\\\right)E(∣∣gk\\u200b∣∣2∣xk\\u200b) достаточно точно отражает поведение метода вблизи решения: даже если точка xkx_kxk\\u200b оказалась по какой-то причине близка к решению x∗x_\\\\astx∗\\u200b (или даже просто совпала с решением), то E(∣∣gk∣∣2∣xk)\\\\mathbb{E}\\\\left(\\\\vert\\\\vert g_k\\\\vert\\\\vert^2 \\\\mid x_k\\\\right)E(∣∣gk\\u200b∣∣2∣xk\\u200b) и, в частности, E(∣∣gk−∇f(xk)∣∣2∣xk)\\\\mathbb{E}\\\\left(\\\\vert\\\\vert g_k - \\\\nabla f(x_k)\\\\vert\\\\vert^2 \\\\mid x_k\\\\right)E(∣∣gk\\u200b−∇f(xk\\u200b)∣∣2∣xk\\u200b) будут порядка σ2\\\\sigma^2σ2 или σ∗2\\\\sigma_{\\\\ast}^2σ∗2\\u200b. Следовательно, при следующем шаге метод с большой вероятностью отдалится от/выйдет из решения, поскольку E(14∣∣xk+1−xk∣∣2)=α2E(14∣∣gk∣∣2)∼α2σ2\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_{k+1} - x_k\\\\vert\\\\vert^2\\\\right) = \\\\alpha^2\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k\\\\vert\\\\vert^2\\\\right) \\\\sim \\\\alpha^2\\\\sigma^2E(41\\u200b∣∣xk+1\\u200b−xk\\u200b∣∣2)=α2E(41\\u200b∣∣gk\\u200b∣∣2)∼α2σ2 или α2σ∗2\\\\alpha^2\\\\sigma_{\\\\ast}^2α2σ∗2\\u200b.\\nИз приведённых выше рассуждений видно, что дисперсия стох. градиента мешает методу сходится линейно к точному решению. Поэтому хотелось бы как-то поменять правило вычисления стох. градиента, чтобы выполнялись 3 важных свойства: (1) новый стох. градиент должен быть не сильно дороже в плане вычислений, чем подсчёт стох. градиента в SGD (градиента слагаемого), (2) новый стох. градиент должен быть несмещённой оценкой полного градиента ∇f(xk)\\\\nabla f(x_k)∇f(xk\\u200b), и (3) дисперсия нового стох. градиента должна уменьшаться в процессе работы метода. Например, можно рассмотреть следующий стох. градиент:\\ngk=∇fjk(xk)+sk,g_k = \\\\nabla f_{j_k}(x_k) + s_k, \\ngk\\u200b=∇fjk\\u200b\\u200b(xk\\u200b)+sk\\u200b,где jkj_kjk\\u200b выбирается случайно равновероятно из множества {1,2,…,n}\\\\{1, 2, \\\\ldots, n\\\\}{1,2,…,n} и E(sk∣xk)=0\\\\mathbb{E}\\\\left(s_k\\\\mid x_k\\\\right) = 0E(sk\\u200b∣xk\\u200b)=0. В таком случае, будет выполнено свойство (2) из списка выше. Чтобы достичь желаемой цели, необходимо как-то специфицировать выбор случайного вектора sks_ksk\\u200b. Исторически одним из первых способов выбора sks_ksk\\u200b был sk=−∇fjk(wk)+∇f(wk)s_k = -\\\\nabla f_{j_k}(w_k) + \\\\nabla f(w_k)sk\\u200b=−∇fjk\\u200b\\u200b(wk\\u200b)+∇f(wk\\u200b), где точка wkw_kwk\\u200b обновляется раз в m∼nm \\\\sim nm∼n итераций:\\nwk+1={wk,if\\xa0k+1mod\\u2009\\u2009m≠0,xk+1,if\\xa0k+1mod\\u2009\\u2009m=0.    w_{k+1} = \\\\begin{cases} w_k, & \\\\text{if } k+1 \\\\mod m \\\\neq 0,\\\\\\\\ x_{k+1}, & \\\\text{if } k+1 \\\\mod m = 0. \\\\end{cases}\\nwk+1\\u200b={wk\\u200b,xk+1\\u200b,\\u200bif\\xa0k+1modm\\ue020=0,if\\xa0k+1modm=0.\\u200bДанный метод называется Stochastic Variance Reduced Gradient (SVRG). Данный методы был предложен и проанализирован в NeurIPS статье Джонсона и Жанга в 2013 году. Теперь же убедимся, что метод удовлетворяет всем трём отмеченным свойствам. Начнём с несмещённости:\\ngk=∇fjk(xk)−∇fjk(wk)+∇f(wk),    g_k = \\\\nabla f_{j_k}(x_k) - \\\\nabla f_{j_k}(w_k) + \\\\nabla f(w_k),\\ngk\\u200b=∇fjk\\u200b\\u200b(xk\\u200b)−∇fjk\\u200b\\u200b(wk\\u200b)+∇f(wk\\u200b),E(gk∣xk)=1n∑i=1n(∇fi(xk)−∇fi(wk)+∇f(wk))=∇f(xk).    \\\\mathbb{E}\\\\left(g_k\\\\mid x_k\\\\right) = \\\\frac{1}{n}\\\\sum\\\\limits_{i=1}^n \\\\left(\\\\nabla f_{i}(x_k) - \\\\nabla f_{i}(w_k) + \\\\nabla f(w_k)\\\\right) = \\\\nabla f(x_k).\\nE(gk\\u200b∣xk\\u200b)=n1\\u200bi=1∑n\\u200b(∇fi\\u200b(xk\\u200b)−∇fi\\u200b(wk\\u200b)+∇f(wk\\u200b))=∇f(xk\\u200b).Далее, вычисление gkg_kgk\\u200b подразумевает 2 подсчёта градентов слагаемых при kmod\\u2009\\u2009m≠0k \\\\mod m \\\\neq 0kmodm\\ue020=0 и n+2n+2n+2 подсчёта градентов слагаемых при kmod\\u2009\\u2009m≠0k \\\\mod m \\\\neq 0kmodm\\ue020=0. Таким образом, за mmm последователльных итераций SVRG происходит вычисление 2(m−1)+n+2=2m+n2(m-1) + n + 2 = 2m + n2(m−1)+n+2=2m+n градиентов слагаемых, в то время как SGD требуется mmm подсчётов градиентов слагаемых. Если m=nm = nm=n (стандартный выбор параметра mmm), то nnn итераций SVRG лишь в 3 раза дороже, чем nnn итераций SGD. Иными словами, в среднем итерация SVRG не сильно дороже итерации SGD.\\nНаконец, если мы предположим, что метод сходится E(14∣∣xk−x∗∣∣2)→0\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_\\\\ast\\\\vert\\\\vert^2\\\\right) \\\\to 0E(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2)→0 (а он действительно сходится, см., например, доказательство вот тут), то получим, что E(14∣∣wk−x∗∣∣2)→0\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert w_k - x_\\\\ast\\\\vert\\\\vert^2\\\\right) \\\\to 0E(41\\u200b∣∣wk\\u200b−x∗\\u200b∣∣2)→0, а значит E(14∣∣xk−wk∣∣2)→0\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - w_k\\\\vert\\\\vert^2\\\\right) \\\\to 0E(41\\u200b∣∣xk\\u200b−wk\\u200b∣∣2)→0 и E(14∣∣∇f(wk)∣∣2)→0\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert\\\\nabla f(w_k)\\\\vert\\\\vert^2\\\\right) \\\\to 0E(41\\u200b∣∣∇f(wk\\u200b)∣∣2)→0. Но тогда в силу Липшицевости градиентов fif_ifi\\u200b для всех i=1,…,ni=1,\\\\ldots,ni=1,…,n имеем:\\nE(14∣∣gk∣∣2)=E(14∣∣∇fjk(xk)−∇fjk(wk)+∇f(wk)∣∣2)≤2E(14∣∣∇fjk(xk)−∇fjk(wk)∣∣2)+2E(14∣∣∇f(wk)∣∣2)≤2Lmax\\u2061E(14∣∣xk−wk∣∣2)+2E(14∣∣∇f(wk)∣∣2)→0,    \\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert g_k\\\\vert\\\\vert^2\\\\right) = \\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert\\\\nabla f_{j_k}(x_k) - \\\\nabla f_{j_k}(w_k) + \\\\nabla f(w_k)\\\\vert\\\\vert^2\\\\right)\\\\\\\\\\n    \\\\leq 2\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert\\\\nabla f_{j_k}(x_k) - \\\\nabla f_{j_k}(w_k)\\\\vert\\\\vert^2\\\\right) + 2\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert\\\\nabla f(w_k)\\\\vert\\\\vert^2\\\\right)\\\\\\\\\\n    \\\\leq 2L_{\\\\max} \\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - w_k\\\\vert\\\\vert^2\\\\right) + 2\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert\\\\nabla f(w_k)\\\\vert\\\\vert^2\\\\right) \\\\to 0,\\nE(41\\u200b∣∣gk\\u200b∣∣2)=E(41\\u200b∣∣∇fjk\\u200b\\u200b(xk\\u200b)−∇fjk\\u200b\\u200b(wk\\u200b)+∇f(wk\\u200b)∣∣2)≤2E(41\\u200b∣∣∇fjk\\u200b\\u200b(xk\\u200b)−∇fjk\\u200b\\u200b(wk\\u200b)∣∣2)+2E(41\\u200b∣∣∇f(wk\\u200b)∣∣2)≤2Lmax\\u200bE(41\\u200b∣∣xk\\u200b−wk\\u200b∣∣2)+2E(41\\u200b∣∣∇f(wk\\u200b)∣∣2)→0,а значит, дисперсия gkg_kgk\\u200b стремится к нулю.\\nПриведённые выше рассуждения не являются формальным доказательством сходимости метода, но частично объясняют, почему метод сходится и, самое главное, объясняют интуицию позади формул, задающих метод. Строгое доказательство можно прочитать вот тут. Мы же здесь приведём результат о сходимости немного другого метода — Loopless Stochastic Variance Reduced Gradient (L-SVRG), который был предложен в 2015 году и переоткрыт в 2019 году. Основное отличие от SVRG состоит в том, что точка wkw_kwk\\u200b теперь обновляется на каждой итерации с некоторой маленькой вероятностью p∼1/np \\\\sim 1/np∼1/n:\\nwk+1={wk,с\\xa0вероятностью\\xa01−p,xk,с\\xa0вероятностью\\xa0p.    w_{k+1} = \\\\begin{cases} w_k, & \\\\text{с вероятностью } 1-p,\\\\\\\\ x_{k}, & \\\\text{с вероятностью } p. \\\\end{cases}\\nwk+1\\u200b={wk\\u200b,xk\\u200b,\\u200bс\\xa0вероятностью\\xa01−p,с\\xa0вероятностью\\xa0p.\\u200bИными словами, L-SVRG имеет случайную длину цикла, в котором wkw_kwk\\u200b не обновляется. Вся интуиция и все наблюдения приведённые для SVRG выше, справедливы и для L-SVRG.\\nМожно доказать следующий результат.\\nТеорема. Предположим, что fff является LLL-гладкой, μ\\\\muμ-сильно выпуклой и имеет вид суммы, функции fif_ifi\\u200b являются выпуклыми и LiL_iLi\\u200b-гладкими для всех i=1,…,ni=1,\\\\ldots, ni=1,…,n, и размер шага удовлетворяет 0<α≤1/6Lmax\\u20610 < \\\\alpha \\\\leq 1/6L_{\\\\max}0<α≤1/6Lmax\\u200b, где Lmax\\u2061=max\\u2061i∈1,…,nLiL_{\\\\max} = \\\\max_{i\\\\in 1,\\\\ldots,n} L_{i}Lmax\\u200b=maxi∈1,…,n\\u200bLi\\u200b. Тогда для всех k≥0k \\\\geq 0k≥0 для итераций L-SVRG выполняется неравенство\\nE(14∣∣xk−x∗∣∣2)≤(1−min\\u2061{αμ,p2})kV0,        \\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2\\\\right) \\\\leq \\\\left(1 - \\\\min\\\\left\\\\{\\\\alpha\\\\mu, \\\\frac{p}{2}\\\\right\\\\}\\\\right)^kV_0,\\nE(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2)≤(1−min{αμ,2p\\u200b})kV0\\u200b,где V0=∣∣x0−x∗∣∣2+4α2pσ02V_0 = \\\\vert\\\\vert x_0 - x_\\\\ast\\\\vert\\\\vert^2 + \\\\tfrac{4\\\\alpha^2}{p}\\\\sigma_0^2V0\\u200b=∣∣x0\\u200b−x∗\\u200b∣∣2+p4α2\\u200bσ02\\u200b, σ02=1n∑i=1n∣∣∇fi(x0)−∇fi(x∗)∣∣\\\\sigma_0^2 = \\\\tfrac{1}{n}\\\\sum_{i=1}^n \\\\vert\\\\vert\\\\nabla f_i(x_0) - \\\\nabla f_i(x_\\\\ast)\\\\vert\\\\vertσ02\\u200b=n1\\u200b∑i=1n\\u200b∣∣∇fi\\u200b(x0\\u200b)−∇fi\\u200b(x∗\\u200b)∣∣.\\nЗамечание. В частности, если α=1/6Lmax\\u2061\\\\alpha = 1/6L_{\\\\max}α=1/6Lmax\\u200b и p=1/np = 1/np=1/n, то\\nE(14∣∣xk−x∗∣∣2)≤(1−min\\u2061{μ6Lmax\\u2061,12n})kV0.    \\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_{\\\\ast}\\\\vert\\\\vert^2\\\\right) \\\\leq \\\\left(1 - \\\\min\\\\left\\\\{\\\\frac{\\\\mu}{6L_{\\\\max}}, \\\\frac{1}{2n}\\\\right\\\\}\\\\right)^kV_0.\\nE(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2)≤(1−min{6Lmax\\u200bμ\\u200b,2n1\\u200b})kV0\\u200b.Следовательно, чтобы гарантировать E(14∣∣xk−x∗∣∣2)≤ε\\\\mathbb{E}\\\\left(\\\\vphantom{\\\\frac14}\\\\vert\\\\vert x_k - x_\\\\ast\\\\vert\\\\vert^2\\\\right) \\\\leq \\\\varepsilonE(41\\u200b∣∣xk\\u200b−x∗\\u200b∣∣2)≤ε, L-SVRG требуется\\nO((n+Lmax\\u2061μ)log\\u2061(V0ε))  \\\\cal O\\\\left(\\\\left(n + \\\\frac{L_{\\\\max} }{\\\\mu}\\\\right)\\\\log\\\\left(\\\\frac{V_0}{\\\\varepsilon}\\\\right)\\\\right)\\nO((n+μLmax\\u200b\\u200b)log(εV0\\u200b\\u200b))итераций/подсчётов градиентов слагаемых (в среднем). Напомним, что чтобы гарантировать то же самое, градиентному спуску (GD) необходимо сделать\\nO(nLμlog\\u2061(L∣∣x0−x∗∣∣2με))  \\\\cal O\\\\left(n\\\\frac{L}{\\\\mu}\\\\log\\\\left(\\\\frac{L \\\\vert\\\\vert x_0 - x_\\\\ast\\\\vert\\\\vert^2}{\\\\mu\\\\varepsilon}\\\\right)\\\\right)\\nO(nμL\\u200blog(μεL∣∣x0\\u200b−x∗\\u200b∣∣2\\u200b))подсчётов градиентов слагаемых, поскольку каждая итерация GD требует nnn подсчётов градиентов слагаемых (нужно вычислять полный градиент ∇f(x)=1n∑i=1n∇fi(x)\\\\nabla f(x) = \\\\tfrac{1}{n}\\\\sum_{i=1}^n \\\\nabla f_i(x)∇f(x)=n1\\u200b∑i=1n\\u200b∇fi\\u200b(x)). Можно показать, что L≤Lmax\\u2061≤nLL \\\\leq L_{\\\\max} \\\\leq nLL≤Lmax\\u200b≤nL, поэтому в худшем случае полученная оценка для L-SVRG не лучше, чем для GD. Однако в случае, когда Lmax\\u2061=O(L)L_{\\\\max} = \\\\cal O(L)Lmax\\u200b=O(L), L-SVRG имеет сложность значительно лучше, чем GD (если пренебречь логарифмическими множителями).\\nВ заключение этого раздела, хотелось бы отметить, что существуют и другие методы редукции дисперсии. Одним из самых популярных среди них является SAGA. В отличие от SVRG/L-SVRG, в методе SAGA хранится набор градиентов ∇f1(wk1),∇f2(wk2),…,∇fn(wkn)\\\\nabla f_1(w_k^1), \\\\nabla f_2(w_k^2), \\\\ldots, \\\\nabla f_n(w_k^n)∇f1\\u200b(wk1\\u200b),∇f2\\u200b(wk2\\u200b),…,∇fn\\u200b(wkn\\u200b). Здесь точка wkiw_k^iwki\\u200b обозначает точку, в которой в последний раз был подсчитан градиент функции iii до итерации kkk. Формально это можно записать следующим образом:\\nw01=w02=…=w0n,    w_0^1 = w_0^2 = \\\\ldots = w_0^n,\\nw01\\u200b=w02\\u200b=…=w0n\\u200b,gk=∇fjk(xk)−∇fjk(wkjk)+1n∑i=1n∇fi(wki),    g_k = \\\\nabla f_{j_k}(x_k) - \\\\nabla f_{j_k}(w_k^{j_k}) + \\\\frac{1}{n}\\\\sum\\\\limits_{i=1}^n \\\\nabla f_{i}(w_k^{i}),\\ngk\\u200b=∇fjk\\u200b\\u200b(xk\\u200b)−∇fjk\\u200b\\u200b(wkjk\\u200b\\u200b)+n1\\u200bi=1∑n\\u200b∇fi\\u200b(wki\\u200b),wk+1jk=xk,wk+1i=wki\\xa0для\\xa0всех\\xa0i≠jk,    w_{k+1}^{j_k} = x_k, \\\\quad w_{k+1}^i = w_k^i \\\\text{ для всех } i \\\\neq j_k,\\nwk+1jk\\u200b\\u200b=xk\\u200b,wk+1i\\u200b=wki\\u200b\\xa0для\\xa0всех\\xa0i\\ue020=jk\\u200b,xk+1=xk−αgk.    x_{k+1} = x_k - \\\\alpha g_k.\\nxk+1\\u200b=xk\\u200b−αgk\\u200b.Основное преимущество SAGA состоит в том, что не требуется вычислять полный градиент всей суммы по ходу работы метода, однако в начале требуется посчитать градиенты всех слагаемых (отмечаем здесь, что эта операция может быть гораздо дороже по времени, чем вычисление полного градиента) и, более того, требуется хранить nnn векторов, что может быть недопустимо для больших датасетов. В плане теоретических гарантий SAGA и L-SVRG не отличимы.\\nНиже приведён график с траекториями SGD (с постоянным шагом), L-SVRG и SAGA при решении задачи логистической регрессии. Как можно видеть из графика, SGD достаточно быстро достигает не очень высокой точности и начинает осциллировать вокруг решения. В то же время, L-SVRG и SAGA достигают той же точности медленнее, но зато не осциллируют вокруг решения, а продолжают сходится (причём линейно).\\nСравнение работы SGD, L-SVRG и SAGA при решении задачи логистической регрессии на датасете gisette из библиотеки LIBVSM.\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф14.3. Методы второго порядкаОт\\xa0метода Ньютона до\\xa0LBFGSСледующий параграф15.1. Введение в онлайн-обучениеЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_19.html', 'title': 'Модели с латентными переменными'}, page_content='Модели с латентными переменнымиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/74.1.Вероятностный подход в ML4.2.Экспоненциальный класс распределений и принцип максимальной энтропии4.3.Обобщённые линейные модели4.4.Как оценивать вероятности4.5.Генеративный подход к классификации4.6.Байесовский подход к оцениванию4.7.Модели с латентными переменнымиЗачем нужны модели с латентными переменнымиСмеси распределенийКак генерировать из смеси распределенийМодели со скрытыми переменнымиEM-алгоритмРазделение смеси гауссианВероятностный PCA5.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Модели с латентными переменными4.7. Модели с латентными переменнымиАвторыУшаков РоманЗачем нужны модели с латентными переменными\\nПредположим, что мы делаем анализ данных для банка, и нам предоставили данные о годовых зарплатах клиентов.\\n\\nВ этом графике заметны три моды, которым, наверное, соответствуют три кластера клиентов. Неопытный аналитик мог бы проигнорировать это и попытаться описать график в отчете для руководства с помощью двух чисел — средней зарплаты и стандартного отклонения зарплат.\\nОднако данные с гистограммы ниже имеют точно такое же среднее и стандартное отклонение, как и мультимодальные данные выше. Распределение совсем другое, правда?Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nОчевидно, что графики выглядят совершенно по-разному, и правильная интерпретация первого графика может принести бизнесу дополнительные деньги (скажем, если банк научится предлагать клиентам из каждого кластера более кастомизированные предложения). Так что не надо пытаться описывать мультимодальные данные с помощью унимодальных распределений.\\n\\nПроблема заключается в том, что нам неизвестно, к какому кластеру относится каждый клиент, и неизвестны характеристики кластеров — как же их тогда описать?\\nДля каждого кластера можно попытаться задать свои параметры (среднее и дисперсию). Но как определить, из какого кластера конкретный клиент в выборке? Более того, один клиент может, например, «на 0.7» относиться к одному кластеру и «на 0.3» к другому.\\n\\nКак решать такую задачу «мягкой» кластеризации («мягкой», потому что один объект может относиться к нескольким кластерам)? Мы могли бы действовать итерационно. Сначала зададим начальное приближение на параметры распределений.\\nНапример, в нашем случае с клиентами банка из графика можно предположить, что среднее для первой кластера 30, для второго — 40, для третьей — 50, а стандартные отклонения у всех равняются, скажем, 10. Зная эти начальные параметры, мы можем для каждого клиента посчитать степень принадлежности к каждому из трёх кластеров (важно не забыть отнормировать эти числа, чтобы их сумма действительно равнялась единице).\\nДальше мы бы могли пересчитать наши средние и дисперсии, «взвешивая» вклад объектов пропорционально степени их принадлежности к каждому кластеру, и таким образом уточнить средние и дисперсии для всех трёх кластеров. Повторяя эти два шага последовательно, мы получили бы средние и дисперсии кластеров, а для каждого объекта — степени принадлежности к кластерам. Это — EM-алгоритм, подробнее о нём мы поговорим ниже.\\nКстати, оказывается, что и метод кластеризации K-средних во многом сродни EM-алгоритму (и на самом деле представляет собой его предельный случай). Действительно, мы сначала случайно расставляем центры кластеров.\\nЗатем мы для каждого объекта пересчитываем расстояние до центра каждого кластера, после чего получаем «вес» объекта в каждом кластере («вес» в том смысле, что чем ближе объект к центру кластера, тем больше этот объект учитывается при пересчете центра этого кластера) через нормировку расстояний.\\nТеперь, если мы применяем настоящий метод K-средних, то приписываем объект к кластеру с самым большим «весом», после чего опять пересчитываем центры кластеров и потом опять измеряем вес для каждого объекта кластера. Строгое же применение EM-алгоритма даёт «мягкую» версию метода K-средних.\\nСмеси распределений\\nГоворят, что распределение p(x)p(x)p(x) является смесью распределений, если его плотность имеет вид\\np(x)=∑k=1Kπkpk(x),∑k=1Kπk=1,πk≥0,p(x)\\n=\\n\\\\sum_{k = 1}^{K} \\\\pi_k p_k(x),\\n\\\\qquad\\n\\\\sum_{k = 1}^{K} \\\\pi_k = 1,\\n\\\\qquad\\n\\\\pi_k \\\\geq 0,\\np(x)=k=1∑K\\u200bπk\\u200bpk\\u200b(x),k=1∑K\\u200bπk\\u200b=1,πk\\u200b≥0,где:\\n\\nKKK — число компонент;\\nπk\\\\pi_kπk\\u200b — вероятности компонент;\\npk(x)p_k(x)pk\\u200b(x) — функции правдоподобия, то есть функции вероятности компонент (в дискретном случае) или их плотности (в абсолютно непрерывном случае).\\n\\nПроиллюстрируем это понятие на примере с банком. Будем считать, что распределения компонент смеси принадлежат некоторому параметрическому семейству: pk(x)=ϕ(x∣θk)p_k(x) = \\\\phi ( x \\\\vert \\\\theta_k )pk\\u200b(x)=ϕ(x∣θk\\u200b) (например, гауссовскому с параметром θk=(μk,σk)\\\\theta_k = (\\\\mu_k, \\\\sigma_k)θk\\u200b=(μk\\u200b,σk\\u200b)).\\nМы можем говорить, что каждое из распределений pk(x)p_k(x)pk\\u200b(x) задаёт свой кластер, причём каждый кластер имеет некоторую априорную вероятность πk\\\\pi_kπk\\u200b. Если у нас нет дополнительных данных, разумно положить πk=13\\\\pi_k = \\\\frac{1}{3}πk\\u200b=31\\u200b.\\nЕсли же нам, к примеру, известно, что какой-нибудь кластер описывает сравнительно малочисленную группу людей, эти вероятности окажутся различными. Таким образом, мы проинтерпретировали нашу мягкую кластеризацию в терминах смеси распределений.\\nКак генерировать из смеси распределений\\nРассмотрим следующий эксперимент: сначала из дискретного распределения  {π1,…,πK}\\\\{\\\\pi_1, \\\\dots, \\\\pi_K\\\\}{π1\\u200b,…,πK\\u200b} выбирается номер kkk, а затем из распределения ϕ(x∣θk)\\\\phi(x \\\\vert \\\\theta_k)ϕ(x∣θk\\u200b) выбирается значение xxx. Покажем, что распределение переменной xxx будет представлять собой смесь.\\nВведем скрытую переменную zzz, отвечающую за то, к какой компоненте смеси будет относиться очередной xxx. Пусть она представляет собой KKK-мерный бинарный случайный вектор, ровно одна компонента которого равна единице:\\nz∈{0,1}K,∑k=1Kzk=1.z \\\\in \\\\{0, 1\\\\}^K,\\n\\\\qquad\\n\\\\sum_{k = 1}^{K} z_k = 1.\\nz∈{0,1}K,k=1∑K\\u200bzk\\u200b=1.Вероятность того, что единице будет равна kkk-я компонента, положим равной πk\\\\pi_kπk\\u200b:\\np(zk=1)=πk.p(z_k = 1) = \\\\pi_k.\\np(zk\\u200b=1)=πk\\u200b.Запишем распределение сразу всего вектора:\\np(z)=∏k=1Kπkzk.p(z) = \\\\prod_{k = 1}^{K} \\\\pi_k^{z_k}.\\np(z)=k=1∏K\\u200bπkzk\\u200b\\u200b.Теперь, когда номер компоненты смеси известен, сгенерируем xxx из распределения ϕ(x∣θk)\\\\phi(x \\\\vert \\\\theta_k)ϕ(x∣θk\\u200b):\\np(x∣zk=1)=ϕ(x∣θk),p(x \\\\vert z_k = 1)\\n=\\n\\\\phi(x \\\\vert \\\\theta_k),\\np(x∣zk\\u200b=1)=ϕ(x∣θk\\u200b),или, что то же самое,\\np(x∣z)=∏k=1K[ϕ(x∣θk)]zk.p(x \\\\vert z)\\n=\\n\\\\prod_{k = 1}^{K}\\n\\\\Bigl[\\n    \\\\phi(x \\\\vert \\\\theta_k)\\n\\\\Bigr]^{z_k}.\\np(x∣z)=k=1∏K\\u200b[ϕ(x∣θk\\u200b)]zk\\u200b.Проверим, что xxx имеет нужное нам распределение. Запишем совместное распределение переменных xxx и zzz:\\np(x,z)=p(z)p(x∣z)=∏k=1K[πkϕ(x∣θk)]zk.p(x, z)\\n=\\np(z) p(x \\\\vert z)\\n=\\n\\\\prod_{k = 1}^{K}\\n\\\\Bigl[\\n    \\\\pi_k \\\\phi(x \\\\vert \\\\theta_k)\\n\\\\Bigr]^{z_k}.\\np(x,z)=p(z)p(x∣z)=k=1∏K\\u200b[πk\\u200bϕ(x∣θk\\u200b)]zk\\u200b.Чтобы найти распределение переменной xxx, нужно избавиться от скрытой переменной:\\np(x)=∑zp(x,z).p(x)\\n=\\n\\\\sum_{z} p(x, z).\\np(x)=z∑\\u200bp(x,z).Суммирование здесь ведется по всем возможным значениям zzz, то есть по всем KKK-мерным бинарным векторам с одной единицей:\\np(x)=∑zp(x,z)=∑k=1Kπkϕ(x∣θk).p(x)\\n=\\n\\\\sum_{z} p(x, z)\\n=\\n\\\\sum_{k = 1}^{K}\\n\\\\pi_k \\\\phi(x \\\\vert \\\\theta_k).\\np(x)=z∑\\u200bp(x,z)=k=1∑K\\u200bπk\\u200bϕ(x∣θk\\u200b).Мы получили, что распределение сгенерированной переменной xxx в описанном эксперименте представляет собой смесь KKK компонент.\\nМодели со скрытыми переменными\\nРассмотрим вероятностную модель с наблюдаемыми переменными XXX и параметрами Θ\\\\ThetaΘ, для которой задано правдоподобие log\\u2061p(X∣Θ)\\\\log p(X \\\\vert \\\\Theta)logp(X∣Θ).\\nПредположим, что в модели также существуют скрытые переменные ZZZ, описывающие её внутреннее состояние и, возможно, недоступные для непосредственного наблюдения (как то, к какому из кластеров относится клиент). Тогда правдоподобие log\\u2061p(X∣Θ)\\\\log p(X \\\\vert \\\\Theta)logp(X∣Θ) называется неполным, а правдоподобие log\\u2061p(X,Z∣Θ)\\\\log p(X, Z \\\\vert \\\\Theta)logp(X,Z∣Θ) — полным. Они связаны соотношением\\nlog\\u2061p(X∣Θ)=log\\u2061{∑Zp(X,Z∣Θ)}.\\\\log p(X \\\\vert \\\\Theta) = \\\\log \\\\Biggl\\\\{ \\\\sum_{Z} p(X, Z \\\\vert \\\\Theta) \\\\Biggr\\\\}.\\nlogp(X∣Θ)=log{Z∑\\u200bp(X,Z∣Θ)}.Нашей основной целью будет создать хорошую модель XXX, то есть оценить параметры Θ\\\\ThetaΘ. И оказывается, что с помощью введения скрытых переменных нередко удаётся существенно упростить правдоподобие и эффективно решить задачу.\\nРассмотрим пример со смесями распределений. В качестве наблюдаемых переменных здесь выступает выборка X={x1,…,xℓ}X = \\\\{x_1, \\\\dots, x_\\\\ell\\\\}X={x1\\u200b,…,xℓ\\u200b}, в качестве скрытых переменных Z={z1,…,zℓ}Z = \\\\{z_1, \\\\dots, z_\\\\ell\\\\}Z={z1\\u200b,…,zℓ\\u200b} — номера компонент, из которых сгенерированы объекты (здесь каждый из ziz_izi\\u200b — KKK-мерный вектор), в качестве параметров — априорные вероятности и параметры компонент Θ=(π1,…,πK,θ1,…,θK)\\\\Theta = (\\\\pi_1, \\\\dots, \\\\pi_K, \\\\theta_1, \\\\dots, \\\\theta_K)Θ=(π1\\u200b,…,πK\\u200b,θ1\\u200b,…,θK\\u200b).\\nНеполное правдоподобие выглядит так:\\nlog\\u2061p(X∣Θ)=∑i=1llog\\u2061{∑k=1Kπkp(xi∣θk)}.\\\\log p(X \\\\vert \\\\Theta) = \\\\sum_{i = 1}^{l} \\\\log \\\\Biggl\\\\{\\\\sum_{k = 1}^{K} \\\\pi_k p(x_i \\\\vert \\\\theta_k) \\\\Biggr\\\\}.\\nlogp(X∣Θ)=i=1∑l\\u200blog{k=1∑K\\u200bπk\\u200bp(xi\\u200b∣θk\\u200b)}.Правдоподобие здесь имеет вид логарифма суммы. Если приравнять нулю его градиент, то получатся сложные уравнения, не имеющие аналитического решения. Данное правдоподобие сложно вычислять: оно не является выпуклым (а точнее, вогнутым) и может иметь много локальных экстремумов, поэтому применение обычных итерационных методов для его непосредственной максимизации приводит к медленной сходимости.\\nРассмотрим теперь полное правдоподобие:\\nlog\\u2061p(X,Z∣Θ)=∑i=1lp(xi,zi∣Θ)=∑i=1l∑k=1Kzik{log\\u2061πk+log\\u2061ϕ(xi∣θk)}.\\\\log p(X, Z \\\\vert \\\\Theta) = \\\\sum_{i = 1}^{l}p(x_i, z_i\\\\vert\\\\Theta) = \\\\sum_{i = 1}^{l} \\\\sum_{k = 1}^{K} z_{ik} \\\\Bigl\\\\{ \\\\log \\\\pi_k + \\\\log \\\\phi(x_i \\\\vert \\\\theta_k)\\n\\\\Bigr\\\\}.\\nlogp(X,Z∣Θ)=i=1∑l\\u200bp(xi\\u200b,zi\\u200b∣Θ)=i=1∑l\\u200bk=1∑K\\u200bzik\\u200b{logπk\\u200b+logϕ(xi\\u200b∣θk\\u200b)}.Оно имеет вид суммы логарифмов, и это позволяет аналитически найти оценки максимального правдоподобия на параметры Θ\\\\ThetaΘ при известных переменных XXX и ZZZ. В общем случае ZZZ также стараются выбирать таким способом, чтобы распределение p(X,Z∣Θ)p(X, Z \\\\vert \\\\Theta)p(X,Z∣Θ) оказалось «лучше» исходного. В каком именно смысле, мы увидим дальше.\\nПроблема, впрочем, заключается в том, что нам не известны скрытые переменные ZZZ, поэтому их необходимо оценивать одновременно с параметрами, что никак не легче максимизации неполного правдоподобия. Осуществить это позволяет EM-алгоритм.\\nEM-алгоритм\\nEM-алгоритм решает задачу максимизации полного правдоподобия путём попеременной оптимизации по параметрам и по скрытым переменным.\\nОпишем сначала наивный способ оптимизации. Зафиксируем некоторое начальное приближение для параметров Θold\\\\Theta^{\\\\text{old}}Θold. При известных наблюдаемых переменных XXX и параметрах Θold\\\\Theta^{\\\\text{old}}Θold мы можем оценить скрытые переменные, найдя их наиболее правдоподобные значения:\\nZ∗=argmax\\u2061Zp(Z∣X,Θold)=argmax\\u2061Zp(X,Z∣Θold).Z^* = \\\\underset{Z}{\\\\operatorname{arg max}} p(Z \\\\vert X, \\\\Theta^\\\\text{old}) = \\\\underset{Z}{\\\\operatorname{arg max}} p(X, Z \\\\vert \\\\Theta^\\\\text{old}).\\nZ∗=Zargmax\\u200bp(Z∣X,Θold)=Zargmax\\u200bp(X,Z∣Θold).Зная скрытые переменные, мы можем теперь найти следующее приближение для параметров:\\nΘnew=argmax\\u2061Θp(X,Z∗∣Θ).\\\\Theta^\\\\text{new} = \\\\underset{\\\\Theta}{\\\\operatorname{arg max}} p(X, Z^* \\\\vert \\\\Theta).\\nΘnew=Θargmax\\u200bp(X,Z∗∣Θ).Повторяя итерации до сходимости, мы получим некоторый итоговый вектор параметров Θ*\\\\Theta^\\\\text{*}Θ*.\\nДанная процедура, однако, далека от идеальной — и ниже мы предложим решение, которое приводит к более качественным результатам.\\nВоспользуемся байесовским подходом. Точечные оценки параметров несут меньше информации, чем их распределение; учтём это и будем оптимизировать не ZZZ, а условное распределение ZZZ.\\nКак и прежде, зафиксируем вектор параметров Θold\\\\Theta^\\\\text{old}Θold, но вместо точечной оценки вычислим апостериорное распределение на скрытых переменных p(Z∣X,Θold)p(Z \\\\vert X, \\\\Theta^\\\\text{old})p(Z∣X,Θold), которое будет в некотором смысле оптимальным образом описывать распределение ZZZ при известных XXX и Θ\\\\ThetaΘ. В этом заключается E-шаг EM-алгоритма.\\nОтметим, что вычислить p(Z∣XΘ)p(Z \\\\vert X \\\\Theta)p(Z∣XΘ) аналитически возможно не для всех распределений, и скрытые переменные стоит подбирать так, чтобы это всё-таки получилось.\\nТеперь мы должны произвести оптимизацию по Θ\\\\ThetaΘ. Для этого возьмём логарифм полного правдоподобия log\\u2061p(X,Z∣Θ)\\\\log p(X, Z \\\\vert \\\\Theta)logp(X,Z∣Θ) и усредним его по всем возможным значениям скрытых переменных ZZZ:\\nQ(Θ,Θold)=EZ∼p(Z∣X,Θold)log\\u2061p(X,Z∣Θ)=∑Zp(Z∣X,Θold)log\\u2061p(X,Z∣Θ)Q(\\\\Theta, \\\\Theta^\\\\text{old}) = \\\\mathbb{E}_{Z \\\\sim p(Z \\\\vert X, \\\\Theta^\\\\text{old})} \\\\log p(X, Z \\\\vert \\\\Theta) = \\\\sum_{Z} p(Z \\\\vert X, \\\\Theta^\\\\text{old}) \\\\log p(X, Z \\\\vert \\\\Theta)\\nQ(Θ,Θold)=EZ∼p(Z∣X,Θold)\\u200blogp(X,Z∣Θ)=Z∑\\u200bp(Z∣X,Θold)logp(X,Z∣Θ)Формально говоря, мы нашли матожидание логарифма полного правдоподобия по апостериорному распределению на скрытых переменных.\\nНа M-шаге новый вектор параметров находится как максимизатор данного матожидания:\\nΘnew=argmax\\u2061ΘQ(Θ,Θold)=argmax\\u2061Θ∑Zp(Z∣X,Θold)log\\u2061p(X,Z∣Θ).\\\\Theta^\\\\text{new} = \\\\underset{\\\\Theta}{\\\\operatorname{arg max}} Q(\\\\Theta, \\\\Theta^\\\\text{old}) = \\\\underset{\\\\Theta}{\\\\operatorname{arg max}} \\\\sum_{Z} p(Z \\\\vert X, \\\\Theta^\\\\text{old}) \\\\log p(X, Z \\\\vert \\\\Theta).\\nΘnew=Θargmax\\u200bQ(Θ,Θold)=Θargmax\\u200bZ∑\\u200bp(Z∣X,Θold)logp(X,Z∣Θ).EM-алгоритм состоит в чередовании E-шага и M-шага.\\nМожно показать, что такой итерационной процесс всегда не уменьшает правдоподобие и сходится.\\nВ двух словах: почему это работаетНапомним, что наша цель — найти параметры Θ\\\\ThetaΘ, максимизирующие log\\u2061p(X∣Θ)\\\\log p(X \\\\vert \\\\Theta)logp(X∣Θ). Преобразуем этот логарифм правдоподобия, введя некоторое (пока произвольное) распределение q(Z)q(Z)q(Z) на латентных переменных:\\nlog\\u2061p(X∣Θ)=(∑Zq(Z)dZ)⋅log\\u2061p(X∣Θ)=\\\\log p(X \\\\vert \\\\Theta) = \\\\left(\\\\sum_Zq(Z)dZ\\\\right)\\\\cdot \\\\log p(X \\\\vert \\\\Theta) = \\nlogp(X∣Θ)=(Z∑\\u200bq(Z)dZ)⋅logp(X∣Θ)==∑Zq(Z)log\\u2061p(X∣Θ)dZ=∑Zq(Z)log\\u2061p(X,Z∣Θ)p(Z∣X,Θ)dZ= \\\\sum_Zq(Z)\\\\log p(X \\\\vert \\\\Theta)dZ = \\\\sum_Zq(Z)\\\\log{\\\\frac{p(X, Z \\\\vert \\\\Theta)}{p(Z \\\\vert X, \\\\Theta)}}dZ\\n=Z∑\\u200bq(Z)logp(X∣Θ)dZ=Z∑\\u200bq(Z)logp(Z∣X,Θ)p(X,Z∣Θ)\\u200bdZЗдесь мы применили то, что p(b)=p(a,b)p(a∣b)p(b) = \\\\frac{p(a, b)}{p(a\\\\vert b)}p(b)=p(a∣b)p(a,b)\\u200b в силу равенства p(a∣b)=p(a,b)p(b)p(a\\\\vert b) = \\\\frac{p(a, b)}{p(b)}p(a∣b)=p(b)p(a,b)\\u200b. Далее, домножим числитель и знаменатель под логарифмом на q(Z)q(Z)q(Z):\\n=∑Zq(Z)log\\u2061p(X,Z∣Θ)p(Z∣X,Θ)dZ=∑Zq(Z)log\\u2061p(X,Z∣Θ)q(Z)p(Z∣X,Θ)q(Z)dZ== \\\\sum_Zq(Z)\\\\log{\\\\frac{p(X, Z \\\\vert \\\\Theta)}{p(Z \\\\vert X, \\\\Theta)}}dZ = \\n\\\\sum_Zq(Z)\\\\log{\\\\frac{p(X, Z \\\\vert \\\\Theta)q(Z)}{p(Z \\\\vert X, \\\\Theta)q(Z)}}dZ ==Z∑\\u200bq(Z)logp(Z∣X,Θ)p(X,Z∣Θ)\\u200bdZ=Z∑\\u200bq(Z)logp(Z∣X,Θ)q(Z)p(X,Z∣Θ)q(Z)\\u200bdZ==∑Zq(Z)log\\u2061p(X,Z∣Θ)q(Z)dZ⏟=:L(q,Θ)+∑Zq(Z)log\\u2061q(Z)p(Z∣X,Θ)dZ⏟=KL(q(Z)∥p(Z∣X,Θ))= \\\\underbrace{\\\\sum_Zq(Z)\\\\log{\\\\frac{p(X, Z \\\\vert \\\\Theta)}{q(Z)}}dZ}_{=:\\\\mathcal{L}(q, \\\\Theta)} +\\n\\\\underbrace{\\\\sum_Zq(Z)\\\\log{\\\\frac{q(Z)}{p(Z \\\\vert X, \\\\Theta)}}dZ}_{=KL(q(Z)\\\\parallel p(Z \\\\vert X, \\\\Theta))}==:L(q,Θ)Z∑\\u200bq(Z)logq(Z)p(X,Z∣Θ)\\u200bdZ\\u200b\\u200b+=KL(q(Z)∥p(Z∣X,Θ))Z∑\\u200bq(Z)logp(Z∣X,Θ)q(Z)\\u200bdZ\\u200b\\u200bВторое слагаемое — это расстояние Кульбака-Лейблера между распределениями q(Z)q(Z)q(Z) и p(Z∣X,Θ)p(Z \\\\vert X, \\\\Theta)p(Z∣X,Θ). Так как расстояние Кульбака-Лейблера всегда неотрицательно, мы получаем, что log\\u2061p(X∣Θ)⩾L(q,Θ)\\\\log p(X \\\\vert \\\\Theta) \\\\geqslant \\\\mathcal{L}(q, \\\\Theta)logp(X∣Θ)⩾L(q,Θ). Первое слагаемое, L(q,Θ)\\\\mathcal{L}(q, \\\\Theta)L(q,Θ), называется вариационной нижней оценкой на log\\u2061p(X∣Θ)\\\\log p(X \\\\vert \\\\Theta)logp(X∣Θ), и, максимизируя его, мы будем также увеличивать и всю сумму — поэтому в дальнейшем мы позволим себе ограничиться работой только с ним.\\nВариационную нижнюю оценку мы будем попеременно оптимизировать по qqq (да-да, по распределению) и по Θ\\\\ThetaΘ.\\nE-шаг состоит в максимизации по qqq при фиксированных Θ\\\\ThetaΘ. Так как\\nL(q,Θ)=log\\u2061p(X∣Θ)−KL(q(Z)∥p(Z∣X,Θ)),\\\\mathcal{L}(q, \\\\Theta) = \\\\log p(X \\\\vert \\\\Theta) - KL(q(Z)\\\\parallel p(Z \\\\vert X, \\\\Theta)),\\nL(q,Θ)=logp(X∣Θ)−KL(q(Z)∥p(Z∣X,Θ)),а log\\u2061p(X∣Θ)\\\\log p(X \\\\vert \\\\Theta)logp(X∣Θ) от qqq не зависит, максимизировать L(q,Θ)\\\\mathcal{L}(q, \\\\Theta)L(q,Θ) по qqq — это то же самое, что минимизировать KL(q(Z)∥p(Z∣X,Θ))KL(q(Z)\\\\parallel p(Z \\\\vert X, \\\\Theta))KL(q(Z)∥p(Z∣X,Θ)). Расстояние Кульбака-Лейблера минимально, когда распределения совпадают, то есть мы должны обновить qqq по правилу\\nq(Z)=p(Z∣X,Θ)q(Z) = p(Z \\\\vert X, \\\\Theta)\\nq(Z)=p(Z∣X,Θ)На M-шаге мы максимизируем по Θ\\\\ThetaΘ при фиксированном qqq. Преобразуем вариационную нижнюю оценку:\\nL(q,Θ)=∑Zq(Z)log\\u2061p(X,Z∣Θ)q(Z)dZ=\\\\mathcal{L}(q, \\\\Theta) = \\\\sum_Zq(Z)\\\\log{\\\\frac{p(X, Z \\\\vert \\\\Theta)}{q(Z)}}dZ =\\nL(q,Θ)=Z∑\\u200bq(Z)logq(Z)p(X,Z∣Θ)\\u200bdZ==∑Zq(Z)log\\u2061p(X,Z∣Θ)dZ−∑Zq(Z)log\\u2061q(Z)dZ= \\\\sum_Zq(Z)\\\\log{p(X, Z \\\\vert \\\\Theta)}dZ - \\\\sum_Zq(Z)\\\\log{q(Z)}dZ\\n=Z∑\\u200bq(Z)logp(X,Z∣Θ)dZ−Z∑\\u200bq(Z)logq(Z)dZЗаметим, что ∑Zq(Z)log\\u2061q(Z)dZ\\\\sum_Zq(Z)\\\\log{q(Z)}dZ∑Z\\u200bq(Z)logq(Z)dZ — это энтропия распределения qqq, и она не зависит от Θ\\\\ThetaΘ. Таким образом, нам достаточно максимизировать по Θ\\\\ThetaΘ математическое ожидание\\n∑Zq(Z)log\\u2061p(X,Z∣Θ)dZ=EZ∼q(Z)log\\u2061p(X,Z∣Θ)\\\\sum_Zq(Z)\\\\log{p(X, Z \\\\vert \\\\Theta)}dZ = \\\\mathbb{E}_{Z\\\\sim q(Z)}\\\\log{p(X, Z \\\\vert \\\\Theta)}\\nZ∑\\u200bq(Z)logp(X,Z∣Θ)dZ=EZ∼q(Z)\\u200blogp(X,Z∣Θ)Жёсткий EM-алгоритм\\nНе всегда получается подобрать латентные переменные ZZZ так, чтобы p(Z∣X,Θ)p(Z \\\\vert X, \\\\Theta)p(Z∣X,Θ) можно было выразить аналитически, то есть на E-шаге не удаётся минимизировать L(q,Θ)\\\\mathcal{L}(q, \\\\Theta)L(q,Θ) по qqq.\\nВ такой ситуации иногда приходится брать оптимум не по всему пространству распределений, а только по некоторому семейству — например, параметрическому, в котором оптимизацию можно проводить градиентными методами. В максимально упрощённой ситуации мы возьмём семейство дельта-функций, то есть вместо распределения на ZZZ будем брать просто точечную оценку. Такая модификация EM-алгоритма называется жёстким EM-алгоритмом.\\nВ начале параграфа мы упоминали кластеризацию методом K-средних и отмечали, что EM-алгоритм даёт «мягкую» версию алгоритма: на E-шаге мы не приписываем однозначно точку к какому-то из кластеров (то есть не берём точечную оценку скрытой переменной «номер кластера, к которому принадлежит точка»), а сопоставляем ей вероятности принадлежности каждому из кластеров (то есть распределение на скрытые переменные). Настоящий метод K-средних как раз таки соответствует жёсткому EM-алгоритму.\\nРазделение смеси гауссиан\\nПусть теперь нам известно, что NNN точек были семплированы из K разных гауссовских распределений и нам неизвестно, какая точка из какого распределения пришла в выборку. Нам нужно оценить параметры (μ1\\\\mu_1μ1\\u200b, σ1\\\\sigma_1σ1\\u200b) для первого распределения, (μ2\\\\mu_2μ2\\u200b, σ2\\\\sigma_2σ2\\u200b) для второго и, соответственно, (μk\\\\mu_kμk\\u200b, σk\\\\sigma_kσk\\u200b) для kkk-го распределения.\\nЕсли мы знаем, что точка xix_ixi\\u200b пришла из распределения ziz_izi\\u200b, то её правдоподобие в равно:\\np(xi∣zi,θ)=12πσziexp\\u2061(−(xi−μzi)22σzi2)p(x_i \\\\mid z_i, \\\\theta) = \\\\frac1{\\\\sqrt{ 2 \\\\pi }\\\\sigma_{z_i}} \\\\exp\\\\left( -\\\\frac{ (x_i - \\\\mu_{z_i})^2 }{2 \\\\sigma_{z_i}^2} \\\\right)   \\np(xi\\u200b∣zi\\u200b,θ)=2π\\u200bσzi\\u200b\\u200b1\\u200bexp(−2σzi\\u200b2\\u200b(xi\\u200b−μzi\\u200b\\u200b)2\\u200b)Напомним, что ziz_izi\\u200b — это латентная переменная, обозначающая номер гауссианы (от 1 до KKK), из которой была просемплирована точка xix_ixi\\u200b. Например, если точка xix_ixi\\u200b просемплирована из распределения с номером 3, то в формулу вместо (μzi\\\\mu_{z_i}μzi\\u200b\\u200b, σzi\\\\sigma_{z_i}σzi\\u200b\\u200b) нужно подставлять (μ3,σ3)(\\\\mu_3, \\\\sigma_3)(μ3\\u200b,σ3\\u200b).\\nВоспользуемся EM-алгоритмом для нахождения параметров (μzi\\\\mu_{z_i}μzi\\u200b\\u200b, σzi\\\\sigma_{z_i}σzi\\u200b\\u200b). Сначала инициализируем их:\\nΘold=μ1_old,σ1_old,μ2_old,σ2_old,…,μK_old,σK_old\\\\Theta_{\\\\text{old}}={ \\\\mu_{1\\\\_{\\\\text{old}}}, \\\\sigma_{1\\\\_{\\\\text{old}}}, \\\\mu_{2\\\\_{\\\\text{old}}}, \\\\sigma_{2\\\\_{\\\\text{old}}}, \\\\ldots, \\\\mu_{K\\\\_{\\\\text{old}}}, \\\\sigma_{K\\\\_{\\\\text{old}}}\\\\\\\\}\\nΘold\\u200b=μ1_old\\u200b,σ1_old\\u200b,μ2_old\\u200b,σ2_old\\u200b,…,μK_old\\u200b,σK_old\\u200bЗная Θ_old\\\\Theta\\\\_{\\\\text{old}}Θ_old, выполним E-шаг: нужно найти p(Z∣X,Θ)p(Z \\\\mid X, \\\\Theta)p(Z∣X,Θ) или что то же самое, для каждого объекта xix_ixi\\u200b найти распределение на вероятности p(zi=k∣xi,Θ_old)p(z_i = k \\\\mid x_i, \\\\Theta\\\\_{\\\\text{old}})p(zi\\u200b=k∣xi\\u200b,Θ_old).\\nКак найти апостериорную вероятность p(zi=k∣xi,Θ_old)p(z_i = k \\\\mid x_i, \\\\Theta\\\\_{\\\\text{old}})p(zi\\u200b=k∣xi\\u200b,Θ_old), если мы знаем xix_ixi\\u200b и у нас есть приближение Θ_old\\\\Theta\\\\_{\\\\text{old}}Θ_old?\\nОтвет — по формуле Байеса:\\np(zi=k∣xi,Θold)=p(xi∣zi=k,Θold)⋅p(zi=k)p(xi∣Θold)=p(xi∣zi=k,Θold)⋅p(zi=k)∑k=1Kp(xi∣zi=k,Θold)⋅p(zi=k)p(z_i = k \\\\mid x_i, \\\\Theta_{\\\\text{old}}) = \\\\frac{p(x_i \\\\mid z_i = k, \\\\Theta_{\\\\text{old}}) \\\\cdot p(z_i = k) } {p(x_i | \\\\Theta_{\\\\text{old}} )} = \\\\frac {p(x_i \\\\mid z_i = k, \\\\Theta_{\\\\text{old}}) \\\\cdot p(z_i = k) } { \\\\sum_{k=1}^K p(x_i \\\\mid z_i = k, \\\\Theta_{\\\\text{old}}) \\\\cdot p(z_i = k) }\\np(zi\\u200b=k∣xi\\u200b,Θold\\u200b)=p(xi\\u200b∣Θold\\u200b)p(xi\\u200b∣zi\\u200b=k,Θold\\u200b)⋅p(zi\\u200b=k)\\u200b=∑k=1K\\u200bp(xi\\u200b∣zi\\u200b=k,Θold\\u200b)⋅p(zi\\u200b=k)p(xi\\u200b∣zi\\u200b=k,Θold\\u200b)⋅p(zi\\u200b=k)\\u200bгде p(zi=k)p(z_i = k)p(zi\\u200b=k) — априорная вероятность того, что объекта xix_ixi\\u200b получен из распределения с номером kkk. На первом шаге априорную вероятность можно положить равной p(zi=k)=1Kp(z_i = k) = \\\\frac{1}{K}p(zi\\u200b=k)=K1\\u200b для всех гауссиан.\\nВведём обозначение\\nuik:=p(xi∣zi=k,Θold)=1(2π)σkoldexp\\u2061(−(xi−μkold)22σkold2)u_{ik} := p(x_i \\\\mid z_i = k, \\\\Theta_{\\\\text{old}}) =  \\\\frac1{\\\\sqrt{(2 \\\\pi)} \\\\sigma_{k_{\\\\text{old}}} } \\\\exp\\\\left( -\\\\frac{(x_i - \\\\mu_{k_{\\\\text{old}}})^2 }{2 \\\\sigma_{k_{\\\\text{old}}}^2} \\\\right)\\nuik\\u200b:=p(xi\\u200b∣zi\\u200b=k,Θold\\u200b)=(2π)\\u200bσkold\\u200b\\u200b1\\u200bexp(−2σkold\\u200b2\\u200b(xi\\u200b−μkold\\u200b\\u200b)2\\u200b)— правдоподобие того, что объект xix_ixi\\u200b пришел из нормального распределения с параметрами (μkold,σkold2)(\\\\mu_{k_{\\\\text{old}}}, \\\\sigma_{k_{\\\\text{old}}}^2)(μkold\\u200b\\u200b,σkold\\u200b2\\u200b).\\nТогда по формуле Байеса:\\np(zi=k∣xi,θold)=uik⋅1K∑k=1Kuik⋅1Kp(z_i = k| x_i, \\\\theta_{\\\\text{old}}) = \\\\frac{ u_{ik} \\\\cdot \\\\frac{1}{K}} { \\\\sum_{k=1}^K u_{ik} \\\\cdot \\\\frac{1}{K} } \\np(zi\\u200b=k∣xi\\u200b,θold\\u200b)=∑k=1K\\u200buik\\u200b⋅K1\\u200buik\\u200b⋅K1\\u200b\\u200bВот так для каждого объекта xix_ixi\\u200b по начальному приближению θold\\\\theta_{\\\\text{old}}θold\\u200b мы посчитаем распределение p(zi)p(z_i )p(zi\\u200b) — с какими вероятностями объект xix_ixi\\u200b приходит из той или иной компоненты смеси.\\nТеперь выведем формулы для М-шага.\\nθ=argmax\\u2061ΘEq(Z)log\\u2061p(X∣Z,Θ)=argmax\\u2061Θ∑i=1N∑k=1Kp(zi=k∣xi,Θ)⋅log\\u2061p(xi∣zi=k,Θ) \\\\theta = \\\\underset{\\\\Theta}{\\\\operatorname{arg max}} \\\\mathbb{E}_{q(Z)} \\\\log p(X | Z, \\\\Theta) = \\\\underset{\\\\Theta}{\\\\operatorname{arg max}} \\\\sum_{i=1}^N \\\\sum_{k=1}^K p(z_i = k | x_i, \\\\Theta) \\\\cdot \\\\log p(x_i | z_i = k, \\\\Theta)  \\nθ=Θargmax\\u200bEq(Z)\\u200blogp(X∣Z,Θ)=Θargmax\\u200bi=1∑N\\u200bk=1∑K\\u200bp(zi\\u200b=k∣xi\\u200b,Θ)⋅logp(xi\\u200b∣zi\\u200b=k,Θ)=∑i=1N∑k=1Kp(zi=k∣xi,Θ)⋅(log\\u20611K−12σk2(xi−μk)2−12log\\u2061σk2)+const= \\\\sum_{i=1}^N \\\\sum_{k=1}^K p(z_i = k | x_i, \\\\Theta) \\\\cdot \\\\left ( \\\\log \\\\frac{1}{K} - \\\\frac{1}{2 \\\\sigma_k^2} (x_i - \\\\mu_k)^2 - \\\\frac{1}{2} \\\\log \\\\sigma_k^2 \\\\right) + const \\n=i=1∑N\\u200bk=1∑K\\u200bp(zi\\u200b=k∣xi\\u200b,Θ)⋅(logK1\\u200b−2σk2\\u200b1\\u200b(xi\\u200b−μk\\u200b)2−21\\u200blogσk2\\u200b)+constЗапишем производную и приравняем к 000, чтобы найти экстремум:\\n0=∂Eq(Z)log\\u2061p(X∣Z,Θ)∂μk=−∑i=1Np(zi=k∣xi,Θ)⋅xi−μkσk2 0 = \\\\frac{\\\\partial \\\\mathbb{E}_{q(Z)} \\\\log p(X | Z, \\\\Theta) }{\\\\partial \\\\mu_k} = - \\\\sum_{i=1}^N p(z_i = k | x_i, \\\\Theta) \\\\cdot \\\\frac{x_i - \\\\mu_k}{\\\\sigma_k^2} \\n0=∂μk\\u200b∂Eq(Z)\\u200blogp(X∣Z,Θ)\\u200b=−i=1∑N\\u200bp(zi\\u200b=k∣xi\\u200b,Θ)⋅σk2\\u200bxi\\u200b−μk\\u200b\\u200bОтсюда\\nμk=∑i=1Np(zi=k∣xi,θ)⋅xi∑i=1Np(zi=k∣xi,θ)\\\\mu_k = \\\\frac{ \\\\sum_{i=1}^N p(z_i = k | x_i, \\\\theta) \\\\cdot x_i } {\\\\sum_{i=1}^N p(z_i = k | x_i, \\\\theta) } \\nμk\\u200b=∑i=1N\\u200bp(zi\\u200b=k∣xi\\u200b,θ)∑i=1N\\u200bp(zi\\u200b=k∣xi\\u200b,θ)⋅xi\\u200b\\u200bМы получили конечную формулу для пересчета μk\\\\mu_kμk\\u200b по ziz_izi\\u200b и предыдущему значению θ\\\\thetaθ. Причем у этой формулы есть простая интерпретация — каждый объект мы взвешиваем с его вероятностью принадлежности к этому классу p(zi=k∣x,θ)p(z_i = k \\\\mid x, \\\\theta)p(zi\\u200b=k∣x,θ).\\nТеперь посчитаем производную по σk2\\\\sigma_k^2σk2\\u200b (обратите внимание, что именно по квадрату σk\\\\sigma_kσk\\u200b):\\n∂Eq(Z)log\\u2061p(X∣Z,Θ)∂σk2=∑i=1Np(zi=k∣xi,θ)⋅((xi−μk)22σk4−12σk2)=0\\\\frac{\\\\partial \\\\mathbb{E}_{q(Z)} \\\\log p(X | Z, \\\\Theta) }{\\\\partial \\\\sigma_k^2} = \\\\sum_{i=1}^N   p(z_i = k | x_i, \\\\theta) \\\\cdot \\\\left( \\\\frac{(x_i - \\\\mu_k)^2}{2 \\\\sigma_k^4} - \\\\frac{1}{2 \\\\sigma_k^2} \\\\right) = 0\\n∂σk2\\u200b∂Eq(Z)\\u200blogp(X∣Z,Θ)\\u200b=i=1∑N\\u200bp(zi\\u200b=k∣xi\\u200b,θ)⋅(2σk4\\u200b(xi\\u200b−μk\\u200b)2\\u200b−2σk2\\u200b1\\u200b)=0Стало быть,\\nσk2=∑i=1Np(zi=k∣xi,Θ)(xi−μk)2∑i=1Np(zi=k∣xi,Θ)\\\\sigma_k^2 = \\\\frac{ \\\\sum_{i=1}^N  p(z_i = k | x_i, \\\\Theta) (x_i - \\\\mu_k)^2 }{ \\\\sum_{i=1}^N p(z_i = k | x_i, \\\\Theta) }\\nσk2\\u200b=∑i=1N\\u200bp(zi\\u200b=k∣xi\\u200b,Θ)∑i=1N\\u200bp(zi\\u200b=k∣xi\\u200b,Θ)(xi\\u200b−μk\\u200b)2\\u200bМы снова получили интерпретируемый результат: подсчитывая дисперсию для kkk-ой гауссианы, мы учитываем вес каждого объекта при подсчете среднеквадратичноого отклонения. То есть веса — вероятности происхождения из той или иной компоненты смеси. Сравните эту формулу с формулой для подсчета выборочной дисперсии, где каждый из NNN объектов вносит одинаковый вклад в дисперсию с весом 1N\\\\frac{1}{N}N1\\u200b:\\nσ2=∑iN(xi−μ)2N\\\\sigma^2 = \\\\frac{\\\\sum_i^N (x_i - \\\\mu)^2}{N}\\nσ2=N∑iN\\u200b(xi\\u200b−μ)2\\u200bВы можете «пощупать» EM-алгоритм в задаче разделения вероятностной смеси с помощью интерактивной визуализации — попробуйте сделать E и M шаги и последить за изменениями параметров: после одной итераций алгоритма можно выбрать точку на графике и наблюдать за вероятностью её принадлежности к разным кластерам.\\nВероятностный PCA\\nТеперь давайте рассмотрим простой пример того, как введение латентных переменных может помочь выделять новые информативные признаки в данных.\\nПредположим, что мы имеем выборку данных xix_ixi\\u200b (вектор-строку), где каждый объект имеет DDD признаков (предположим, что число DDD очень большое). Это достаточно типичная ситуация, например, при работе с текстами или изображениями.\\nТеперь введём следующую вероятностную модель xi=ziWT+εx_i = z_i W^T + \\\\varepsilonxi\\u200b=zi\\u200bWT+ε, где ziz_izi\\u200b — латентный вектор-строка меньшей размерности TTT, а ε∼N(0,σ2E)\\\\varepsilon \\\\sim \\\\mathcal{N}(0, \\\\sigma^2 E)ε∼N(0,σ2E), где EEE — единичная матрица размером D×DD \\\\times DD×D, σ\\\\sigmaσ — скаляр больший 0.\\nЧто означает эта модель? Она означает, что наши сложные многоразмерные данные xix_ixi\\u200b могут иметь более простое малоразмерное представление ziz_izi\\u200b, а отображение zi→xiz_i \\\\rightarrow x_izi\\u200b→xi\\u200b линейно с точностью до нормально распределенного шума.\\nЗаметим, что так как ε∼N(0,σ2E)\\\\varepsilon \\\\sim  \\\\mathcal{N}(0, \\\\sigma^2 E)ε∼N(0,σ2E), отсюда следует, что xi∼N(ziWT,σ2E)x_i \\\\sim \\\\mathcal{N} (z_i W^T,  \\\\sigma^2 E)xi\\u200b∼N(zi\\u200bWT,σ2E). Зададим априорное распределение на ziz_izi\\u200b как стандартное нормальное zi∼N(0,E)z_i \\\\sim \\\\mathcal{N}(0, E)zi\\u200b∼N(0,E) и распишем совместное распределение (xi,zi)(x_i, z_i)(xi\\u200b,zi\\u200b) через условное и априорное:\\np(xi,zi∣W,σ)=p(xi∣zi,W,σ)p(zi)p(x_i, z_i \\\\vert W, \\\\sigma) = p(x_i \\\\vert z_i, W, \\\\sigma ) p(z_i) \\np(xi\\u200b,zi\\u200b∣W,σ)=p(xi\\u200b∣zi\\u200b,W,σ)p(zi\\u200b)Чтобы восстановить параметры WWW, σ\\\\sigmaσ и латентные переменные ziz_izi\\u200b, снова воспользуемся EM-алгоритмом.\\nНа E-шаге мы оцениваем распределение на ziz_izi\\u200b при фиксированных WWW и σ\\\\sigmaσ:\\nПо формуле Байеса распределение на ziz_izi\\u200b при условии xix_ixi\\u200b:\\np(zi∣xi,W,σ)=p(xi∣zi,W,σ)⋅p(zi)p(xi∣W,σ)p(z_i \\\\vert x_i, W, \\\\sigma) = \\\\frac{p(x_i \\\\vert z_i, W, \\\\sigma) \\\\cdot p(z_i)}{p(x_i \\\\vert W, \\\\sigma)}\\np(zi\\u200b∣xi\\u200b,W,σ)=p(xi\\u200b∣W,σ)p(xi\\u200b∣zi\\u200b,W,σ)⋅p(zi\\u200b)\\u200bС точностью до констант и слагаемых, которые не зависят от ziz_izi\\u200b, логарифм правдоподобия равен:\\nlog\\u2061p(zi∣xi,W,σ)∼−12σ2(xi−ziWT)(xi−ziWT)T−12ziziT\\\\log p(z_i \\\\vert x_i, W, \\\\sigma) \\\\sim - \\\\frac{1}{2 \\\\sigma^2} (x_i - z_iW^T) (x_i - z_i W^T)^T - \\\\frac{1}{2} z_i z_i^T  \\nlogp(zi\\u200b∣xi\\u200b,W,σ)∼−2σ21\\u200b(xi\\u200b−zi\\u200bWT)(xi\\u200b−zi\\u200bWT)T−21\\u200bzi\\u200bziT\\u200blog\\u2061p(zi∣xi,W,σ)∼−12σ2ziWTWziT−12ziziT+12σ2(xiWziT+ziWTxiT)\\\\log p(z_i \\\\vert x_i, W, \\\\sigma) \\\\sim -\\\\frac{1}{2 \\\\sigma^2} z_i W^T W z_i^T - \\\\frac{1}{2} z_i z_i^T + \\\\frac{1}{2\\\\sigma^2} (x_i W z_i^T + z_i W^T x_i^T) \\nlogp(zi\\u200b∣xi\\u200b,W,σ)∼−2σ21\\u200bzi\\u200bWTWziT\\u200b−21\\u200bzi\\u200bziT\\u200b+2σ21\\u200b(xi\\u200bWziT\\u200b+zi\\u200bWTxiT\\u200b)log\\u2061p(zi∣xi,W,σ)∼−12σ2zi(WTW+σ2E)ziT+12σ2(xiWziT+ziWTxiT)\\\\log p(z_i \\\\vert x_i, W, \\\\sigma) \\\\sim - \\\\frac{1}{2 \\\\sigma^2} z_i (W^T W + \\\\sigma^2 E) z_i^T + \\\\frac{1}{2\\\\sigma^2} (x_i W z_i^T + z_i W^T x_i^T) \\nlogp(zi\\u200b∣xi\\u200b,W,σ)∼−2σ21\\u200bzi\\u200b(WTW+σ2E)ziT\\u200b+2σ21\\u200b(xi\\u200bWziT\\u200b+zi\\u200bWTxiT\\u200b)Обозначим M:=WTW+σ2EM:= W^T W + \\\\sigma^2 EM:=WTW+σ2E, тогда\\nlog\\u2061p(zi∣xi,W,σ)∼−12σ2zi(σ2M−1)−1ziT+12(zi(σ2M−1)−1⋅M−1WTxiT+xiWM−1⋅(σ2M−1)−1zi)\\\\log p(z_i \\\\vert x_i, W, \\\\sigma) \\\\sim -\\\\frac{1}{2 \\\\sigma^2} z_i (\\\\sigma^2 M^{-1})^{-1} z_i^T + \\\\frac{1}{2} \\\\left( z_i (\\\\sigma^2 M^{-1})^{-1} \\\\cdot M^{-1} W^T x_i^T + x_i W M^{-1} \\\\cdot (\\\\sigma^2 M^{-1})^{-1} z_i  \\\\right) \\nlogp(zi\\u200b∣xi\\u200b,W,σ)∼−2σ21\\u200bzi\\u200b(σ2M−1)−1ziT\\u200b+21\\u200b(zi\\u200b(σ2M−1)−1⋅M−1WTxiT\\u200b+xi\\u200bWM−1⋅(σ2M−1)−1zi\\u200b)log\\u2061p(zi∣xi,W,σ)∼−12(zi−xiWM−1)⋅(σ2M)−1⋅(zi−xiWM−1)T\\\\log p(z_i \\\\vert x_i, W, \\\\sigma) \\\\sim -\\\\frac{1}{2}  (z_i  - x_i W M^{-1}) \\\\cdot (\\\\sigma^2 M)^{-1} \\\\cdot (z_i - x_i W M^{-1})^T\\nlogp(zi\\u200b∣xi\\u200b,W,σ)∼−21\\u200b(zi\\u200b−xi\\u200bWM−1)⋅(σ2M)−1⋅(zi\\u200b−xi\\u200bWM−1)TЕсли теперь взять от этого экспоненту, увидим, что p(zi∣xi,W,σ)∼N(xiWM−1,σ2M)p(z_i \\\\vert x_i, W, \\\\sigma) \\\\sim \\\\mathcal{N} (x_i W M^{-1}, \\\\sigma^2 M)p(zi\\u200b∣xi\\u200b,W,σ)∼N(xi\\u200bWM−1,σ2M).\\nM-шаг.\\nТеперь мы оптимизировать по WWW и σ2\\\\sigma^2σ2:\\nEp(Z∣X,W,σ)log\\u2061p(X,Z∣W,σ)=∑inEp(zi∣xi,W,σ)log\\u2061p(xi,zi∣W,σ)→min\\u2061W,σ\\\\mathbb{E}_{p(Z \\\\vert X, W, \\\\sigma)} \\\\log p(X, Z \\\\vert W, \\\\sigma) = \\\\sum_i^n \\\\mathbb{E}_{p(z_i \\\\vert x_i, W, \\\\sigma)} \\\\log p(x_i, z_i \\\\vert W, \\\\sigma)   \\\\rightarrow \\\\min_{W,\\\\sigma}\\nEp(Z∣X,W,σ)\\u200blogp(X,Z∣W,σ)=i∑n\\u200bEp(zi\\u200b∣xi\\u200b,W,σ)\\u200blogp(xi\\u200b,zi\\u200b∣W,σ)→W,σmin\\u200bПриравняв производные к 000, можно найти:\\nWnew=(∑in(Ezi)xiT)⋅(∑inE(ziTzi))−1W_{\\\\text{new}} = \\\\left( \\\\sum_i^n ( \\\\mathbb{E} z_i ) x_i^T\\\\right) \\\\cdot \\\\left( \\\\sum_i^n \\\\mathbb{E} (z_i^T z_i) \\\\right)^{-1}\\nWnew\\u200b=(i∑n\\u200b(Ezi\\u200b)xiT\\u200b)⋅(i∑n\\u200bE(ziT\\u200bzi\\u200b))−1σnew=1ND∑in[∣∣xi∣∣2−2xiWnewEziT+tr(WnewTWnewE(ziTzi))]\\\\sigma_{\\\\text{new}} = \\\\frac{1}{ND} \\\\sum_i^n \\\\left[  \\\\vert \\\\vert x_i \\\\vert \\\\vert^2 - 2x_i W_{\\\\text{new}} \\\\mathbb{E} z_i^T + \\\\text{tr} \\\\left( W_{\\\\text{new}}^T W_{\\\\text{new}} \\\\mathbb{E} (z_i^T z_i)   \\\\right) \\\\right]\\nσnew\\u200b=ND1\\u200bi∑n\\u200b[∣∣xi\\u200b∣∣2−2xi\\u200bWnew\\u200bEziT\\u200b+tr(WnewT\\u200bWnew\\u200bE(ziT\\u200bzi\\u200b))]Вероятностный PCA хорош тем, что:\\n\\n\\nкак любая байесовская модель, может служить промежуточным участком в более сложной вероятностной модели;\\n\\n\\nесли в данных есть пропуски, то вероятностный PCA легко обобщается и на этот случай с добавлением дополнительных скрытых переменных;\\n\\n\\nтак как параметры W,σW, \\\\sigmaW,σ и оценки на ziz_izi\\u200b получаются через итерационный EM-алгоритм, то вероятностный PCA может быть вычислительно эффективнее. Так, в вычислениях и промежуточных формулах нигде не используется матрица XTX∈RD×DX^T X \\\\in \\\\mathcal{R}^{D \\\\times D}XTX∈RD×D, и все рассматриваемые матрицы имеют меньший размер.\\n\\n\\nСвязь с обычным PCA\\nКак вероятностный PCA связан с обычным, который мы изучили в теме про разложение матриц?\\nНапомним, что в обычном SVD-разложении мы полагали, что xi∼ziΣ^V^Tx_i \\\\sim z_i \\\\hat{\\\\Sigma} \\\\hat{V}^Txi\\u200b∼zi\\u200bΣ^V^T. Давайте опять положим, что разница между xix_ixi\\u200b и ziΣ^V^Tz_i \\\\hat{\\\\Sigma} \\\\hat{V}^Tzi\\u200bΣ^V^T есть гауссовский шум с нулевым средним ε∼N(0,σ2E)\\\\varepsilon \\\\sim \\\\mathcal{N}(0, \\\\sigma^2 E)ε∼N(0,σ2E):\\nxi=ziΣ^V^T+εx_i = z_i \\\\hat{\\\\Sigma} \\\\hat{V}^T + \\\\varepsilon\\nxi\\u200b=zi\\u200bΣ^V^T+εили\\nxi=N(ziΣ^V^T,σ2E)x_i = \\\\mathcal{N} (z_i \\\\hat{\\\\Sigma} \\\\hat{V}^T , \\\\sigma^2 E)\\nxi\\u200b=N(zi\\u200bΣ^V^T,σ2E)Если зададим априорное распределение на ziz_izi\\u200b как стандартное нормальное p(zi)∼N(0,E)p(z_i) \\\\sim \\\\mathcal{N}(0, E)p(zi\\u200b)∼N(0,E), тогда ziVT∼N(0,V^TΣ^2V^)z_i V^T \\\\sim \\\\mathcal{N}(0, \\\\hat{V}^T \\\\hat{\\\\Sigma}^2 \\\\hat{V})zi\\u200bVT∼N(0,V^TΣ^2V^) и соответственно xi∼N(0,V^TΣ^2V^+σ2E)x_i \\\\sim \\\\mathcal{N}(0, \\\\hat{V}^T \\\\hat{\\\\Sigma}^2 \\\\hat{V} + \\\\sigma^2 E)xi\\u200b∼N(0,V^TΣ^2V^+σ2E).\\nТеперь сделаем обратную замену WT=Σ^V^TW^T = \\\\hat{\\\\Sigma} \\\\hat{V}^TWT=Σ^V^T и убедимся, что оценка максимального правдоподобия в точности равна Σ^V^T\\\\hat{\\\\Sigma} \\\\hat{V}^TΣ^V^T.\\nlog\\u2061p(xi∣W,σ)=−N2log\\u2061det\\u2061(WTW+σ2E)−12∑ixi(WTW+σ2E)−1xiT+const\\\\log p(x_i \\\\vert W, \\\\sigma) = - \\\\frac{N}{2} \\\\log \\\\det (W^T W + \\\\sigma^2 E) - \\\\frac{1}{2} \\\\sum_i x_i (W^T W + \\\\sigma^2 E)^{-1} x_i^T + const\\nlogp(xi\\u200b∣W,σ)=−2N\\u200blogdet(WTW+σ2E)−21\\u200bi∑\\u200bxi\\u200b(WTW+σ2E)−1xiT\\u200b+const(напомним, что xix_ixi\\u200b и ziz_izi\\u200b это вектор-строки). Заметим, что число есть след матрицы, состоящей из этого числа, поэтому можно преобразовать вторую часть, как\\n−12∑ixi(WTW+σ2E)−1xiT=−12tr(∑ixi(WTW+σ2E)−1xiT)=-\\\\frac{1}{2} \\\\sum_i x_i (W^T W + \\\\sigma^2 E)^{-1} x_i^T = -\\\\frac{1}{2} \\\\textit{tr} \\\\left( \\\\sum_i x_i ( W^T W + \\\\sigma^2 E)^{-1} x_i^T  \\\\right) =\\n−21\\u200bi∑\\u200bxi\\u200b(WTW+σ2E)−1xiT\\u200b=−21\\u200btr(i∑\\u200bxi\\u200b(WTW+σ2E)−1xiT\\u200b)==−12tr((WTW+σ2E)−1⋅(∑ixiTxi))== -\\\\frac{1}{2} \\\\textit{tr} \\\\left( (W^T W + \\\\sigma^2 E)^{-1} \\\\cdot \\\\left( \\\\sum_i x_i^T x_i \\\\right) \\\\right) = \\n=−21\\u200btr((WTW+σ2E)−1⋅(i∑\\u200bxiT\\u200bxi\\u200b))==−12tr((WTW+σ2E)−1⋅XXT)= -\\\\frac{1}{2} \\\\textit{tr} \\\\left( (W^T W + \\\\sigma^2 E)^{-1} \\\\cdot XX^T \\\\right) \\n=−21\\u200btr((WTW+σ2E)−1⋅XXT)Отсюда следует, что\\nlog\\u2061p(xi∣W,σ)=−N2log\\u2061det\\u2061(WTW+σ2E)−12tr((WTW+σ2E)−1⋅XXT)\\\\log p(x_i \\\\vert W, \\\\sigma) = -\\\\frac{N}{2} \\\\log \\\\det (W^T W + \\\\sigma^2 E) - \\\\frac{1}{2}  \\\\textit{tr} \\\\left( (W^T W + \\\\sigma^2 E)^{-1} \\\\cdot XX^T \\\\right) \\nlogp(xi\\u200b∣W,σ)=−2N\\u200blogdet(WTW+σ2E)−21\\u200btr((WTW+σ2E)−1⋅XXT)Приравняв производную по WWW к нулю, найдем:\\nWMLT=(Σ^2−σ2E)12V^TW^T_{\\\\textit{ML}} = (\\\\hat{\\\\Sigma}^2 - \\\\sigma^2 E)^{\\\\frac{1}{2}} \\\\hat{V}^T\\nWMLT\\u200b=(Σ^2−σ2E)21\\u200bV^TОценка максимума правдоподобия на σ2\\\\sigma^2σ2:\\nσML2=1D−T∑j=T+1Dσj2,\\\\sigma^2_{\\\\textit{ML}} = \\\\frac{1}{D - T} \\\\sum_{j=T+1}^{D} \\\\sigma_j^2,\\nσML2\\u200b=D−T1\\u200bj=T+1∑D\\u200bσj2\\u200b,Эту оценку можно интерпретировать как среднюю потерю дисперсии по всем проигнорированным сингулярным направлениям. Если же σ2\\\\sigma^2σ2 — константа, то при σ→0\\\\sigma \\\\rightarrow 0σ→0 получаем обычный PCA.\\nДругой способ получить обычный PCA — это вместо обычного EM-алгоритма воспользоваться его жёсткой модификацией.\\n\\nТеперь предлагаем вам потренировать изученный материал на практике. Предлагаем вам выполнить лабораторную работу, которая покрывает большинство тем главы “Вероятностные модели”. Скачайте ноутбук с лабораторной работой. В нём вы найдете описания заданий и дополнительные материалы. Задания из лабораторной прикреплены к этому параграфу в виде задач в системе Яндекс Контест. Чтобы проверить себя, отправляйте решения по соответствующим задачам в систему. Успехов в практике!\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьВыполните задачи урока0 / 9 выполненоВыполнять задачиСообщить об ошибкеПредыдущий параграф4.6. Байесовский подход к оцениваниюБайесовская статистика. Априорные и\\xa0апостериорные распределения на\\xa0параметры моделей. MAP-оценки. Байесовский подход к\\xa0выбору моделей. Байесовский подход для задачи линейной регресииСледующий параграф5.1. Нейронные сетиКраткий путеводитель по\\xa0разделуЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_39.html', 'title': 'Контентные рекомендации'}, page_content='Контентные рекомендацииЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/49.1.Введение в рекомендательные системы9.2.Рекомендации на основе матричных разложений9.3.Контентные рекомендацииВведениеФакторизационные машиныDSSM (deep sematic similiarity model)Трансформеры для рекомендаций9.4.Хорошие свойства рекомендательных систем10.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Контентные рекомендации9.3. Контентные рекомендацииАвторыИлья БойцовРоман ГасымовВведение\\nВсе рекомендательные системы можно поделить на три типа в зависимости от того, какую информацию они используют для построения рекомендаций:\\n\\nКонтентные;\\nКоллаборативые;\\nГибридные.\\n\\nВ данном разделе мы подробнее рассмотрим основные алгоритмы построения контентных рекомендаций.\\nОсновная идея контентных рекомендаций состоит в том, что для их построения будут использоваться атрибуты объектов и пользователей. На основе данных атрибутов мы можем найти релевантные данному пользователю объекты и рекомендовать их.\\nПредставим, например, что мы работаем в музыкальном онлайн-сервисе и хотим подбирать наиболее релевантную музыку нашим пользователям. Допустим у нас есть пользователь Иван, который интересуется русским роком. Тогда наша система может рекомендовать Ивану музыку этого или подобных жанров.\\nМожно придумать много различных атрибутов трека: жанр, автор, год выхода, продолжительность и так далее. Также можно использовать дополнительную информацию о пользователе: возраст, уровень дохода и тому подобные.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nКакими бывают контентные признаки\\nДопустим, мы работаем в музыкальном сервисе. Тогда в качестве признаков объектов можно использовать:\\n\\nСтандартные статистики объекта: количество лайков, кликов, полных прослушиваний;\\nПризнаки автора: количество слушателей, жанр;\\nНеструктурированные данные: названия треков, обложки альбомов или даже предобученные эмбеддинги треков целиком.\\n\\nВ качестве признаков пользователей можно использовать:\\n\\nИнформацию про пользователя, если она нам доступна: возраст, пол, язык, насколько долго пользуется сервисом;\\nИнформацию про контекст запроса: с какого устройства был сделан, в какое время.\\nИнформацию про друзей пользователя и их взаимодействия. Например, усреднённый эмбеддинг всех треков, которые слушал каждый из друзей. Или же можно обучить RNN или Transformer на истории и результат конкатенировать к остальным признакам.\\n\\nФакторизационные машины\\nНачнём с постановки задачи. Пусть I – множество объектов (айтемов), U - множество пользователей. Для каждой пары объект-пользователь построим вектор размерности ∣U∣+∣I∣\\\\vert U\\\\vert + \\\\vert I\\\\vert∣U∣+∣I∣ взаимодействия этой пары, в котором единицы стоят на месте соответствующих пользователя и объекта:\\n\\nПредсказывать будем пользовательские рейтинги объектов a(x)a(x)a(x).\\nМожно рассмотреть простейшую регрессионную модель:\\na(x)=w0+∑t=1∣U∣+∣I∣wtxta(x) = w_0 + \\\\sum_{t=1}^{\\\\vert U\\\\vert + \\\\vert I\\\\vert} w_t x_t \\na(x)=w0\\u200b+t=1∑∣U∣+∣I∣\\u200bwt\\u200bxt\\u200bЗаметим, что к этой модели легко добавить любые фичи объектов, пользователей или пар объект-пользователь:\\n\\nДальше будем обозначать через nnn общее число фичей. Модель можно обогатить признаками, отвечающими за взаимодействия второго порядка:\\na(x)=w0+∑t=1nwtxt+∑r=1n∑s=r+1nwrsxrxsa(x) = w_0 + \\\\sum_{t=1}^n w_t x_t + \\\\sum_{r=1}^n \\\\sum_{s=r+1}^n w_{rs} x_r x_s \\na(x)=w0\\u200b+t=1∑n\\u200bwt\\u200bxt\\u200b+r=1∑n\\u200bs=r+1∑n\\u200bwrs\\u200bxr\\u200bxs\\u200bМатрицу W=(wrs)W = (w_{rs})W=(wrs\\u200b) можно считать симметричной: в любом случае, мы используем только её верхний треугольник.\\nИз-за использования попарных взаимодействий пользователей и объектов в полученной модели будет n(n+1)2+n+1\\\\frac{n(n+1)}{2} +n + 12n(n+1)\\u200b+n+1 параметр, и так как n⩾∣U∣+∣I∣n\\\\geqslant\\\\vert U\\\\vert + \\\\vert I\\\\vertn⩾∣U∣+∣I∣ может быть очень большим, работать с такой моделью может оказаться непросто.\\nДля решения этой проблемы можно использовать следующий трюк. Сопоставим каждому признаку xtx_txt\\u200b вектор vt∈Rkv_t \\\\in \\\\mathbb{R}^kvt\\u200b∈Rk для некоторого не очень большого kkk и представим модель в виде:\\na(x)=w0+∑t=1nwtxt+∑r=1n∑s=r+1m<vr,vs>xrxsa(x) = w_0 + \\\\sum_{t=1}^n w_t x_t  + \\\\sum_{r=1}^n\\\\sum_{s=r+1}^m <v_r, v_s> x_r x_s \\na(x)=w0\\u200b+t=1∑n\\u200bwt\\u200bxt\\u200b+r=1∑n\\u200bs=r+1∑m\\u200b<vr\\u200b,vs\\u200b>xr\\u200bxs\\u200bТаким образом, мы заменяем симметричную матрицу коэффициентов WWW на её низкоранговое приближение VTVV^T VVTV, где VVV – матрица n×kn \\\\times kn×k с векторами viv_ivi\\u200b по столбцам. Число параметров модели при этом можно снизить до nk+n+1nk + n + 1nk+n+1. На практике матрица WWW разреженная, и, как правило, даже при небольшом kkk получается её неплохо приблизить. В то же время, при небольших kkk модель обладает лучшей обобщающей способностью.\\nВычислить ∑r=1n∑s=1mwrsxrxs\\\\sum_{r=1}^n \\\\sum_{s=1}^m w_{rs} x_r x_s∑r=1n\\u200b∑s=1m\\u200bwrs\\u200bxr\\u200bxs\\u200b по можно за O(nk)O(nk)O(nk):\\n∑r=1n∑s=r+1n<vr,vs>xrxs=\\\\sum_{r=1}^n \\\\sum_{s=r+1}^n <v_r, v_s> x_r x_s = \\nr=1∑n\\u200bs=r+1∑n\\u200b<vr\\u200b,vs\\u200b>xr\\u200bxs\\u200b==12∑r=1n∑s=1n<vr,vs>xrxs−12∑r=1n<vi,vi>xixi== \\\\frac12 \\\\sum_{r=1}^n \\\\sum_{s=1}^n <v_r, v_s> x_r x_s - \\\\frac12 \\\\sum_{r=1}^n <v_i, v_i> x_i x_i = \\n=21\\u200br=1∑n\\u200bs=1∑n\\u200b<vr\\u200b,vs\\u200b>xr\\u200bxs\\u200b−21\\u200br=1∑n\\u200b<vi\\u200b,vi\\u200b>xi\\u200bxi\\u200b==12∑r=1n∑s=1n∑f=1kvrfvsfxrxs−12∑r=1n∑f=1kvrfvsfxrxs== \\\\frac12 \\\\sum_{r=1}^n \\\\sum_{s=1}^n \\\\sum_{f=1}^k v_{rf} v_{sf} x_r x_s - \\\\frac12 \\\\sum_{r=1}^n \\\\sum_{f=1}^k v_{rf} v_{sf} x_r x_s = \\n=21\\u200br=1∑n\\u200bs=1∑n\\u200bf=1∑k\\u200bvrf\\u200bvsf\\u200bxr\\u200bxs\\u200b−21\\u200br=1∑n\\u200bf=1∑k\\u200bvrf\\u200bvsf\\u200bxr\\u200bxs\\u200b==12∑f=1k(∑r=1nvrfxr)⋅(∑s=1nvsfxs)−∑r=1nvrf2xr2)== \\\\frac12 \\\\sum_{f=1}^k \\\\left(\\\\sum_{r=1}^n v_{rf} x_r) \\\\cdot (\\\\sum_{s=1}^n v_{sf} x_s) - \\\\sum_{r=1}^n v_{rf}^2 x_r^2 \\\\right) = \\n=21\\u200bf=1∑k\\u200b(r=1∑n\\u200bvrf\\u200bxr\\u200b)⋅(s=1∑n\\u200bvsf\\u200bxs\\u200b)−r=1∑n\\u200bvrf2\\u200bxr2\\u200b)==12∑f=1k((∑r=1nvrfxr)2−∑r=1nvrf2xr2)= \\\\frac12 \\\\sum_{f=1}^k \\\\left((\\\\sum_{r=1}^n v_{rf} x_r)^2 - \\\\sum_{r=1}^n v_{rf}^2 x_r^2 \\\\right)\\n=21\\u200bf=1∑k\\u200b((r=1∑n\\u200bvrf\\u200bxr\\u200b)2−r=1∑n\\u200bvrf2\\u200bxr2\\u200b)Итоговая модель имеет вид\\na(x)=w0+∑r=1nwrxr+12∣∣∑r=1nvrxr∣∣22−12∑r=1n∣∣vr∣∣22xr2a(x) = w_0 + \\\\sum_{r=1}^n w_r x_r + \\\\frac12 \\\\left|\\\\left| \\\\sum_{r=1}^n v_r x_r \\\\right|\\\\right|_2^2 - \\\\frac12 \\\\sum_{r=1}^n ||v_r||_2^2x_r^2\\na(x)=w0\\u200b+r=1∑n\\u200bwr\\u200bxr\\u200b+21\\u200b\\u200b\\u200br=1∑n\\u200bvr\\u200bxr\\u200b\\u200b\\u200b22\\u200b−21\\u200br=1∑n\\u200b∣∣vr\\u200b∣∣22\\u200bxr2\\u200bДанная модель и называется факторизационной машиной.\\nПервоначально факторизационные машины использовали только коллаборативный сигнал, но, как мы уже видели, в такую модель можно естественным образом добавить и контентную информацию.\\nФакторизацонную машину можно обучать для решения разных задач. Например:\\n\\nПредсказание рейтинга. Ответ модели a(x)a(x)a(x) можно интерпретировать, как вещественный рейтинг, и решать задачу регрессии.\\nБинарную классификацию рекомендовать/не рекомендовать. Тогда a(x)a(x)a(x) имеет смысл логита, и мы можем оптимизировать оптимизировать log loss или hinge loss.\\nРанжирование объектов. Тогда a(x)a(x)a(x) – это ранжирующая функция.\\n\\nМодель обычно обучается градиентным спуском.\\nFFM – Field-aware Factorization Machines\\nОригинальная статья\\nСтатья про практическое применение\\nКак следующий этап развития факториационных машин, появилась идея иметь несколько различных латентных представлений для каждой из фичей.\\nПример: есть три разных по своей природе признака: год выпуска, цвет и марка автомобиля. В факторизационной машине для учёта взаимодействия год-цвет и год-марка используется один и тот же вектор для года. Но так как эти признаки разные по смыслу, то и характер их взаимодействия может отличаться.\\nИдея: использовать 2 разных вектора для признака «год выпуска» при учёте взаимодействий год-цвет и год-марка. Таким образом, модель принимает вид:\\na(x)=w0+∑t=1nwtxt+∑r=1n∑s=r+1m<vr,s,vs,r>xrxsa(x) = w_0 + \\\\sum_{t=1}^n w_t x_t  + \\\\sum_{r=1}^n\\\\sum_{s=r+1}^m <v_{r,s}, v_{s,r}> x_r x_s\\na(x)=w0\\u200b+t=1∑n\\u200bwt\\u200bxt\\u200b+r=1∑n\\u200bs=r+1∑m\\u200b<vr,s\\u200b,vs,r\\u200b>xr\\u200bxs\\u200b\\nАвторы статьи выложили исходный код своей библиотеки libffm, с помощью которой они смогли войти в топ-3 сразу в трёх соревнованиях на kaggle (Criteo, Avazu, Outbrain). Подробнее об этом можно почитать вот тут.\\nDSSM (deep sematic similiarity model)\\nТеперь рассмотрим ещё одну популярную модель, которая использует контентную информацию для построения рекомендаций – DSSM.\\nОригинальная статья\\nВ оригинальной статье DSSM была использована для нахождения «схожести» между поисковым запросом и документом. Для этого она использовала текст запроса и текст документа.\\nDSSM представляет из себя «двуногую» (two-tower) нейронную сеть. В исходной постановке на первый вход подаётся текст запроса, а на второй – текст документа. Далее, независимо для текста запроса и текста документа строятся эмбеддинги. Итоговая «схожесть» вычисляется, как косинусная мера близости между ними.\\nНа схеме ниже Q – это запрос (query), а D – документ (document).\\n\\nНекоторые авторы пытались в качестве меры близости рассматривать вместо косинусной меры обучаемый MLP, но это оказалось гиблой идеей.\\nЭта архитектура оказалась крайне удобной при использовании на практике, так как эмбеддинги пользователя и объекта можно предподсчитать независимо и дальше хранить сразу готовые представления для них, а при запросе к рекомендациям просто пересчитывать меру близости, что ускоряет применение модели.\\nДанная идея хорошо обобщается на построение рекомендаций. Поиск релевантных объектов можно представить, как задачу ранжирования, где вместо текстов запроса и документа мы будем иметь некоторую контентную информацию о пользователе и объекте.\\nОбучение DSSM\\nДавайте считать, что мы для каждого запроса qqq предсказываем один релевантный документ.\\nОбозначим через yqy_qyq\\u200b и ydy_dyd\\u200b построенные моделью эмбеддинги запроса qqq и документа ddd соответственно. Будем вычислять условную вероятность клика по документу ddd при условии запроса qqq следующим образом:\\nP(d∣q)=exp\\u2061(b0R(q,d))∑i=1Dexp\\u2061(b0R(q,di))P(d\\\\vert q)  = \\\\frac{\\\\exp(b_0 R(q, d))}{\\\\sum_{i=1}^D \\\\exp(b_0 R(q, d_i))}\\nP(d∣q)=∑i=1D\\u200bexp(b0\\u200bR(q,di\\u200b))exp(b0\\u200bR(q,d))\\u200bгде\\nR(q,d)=cos\\u2061(yq,yd)=yqTyd∣∣yq∣∣⋅∣∣yd∣∣R(q, d) = \\\\cos(y_q, y_d) = \\\\frac{y_q^T y_d}{||y_q|| \\\\cdot ||y_d||}\\nR(q,d)=cos(yq\\u200b,yd\\u200b)=∣∣yq\\u200b∣∣⋅∣∣yd\\u200b∣∣yqT\\u200byd\\u200b\\u200bЗдесь b0b_0b0\\u200b – коэффициент сглаживания, который подбирается эмпирически, а DDD – число всех документов.\\nЕсли в качестве функции потерь мы выбираем кросс-энтропию, то на паре запрос-кликнутый документ (q,d+)(q, d^+)(q,d+) она принимает вид\\nL(q,d+)=−log(P(d+∣q)).\\\\mathcal{L}(q, d^+) = -log(P(d^+\\\\vert q)).\\nL(q,d+)=−log(P(d+∣q)).Но вычислять градиент такого функционала для каждого примера дорого, ведь для этого придётся для каждого запроса находить вероятность клика по всем документам. Что же делать? На помощь приходит negative sampling. Заметим, что среди документов ddd в знаменателе P(d∣q)P(d\\\\vert q)P(d∣q) есть лишь один кликнутый, а остальные тысячи и миллионы являются отрицательными примерами. Есть смысл на каждом шаге оптимизации рассматривать не все из них, а только небольшую выборку, вместо полной суммы\\n∑i=1Dexp\\u2061(b0R(q,di))\\\\sum_{i=1}^D \\\\exp(b_0 R(q, d_i))\\ni=1∑D\\u200bexp(b0\\u200bR(q,di\\u200b))беря\\nexp\\u2061(b0R(q,d+))+∑i=1kexp\\u2061(b0R(q,di−)),\\\\exp(b_0 R(q, d^+)) + \\\\sum_{i=1}^k \\\\exp(b_0 R(q, d^-_i)),\\nexp(b0\\u200bR(q,d+))+i=1∑k\\u200bexp(b0\\u200bR(q,di−\\u200b)),где d1−,…,dk−d^-_1,\\\\ldots,d^-_kd1−\\u200b,…,dk−\\u200b – подобранные для запроса qqq негативные примеры. Генерировать их можно по-разному; на практике чаще всего используют одну из следующих стратегий:\\n\\nРавновероятно выбирать подмножество документов из некликнутых. В оригинальной статье предлагают брать позитивные и негативные в соотношении 4:14:14:1.\\nС большей вероятностью выбирать те из некликнутых документов, популярность которых выше.\\nНа каждой эпохе обучения выбирать некликнутые документы, получившие максимальный скор для этого запроса на предыдущей эпохе.\\n\\nДругие функции потерь\\nPairwise loss\\nЗадачу построения рекомендаций можно решать, как задачу ранжирования. Например, это можно делать с помощью попарного лосса. А именно, рассмотрим пару объектов, в которой i1i_1i1\\u200b – релевантный, а i2i_2i2\\u200b не релевантный для пользователя uuu. Тогда мы можем использовать один из двух вариантов функции потерь:\\n\\n\\nL(R(u,i1),R(u,i2))=CrossEntropy(1.0,σ(R(u,i1)−R(u,i2)))\\\\mathcal{L}(R(u, i_1), R(u, i_2)) = \\\\text{CrossEntropy}(1.0, \\\\sigma(R(u, i_1) - R(u, i_2)))L(R(u,i1\\u200b),R(u,i2\\u200b))=CrossEntropy(1.0,σ(R(u,i1\\u200b)−R(u,i2\\u200b))). Тем самым модель будет учиться ранжировать положительные примеры выше отрицательных.\\n\\n\\nL(R(u,i1),R(u,i2))=max\\u2061(0,α−R(u,i1)+R(u,i2))\\\\mathcal{L}(R(u, i_1), R(u, i_2)) = \\\\max(0, \\\\alpha - R(u, i_1) + R(u, i_2))L(R(u,i1\\u200b),R(u,i2\\u200b))=max(0,α−R(u,i1\\u200b)+R(u,i2\\u200b)) (triplet loss). При этом модель обучается так, чтобы положительный и отрицательный примеры как можно больше отличались. Эта функция потерь довольно популярна не только в DSSM сетках, но и в целом в задачах, где нужно обучить парные представления (yq,yd)(y_q, y_d)(yq\\u200b,yd\\u200b) объектов (q,d)(q, d)(q,d) из разных доменов так, чтобы для релевантных друг другу qqq и ddd эмбеддинги оказывались близкими, а для не релевантных далёкими.\\n\\n\\nFull Product Softmax loss\\nРассмотрим батч (u1,i1,r1),…,(uM,iM,rM)(u_1, i_1, r_1),\\\\ldots, (u_M, i_M, r_M)(u1\\u200b,i1\\u200b,r1\\u200b),…,(uM\\u200b,iM\\u200b,rM\\u200b) размера MMM, где utu_tut\\u200b – пользователь, iti_tit\\u200b – пользователю, а rtr_trt\\u200b – таргет, степень релевантности объекта пользователю. Построим по ним:\\n\\nматрицу эмбеддингов пользователей U∈RM×DU \\\\in \\\\mathbb{R}^{M \\\\times D}U∈RM×D;\\nматрицу эмбеддингов объектов W∈RM×DW \\\\in \\\\mathbb{R}^{M \\\\times D}W∈RM×D;\\nвектор таргетов r∈RMr \\\\in \\\\mathbb{R}^Mr∈RM.\\n\\nРассмотрим матрицу\\nsoftmax(αUWT+β),UWT∈RM×M,\\\\text{softmax}(\\\\alpha U W^T + \\\\beta), U W^T \\\\in \\\\mathbb{R}^{M \\\\times M},\\nsoftmax(αUWT+β),UWT∈RM×M,где softmax берётся по строкам\\n\\nРассмотрим функцию потерь вид\\nL=−I{r>0}T⋅log\\u2061(diag(softmax(αUWT+β)))L = -\\\\mathbb{I}\\\\{r > 0\\\\}^T \\\\cdot \\\\log(\\\\text{diag}(\\\\text{softmax}(\\\\alpha U W^T + \\\\beta)))\\nL=−I{r>0}T⋅log(diag(softmax(αUWT+β)))Эта функция потерь старается сделать так, чтобы для релевантных друг другу (с r>0r > 0r>0) пар (u,i)(u, i)(u,i) скалярное произведение эмбеддингов ⟨wu,wi⟩\\\\langle w_u, w_i\\\\rangle⟨wu\\u200b,wi\\u200b⟩ было максимальным.\\nТрансформеры для рекомендаций\\nВ 2018 году появилась архитектура трансформеров на основе механизма внимания. Модели на основе трансформеров показали state-of-the-art результаты на большом числе NLP задач, а впоследствии оказалось, что они отлично подходят и для задач компьютерного зрения. С их помощью можно решать и задачи рекомендаций. Аналогия заключается в следующем: если в NLP трансформеры работают с последовательностями токенов, то в рекомендациях в качестве последовательности можно взять историю событий пользователя. Каждый элемент последовательности – это взаимодействие пользователя с объектом, например, клик на объект.\\nКлассические модели рекомендаций часто игнорируют тот факт, что история пользователя – это направленная последовательность, в которой порядок событий имеет значение. Трансформеры позволяют учитывать как порядок событий, так и сложные паттерны в поведении и интересах пользователя. Например, исследователи из Alibaba представили модель, которую назвали Behaviour Sequence Transformer. Авторы заявляют, что модель используется в продакшене. Модель решает задачу Click Through Rate (CTR) prediction – предсказание вероятности клика по объекту.\\n\\nНа вход модели подается история кликов пользователя, на основе которой нужно предсказать вероятность клика по заданному объекту. Роль архитектуры трансформера здесь в том, чтобы качественно закодировать представление пользователя, после чего применяется обычный multi layer perceptron (MLP) для предсказания вероятности.\\nПомимо архитектур, которые специально разрабатываются под задачи рекомендаций, трансформеры можно использовать и как обособленные предобученные модели для построения векторых представлений текстов или изображений, которые затем подаются как признаки для решения downstream задач в домене рекомендаций. Несмотря на очевидные преимущества трансформеров с точки зрения качества, их использование в продакшене часто ограничивается имеющимися вычислительными ресурсами. Это особенно актуально для рекомендаций, где модели важно применять непосредственно в момент запроса пользователя.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф9.2. Рекомендации на основе матричных разложенийСледующий параграф9.4. Хорошие свойства рекомендательных системЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_9.html', 'title': 'Градиентный бустинг'}, page_content='Градиентный бустингЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/52.1.Линейные модели2.2.Метрические методы2.3.Решающие деревья2.4.Ансамбли в машинном обучении2.5.Градиентный бустингИнтуицияПример с задачей регрессии: формальное описаниеОбобщение на другие функции потерьПочитать по теме3.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Градиентный бустинг2.5. Градиентный бустингАвторы Елистратова ЕвгенияЛунёв КириллКак устроено самое мощное семейство не-нейросетевых моделей: градиентный бустинг над решающими деревьямиВ прошлых разделах мы научились соединять базовые алгоритмы в ансамбль с помощью бэггинга (и, в частности, строить из решающих деревьев случайные леса). Теперь мы рассмотрим другой способ объединять базовые алгоритмы в композицию — градиентный бустинг.\\nВ ходе обучения случайного леса каждый базовый алгоритм строится независимо от остальных. Бустинг, в свою очередь, воплощает идею последовательного построения линейной комбинации алгоритмов. Каждый следующий алгоритм старается уменьшить ошибку текущего ансамбля.\\nБустинг, использующий деревья решений в качестве базовых алгоритмов, называется градиентным бустингом над решающими деревьями, (Gradient Boosting on Decision Trees, GBDT).\\nОн отлично работает на выборках с «табличными», неоднородными данными. Пример таких данных — описание пользователя Яндекса через его возраст, пол, среднее число поисковых запросов в день, число заказов такси и так далее. Такой бустинг способен эффективно находить нелинейные зависимости в данных различной природы.\\nЭтим свойством обладают все алгоритмы, которые используют деревья решений, однако именно GBDT обычно выигрывает в подавляющем большинстве задач. Благодаря этому он широко применяется во многих конкурсах по машинному обучению и задачах из индустрии:\\n\\nпоисковом ранжировании;\\nрекомендательных системах;\\nтаргетировании рекламы;\\nпредсказании погоды;\\nвыбора пункта назначения такси и многих других.\\nВступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nНе так хорошо бустинг проявляет себя на однородных данных: текстах, изображениях, звуке, видео. В таких задачах нейросетевые подходы почти всегда демонстрируют лучшее качество.\\nИ хотя деревья решений — традиционный выбор для объединения в ансамбли, никто не запрещает использовать и другие алгоритмы (например, линейные модели) в качестве базовых. Эта возможность реализована в пакете XGBoost.\\nСтоит только понимать, что построенная композиция окажется линейной комбинацией линейных моделей, то есть опять-таки линейной моделью - или нейросетью с одним полносвязным слоем. Это уменьшает возможности ансамбля эффективно определять нелинейные зависимости в данных. Поэтому в этом параграфе мы рассмотрим только бустинг над решающими деревьями.\\nИнтуиция\\nРассмотрим задачу регрессии с квадратичной функцией потерь:\\nL(y,x)=12∑i=1N(yi−a(xi))2→min\\u2061\\\\mathcal{L}(y, x) = \\\\frac{1}{2}\\\\sum^{N}_{i=1}\\\\left(y_i -  a(x_i)\\\\right)^{2} \\\\rightarrow \\\\min\\nL(y,x)=21\\u200bi=1∑N\\u200b(yi\\u200b−a(xi\\u200b))2→minДля решения будем строить композицию из KKK базовых алгоритмов:\\na(x)=aK(x)=b1(x)+b2(x)+⋯+bK(x)a(x) = a_K(x) = b_1(x) + b_2(x) + \\\\dots +b_K(x)\\na(x)=aK\\u200b(x)=b1\\u200b(x)+b2\\u200b(x)+⋯+bK\\u200b(x)Если мы обучим единственное решающее дерево, то качество такой модели, скорее всего, будет низким. Однако мы знаем, на каких объектах построенное дерево давало точные предсказания, а на каких ошибалось.\\nПопробуем использовать эту информацию и обучим ещё одну модель. Допустим, что предсказание первой модели на объекте xlx_lxl\\u200b на 10 больше, чем необходимо (т.е. b1(xl)=yl+10b_1(x_l) = y_l + 10b1\\u200b(xl\\u200b)=yl\\u200b+10). Если бы мы могли обучить новую модель, которая на xlx_lxl\\u200b будет выдавать ответ −10-10−10, то сумма ответов этих двух моделей на объекте xlx_lxl\\u200b в точности совпала бы с истинным значением:\\nb1(xl)+b2(xl)=(yl+10)+(−10)=ylb_1(x_l) + b_2(x_l) = (y_l + 10) + (-10) = y_l \\nb1\\u200b(xl\\u200b)+b2\\u200b(xl\\u200b)=(yl\\u200b+10)+(−10)=yl\\u200bДругими словами, если вторая модель научится предсказывать разницу между реальным значением и ответом первой, то это позволит уменьшить ошибку композиции.\\nВ реальности вторая модель тоже не сможет обучиться идеально, поэтому обучим третью, которая будет «компенсировать» неточности первых двух. Будем продолжать так, пока не построим композицию из KKK алгоритмов.\\nДля объяснения метода градиентного бустинга полезно воспользоваться следующей аналогией. Бустинг можно представить как гольфиста, цель которого — загнать мяч в лунку с координатой ybally_{\\\\text{ball}}yball\\u200b. Положение мяча здесь – ответ композиции a(xball)a(x_{\\\\text{ball}})a(xball\\u200b). Гольфист мог бы один раз ударить по мячу, не попасть в лунку и пойти домой, но настырность заставляет его продолжить.\\n\\n\\n\\nИсточник\\n\\n\\nПо счастью, ему не нужно начинать каждый раз с начальной позиции. Следующий удар гольфиста переводит мяч из текущего положения ak(xball)a_k(x_{\\\\text{ball}})ak\\u200b(xball\\u200b) в положение ak+1(xball)a_{k+1}(x_{\\\\text{ball}})ak+1\\u200b(xball\\u200b). Каждый следующий удар — это та поправка, которую вносит очередной базовый алгоритм в композицию. Если гольфист все делает правильно, то функция потерь будет уменьшаться:\\nL(y,ak+1(x))<L(y,ak(x)),\\\\mathcal{L}(y, a_{k+1}(x)) < \\\\mathcal{L}(y, a_{k}(x)),\\nL(y,ak+1\\u200b(x))<L(y,ak\\u200b(x)),то есть мяч постепенно будет приближаться к лунке.\\nУдары при этом делаются не хаотично. Гольфист оценивает текущее положение мяча относительно лунки и следующим ударом старается нивелировать те проблемы, которые он создал себе всеми предыдущими.  Подбираясь к лунке, он будет бить всё аккуратнее и, возможно, даже возьмет другую клюшку, но точно не будет лупить так же, как из первоначальной позиции. В итоге комбинация всех ударов рано или поздно перенесет мяч в лунку.\\nПодобно тому, как гольфист постепенно подводит мяч к цели, бустинг с каждым новым базовым алгоритмом всё больше приближает предсказание к истинному значению метки объекта.\\nРассмотрим теперь другую аналогию — разложение функции в ряд Тейлора. Из курса математического анализа известно, что (достаточно хорошую) бесконечно дифференцируемую функцию f(x)f(x)f(x) на интервале x∈(a−R,a+R)x \\\\in \\\\left(a - R, a + R\\\\right)x∈(a−R,a+R) можно представить в виде бесконечной суммы степенных функций:\\nf(x)=∑n=0∞f(n)(a)n!(x−a)n.f(x) = \\\\sum\\\\limits_{n = 0}^{\\\\infty}\\\\frac{f^{(n)}\\\\left(a\\\\right)}{n!}\\\\left(x - a\\\\right)^{n}.\\nf(x)=n=0∑∞\\u200bn!f(n)(a)\\u200b(x−a)n.Одна, самая первая степенная функция в разложении, очень грубо приближает f(x)f(x)f(x). Прибавляя к ней следующую, мы получим более точное приближение. Каждая следующая элементарная функция увеличивает точность приближения, но менее заметна в общей сумме. Если нам не требуется абсолютно точное разложение, вместо бесконечного ряда Тейлора мы можем ограничиться суммой его первых kkk элементов. Таким образом, интересующую нас функцию мы с некоторой точностью представили в виде суммы «простых» функций.\\nПеренесём эту идею на задачи машинного обучения. В машинном обучении мы пытаемся по выборке (xi,yi)(x_i, y_i)(xi\\u200b,yi\\u200b) восстановить неизвестную истинную зависимость. Прежде всего, мы выбираем подходящий алгоритм. Мы можем выбрать «сложный» алгоритм, который сразу хорошо выучит истинную зависимость.\\nА можем обучить «простой», который выучит истинную зависимость посредственно. Затем мы добавим к нему ещё один такой простой алгоритм, чтобы уточнить предсказание первого алгоритма. Продолжая этот процесс, мы получим сумму простых алгоритмов, где первый алгоритм грубо приближает истинную зависимость, а каждый следующий делает приближение всё точнее.\\nПример с задачей регрессии: формальное описание\\nРассмотрим тот же пример с задачей регрессии и квадратичной функцией потерь:\\nL(y,x)=12∑i=1N(yi−a(xi))2→min\\u2061\\\\mathcal{L}(y, x) = \\\\frac{1}{2}\\\\sum\\\\limits^{N}_{i=1}\\\\left(y_i -  a(x_i)\\\\right)^{2} \\\\rightarrow \\\\min\\nL(y,x)=21\\u200bi=1∑N\\u200b(yi\\u200b−a(xi\\u200b))2→minДля решения также будем строить композицию из KKK базовых алгоритмов семейства B\\\\mathcal{B}B:\\na(x)=aK(x)=b1(x)+b2(x)+⋯+bK(x)a(x) = a_K(x) = b_1(x) + b_2(x) + \\\\dots + b_K(x)\\na(x)=aK\\u200b(x)=b1\\u200b(x)+b2\\u200b(x)+⋯+bK\\u200b(x)В качестве базовых алгоритмов выберем, как и условились в начале параграфа, семейство B\\\\mathcal{B}B решающих деревьев некоторой фиксированной глубины.\\nИспользуя известные нам методы построения решающих деревьев, обучим алгоритм b1(x)∈Bb_1(x) \\\\in \\\\mathcal{B}b1\\u200b(x)∈B, который наилучшим образом приближает целевую переменную:\\nb1(x)=argminb∈B\\u2009L(y,b(x))b_1(x) = \\\\underset{b\\\\in \\\\mathcal{B}}{\\\\mathrm{argmin}} \\\\, \\\\mathcal{L}(y, b(x))\\nb1\\u200b(x)=b∈Bargmin\\u200bL(y,b(x))Построенный алгоритм b1(x)b_1(x)b1\\u200b(x), скорее всего, работает не идеально. Более того, если базовый алгоритм работает слишком хорошо на обучающей выборке, то высока вероятность переобучения: низкий уровень смещения, но высокий уровень разброса. Далее вычислим, насколько сильно отличаются предсказания этого дерева от истинных значений:\\nsi1=yi−b1(xi)s_i^{1} = y_i - b_1(x_i)\\nsi1\\u200b=yi\\u200b−b1\\u200b(xi\\u200b)Теперь мы хотим скорректировать b1(x)b_1(x)b1\\u200b(x) с помощью b2(x)b_2(x)b2\\u200b(x). В идеале так, чтобы b2(x)b_2(x)b2\\u200b(x) идеально предсказывал величины si1s_i^{1}si1\\u200b, ведь в этом случае\\na2(xi)=b1(xi)+b2(xi)=   a_2(x_i) = b_1(x_i) + b_2(x_i) = \\na2\\u200b(xi\\u200b)=b1\\u200b(xi\\u200b)+b2\\u200b(xi\\u200b)==b1(xi)+si1=b1(xi)+(yi−b1(xi))=yi   = b_1(x_i) + s_i^1 = b_1(x_i) + (y_i - b_1(x_i)) = y_i \\n=b1\\u200b(xi\\u200b)+si1\\u200b=b1\\u200b(xi\\u200b)+(yi\\u200b−b1\\u200b(xi\\u200b))=yi\\u200bНайти совершенный алгоритм, скорее всего, не получится, но по крайней мере мы можем выбрать из семейства наилучшего представителя для такой задачи. Итак, второе решающее дерево будет обучаться предсказывать разности si1s_i^1si1\\u200b:\\nb2(x)=argminb∈B\\u2009L(s1,b(x))b_2(x) = \\\\underset{b\\\\in \\\\mathcal{B}}{\\\\mathrm{argmin}} \\\\, \\\\mathcal{L}(s^1, b(x))\\nb2\\u200b(x)=b∈Bargmin\\u200bL(s1,b(x))Ожидается, что композиция из двух таких моделей a2(x)=b1(x)+b2(x)a_2(x) = b_1(x) + b_2(x)a2\\u200b(x)=b1\\u200b(x)+b2\\u200b(x) станет более качественно предсказывать целевую переменную yyy.\\nДалее рассуждения повторяются до построения всей композиции. На kkk-ом шаге вычисляется разность между правильным ответом и текущим предсказанием композиции из k−1k - 1k−1 алгоритмов:\\nsik−1=yi−∑j=1k−1bj(xi)=yi−ak−1(xi)s_i^{k - 1} = y_i - \\\\sum_{j=1}^{k - 1} b_{j}(x_i) = y_i - a_{k - 1}(x_i)\\nsik−1\\u200b=yi\\u200b−j=1∑k−1\\u200bbj\\u200b(xi\\u200b)=yi\\u200b−ak−1\\u200b(xi\\u200b)Затем kkk-й алгоритм учится предсказывать эту разность:\\nbk(x)=argminb∈B\\u2009L(sk−1,b(x)),b_k(x) = \\\\underset{b\\\\in \\\\mathcal{B}}{\\\\mathrm{argmin}} \\\\, \\\\mathcal{L}(s^{k - 1}, b(x)), \\nbk\\u200b(x)=b∈Bargmin\\u200bL(sk−1,b(x)),а композиция в целом обновляется по формуле\\nak(x)=ak−1(x)+bk(x)a_k(x) = a_{k - 1}(x) + b_k(x)\\nak\\u200b(x)=ak−1\\u200b(x)+bk\\u200b(x)Обучение KKK базовых алгоритмов завершает построение композиции.\\nОбобщение на другие функции потерь\\nИнтуиция\\nОтметим теперь важное свойство функции потерь в рассмотренном выше примере с регрессией. Для этого посчитаем производную функции потерь по предсказанию z=ak(xi)z = a_k(x_i)z=ak\\u200b(xi\\u200b) модели для iii-го объекта:\\n∂L(yi,z)∂z∣z=ak(xi)=∂∂z12(yi−z)2∣z=ak(xi)=ak(xi)−yi\\\\frac{\\\\partial{\\\\mathcal{L}(y_i,z)}}{\\\\partial{z}}\\\\bigg|_{z=a_k(x_i)} = \\\\frac{\\\\partial}{\\\\partial{z}}\\\\frac{1}{2}\\\\left(y_i -  z\\\\right)^{2}\\\\bigg|_{z=a_k(x_i)} = a_k(x_i) - y_i \\n∂z∂L(yi\\u200b,z)\\u200b\\u200bz=ak\\u200b(xi\\u200b)\\u200b=∂z∂\\u200b21\\u200b(yi\\u200b−z)2\\u200bz=ak\\u200b(xi\\u200b)\\u200b=ak\\u200b(xi\\u200b)−yi\\u200bВидим, что разность, на которую обучается kkk-й алгоритм, выражается через производную:\\nsik=yi−ak(xi)=−∂L(yi,z)∂z∣z=ak(xi)s_i^{k} = y_i -a_k(x_i) = -\\\\frac{\\\\partial{\\\\mathcal{L}(y_i,z)}}{\\\\partial{z}}\\\\bigg|_{z=a_k(x_i)}\\nsik\\u200b=yi\\u200b−ak\\u200b(xi\\u200b)=−∂z∂L(yi\\u200b,z)\\u200b\\u200bz=ak\\u200b(xi\\u200b)\\u200bТаким образом, для каждого объекта xix_ixi\\u200b очередной алгоритм в бустинге обучается предсказывать антиградиент функции потерь по предсказанию модели −∂L(yi,z)∂z-\\\\frac{\\\\partial{\\\\mathcal{L}(y_i,z)}}{\\\\partial{z}}−∂z∂L(yi\\u200b,z)\\u200b в точке ak(xi)a_k(x_i)ak\\u200b(xi\\u200b) предсказания текущей части композиции на объекте xix_ixi\\u200b.\\nПочему же это важно? Дело в том, что это наблюдение позволяет обобщить подход построения бустинга на произвольную дифференцируемую функцию потерь. Для этого мы заменяем обучение на разность siks_i^ksik\\u200b обучением на антиградиент функции потерь (−gik)(-g_i^k)(−gik\\u200b), где\\ngik=∂L(yi,z)∂z∣z=ak(xi)g_i^k =\\\\frac{\\\\partial{\\\\mathcal{L}(y_i,z)}}{\\\\partial{z}}\\\\bigg|_{z=a_k(x_i)}\\ngik\\u200b=∂z∂L(yi\\u200b,z)\\u200b\\u200bz=ak\\u200b(xi\\u200b)\\u200bВспомните аналогию с гольфистом: обучение композиции можно представить как перемещение предсказания из точки (ak(x1),ak(x2),…,ak(xN))(a_k(x_1), a_k(x_2), \\\\dots, a_k(x_N))(ak\\u200b(x1\\u200b),ak\\u200b(x2\\u200b),…,ak\\u200b(xN\\u200b)) в точку (ak+1(x1),ak+1(x2),…,ak+1(xN))(a_{k+1}(x_1), a_{k+1}(x_2), \\\\dots, a_{k+1}(x_N))(ak+1\\u200b(x1\\u200b),ak+1\\u200b(x2\\u200b),…,ak+1\\u200b(xN\\u200b)). В конечном итоге мы ожидаем, что точка (aK(x1),aK(x2),…,aK(xN))(a_K(x_1), a_K(x_2), \\\\dots, a_K(x_N))(aK\\u200b(x1\\u200b),aK\\u200b(x2\\u200b),…,aK\\u200b(xN\\u200b)) будет располагаться как можно ближе к точке с истинными значениями (y1,y2,…,yN)(y_1, y_2, \\\\dots, y_N)(y1\\u200b,y2\\u200b,…,yN\\u200b).\\n\\nВ случае квадратичной функции потерь интуиция вполне подкрепляется математикой. Изменится ли что-либо в наших действиях, если мы поменяем квадратичную функцию потерь на любую другую? С одной стороны, мы, как и прежде, можем двигаться в направлении уменьшения разности предсказания и истинного значения: любая функция потерь поощряет такие шаги для каждого отдельного объекта, ведь для любой адекватной функции потерь выполнено L(y,y)=0\\\\mathcal{L}(y, y) = 0L(y,y)=0.\\nНо мы можем посмотреть на задачу и с другой стороны: не с точки зрения уменьшения расстояния между вектором предсказаний и вектором истинных значений, а с точки зрения уменьшения значения функции потерь. Для наискорейшего уменьшения функции потерь нам необходимо шагнуть в сторону её антиградиента по вектору предсказаний текущей композиции, то есть как раз таки в сторону вектора (−g1k,…,−gNk)(-g_1^k,\\\\dots,-g_N^k)(−g1k\\u200b,…,−gNk\\u200b). Это направление не обязано совпадать с шагом по направлению уменьшения разности предсказания и истинного значения. Например, может возникнуть гипотетическая ситуация, как на рисунке ниже:\\n\\nВ изображённом примере рассматриваются два объекта x1x_1x1\\u200b и x2x_2x2\\u200b. Текущее предсказание для них — (ak(x1),ak(x2))(a_k(x_1), a_k(x_2))(ak\\u200b(x1\\u200b),ak\\u200b(x2\\u200b)), а окружность определяет варианты следующего шага: первый вариант — пойти в направлении (s1k,s2k)(s_1^k, s_2^k)(s1k\\u200b,s2k\\u200b), как делалось ранее; второй — пойти в направлении антиградиента. Также показаны линии уровня значений функции потерь. Функция потерь в этом примере устроена таким образом, что L2<L1L_2 < L_1L2\\u200b<L1\\u200b, из-за чего шаг по антиградиенту оказывается более выгодным.\\nДвижение в сторону антиградиента более выгодно с точки зрения минимизации функции потерь — плюс оно также позволяет справляться с ситуациями, когда явно посчитать остаток (разницу между целевым значением и предсказанием) не представляется возможным.\\nОдин из таких примеров — задача ранжирования. В задаче ранжирования объекты в датасете разбиты на группы и требуется построить модель, по предсказаниям которой можно было бы «правильно» упорядочить документы в каждой группе (обычно по убыванию предсказания модели).\\nЧто значит упорядочить «правильно»? Это значит, что полученная по предсказаниям модели перестановка объектов в группе должна быть близка к идеальной по некоторой метрике.\\nКак задается идеальная перестановка? Есть два способа:\\n\\nПервый способ — проставить каждому объекту число yyy, по которому можно отсортировать объекты для получения идеальной перестановки. Это число можно рассматривать как таргет и обучать модель регрессии — в некоторых случаях это даже будет работать хорошо.\\nВторой способ — задать набор пар объектов, которые обозначают их порядок относительно друг друга в идеальной перестановке. То есть пара (i,j)(i, j)(i,j) означает, что объект с номером iii должен стоять раньше в перестановке, чем объект с номером jjj.\\n\\nВо втором способе таргетов у объектов нет, но дифференцируемая функция потерь есть — в библиотеке CatBoost она называется PairLogit и вычисляется по формуле:\\nPairLogit=−∑p,n∈Pairs(log(11+e−(ap−an)))∣Pairs∣,   PairLogit = \\\\frac{-\\\\sum\\\\limits_{p, n \\\\in Pairs} \\\\left(log(\\\\displaystyle\\\\frac{1}{1 + e^{- (a_{p} - a_{n})}})\\\\right)}{|Pairs|},\\nPairLogit=∣Pairs∣−p,n∈Pairs∑\\u200b(log(1+e−(ap\\u200b−an\\u200b)1\\u200b))\\u200b,где apa_pap\\u200b и ana_nan\\u200b — это предсказания модели на объектах ppp и nnn соответственно. Градиент такой функции потерь посчитать можно, а разницу между предсказанием и истинным значением — нет.\\nМатематическое обоснование\\nПопробуем записать наши интуитивные соображения более формально. Пусть L\\\\mathcal{L}L – дифференцируемая функция потерь, а наш алгоритм a(x)a(x)a(x) представляет собой композицию базовых алгоритмов:\\na(x)=ak(x)=b1(x)+…+bk(x)   a(x) = a_k(x) = b_1(x) + \\\\ldots + b_k(x)\\na(x)=ak\\u200b(x)=b1\\u200b(x)+…+bk\\u200b(x)Мы строим нашу композицию «жадно»:\\nak(x)=ak−1(x)+bk(x),   a_k(x) = a_{k - 1}(x) + b_k(x),\\nak\\u200b(x)=ak−1\\u200b(x)+bk\\u200b(x),где вновь добавляемый базовый алгоритм bkb_kbk\\u200b обучается так, чтобы улучшить предсказания текущей композиции:\\nbk=argminb∈B∑i=1NL(yi,ak−1(xi)+b(xi))   b_k = \\\\underset{b\\\\in \\\\mathcal{B}}{\\\\mathrm{argmin}} \\\\sum_{i = 1}^N \\\\mathcal{L}(y_i, a_{k - 1}(x_i) + b(x_i))\\nbk\\u200b=b∈Bargmin\\u200bi=1∑N\\u200bL(yi\\u200b,ak−1\\u200b(xi\\u200b)+b(xi\\u200b))Модель b0b_0b0\\u200b выбирается так, чтобы минимизировать потери на обучающей выборке:\\nb0=argminb∈B∑i=1NL(yi,b(xi))   b_0 = \\\\underset{b\\\\in \\\\mathcal{B}}{\\\\mathrm{argmin}} \\\\sum_{i = 1}^N \\\\mathcal{L}(y_i, b(x_i))\\nb0\\u200b=b∈Bargmin\\u200bi=1∑N\\u200bL(yi\\u200b,b(xi\\u200b))Для построения базовых алгоритмов на следующих шагах рассмотрим разложение Тейлора функции потерь L\\\\mathcal LL до первого члена в окрестности точки (yi,ak−1(xi))(y_i, a_{k - 1}(x_i))(yi\\u200b,ak−1\\u200b(xi\\u200b)):\\nL(yi,ak−1(xi)+b(xi))≈L(yi,ak−1(xi))+b(xi)∂L(yi,z)∂z∣z=ak−1(xi)=L(yi,ak−1(xi))+b(xi)gik−1   \\\\mathcal{L}(y_i, a_{k - 1}(x_i) + b(x_i)) \\\\approx \\n   \\\\mathcal{L}(y_i, a_{k - 1}(x_i)) + b(x_i) \\\\frac{\\\\partial \\\\mathcal{L}(y_i, z)}{\\\\partial z} \\\\bigg|_{z = a_{k - 1}(x_i)} \\n   = \\\\mathcal{L}(y_i, a_{k - 1}(x_i)) + b(x_i) g_i^{k - 1}\\nL(yi\\u200b,ak−1\\u200b(xi\\u200b)+b(xi\\u200b))≈L(yi\\u200b,ak−1\\u200b(xi\\u200b))+b(xi\\u200b)∂z∂L(yi\\u200b,z)\\u200b\\u200bz=ak−1\\u200b(xi\\u200b)\\u200b=L(yi\\u200b,ak−1\\u200b(xi\\u200b))+b(xi\\u200b)gik−1\\u200bИзбавившись от постоянных членов, мы получим следующую оптимизационную задачу:\\nbk≈argminb∈B∑i=1Nb(xi)gik−1   b_k \\\\approx \\\\underset{b\\\\in \\\\mathcal{B}}{\\\\mathrm{argmin}} \\\\sum_{i = 1}^N b(x_i) g_i^{k - 1}\\nbk\\u200b≈b∈Bargmin\\u200bi=1∑N\\u200bb(xi\\u200b)gik−1\\u200bПоскольку суммируемое выражение — это скалярное произведение двух векторов, его значение минимизируют b(xi)b(x_i)b(xi\\u200b), пропорциональные значениям −gik−1-g_i^{k - 1}−gik−1\\u200b. Поэтому на каждой итерации базовые алгоритмы bkb_kbk\\u200b обучаются предсказывать значения антиградиента функции потерь по текущим предсказаниям композиции.\\nИтак, использованная нами интуиция шага в сторону «уменьшения остатка» удивительным образом привела к оптимальным смещениям в случае квадратичной функции потерь, но для других функций потерь это не так: для них смещение происходит в сторону антиградиента.\\nПолучается, что в общем случае на каждой итерации базовые алгоритмы должны приближать значения антиградиента функции потерь. Однако есть частный случай, в котором в качестве таргета для базового алгоритма выгоднее использовать именно «остатки» — это касается функции потерь MAE. Её производная равна -1, 0 или +1.\\nПриближая базовым алгоритмом антиградиент MAE, количество итераций до сходимости будет расти пропорционально масштабу таргета. То есть, если домножить целевое значение на 10, то потребуется в 10 раз больше итераций градиентного бустинга. Использование остатков в качестве таргета для базового алгоритма не имеет такой проблемы. Аналогичные рассуждения верны также для функции MAPE, в которой проблема с масштабом таргета может проявляться еще сильнее.\\nОбучение базового алгоритма\\nПри построении очередного базового алгоритма bk+1b_{k+1}bk+1\\u200b мы решаем задачу регрессии с таргетом, равным антиградиенту функции потерь исходной задачи на предсказании ak=b1+…+bka_k = b_1 + \\\\ldots + b_kak\\u200b=b1\\u200b+…+bk\\u200b.\\nТеоретически можно воспользоваться любым методом построения регрессионного дерева. Важно выбрать оценочную функцию SSS, которая будет показывать, насколько текущая структура дерева хорошо приближает антиградиент. Её нужно будет использовать для построения критерия ветвления:\\n∣R∣⋅S(R)−∣Rright∣⋅S(Rright)−∣Rleft∣⋅S(Rleft)→max\\u2061,|R| \\\\cdot S(R) - |R_{right}| \\\\cdot S(R_{right}) - |R_{left}| \\\\cdot S(R_{left}) \\\\rightarrow \\\\max,\\n∣R∣⋅S(R)−∣Rright\\u200b∣⋅S(Rright\\u200b)−∣Rleft\\u200b∣⋅S(Rleft\\u200b)→max,где S(R)S(R)S(R) — значение функции SSS в вершине RRR, S(Rleft),S(Rright)S(R_{left}), S(R_{right})S(Rleft\\u200b),S(Rright\\u200b) — значения в левом и правом сыновьях RRR после добавления предиката, ∣\\u2009⋅\\u2009∣\\\\mid \\\\, \\\\cdot \\\\, \\\\mid∣⋅∣ — количество элементов, пришедших в вершину.\\nНапример, можно использовать следующие оценочные функции:\\nL2(g,p)=∑i=1N(pi−gi)2,Cosine(g,p)=−∑i=1N(pi⋅gi)∑i=1Npi2⋅∑i=1Ngi2, L_2(g, p) = \\\\sum\\\\limits_{i=1}^N\\\\left(p_i - g_i\\\\right)^2,\\\\\\\\\\nCosine(g, p) = -\\\\frac{\\\\sum\\\\limits_{i=1}^N(p_i \\\\cdot g_i)}{\\\\sqrt{\\\\sum\\\\limits_{i=1}^Np_i^2} \\\\cdot \\\\sqrt{\\\\sum\\\\limits_{i=1}^Ng_i^2}},\\nL2\\u200b(g,p)=i=1∑N\\u200b(pi\\u200b−gi\\u200b)2,Cosine(g,p)=−i=1∑N\\u200bpi2\\u200b\\u200b⋅i=1∑N\\u200bgi2\\u200b\\u200bi=1∑N\\u200b(pi\\u200b⋅gi\\u200b)\\u200b,где pip_ipi\\u200b — предсказание дерева на объекте xix_ixi\\u200b, gig_igi\\u200b — антиградиент, на который учится дерево, p=pii=1Np = {p_i}{i=1}^Np=pi\\u200bi=1N, g=giNg = { g_i }^Ng=gi\\u200bN. Функция L2L_2L2\\u200b представляет собой среднеквадратичную ошибку, а функция CosineCosineCosine определяет близость через косинусное расстояние между векторами предсказаний и антиградиентов.\\nВ итоге обучение базового алгоритма проходит в два шага:\\n\\nпо функции потерь вычисляется целевая переменная для обучения следующего базового алгоритма:\\n\\ngik=∂L(yi,z)∂z∣z=ak(xi)g_i^k =\\\\frac{\\\\partial{\\\\mathcal{L}(y_i,z)}}{\\\\partial{z}}\\\\bigg|_{z=a_k(x_i)}\\ngik\\u200b=∂z∂L(yi\\u200b,z)\\u200b\\u200bz=ak\\u200b(xi\\u200b)\\u200b\\nстроится регрессионное дерево на обучающей выборке (xi,−gik)(x_i, -g_i^k)(xi\\u200b,−gik\\u200b), минимизирующее выбранную оценочную функцию.\\n\\nНа практике\\nПоскольку для построения градиентного бустинга достаточно уметь считать градиент функции потерь по предсказаниям, с его помощью можно решать широкий спектр задач. В библиотеках градиентного бустинга даже реализована возможность создавать свои функции потерь: для этого достаточно уметь вычислять ее градиент, зная истинные значения и текущие предсказания для элементов обучающей выборки.\\nТипичный градиентный бустинг имеет в составе несколько тысяч деревьев решений, которые необходимо строить последовательно. Построение решающего дерева на выборках типичного размера и современном железе, даже с учетом всех оптимизаций, требует небольшого, но всё-таки заметного времени (0.1-1c), которое для всего ансамбля превратится в десятки минут. Это не так быстро, как обучение линейных моделей, но всё-таки значительно быстрее, чем обучение типичных нейросетей.\\nТемп обучения (learning rate)\\nОбучение композиции с помощью градиентного бустинга может привести к переобучению, если базовые алгоритмы слишком сложные. Например, если сделать решающие деревья слишком глубокими (более 10 уровней), то при обучении бустинга ошибка на обучающей выборке даже при довольно скромном KKK может приблизиться к нулю, то есть предсказание будет почти идеальным, но на тестовой выборке всё будет плохо.\\nСуществует два решения этой проблемы.\\n\\n\\nВо-первых, необходимо упростить базовую модель, уменьшив глубину дерева (либо примерив какие-либо ещё техники регуляризации).\\n\\n\\nВо-вторых, мы можем ввести параметр, называемый темпом обучения (learning rate) η∈(0,1]\\\\eta \\\\in (0, 1]η∈(0,1]:\\n\\n\\nak+1(x)=ak(x)+ηbk+1(x)a_{k+1}(x) = a_{k}(x) + \\\\eta b_{k+1}(x) \\nak+1\\u200b(x)=ak\\u200b(x)+ηbk+1\\u200b(x)Присутствие этого параметра означает, что каждый базовый алгоритм вносит относительно небольшой вклад во всю композицию: если расписать сумму целиком, она будет иметь вид\\nak+1(x)=b1(x)+ηb2(x)+ηb3(x)+…+ηbk+1(x)a_{k+1}(x) = b_1(x) + \\\\eta b_2(x) + \\\\eta b_3(x) + \\\\ldots + \\\\eta b_{k+1}(x)\\nak+1\\u200b(x)=b1\\u200b(x)+ηb2\\u200b(x)+ηb3\\u200b(x)+…+ηbk+1\\u200b(x)Значение параметра обычно определяется эмпирически по входным данным. В библиотеке CatBoost темп обучения может быть выбран автоматически по набору данных. Для этого используется заранее обученная линейная модель, предсказывающая темп обучения по мета-параметрам выборки данных: числу объектов, числу признаков и другим.\\nТемп обучения связан с количеством итераций градиентного бустинга. Чем меньше learning rate, тем больше итераций потребуется сделать для достижения того же качества на обучающей выборке.\\nFeature importance\\nОтдельные деревья решений можно легко интерпретировать, просто визуализируя их структуру. Но в модели градиентного бустинга содержатся сотни деревьев, и поэтому её нелегко интерпретировать с помощью визуализации входящих в неё деревьев. При этом хотелось бы, как минимум, понимать, какие именно признаки в данных оказывают наибольшее влияние на предсказание композиции.\\nМожно сделать следующее наблюдение: признаки из верхней части дерева влияют на окончательное предсказание для большей доли обучающих объектов, чем признаки, попавшие на более глубокие уровни.\\nТаким образом, ожидаемая доля обучающих объектов, для которых происходило ветвление по данному признаку, может быть использована в качестве оценки его относительной важности для итогового предсказания. Усредняя полученные оценки важности признаков по всем решающим деревьям из ансамбля, можно уменьшить дисперсию такой оценки и использовать ее для отбора признаков. Этот метод известен как MDI (mean decrease in impurity).\\nСуществуют и другие методы оценки важности признаков для ансамблей: например, Permutation feature importance (см. описание в sklearn) и множество разных подходов, предлагаемых в библиотеке CatBoost. Все эти техники отбора признаков применимы также и для случайных лесов.\\nРеализации\\nДля общего развития имеет смысл посмотреть реализацию в sklearn, но на практике она весьма медленная и не такая уж умная.\\nХороших реализаций GBDT есть, как минимум, три: LightGBM, XGBoost и CatBoost. Исторически они отличались довольно сильно, но за последние годы успели скопировать друг у друга все хорошие идеи.\\nФорма деревьев\\nОдно из основных отличий LightGBM, XGBoost и CatBoost — форма решающих деревьев.\\nLightGBM строит деревья по принципу: «На каждом шаге делим вершину с наилучшим скором», а основным критерием остановки выступает максимально допустимое количество вершин в дереве. Это приводит к тому, что деревья получаются несимметричными, то есть поддеревья могут иметь разную глубину — например, левое поддерево может иметь глубину 222, а правое может разрастись до глубины 151515.\\nС одной стороны, это позволяет быстро подогнаться под обучающие данные. С другой — бесконтрольный рост дерева в глубину неизбежно ведет к переобучению, поэтому LightGBM позволяет помимо количества вершин ограничивать и максимальную глубину. Впрочем, это ограничение обычно все равно выше, чем для XGBoost и CatBoost.\\n\\nXGBoost строит деревья по принципу: «Строим дерево последовательно по уровням до достижения максимальной глубины». Отдельного ограничения на количество вершин нет, так как оно естественным образом получается из ограничения на глубину дерева. В XGBoost деревья «стремятся» быть симметричными по глубине, и в идеале получается полное бинарное дерево, если это не противоречит другим ограничениям (например, ограничению на минимальное количество объектов в листе). Такие деревья обычно являются более устойчивыми к переобучению.\\n\\nCatBoost строит деревья по принципу: «Все вершины одного уровня имеют одинаковый предикат». Одинаковые сплиты во всех вершинах одного уровня позволяют избавиться от ветвлений (конструкций if-else) в коде инференса модели с помощью битовых операций и получить более эффективный код, который в разы ускоряет применение модели, в особенности в случае применения на батчах.\\nКроме этого, такое ограничение на форму дерева выступает в качестве сильной регуляризации, что делает модель более устойчивой к переобучению. Основной критерий остановки, как и в случае XGBoost, — ограничение на глубину дерева. Однако, в отличие от XGBoost, в CatBoost всегда создаются полные бинарные деревья, несмотря на то, что в некоторые поддеревья может не попасть ни одного объекта из обучающей выборки.\\n\\nГде используется градиентный бустинг\\nЕсли коротко — везде.\\nСегодня это один из двух главных подходов, которые используются на практике (второй — это нейронные сети, конечно). Формально градиентный бустинг слабее и менее гибок, чем сети, но выигрывает в простоте настройки темпа обучения и применения, размере и интерпретируемости модели.\\nВо многих компаниях, так или иначе связанных с ML, он используется для всех задач, которые не связаны с однородными данными (картинками, текстами, и так далее). Типичный поисковый запрос в Яндексе, выбор отеля на Booking.com или сериала на вечер в Netflix задействует несколько десятков моделей GBDT.\\nВпрочем, в будущем можно ожидать плавного исчезновения этого подхода, так как улучшение архитектур глубинного обучения и дальнейшее развитие железа нивелирует его преимущество по сравнению с нейросетями.\\nПочитать по теме\\n\\nСерия блог-постов о градиентном бустинге от Terence Parr and Jeremy Howard\\nРаздел документации sklearn с теоретическими выкладками для градиентного бустинга\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф2.4. Ансамбли в машинном обученииКак смешать несколько моделей в\\xa0одну. Стэкинг, бэггинг, случайные лесаСледующий параграф3.1. Метрики классификации и регрессииКак оценить качество модели для классификации или регрессии и\\xa0почему для разных задач нужны разные метрикиЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_54.html', 'title': 'Implicit bias'}, page_content='Implicit biasЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/613.1.Введение в теорию глубокого обучения13.2.Обобщающая способность – классическая теория13.3.PAC-байесовские оценки риска13.4.Сети бесконечной ширины13.5.Ландшафт функции потерь13.6.Implicit biasСлучай линейных сетей14.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Implicit bias13.6. Implicit biasАвторыГоликов ЕвгенийПод термином implicit bias мы будем понимать явление, состоящее в том, что алгоритм обучения среди всех возможных моделей с нулевым эмпирическим риском выбирает определённые. Это явление можно наблюдать уже на очень простом примере. Рассмотрим задачу линейной регрессии с квадратичной функцией потерь. Пусть имеется mmm обучающих примеров в евклидовом пространстве размерности N>mN > mN>m. В этом случае наша задача 12∥Xw−y∥22→min\\u2061w\\\\frac{1}{2} \\\\| X w - y \\\\|_2^2 \\\\to \\\\min_w21\\u200b∥Xw−y∥22\\u200b→minw\\u200b недоопределена: семейство решений составляет линейное многообразие в пространстве весов. Предположим, что матрица объекты-признаки X∈Rm×NX \\\\in \\\\mathbb{R}^{m \\\\times N}X∈Rm×N имеет полный ранг по строкам: rkX=m<N\\\\text{rk} X = m < NrkX=m<N.\\nРассмотрим динамику градиентного спуска с шагом η\\\\etaη и нулевой инициализацией весов:\\nwk+1=wk+ηXT(y−Xwk),w0=0.w_{k+1} = w_k + \\\\eta X^T (y - X w_k),\\n\\\\quad\\nw_0 = 0.\\nwk+1\\u200b=wk\\u200b+ηXT(y−Xwk\\u200b),w0\\u200b=0.Нам будет удобнее вместо дискретного градиентного спуска рассматривать его непрерывный аналог\\ndwdt=XT(y−Xw),w(t)=0,\\\\frac{dw}{dt} = X^T (y - X w),\\n\\\\quad\\nw(t) = 0,\\ndtdw\\u200b=XT(y−Xw),w(t)=0,Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступитьпереход к которому соответствует стремлению η\\\\etaη к нулю и выбору параметризации по ttt, для которой k=[t/η]k = [t / \\\\eta]k=[t/η] (округление берётся в любую сторону).\\nРассмотрим сингулярное разложение X=UΣVTX = U \\\\Sigma V^TX=UΣVT, где UUU и VVV ортогональны, а Σ\\\\SigmaΣ – прямоугольная диагональная матрица. Тогда\\nXTX=VΣTΣVT∈RN×NX^T X = V \\\\Sigma^T \\\\Sigma V^T \\\\in \\\\mathbb{R}^{N \\\\times N}\\nXTX=VΣTΣVT∈RN×Nи rk(XTX)=m\\\\text{rk}(X^T X) = mrk(XTX)=m.\\nОбозначая w~=VTw\\\\tilde w = V^T ww~=VTw и y~=VTXTy=ΣTUTy\\\\tilde y = V^T X^T y = \\\\Sigma^T U^T yy~\\u200b=VTXTy=ΣTUTy, получаем:\\ndw~dt=y~−ΣTΣw~,w~(0)=0.\\\\frac{d\\\\tilde w}{dt} = \\\\tilde y - \\\\Sigma^T \\\\Sigma \\\\tilde w,\\n\\\\quad\\n\\\\tilde w(0) = 0.\\ndtdw~\\u200b=y~\\u200b−ΣTΣw~,w~(0)=0.В координатном виде имеем следующее:\\ndw~idt=y~i−σi2w~i,w~i(0)=0∀i∈[1:m]\\\\frac{d\\\\tilde w_i}{dt} = \\\\tilde y_i - \\\\sigma_i^2 \\\\tilde w_i,\\n\\\\quad\\n\\\\tilde w_i(0) = 0\\n\\\\qquad\\n\\\\forall i \\\\in [1:m]\\ndtdw~i\\u200b\\u200b=y~\\u200bi\\u200b−σi2\\u200bw~i\\u200b,w~i\\u200b(0)=0∀i∈[1:m]и\\ndw~idt=0,w~i(0)=0∀i∈[m+1:N]\\\\frac{d\\\\tilde w_i}{dt} = 0,\\n\\\\quad\\n\\\\tilde w_i(0) = 0\\n\\\\qquad\\n\\\\forall i \\\\in [m+1:N]\\ndtdw~i\\u200b\\u200b=0,w~i\\u200b(0)=0∀i∈[m+1:N]так как ΣTΣ\\\\Sigma^T \\\\SigmaΣTΣ – диагональная матрица, у которой лишь первые mmm элементов на диагонали не равны нулю, и у вектора y~=ΣTUTy\\\\tilde y = \\\\Sigma^T U^T yy~\\u200b=ΣTUTy тоже лишь первые mmm координат ненулевые.\\nТак как все σi\\\\sigma_iσi\\u200b для i=1,…,mi=1,\\\\ldots,mi=1,…,m ненулевые, при t→∞t \\\\to \\\\inftyt→∞ получаем следующее решение:\\nw~i(∞)=y~iσi2∀i∈{1,…,m},w~i(∞)=0∀i∈{m+1,…,N}.\\\\tilde w_i(\\\\infty) = \\\\frac{\\\\tilde y_i}{\\\\sigma_i^2}\\n\\\\quad \\\\forall i \\\\in\\\\{1,\\\\ldots,m\\\\},\\n\\\\qquad\\n\\\\tilde w_i(\\\\infty) = 0 \\n\\\\quad \\\\forall i \\\\in \\\\{m+1,\\\\ldots,N\\\\}.\\nw~i\\u200b(∞)=σi2\\u200by~\\u200bi\\u200b\\u200b∀i∈{1,…,m},w~i\\u200b(∞)=0∀i∈{m+1,…,N}.Это можно записать в эквивалентном матричном виде: w~(∞)=(ΣTΣ)+y~=Σ+UTy\\\\tilde w(\\\\infty) = (\\\\Sigma^T \\\\Sigma)^+ \\\\tilde y = \\\\Sigma^+ U^T yw~(∞)=(ΣTΣ)+y~\\u200b=Σ+UTy, где «+» обозначает взятие псевдообратной матрицы.\\nЗначит, w(∞)=VΣ+UTy=X+y=XT(XXT)−1yw(\\\\infty) = V \\\\Sigma^{+} U^T y = X^{+} y = X^T (X X^T)^{-1} yw(∞)=VΣ+UTy=X+y=XT(XXT)−1y в силу того, что матрица XXX имеет полный ранг по строкам.\\nПокажем теперь, что это частное решение, найденное градиентным спуском с нулевой инициализацией, имеет наименьшую евклидову норму среди всех минимумов функции потерь 12∣∣Xw−y∣∣22\\\\frac{1}{2} \\\\vert\\\\vert X w - y \\\\vert\\\\vert_2^221\\u200b∣∣Xw−y∣∣22\\u200b.\\nРассмотрим соответствующую функцию Лагранжа:\\nL(w;λ)=∥w∥222+λT(y−Xw).L(w; \\\\lambda) = \\\\frac{\\\\| w \\\\|_2^2}{2} + \\\\lambda^T (y - X w).\\nL(w;λ)=2∥w∥22\\u200b\\u200b+λT(y−Xw).Здесь λ∈Rm\\\\lambda \\\\in \\\\mathbb{R}^{m}λ∈Rm. Покажем, что пара (w(∞),λ∗=(XXT)−1y)(w(\\\\infty), \\\\lambda^* = (X X^T)^{-1} y)(w(∞),λ∗=(XXT)−1y) является критической точкой этой функции:\\n∇λL(w(∞);λ∗)=y−Xw(∞)=y−XXT(XXT)−1y=0;\\\\nabla_\\\\lambda L(w(\\\\infty); \\\\lambda^*) \\n= y - X w(\\\\infty)\\n= y - X X^T (X X^T)^{-1} y \\n= 0;\\n∇λ\\u200bL(w(∞);λ∗)=y−Xw(∞)=y−XXT(XXT)−1y=0;∇wL(w(∞);λ∗)=w(∞)−XTλ∗=XT((XXT)−1y−(XXT)−1y)=0.\\\\nabla_w L(w(\\\\infty); \\\\lambda^*)\\n= w(\\\\infty) - X^T \\\\lambda^*\\n= X^T((X X^T)^{-1} y - (X X^T)^{-1} y)\\n= 0.\\n∇w\\u200bL(w(∞);λ∗)=w(∞)−XTλ∗=XT((XXT)−1y−(XXT)−1y)=0.В силу выпуклости функции Лагранжа эта критическая точка является точкой глобального минимума.\\nСлучай линейных сетей\\nКаков implicit bias нейронных сетей? Сходится ли градиентный спуск в решение наименьшей нормы и если да, то о какой норме идёт речь? Частичный ответ на этот вопрос удаётся получить для линейных сетей.\\nСледуя работе Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, рассмотрим линейную сеть с одним скрытым слоем:\\nf(x;W1:2)=W2W1x,f(x; W_{1:2}) =\\nW_2 W_1 x,\\nf(x;W1:2\\u200b)=W2\\u200bW1\\u200bx,где W1W_1W1\\u200b и W2W_2W2\\u200b – матрицы n×nn\\\\times nn×n.\\nПоставим задачу многомерной (метка yyy – вектор) регрессии с квадратичной функцией потерь:\\nL(W1:2)=Ex,y∣∣y−f(x;W1:2)∣∣22→min\\u2061W\\\\mathcal{L}(W_{1:2}) = \\\\mathbb{E}_{x,y} \\\\vert\\\\vert y - f(x; W_{1:2}) \\\\vert\\\\vert_2^2 \\\\to \\\\min_W\\nL(W1:2\\u200b)=Ex,y\\u200b∣∣y−f(x;W1:2\\u200b)∣∣22\\u200b→Wmin\\u200bШаг градиентного спуска выглядит следующим образом:\\nW1WT˙=ηEx,yW2T(yxT−W2W1xxT),W2WT˙=ηEx,y(yxT−W2W1xxT)W1T,\\\\dot{W_1\\\\vphantom{W^T}} =\\n\\\\eta \\\\mathbb{E}_{x,y} W_2^T (y x^T - W_2 W_1 x x^T),\\n\\\\quad\\n\\\\dot{W_2\\\\vphantom{W^T}} =\\n\\\\eta \\\\mathbb{E}_{x,y} (y x^T - W_2 W_1 x x^T) W_1^T,\\nW1\\u200bWT˙\\u200b=ηEx,y\\u200bW2T\\u200b(yxT−W2\\u200bW1\\u200bxxT),W2\\u200bWT˙\\u200b=ηEx,y\\u200b(yxT−W2\\u200bW1\\u200bxxT)W1T\\u200b,где точка над WiW_iWi\\u200b означает производную по времени (то есть по ttt).\\nЭто нелинейная система матричных дифференциальных уравнений второго порядка; чтобы проинтегрировать её аналитически, нам придётся сделать ряд предположений.\\nОпределим ковариационную матрицу входов Σxx=ExxT\\\\Sigma_{xx} = \\\\mathbb{E} x x^TΣxx\\u200b=ExxT и матрицу ковариации меток со входами Σxy=EyxT\\\\Sigma_{xy} = \\\\mathbb{E} y x^TΣxy\\u200b=EyxT. Предположим, что данные декоррелированы: Σxx=I\\\\Sigma_{xx} = IΣxx\\u200b=I; этого можно добиться, заменив входы xxx на Σxx−1x\\\\Sigma_{xx}^{-1} xΣxx−1\\u200bx. Что касается матрицы ковариации меток со входами, рассмотрим её сингулярное разложение:\\nΣxy=U2S2,0V0T=∑r=1nsrurvrT.\\\\Sigma_{xy} =\\nU_2 S_{2,0} V_0^T =\\n\\\\sum_{r=1}^{n} s_r u_r v_r^T.\\nΣxy\\u200b=U2\\u200bS2,0\\u200bV0T\\u200b=r=1∑n\\u200bsr\\u200bur\\u200bvrT\\u200b.Назовём srs_rsr\\u200b силой моды с индексом rrr ковариации между метками и входами.\\nСделаем замену координат:\\nW2‾=U2TW2,W1‾=W1V0.\\\\overline{W_2} =\\nU_2^T W_2,\\n\\\\quad\\n\\\\overline{W_1} =\\nW_1 V_0.\\nW2\\u200b\\u200b=U2T\\u200bW2\\u200b,W1\\u200b\\u200b=W1\\u200bV0\\u200b.В новых координатах градиентный спуск принимает вид:\\nW‾˙1=ηW2‾T(S2,0−W2‾W1‾),W‾˙2=η(S2,0−W2‾W1‾)W1‾T.\\\\dot{\\\\overline{W}}_1 =\\n\\\\eta \\\\overline{W_2}^T (S_{2,0} - \\\\overline{W_2} \\\\overline{W_1}),\\n\\\\quad\\n\\\\dot{\\\\overline{W}}_2 =\\n\\\\eta (S_{2,0} - \\\\overline{W_2} \\\\overline{W_1}) \\\\overline{W_1}^T.\\nW˙1\\u200b=ηW2\\u200b\\u200bT(S2,0\\u200b−W2\\u200b\\u200bW1\\u200b\\u200b),W˙2\\u200b=η(S2,0\\u200b−W2\\u200b\\u200bW1\\u200b\\u200b)W1\\u200b\\u200bT.Пусть W1‾=[a1,…,an]\\\\overline{W_1} = [a_1, \\\\ldots, a_n]W1\\u200b\\u200b=[a1\\u200b,…,an\\u200b] и W2‾=[b1,…,bn]T\\\\overline{W_2} = [b_1, \\\\ldots, b_n]^TW2\\u200b\\u200b=[b1\\u200b,…,bn\\u200b]T. Тогда в терминах векторов aaa и bbb\\n1ηa˙α=sαbα−∑γ=1nbγ(bγTaα)=(sα−(bαTaα))bα−∑γ≠α(bγTaα)bγ;\\\\frac{1}{\\\\eta} \\\\dot a_{\\\\alpha} =\\ns_\\\\alpha b_{\\\\alpha} - \\\\sum_{\\\\gamma=1}^n b_\\\\gamma (b_\\\\gamma^T a_\\\\alpha) =\\n(s_\\\\alpha - (b_\\\\alpha^T a_\\\\alpha)) b_{\\\\alpha} - \\\\sum_{\\\\gamma\\\\neq\\\\alpha} (b_\\\\gamma^T a_\\\\alpha) b_\\\\gamma;\\nη1\\u200ba˙α\\u200b=sα\\u200bbα\\u200b−γ=1∑n\\u200bbγ\\u200b(bγT\\u200baα\\u200b)=(sα\\u200b−(bαT\\u200baα\\u200b))bα\\u200b−γ\\ue020=α∑\\u200b(bγT\\u200baα\\u200b)bγ\\u200b;1ηb˙α=sαaα−∑γ=1n(aγTbα)aγ=(sα−(aαTbα))aα−∑γ≠α(aγTbα)aγ.\\\\frac{1}{\\\\eta} \\\\dot b_{\\\\alpha} =\\ns_\\\\alpha a_{\\\\alpha} - \\\\sum_{\\\\gamma=1}^n (a_\\\\gamma^T b_\\\\alpha) a_\\\\gamma = (s_\\\\alpha - (a_\\\\alpha^T b_\\\\alpha)) a_{\\\\alpha} - \\\\sum_{\\\\gamma\\\\neq\\\\alpha} (a_\\\\gamma^T b_\\\\alpha) a_\\\\gamma.\\nη1\\u200bb˙α\\u200b=sα\\u200baα\\u200b−γ=1∑n\\u200b(aγT\\u200bbα\\u200b)aγ\\u200b=(sα\\u200b−(aαT\\u200bbα\\u200b))aα\\u200b−γ\\ue020=α∑\\u200b(aγT\\u200bbα\\u200b)aγ\\u200b.Получилась система векторных дифференциальных уравнений порядка 2n2n2n, всё ещё нелинейная. К счастью, при определённом предположении об инициализации эта система распадается на nnn независимых систем порядка 222.\\nПредположим, что существует ортогональная матрица R=[r1,…,rn]R = [r_1, \\\\ldots, r_n]R=[r1\\u200b,…,rn\\u200b], такая что при всех α\\\\alphaα имеет место равенство\\naα(0)=a~(0)rαa_\\\\alpha(0) = \\\\tilde a(0) r_\\\\alphaaα\\u200b(0)=a~(0)rα\\u200b и bα(0)=b~(0)rαb_\\\\alpha(0) = \\\\tilde b(0) r_\\\\alphabα\\u200b(0)=b~(0)rα\\u200b\\nдля некоторых скалярных величин a~(0)\\\\tilde a(0)a~(0) и b~(0)\\\\tilde b(0)b~(0).\\nНетрудно заметить, что в этом случае при всех α\\\\alphaα и в любой момент времени ttt имеем aα(t)=a~(t)rαa_\\\\alpha(t) = \\\\tilde a(t) r_\\\\alphaaα\\u200b(t)=a~(t)rα\\u200b и bα(t)=b~(t)rαb_\\\\alpha(t) = \\\\tilde b(t) r_\\\\alphabα\\u200b(t)=b~(t)rα\\u200b для некоторых скалярных величин a~(t)\\\\tilde a(t)a~(t) и b~(t)\\\\tilde b(t)b~(t) .\\nТогда для различных α\\\\alphaα выражения выше становятся независимыми друг от друга:\\nda~dt=η(s−a~b~)b~,db~dt=η(s−a~b~)a~.\\\\frac{d\\\\tilde a}{dt} =\\n\\\\eta (s - \\\\tilde a \\\\tilde b) \\\\tilde b,\\n\\\\qquad\\n\\\\frac{d\\\\tilde b}{dt} =\\n\\\\eta (s - \\\\tilde a \\\\tilde b) \\\\tilde a.\\ndtda~\\u200b=η(s−a~b~)b~,dtdb~\\u200b=η(s−a~b~)a~.Теперь это система нелинейных дифференциальных уравнений второго порядка.\\nЕсли a~=b~\\\\tilde a = \\\\tilde ba~=b~ в начальный момент, то это верно и в любой момент времени. Тогда система выше превращается в одно уравнение первого порядка.\\nВ самом деле, обозначив u=a~b~u = \\\\tilde a \\\\tilde bu=a~b~, получаем:\\nu˙=2η(s−u)u.(1)\\\\dot u = \\n2\\\\eta (s - u) u.\\\\quad(1)\\nu˙=2η(s−u)u.(1)Это уравнение задаёт следующую динамику градиентного спуска для функции потерь E(u)=12(s−u)2E(u) = \\\\frac{1}{2} (s - u)^2E(u)=21\\u200b(s−u)2 (её глобальный минимум – это u=su = su=s).\\nПеред тем, как интегрировать уравнение (1), напомним, как из uuu перейти обратно к исходным W1W_1W1\\u200b и W2W_2W2\\u200b. Имеем для W1W_1W1\\u200b:\\nW1=Wˉ1V0T,Wˉ1=[a~r1,…,a~rn],a~=u.W_1 = \\\\bar W_1 V_0^T,\\n\\\\qquad\\n\\\\bar W_1 = [\\\\tilde a r_1, \\\\ldots, \\\\tilde a r_n],\\n\\\\qquad\\n\\\\tilde a = \\\\sqrt{u}.\\nW1\\u200b=Wˉ1\\u200bV0T\\u200b,Wˉ1\\u200b=[a~r1\\u200b,…,a~rn\\u200b],a~=u\\u200b.Аналогично для W2W_2W2\\u200b:\\nW2=U2Wˉ2,Wˉ2=[b~r1,…,b~rn]T,b~=u.W_2 = U_2 \\\\bar W_2,\\n\\\\qquad\\n\\\\bar W_2 = [\\\\tilde b r_1, \\\\ldots, \\\\tilde b r_n]^T,\\n\\\\qquad\\n\\\\tilde b = \\\\sqrt{u}.\\nW2\\u200b=U2\\u200bWˉ2\\u200b,Wˉ2\\u200b=[b~r1\\u200b,…,b~rn\\u200b]T,b~=u\\u200b.Теперь проинтегрируем уравнение (1) в предположении, что u(0)=u0u(0) = u_0u(0)=u0\\u200b и u(t)=ufu(t) = u_fu(t)=uf\\u200b для выбранного ttt:\\nt=1η∫u0ufdu2u(s−u)\\u2009du=12sη∫u0uf(duu+dus−u)\\u2009du=t\\n= \\\\frac{1}{\\\\eta} \\\\int_{u_0}^{u_f} \\\\frac{du}{2u (s - u)} \\\\, du\\n= \\\\frac{1}{2 s \\\\eta} \\\\int_{u_0}^{u_f} \\\\left(\\\\frac{du}{u} + \\\\frac{du}{s-u}\\\\right) \\\\, du\\n=\\nt=η1\\u200b∫u0\\u200buf\\u200b\\u200b2u(s−u)du\\u200bdu=2sη1\\u200b∫u0\\u200buf\\u200b\\u200b(udu\\u200b+s−udu\\u200b)du==12sη(ln\\u2061(ufu0)−ln\\u2061(uf−su0−s))=12sηln\\u2061(uf(u0−s)u0(uf−s)).= \\\\frac{1}{2 s \\\\eta} \\\\left(\\\\ln\\\\left(\\\\frac{u_f}{u_0}\\\\right) - \\\\ln\\\\left(\\\\frac{u_f-s}{u_0-s}\\\\right)\\\\right)\\n= \\\\frac{1}{2 s \\\\eta} \\\\ln\\\\left(\\\\frac{u_f (u_0-s)}{u_0 (u_f-s)}\\\\right).\\n=2sη1\\u200b(ln(u0\\u200buf\\u200b\\u200b)−ln(u0\\u200b−suf\\u200b−s\\u200b))=2sη1\\u200bln(u0\\u200b(uf\\u200b−s)uf\\u200b(u0\\u200b−s)\\u200b).Рассмотрим время, необходимое, чтобы выучить фиксированную долю силы данной моды uf=ξsu_f = \\\\xi suf\\u200b=ξs, где ξ∈(0,1)\\\\xi \\\\in (0,1)ξ∈(0,1), стартуя из точки из окрестности нуля u0=ϵu_0 = \\\\epsilonu0\\u200b=ϵ. Оно равняется\\nt(ξ)=12sηln\\u2061(ξ1−ξs−ϵϵ)=12sη(ln\\u2061(s/ϵ−1)−ln\\u2061(ξ−1−1)).t^{(\\\\xi)} \\n= \\\\frac{1}{2 s \\\\eta} \\\\ln\\\\left(\\\\frac{\\\\xi}{1-\\\\xi} \\\\frac{s-\\\\epsilon}{\\\\epsilon}\\\\right) \\n= \\\\frac{1}{2 s \\\\eta} (\\\\ln(s/\\\\epsilon - 1) - \\\\ln(\\\\xi^{-1} - 1)).\\n% \\\\sim \\\\frac{1}{2 s \\\\eta} \\\\ln(s/\\\\epsilon) \\\\; \\\\text{при $\\\\epsilon \\\\to 0$}.\\nt(ξ)=2sη1\\u200bln(1−ξξ\\u200bϵs−ϵ\\u200b)=2sη1\\u200b(ln(s/ϵ−1)−ln(ξ−1−1)).Видим, что чем сильнее мода (то есть чем больше sss), тем быстрее она сходится.\\nРассмотрим две моды с силами s1s_1s1\\u200b и s2s_2s2\\u200b, такие что s1>s2s_1 > s_2s1\\u200b>s2\\u200b. Насколько вторая (более слабая) мода выучится к моменту, когда первая уже выучится на долю ξ\\\\xiξ? Из уравнения выше имеем:\\nuf(u0−s)u0(uf−s)=e2sηt;\\\\frac{u_f (u_0-s)}{u_0 (u_f-s)} = e^{2 s \\\\eta t};\\nu0\\u200b(uf\\u200b−s)uf\\u200b(u0\\u200b−s)\\u200b=e2sηt;1−suf=(1−su0)e−2sηt.1 - \\\\frac{s}{u_f} = \\\\left(1 - \\\\frac{s}{u_0}\\\\right) e^{-2 s \\\\eta t}.\\n1−uf\\u200bs\\u200b=(1−u0\\u200bs\\u200b)e−2sηt.Подставляя s=s2s = s_2s=s2\\u200b и t=t1(ξ)t = t_1^{(\\\\xi)}t=t1(ξ)\\u200b, получаем:\\n1−s2uf=(1−s2ϵ)e−2s2ηt1(ξ)=(1−s2ϵ)e−s2s1(ln\\u2061(s/ϵ−1)−ln\\u2061(ξ−1−1))∼∼−s2ϵ(s1ϵ)−s2s1(ξ1−ξ)−s2s1=−s2s1−s2s1(ξ1−ξ)−s2s1ϵs2s1−1.1 - \\\\frac{s_2}{u_f} \\n= \\\\left(1 - \\\\frac{s_2}{\\\\epsilon}\\\\right) e^{-2 s_2 \\\\eta t_1^{(\\\\xi)}}\\n= \\\\left(1 - \\\\frac{s_2}{\\\\epsilon}\\\\right) e^{-\\\\frac{s_2}{s_1} (\\\\ln(s/\\\\epsilon - 1) - \\\\ln(\\\\xi^{-1} - 1))}\\n\\\\sim\\\\\\\\\\\\sim -\\\\frac{s_2}{\\\\epsilon} \\\\left(\\\\frac{s_1}{\\\\epsilon}\\\\right)^{-\\\\frac{s_2}{s_1}} \\\\left(\\\\frac{\\\\xi}{1-\\\\xi}\\\\right)^{-\\\\frac{s_2}{s_1}}\\n= -s_2 s_1^{-\\\\frac{s_2}{s_1}} \\\\left(\\\\frac{\\\\xi}{1-\\\\xi}\\\\right)^{-\\\\frac{s_2}{s_1}} \\\\epsilon^{\\\\frac{s_2}{s_1} - 1}.\\n1−uf\\u200bs2\\u200b\\u200b=(1−ϵs2\\u200b\\u200b)e−2s2\\u200bηt1(ξ)\\u200b=(1−ϵs2\\u200b\\u200b)e−s1\\u200bs2\\u200b\\u200b(ln(s/ϵ−1)−ln(ξ−1−1))∼∼−ϵs2\\u200b\\u200b(ϵs1\\u200b\\u200b)−s1\\u200bs2\\u200b\\u200b(1−ξξ\\u200b)−s1\\u200bs2\\u200b\\u200b=−s2\\u200bs1−s1\\u200bs2\\u200b\\u200b\\u200b(1−ξξ\\u200b)−s1\\u200bs2\\u200b\\u200bϵs1\\u200bs2\\u200b\\u200b−1.Поскольку s2<s1s_2 < s_1s2\\u200b<s1\\u200b, это выражение стремится к минус бесконечности при ϵ→0\\\\epsilon \\\\to 0ϵ→0, из чего следует, что ufu_fuf\\u200b стремится к нулю.\\nЭто означает, что если веса в инициализации лежат в окрестности нуля, то к моменту, когда данная мода выучивается на любую фиксированную долю ξ∈(0,1)\\\\xi \\\\in (0,1)ξ∈(0,1), более слабые моды не успевают выучиться вообще. Таким образом, в любой момент времени ttt матрица W2(t)W1(t)W_2(t) W_1(t)W2\\u200b(t)W1\\u200b(t) является наилучшим малоранговым приближением заданного ранга матрицы корреляций Σyx\\\\Sigma_{yx}Σyx\\u200b, причём чем больше ttt, тем больше ранг. Можно сказать, что градиентный спуск с фиксированным числом шагов «предпочитает» решения малого ранга.\\nВ выводе выше, мы использовали ряд предположений, в частности, что вектора a1:na_{1:n}a1:n\\u200b, образующие матрицу W1‾\\\\overline{W_1}W1\\u200b\\u200b, ортогональны в инициализации. Эмпирически те же выводы оказываются верными и без этого предположения, см. графики в оригинальной работе Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. Можно ли их обосновать строго математически? В работах Towards resolving the implicit bias of gradient descent for matrix factorization и Deep Linear Networks Dynamics доказывается, что самая сильная мода выучивается в первую очередь. Тем не менее, на момент написания этого текста остаётся недоказанным, что все моды выучиваются последовательно от сильных к слабым. К сожалению, implicit bias градиентного спуска для нелинейных сетей пока остаётся почти неизученным.\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф13.5. Ландшафт функции потерьСледующий параграф14.1. Оптимизация в MLКак найти оптимум функции потерь: от\\xa0градиентного спуска до\\xa0AdamЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_42.html', 'title': 'Временные ряды'}, page_content='Временные рядыЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/75.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/510.1.Кластеризация10.2.Временные рядыВведениеПримеры временных рядовПрогнозирование с помощью сведения к задаче регрессииДекомпозиция временных рядов10.3.Аналитика временных рядов10.4.Модели вида ARIMA10.5.Задача ранжирования11.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Временные ряды10.2. Временные рядыАвторыВолков НикитаВведение\\nВременной ряд — значения меняющихся во времени признаков, полученные в некоторые моменты времени.\\n\\nЗадача прогнозирования\\nПусть (yt,t∈N)(y_t, t \\\\in \\\\mathbb{N})(yt\\u200b,t∈N) – временной ряд, для которого известны значения y1,…,yTy_1, \\\\ldots, y_Ty1\\u200b,…,yT\\u200b. Требуется построить прогноз – функцию fff, такую что величина y^T+h=f(y1,…,yT,h)\\\\widehat{y}_{T+h} = f(y_1, \\\\ldots, y_T, h)y\\u200bT+h\\u200b=f(y1\\u200b,…,yT\\u200b,h) как можно лучше приближает значение yT+hy_{T+h}yT+h\\u200b, где h∈{1,…,H}h \\\\in \\\\{1, \\\\ldots, H\\\\}h∈{1,…,H} – количество шагов, на которое нужно построить прогноз, а величина HHH – горизонт прогнозирования, то есть максимальное количество шагов для построения прогноза. Иными словами, прогноз значения ряда в момент времени T+hT+hT+h строится на основе известных значений ряда до момента времени TTT. Кроме этого имеет смысл строить предсказательный интервал, то есть интервал (dT+h,uT+h)(d_{T+h}, u_{T+h})(dT+h\\u200b,uT+h\\u200b), т.ч. P(dT+h⩽yT+h⩽uT+h)⩾α\\\\mathbb{P}(d_{T+h} \\\\leqslant y_{T+h} \\\\leqslant u_{T+h}) \\\\geqslant \\\\alphaP(dT+h\\u200b⩽yT+h\\u200b⩽uT+h\\u200b)⩾α.Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nНапример, пусть yty_tyt\\u200b – значение какого-то признака в момент времени ttt, и у нас есть значения ряда за месяц, то есть y1,…,y30y_1, \\\\ldots, y_{30}y1\\u200b,…,y30\\u200b. Пусть также требуется предсказать значения ряда на неделю вперед. Тогда прогноз на первые сутки вперед будет равен y^31=f(y1,…,y30,1)\\\\widehat{y}_{31} = f(y_1, \\\\ldots, y_{30}, 1)y\\u200b31\\u200b=f(y1\\u200b,…,y30\\u200b,1), а прогноз на пятые сутки y^35=f(y1,…,y30,5)\\\\widehat{y}_{35} = f(y_1, \\\\ldots, y_{30}, 5)y\\u200b35\\u200b=f(y1\\u200b,…,y30\\u200b,5).\\nСпустя некоторое время прогноз можно перестроить. Например, пусть прогноз перестраивается один раз в трое суток. Тогда оценку значения y35y_{35}y35\\u200b мы уточним как y^35=f(y1,…,y33,2)\\\\widehat{y}_{35} = f(y_1, \\\\ldots, y_{33}, 2)y\\u200b35\\u200b=f(y1\\u200b,…,y33\\u200b,2).  При этом может оказаться, что функция fff умеет принимать на вход лишь фиксированное количество предыдущих значений ряда. Например, если она умеем строить прогноз по последним 30 значениям ряда, то запись будет иметь вид y^35=f(y4,…,y33,2)\\\\widehat{y}_{35} = f(y_4, \\\\ldots, y_{33}, 2)y\\u200b35\\u200b=f(y4\\u200b,…,y33\\u200b,2). Иногда для уточнения того, в какой момент построен прогноз значения y35y_{35}y35\\u200b, указывают момент времени построения прогноза. Например, запись y^35\\u2005∣\\u200530\\\\widehat{y}_{35\\\\:\\\\vert\\\\:30}y\\u200b35∣30\\u200b означает, что прогноз на 35-й день построен в день 30, а y^35\\u2005∣\\u200533\\\\widehat{y}_{35\\\\:\\\\vert\\\\:33}y\\u200b35∣33\\u200b – в день 33.\\nЕсли признаков несколько, не обязательно прогнозировать каждый признак. Часто выделяется один или несколько целевых признаков, а остальные признаки являются вспомогательными и могут улучшить прогноз.\\nПрактические примеры:\\n\\nПрогноз погоды на 10 дней вперед.\\nПрогноз осадков на 2 часа вперед.\\n\\nПримеры временных рядов\\nЕжемесячные продажи антидиабетических лекарств в Австралии с июля 1991 по июнь 2008. На этом графике мы можем видеть возрастающий тренд, возможно, даже нелинейный, и кроме этого есть сезонность (периодичность) значений по годам.\\n\\nМаксимальный спрос на электричество в штате Виктория (Австралия) за 30-минутные интервалы с 10 января 2000 в течении 115 дней. Здесь мы можем наблюдать сразу две сезонности – суточную и недельную. Первая сезонность вызвана тем, что люди обычно больше потребляют электричество днем, чем ночью. Недельная сезонность вызвана более высоким потреблением электричества по будням. Если бы мы посмотрели данные за несколько лет, то увидели бы еще одну сезонность – годовую, например, вызванную тем, что в теплое время года работает больше кондиционеров.\\n\\nПочему бы нам не построить прогноз простыми методами, например, линейной регрессией по времени? Общий тренд так можно уловить. Но в остатках (то есть в разности между истинными значениями и прогнозом) этой модели будет достаточно много информации, которую хотелось бы как-то учесть.\\n\\nМожно также добавить квадрат значения времени. Тогда можно уловить квадратичный тренд, но не более.\\n\\nПрогнозирование с помощью сведения к задаче регрессии\\nДавайте для начала поймем, что мы вообще хотим сделать. Посмотрим на этот график, на котором показаны продажи одного из товаров в магазине за разные года.\\n\\nМы знаем значения ряда (зеленые) до момента времени ttt, в данном случае за 4 года с 2013 по 2016 включительно. Предположим также, что в данный момент мы отмечаем Новый год 2017. В этот момент мы хотели бы предсказать (синее) будущие значения ряда (оранжевое) за весь 2017 год на основе четырехлетней истории продаж.\\nОсновная идея – подадим известные (зеленые) значения ряда в какую-то регрессионную функцию, получив тем самым предсказания. При этом можем брать не все известные значения ряда, а только ppp последних значений. Иначе говоря, модель имеет вид\\nyt=f(yt−1,…,yt−p),y_t = f(y_{t-1}, \\\\ldots, y_{t-p}),\\nyt\\u200b=f(yt−1\\u200b,…,yt−p\\u200b),где fff – произвольная функция. Ее можно построить некоторым известным методом машинного обучения, например, линейной регрессией, решающими деревьями, бустингами, нейронными сетями (как сверточными, так и рекуррентными). Разберёмся, какие признаки мы подадим на вход регрессии.\\nПризнаки\\nОбщий принцип\\nНа практике при генерации идей о том, какие признаки можно создавать для построения модели, рекомендуется строить следующий график. На нем нужно отметить момент времени ttt и мысленно поставить себя в этот момент времени. Затем нужно подумать, какие данные нам при этом доступны. В модель можно брать любые признаки, которые доступны к моменту времени ttt. Если все данные поступают сразу, то можно брать все признаки, которые зависят только от значений до момента времени ttt. В реальности часть данных может поступать с задержкой. Например, если данные загружаются в базу данных раз в сутки в полночь, то в полдень нам не доступны данные за последние 12 часов.\\n\\nТакже нужно помнить о том, на сколько времени вперед нужно сделать прогноз. Например, пусть у нас задача состоит в том, чтобы построить прогнозы продаж в магазине с целью планирования новых поставок. После того, как на основе прогноза мы примем решение о составе товаров в новой поставке, необходимо сначала собрать данные товары на складе, потом отправить машину в магазин, и затем еще разложить товар на полки в магазине. На эту процедуру может уходить от нескольких часов до нескольких дней. Тем самым еще до момента начала формирования новой поставки модель прогнозирования продаж должна построить прогноз спроса на товар к тому моменту, когда его выложат на полки.\\nДаты\\nПосмотрим на то, какие признаки можно извлечь\\nПусть дана какая-то дата: 13.04.2021 09:00.\\nОтсюда можно получить следующие признаки:\\n\\nдень недели: [2];\\nмесяц:       [4];\\nгод: [2021];\\nсезон: [весна];\\nпраздник: [0];\\nвыходной: [0];\\nчас: [9].\\n\\nПредыдущие значения ряда.\\nНапример, если мы хотим построить признаки в момент времени ttt для прогнозирования yty_tyt\\u200b, то можно рассмотреть в качестве признаков ppp предыдущих значений ряда yt−1,…,yt−py_{t-1}, \\\\ldots, y_{t-p}yt−1\\u200b,…,yt−p\\u200b.\\n\\n\\n\\n\\nВремя\\n\\n\\nТаргет\\n\\n\\nПризнаки\\n\\n\\n\\n\\nttt\\n\\n\\nyty_tyt\\u200b\\n\\n\\nyt−1,…,yt−py_{t-1}, \\\\ldots, y_{t-p}yt−1\\u200b,…,yt−p\\u200b\\n\\n\\n\\n\\nt−1t-1t−1\\n\\n\\nyt−1y_{t-1}yt−1\\u200b\\n\\n\\nyt−2,…,yt−p−1y_{t-2}, \\\\ldots, y_{t-p-1}yt−2\\u200b,…,yt−p−1\\u200b\\n\\n\\n\\n\\nt−2t-2t−2\\n\\n\\nyt−2y_{t-2}yt−2\\u200b\\n\\n\\nyt−3,…,yt−p−2y_{t-3}, \\\\ldots, y_{t-p-2}yt−3\\u200b,…,yt−p−2\\u200b\\n\\n\\n\\n\\nДля реализации таких признаков можно выполнить сдвиги вперед временного ряда на 1,2,…,p1, 2, \\\\ldots, p1,2,…,p шагов. Например, в таблице для прогнозирования значений ряда мы рассматриваем два предыдущих значений ряда, выполняя тем самым два сдвига вперед. Таким образом, для прогнозирования значения 5 января, которое равно 235, мы берем признаки 230 и 215, которые являются значением ряда за 4 и 3 января соответственно.\\n\\nСкользящее окно\\nНе всегда имеет смысл в качестве признаков в чистом виде брать все предыдущие значения ряда. Например, если данные приходят раз в секунду, то для того чтобы учесть изменения ряда за последний час, пришлось бы создавать 3600 признаков. Вместо этого по предыдущим значениям ряда yt−1,…,yt−py_{t-1}, \\\\ldots, y_{t-p}yt−1\\u200b,…,yt−p\\u200b можно посчитать:\\n\\nсреднее;\\nвзвешенное среднее;\\nэкспоненциальное сглаживание;\\nмедиана;\\nминимум/максимум;\\nстандартное отклонение;\\nлюбую другую статистику.\\n\\nПодобное скользящее окно можно рассматривать и по другим временным факторам, которые мы не прогнозируем.\\nПримеры:\\n\\nСредняя температура на прошлой неделе для предсказания температуры на завтра.\\nСредняя влажность на прошлой неделе для предсказания температуры на завтра.\\n\\nЕсли в задаче данные хорошие и удаётся использовать более-менее стандартные признаки, то можно воспользоваться готовыми инструментами. Если данные не очень приятные, стоит подумать над тем, какие признаки использовать и как реализовать их получение.\\nСезонность\\nЕсли во временном ряду наблюдается сезонность, то стоит использовать сезонные признаки, например, следующие.\\n\\nЗначение переменной сутки/неделю/месяц/год назад. Такие факторы также можно усреднять.\\nСезонность, полученная методами декомпозирования ряда (об этом расскажем ниже).\\n\\nПримеры:\\n\\nЗначение температуры год назад.\\nСреднее значение температуры 23 ноября за 5 последних лет.\\nСреднее значение температуры за 5 последних лет на неделе, в которую входит 23 ноября.\\n\\nСчётчики\\nИдея состоит в том, чтобы группировать данные не только по временным факторам, но и по любым категориальным. Например, пусть сегодня нет ветра. Тогда в качестве признака можно рассмотреть среднюю температуру в безветренные дни по историческим данным.\\nМожно также использовать сразу несколько факторов. Например, мы строим прогноз в апреле. Тогда можно рассматривать среднюю температуру в безветренные дни в апреле по историческим данным.\\nРезюме\\nПодведем итог о том, какие признаки можем использовать для построения нашей модели.\\n\\nИспользуются только данные из прошлого, никакие данные из будущего нельзя использовать при прогнозировании. Нужно также учитывать возможные задержки в поступлении данных.\\nБольшое количество признаков может привести к вычислительным затратам.\\nМожно генерировать и другие признаки с учетом знаний о предметной области.\\n\\nПостроение прогноза\\nМы определились с тем, какие брать признаки. Теперь разберемся с тем, как прогнозировать. Пусть требуется построить прогноз на HHH шагов вперед. Выделяют три основных способа построить прогнозы:\\n\\nРекурсивная стратегия;\\nПрямая стратегия;\\nГибридная стратегия.\\n\\nРекурсивная стратегия\\nДля каждого момента времени t0⩽t⩽Tt_0\\\\leqslant t \\\\leqslant Tt0\\u200b⩽t⩽T создается объект обучающей выборки:\\n\\nПризнаковое описание – история ряда до момента времени t−1t-1t−1.\\nЦелевая метка – значение yty_tyt\\u200b.\\n\\nПо этим данным мы обучаем какую-либо регрессионную модель строить прогнозы на один шаг вперед. При построении прогноза на несколько шагов вперед мы сначала построим прогноз на один шаг. Затем – на второй шаг, используя полученный прогноз на первый шаг в качестве признаков, и далее аналогично.\\nИначе говоря, если для прогнозирования yty_tyt\\u200b признаковое описание имеет вид (yt−p,…,yt−1)(y_{t-p}, \\\\ldots, y_{t-1})(yt−p\\u200b,…,yt−1\\u200b), то для построения прогноза yt+1y_{t+1}yt+1\\u200b рассматривается признаковое описание (yt−p+1,…,yt−1,y^t)(y_{t-p+1}, \\\\ldots, y_{t-1}, \\\\widehat{y}_t)(yt−p+1\\u200b,…,yt−1\\u200b,y\\u200bt\\u200b).\\nНа картинке считаем, что M-2, M-1 и M это названия признаков у построенной модели.\\n\\nПрямая стратегия\\nВ прямой стратегии предполагается, что построением каждого прогноза в рамках горизонта прогнозирования должна заниматься своя модель. Тем самым создается HHH моделей прогнозирования для каждого момента времени t0⩽t⩽t0+H−1t_0\\\\leqslant t \\\\leqslant t_0+H-1t0\\u200b⩽t⩽t0\\u200b+H−1.\\n\\nПризнаковое описание – история ряда до момента времени t0−1t_0-1t0\\u200b−1, причем признаки одни и те же для каждой модели.\\nЦелевая метка – значение yty_tyt\\u200b.\\n\\n\\nГибридная стратегия\\nГибридная стратегия объединяет в себе преимущества рекурсивной и прямой стратегий. Как и в прямой стратегии, создается HHH моделей прогнозирования, но при этом каждая следующая модель использует прогнозы предыдущей подобно тому, как это делает рекурсивная стратегия.\\nИтак, мы должны построить\\n\\nмодель для прогноза на 1 шаг вперед;\\nмодель для прогноза на 2 шага вперед, используя прогноз уже обученной модели на 1 шаг вперед в качестве признака;\\nмодель для прогноза на 3 шага вперед, используя прогноз уже обученных моделей на 1 и 2 шага вперед в качестве признаков;\\nи так далее обучается HHH моделей.\\n\\nПризнаковое описание:\\n\\nистория ряда до момента времени t0−1t_0-1t0\\u200b−1;\\nпредсказание предыдущих моделей для t0,t0−1,…,t−1t_0, t_0-1, \\\\ldots, t-1t0\\u200b,t0\\u200b−1,…,t−1.\\n\\nНа картинке показаны признаковые описания для моделей в такой стратегии.\\n\\nМожно задаться вопросом: что лучше брать при обучении моделей для прогноза на несколько шагов вперёд – истинные значения или же предсказания предыдущих моделей. Если брать истинные, то мы можем точнее построить модели прогнозирования, но, с другой стороны, на этапе применения вы будете использовать прогнозы, а они могут иметь другое распределение, чем истинные данные, в частности, могут иметь смещение и большую дисперсию. В таком случае мы получим плохие следующие прогнозы.\\nМодели для нескольких временных рядов\\nВ реальности очень часто нужно прогнозировать сразу огромное количество временных рядов.\\nПримеры:\\n\\nПредсказание температуры для различных регионов/городов.\\nПредсказания уровня продаж для различных типов товаров (молоко/яблоки/мясо).\\n\\nПроблема:\\n\\nмодель на каждый временной ряд – слишком много ресурсов и не масштабируемо;\\nмало моделей – плохие предсказания для каждого ряда по отдельности.\\n\\nИдея: создавать модели не для каждого временного ряда, а для группы временных рядов. Иначе говоря, разделить объекты на категории, и для каждой категории создавать отдельную модель.\\n\\nОценка качества моделей\\nДля оценка качества моделей прогнозирования временного ряда в основном используются метрики качества регрессии.\\n\\nСредняя квадратичная ошибка\\n\\nMSE=1T−R+1∑t=RT(y^t−yt)2.MSE = \\\\frac{1}{T-R+1} \\\\sum_{t=R}^T \\\\left(\\\\widehat{y}_t - y_t\\\\right)^2.\\nMSE=T−R+11\\u200bt=R∑T\\u200b(y\\u200bt\\u200b−yt\\u200b)2.\\nСредняя абсолютная ошибка\\n\\nMAE=1T−R+1∑t=RT∣y^t−yt∣.MAE = \\\\frac{1}{T-R+1} \\\\sum_{t=R}^T \\\\left\\\\vert\\\\widehat{y}_t - y_t\\\\right\\\\vert.\\nMAE=T−R+11\\u200bt=R∑T\\u200b∣y\\u200bt\\u200b−yt\\u200b∣.Эти метрики показывают, например, на сколько рублей или на сколько единиц товара мы ошибемся.\\nТакже могут использоваться:\\n\\nСредняя абсолютная ошибка в процентах\\n\\nMAPE=100T−R+1∑t=RT∣y^t−ytyt∣.MAPE = \\\\frac{100}{T-R+1} \\\\sum_{t=R}^T \\\\left\\\\vert\\\\frac{\\\\widehat{y}_t - y_t}{y_t}\\\\right\\\\vert.\\nMAPE=T−R+1100\\u200bt=R∑T\\u200b\\u200byt\\u200by\\u200bt\\u200b−yt\\u200b\\u200b\\u200b.\\nВзвешенная средняя ошибка в процентах.\\n\\nWAPE=100⋅∑t=RT∣y^t−yt∣∑t=RT∣yt∣.WAPE = 100 \\\\cdot \\\\dfrac{\\\\sum_{t=R}^T \\\\vert\\\\widehat{y}_t - y_t\\\\vert}{\\\\sum_{t=R}^T \\\\vert y_t\\\\vert}.\\nWAPE=100⋅∑t=RT\\u200b∣yt\\u200b∣∑t=RT\\u200b∣y\\u200bt\\u200b−yt\\u200b∣\\u200b.Эти метрики достаточно популярны из-за того, что позволяют оценить качество в относительных величинах без зависимости от шкалы измерений.\\nКросс-валидация для временных рядов.\\nСтандартные схемы кросс-валидации нельзя применять для временных рядов потому что значения во временных рядах нельзя перемешивать. Существует два способа построить кросс-валидацию на временных рядах.\\nСхема 1.\\n\\nОбучаемся на первых ttt значениях временного ряда y1\\u2005...\\u2005yty_1\\\\:...\\\\:y_{t}y1\\u200b...yt\\u200b, прогнозируем следующие Δt\\\\Delta tΔt значений ряда y^t+1\\u2005...\\u2005y^t+Δt\\\\widehat{y}_{t+1}\\\\:...\\\\:\\\\widehat{y}_{t+\\\\Delta t}y\\u200bt+1\\u200b...y\\u200bt+Δt\\u200b.\\nОбучаемся на y1\\u2005...\\u2005yt+Δty_1\\\\:...\\\\:y_{t+\\\\Delta t}y1\\u200b...yt+Δt\\u200b, прогнозируем y^t+Δt+1\\u2005...\\u2005y^t+2Δt\\\\widehat{y}_{t+\\\\Delta t+1}\\\\:...\\\\:\\\\widehat{y}_{t+2\\\\Delta t}y\\u200bt+Δt+1\\u200b...y\\u200bt+2Δt\\u200b.\\n...\\nОбучаемся на y1\\u2005...\\u2005yt+(k−1)Δty_1\\\\:...\\\\:y_{t+(k-1)\\\\Delta t}y1\\u200b...yt+(k−1)Δt\\u200b, прогнозируем y^t+(k−1)Δt+1\\u2005...\\u2005y^t+kΔt\\\\widehat{y}_{t+(k-1)\\\\Delta t+1}\\\\:...\\\\:\\\\widehat{y}_{t+k\\\\Delta t}y\\u200bt+(k−1)Δt+1\\u200b...y\\u200bt+kΔt\\u200b.\\nНа каждой итерации считаем ошибки и усредняем.\\n\\n\\nСхема 2.\\n\\nОбучаемся на первых ttt значениях временного ряда y1\\u2005...\\u2005yty_1\\\\:...\\\\:y_{t}y1\\u200b...yt\\u200b, прогнозируем следующие Δt\\\\Delta tΔt значений ряда y^t+1\\u2005...\\u2005y^t+Δt\\\\widehat{y}_{t+1}\\\\:...\\\\:\\\\widehat{y}_{t+\\\\Delta t}y\\u200bt+1\\u200b...y\\u200bt+Δt\\u200b.\\nОбучаемся на y1+Δt\\u2005...\\u2005yt+Δty_{1+\\\\Delta t}\\\\:...\\\\:y_{t+\\\\Delta t}y1+Δt\\u200b...yt+Δt\\u200b, прогнозируем y^t+Δt+1\\u2005...\\u2005y^t+2Δt\\\\widehat{y}_{t+\\\\Delta t+1}\\\\:...\\\\:\\\\widehat{y}_{t+2\\\\Delta t}y\\u200bt+Δt+1\\u200b...y\\u200bt+2Δt\\u200b.\\n...\\nОбучаемся на y1+(k−1)Δt\\u2005...\\u2005yt+(k−1)Δty_{1+(k-1)\\\\Delta t}\\\\:...\\\\:y_{t+(k-1)\\\\Delta t}y1+(k−1)Δt\\u200b...yt+(k−1)Δt\\u200b,~прогнозируем~y^t+(k−1)Δt+1\\u2005...\\u2005y^t+kΔt\\\\widehat{y}_{t+(k-1)\\\\Delta t+1}\\\\:...\\\\: \\\\widehat{y}_{t+k\\\\Delta t}y\\u200bt+(k−1)Δt+1\\u200b...y\\u200bt+kΔt\\u200b.\\nСчитаем ошибки и усредняем.\\n\\n\\nЭти две схемы отличаются только размером обучающего множества. В первом случае он постоянно растет, во втором – не меняется, а само обучающее множество при этом сдвигается. Ту или иную схему на практике стоит использовать в зависимости от того, какая решается задача. Например, если данных достаточно много и предполагается онлайн работа модели с периодическим дообучением, то обычно при каждом дообучении размер обучающего множества фиксируют. В таком случае имеет смысл воспользоваться второй схемой, чтобы оценить качество модели, обученной именно на таким количестве данных. Если же данных немного, то для обучения желательно использовать все доступные данные. В таком случае имеет смысл использовать первую схему.\\nОбратите внимание, что во всех случаях размер тестового интервала времени фиксирован. Это необходимое условие, потому как распределение значений метрики на разных размерах данных может отличаться. Вспомните, например, про зависимость дисперсии выборочного от размера выборки.\\nРезюме: стандартные модели ML для временных рядов\\nПреимущества\\n\\nСвободно используют дополнительную информацию – экзогенные факторы или признаки.\\nМного рядов – много моделей. Нейронная сеть может иметь несколько выходов, и это позволяет прогнозировать сразу несколько рядов одной моделью. Пример: прогнозирование продаж различных товаров.\\n\\nНедостатки\\n\\nПредсказательные интервалы напрямую не строятся.\\nИногда работают хуже стандартных моделей.\\nОбработка признаков может быть труднее, чем в других моделях, которые мы рассмотрим далее.\\nИнтерпретация моделей может вызывать трудности у заказчика.\\n\\nДекомпозиция временных рядов\\nДекомпозиция  – процедура разложения временного ряда на три временных компоненты:\\n\\nТренд TtT_tTt\\u200b – плавное долгосрочное изменение временного ряда.\\nСезонность StS_tSt\\u200b – циклические изменения временного ряда с постоянным периодом сезонности sss.\\nОшибка RtR_tRt\\u200b – непрогнозируемая случайная компонента ряда.\\n\\nМожно рассматривать аддитивную декомпозицию, в которой ряд представляется в виде yt=Tt+St+Rty_t = T_t + S_t + R_tyt\\u200b=Tt\\u200b+St\\u200b+Rt\\u200b, а также мультипликативную в виде yt=Tt⋅St⋅Rty_t = T_t \\\\cdot S_t \\\\cdot R_tyt\\u200b=Tt\\u200b⋅St\\u200b⋅Rt\\u200b. Нетрудно понять, что для построения мультипликативного разложения достаточно выполнить аддитивную декомпозицию для ряда log\\u2061yt\\\\log  y_tlogyt\\u200b.\\nДекомпозиция на основе скользящего среднего\\nПусть sss – известный заранее период сезонности. Компоненты разложения вычисляются последовательно по следующим правилам.\\n\\nТренд\\n\\nВычисляется с помощью скользящего окна длины sss: Tt=1s∑i=t−s/2t+s/2yiT_t = \\\\dfrac{1}{s}\\\\sum\\\\limits_{i=t-s/2}^{t+s/2}y_iTt\\u200b=s1\\u200bi=t−s/2∑t+s/2\\u200byi\\u200b.\\n\\n\\nСезонность\\nУсреднение значений по сезону после удаления тренда.\\n\\nВычитаем тренд yt:=yt−Tty_t: = y_t - T_tyt\\u200b:=yt\\u200b−Tt\\u200b;\\nФормируются sss подгрупп: Gi={yi,yi+s,…,yi+ks}G_i = \\\\{y_i, y_{i + s}, \\\\ldots, y_{i + ks}\\\\}Gi\\u200b={yi\\u200b,yi+s\\u200b,…,yi+ks\\u200b};\\niii-е значение сезонности вычисляется усреднением по iii-й группе Si=Gi‾S_i = \\\\overline{G_i}Si\\u200b=Gi\\u200b\\u200b.\\n\\n\\n\\nОшибка\\nRt=yt−Tt−S(t\\xa0mod\\xa0s)R_t = y_t - T_t - S_{(t\\\\ mod\\\\ s)}Rt\\u200b=yt\\u200b−Tt\\u200b−S(t\\xa0mod\\xa0s)\\u200b.\\n\\n\\n\\nSTL-декомпозиция\\nНазвание метода расшифровывается как Seasonal-Trend decomposition using LOESS. Является более продвинутой моделю для декомпозиции временного ряда. Напоминание: LOESS – взвешенная линейная регрессия, где вес объекта пропорционален расстоянию от него до точек обучающей выборки.\\nПринцип работы:\\n\\nИнициализация тренда нулем: T0=0T^0=0T0=0;\\nВ цикле по kkk до сходимости:\\n\\nВычитаем из ряда текущее значение тренда yt:=yt−Ttky_t: = y_t - T_t^kyt\\u200b:=yt\\u200b−Ttk\\u200b.\\nФормируются sss подгрупп: Gi={yi,yi+s,…,yi+ks}G_i = \\\\{y_i, y_{i + s}, \\\\ldots, y_{i + ks}\\\\}Gi\\u200b={yi\\u200b,yi+s\\u200b,…,yi+ks\\u200b}.\\nС помощью LOESS в каждой группе в каждый момент времени предсказывается сезонность StS_tSt\\u200b.\\nВычитаем из ряда полученную сезонность yt:=yt−Sty_t: = y_t - S_tyt\\u200b:=yt\\u200b−St\\u200b.\\nС помощью LOESS предсказывается новое значение тренда Ttk+1T_t^{k + 1}Ttk+1\\u200b.\\n\\n\\n\\nЗамечание. Пропущен шаг работы с выбросами.\\n\\nПреимущества STL-декомпозиции:\\n\\nБольше настраиваемых параметров, позволяющих подогнать модель под любые данные.\\nСезонная компонента может изменяться с течением времени, и это изменение контролируется пользователем.\\nМодель может быть устойчива к выбросам.\\n\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф10.1. КластеризацияМетоды кластеризации: K-Means, агломеративная кластеризация, DBSCAN. Оценка качества кластеризацииСледующий параграф10.3. Аналитика временных рядовЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».'),\n",
       " Document(metadata={'source': 'data/page_15.html', 'title': 'Обобщённые линейные модели'}, page_content=\"Обобщённые линейные моделиЯндекс Образование — Личный кабинетВойти в IDШкольникамСтудентамАбитуриентамПартнёрамХендбукиЖурналAIВойти в ID\\nСодержаниеЗаметкиСодержание1.ВведениеПройдено 0/32.Классическое обучение с учителемПройдено 0/53.Оценка качества моделейПройдено 0/34.Вероятностные моделиПройдено 0/74.1.Вероятностный подход в ML4.2.Экспоненциальный класс распределений и принцип максимальной энтропии4.3.Обобщённые линейные моделиМотивацияОпределениеЧто даёт нам принадлежность экспоненциальному классу?Примеры4.4.Как оценивать вероятности4.5.Генеративный подход к классификации4.6.Байесовский подход к оцениванию4.7.Модели с латентными переменными5.Глубинное обучение - введениеПройдено 0/46.Глубинное обучение - архитектурыПройдено 0/57.Глубинное обучение - практикаПройдено 0/28.Генеративные моделиПройдено 0/69.Рекомендательные системыПройдено 0/410.Практические главыПройдено 0/511.Взаимодействие со средойПройдено 0/212.Теория MLПройдено 0/113.Теория глубокого обученияПройдено 0/614.Оптимизация в MLПройдено 0/415.Онлайн-обучение и стохастическая оптимизацияПройдено 0/416.ТеорминПройдено 0/7Главная/Хендбуки/Учебник по машинному обучению/Обобщённые линейные модели4.3. Обобщённые линейные моделиАвторыФедотов СтаниславАртемьев МихаилКак прокачать линейную модель с\\xa0помощью распределений из\\xa0экспоненциального классаМотивация\\nДо сих пор мы рассматривали в основном модели вида\\ny∼f(x)+εy\\\\sim f(x) + \\\\varepsilon\\ny∼f(x)+εс шумом ε\\\\varepsilonε из того или иного распределения. Но у этих моделей:\\n\\nшум не зависит от xxx;\\nyyy может принимать любые значения.\\n\\nА что, если мы захотим предсказывать время ожидания доставки? Казалось бы, чем дольше время потенциального ожидания, тем больше его дисперсия. А как корректно предсказывать таргет, который принимает только целые значения?\\nОдин из подходов мы обсудим в этом параграфе. Грубо говоря, вместо того, чтобы прибавлять один и тот же шум, мы зафиксируем семейство распределений p(y∣μ(x))p(y\\\\vert\\\\mu(x))p(y∣μ(x)), в котором изменяемым параметром будет зависящее от xxx математическое ожидание μ(x)\\\\mu(x)μ(x).Вступайте в\\xa0сообщество хендбукаЗдесь можно найти единомышленников, экспертов и\\xa0просто интересных собеседников. А\\xa0ещё\\xa0— получить помощь или поделиться знаниями.Вступить\\nВот как могут выглядеть такие модели для случаев, если ppp нормальное с фиксированной дисперсией, экспоненциальное или пуассоновское соответственно:\\n\\nКак видим, такой подход позволяет получать и модели с меняющейся дисперсией шума, и модели с целочисленным таргетом.\\nОпределение\\nМы рассмотрим достаточно широкий класс моделей — обобщённые линейные модели (generalized linear models, GLM). К ним относятся, в частности, линейная и логистическая регрессии. В итоге мы научимся подбирать подходящую регрессионную модель для самых разных типов данных.\\nВспомним, что вероятностную модель линейной регрессии можно записать как\\ny∣x∼N(⟨x,w⟩,τ2),y \\\\vert x \\\\sim\\\\color{red}{\\\\mathcal N}(\\\\langle x, w\\\\rangle, \\\\tau^2),\\ny∣x∼N(⟨x,w⟩,τ2),а вероятностную модель логистической регрессии — как\\ny∣x∼Bern(σ(⟨x,w⟩)),y \\\\vert x \\\\sim \\\\color{red}{Bern}(\\\\color{blue}{\\\\sigma}(\\\\langle x, w\\\\rangle)),\\ny∣x∼Bern(σ(⟨x,w⟩)),где Bern(p)\\\\color{red}{Bern}(p)Bern(p) — распределение Бернулли с параметром ppp, а σ(u)=11+e−u\\\\color{blue}{\\\\sigma}(u) = \\\\frac{1}{1+e^{-u}}σ(u)=1+e−u1\\u200b.\\nИтак, чем в этих терминах отличаются вероятностные модели линейной и логистической регрессии? Во первых, параметризованное семейство распределений для y∣xy \\\\vert xy∣x, а именно, N(∗,σ2)\\\\textcolor{red}{\\\\mathcal{N}(\\\\ast, \\\\sigma^2)}N(∗,σ2) в случае линейной регрессии и Bern\\\\color{red}{Bern}Bern в случае логистической.\\nВо-вторых, в обоих случаях математическое ожидание условного распределения y∣xy\\\\vert xy∣x является функцией от ⟨x,w⟩\\\\langle x, w\\\\rangle⟨x,w⟩. На это можно посмотреть и по-другому: для каждой из задач выбрана функция ggg такая, что g(E(y∣x))=⟨x,w⟩g(\\\\mathbb E(y \\\\vert x)) = \\\\langle x, w\\\\rangleg(E(y∣x))=⟨x,w⟩. Эта функция называется функцией связи (link function).\\nВ случае линейной регрессии  g(u)=ug(u) = ug(u)=u. В самом деле, E(y∣x)=EN(⟨x,w⟩,τ2)=⟨x,w⟩\\\\mathbb E(y \\\\vert x) = \\\\mathbb E{\\\\mathcal N}(\\\\langle x, w\\\\rangle, \\\\tau^2) = \\\\langle x, w\\\\rangleE(y∣x)=EN(⟨x,w⟩,τ2)=⟨x,w⟩. В случае логистической регрессии g(u)=σ−1(u)=logit(u)=log\\u2061u1−ug(u) = \\\\sigma^{-1}(u) = \\\\text{logit}(u) = \\\\log\\\\frac{u}{1-u}g(u)=σ−1(u)=logit(u)=log1−uu\\u200b. Давайте это тоже проверим. В модели логистической регрессии условное распределение y∣xy \\\\vert xy∣x — это распределение Бернулли с вероятностью успеха σ(⟨x,w⟩)\\\\sigma(\\\\langle x, w\\\\rangle)σ(⟨x,w⟩), и этой же вероятности равно его математическое ожидание. Следовательно, g(σ(⟨x,w⟩))=σ−1(σ(⟨x,w⟩))=⟨x,w⟩g(\\\\sigma(\\\\langle x, w\\\\rangle)) = \\\\sigma^{-1}(\\\\sigma(\\\\langle x, w\\\\rangle)) = \\\\langle x, w\\\\rangleg(σ(⟨x,w⟩))=σ−1(σ(⟨x,w⟩))=⟨x,w⟩.\\nОбобщая, можно сказать, что, если данные таковы, что E(Y∣X)\\\\mathbb E(Y \\\\vert X)E(Y∣X) не является линейной функцией от xxx, мы линеаризуем E(Y∣X)\\\\mathbb E(Y \\\\vert X)E(Y∣X) с помощью функции связи ggg.\\nЗамечание: Вообще говоря, нормальное распределение определяется не только своим математическим ожиданием, но и стандартным отклонением. То есть, в отличие от логистической регрессии, модель линейной регрессии не позволяет для данного xxx оценить все параметры распределения y∣xy \\\\vert xy∣x, и дисперсию приходится фиксировать изначально. К счастью, выбор её значения в нормальном распределении не влияет ни на оптимальный вектор весов www, ни на итоговые предсказания E(Y∣X)\\\\mathbb E(Y \\\\vert X)E(Y∣X), которые выдаёт обученная модель.\\nПродолжим. Задав эти две составляющие — параметризованное семейство распределений и функцию связи — мы получим обобщённую линейную модель (GLM).\\nДля нового объекта xxx она выдаст предсказание y^=E(y∣x)=g−1(⟨x,w⟩)\\\\widehat{y} = \\\\mathbb{E}(y\\\\vert x) = g^{-1}(\\\\langle x, w\\\\rangle)y\\u200b=E(y∣x)=g−1(⟨x,w⟩), а выбор класса распределений y∣xy \\\\vert xy∣x потребуется нам для подбора весов www. В принципе, можно выбрать любой класс распределений y∣xy \\\\vert xy∣x и любую монотонную функцию связи ggg, получив некоторую вероятностную модель. Однако обычно для упрощения поиска оптимальных весов www в GLM предполагают, что y∣xy \\\\vert xy∣x принадлежит одному из достаточно простых семейств экспоненциального класса.\\nЧто даёт нам принадлежность экспоненциальному классу?\\nВ контексте GLM обычно рассматривают подкласс экспоненциального класса, состоящий из семейств, представимых в виде\\np(y∣θ,ϕ)=exp\\u2061(yθ−a(θ)ϕ+b(y,ϕ))\\\\color{#348FEA}{p(y \\\\vert \\\\theta, \\\\phi) = \\\\exp\\\\left(\\\\frac{y\\\\theta - a(\\\\theta)}{\\\\phi} + b(y, \\\\phi)\\\\right)}\\np(y∣θ,ϕ)=exp(ϕyθ−a(θ)\\u200b+b(y,ϕ))где θ\\\\thetaθ и ϕ\\\\phiϕ — скалярные параметры, причём ϕ\\\\phiϕ — нечто фиксированное, обычно дисперсия, которая чаще всего полагается равной 111, а значения θ\\\\thetaθ параметризуют распределения из семейства. Нетрудно переписать плотность в более привычном для нас виде, чтобы стало очевидно, что это семейство действительно из экспоненциального класса:\\np(y∣θ,ϕ)=1exp\\u2061(a(θ)ϕ)exp\\u2061(b(y,ϕ))exp\\u2061(yθϕ)p(y \\\\vert \\\\theta, \\\\phi) = \\\\frac1{\\\\exp\\\\left(\\\\frac{a(\\\\theta)}{\\\\phi}\\\\right)}\\\\exp(b(y,\\\\phi))\\\\exp\\\\left(\\\\frac{y\\\\theta}{\\\\phi}\\\\right)\\np(y∣θ,ϕ)=exp(ϕa(θ)\\u200b)1\\u200bexp(b(y,ϕ))exp(ϕyθ\\u200b)Действительно, если вспомнить, что φ\\\\varphiφ — это константа, а не параметр, то получается очень похоже на\\np(y∣ν)=1h(ν)g(y)⋅exp\\u2061(νTu(y))p(y\\\\vert\\\\nu) = \\\\frac1{h(\\\\nu)}g(y)\\\\cdot\\\\exp\\\\left(\\\\nu^Tu(y)\\\\right)\\np(y∣ν)=h(ν)1\\u200bg(y)⋅exp(νTu(y))В частности, мы видим, что u(y)u(y)u(y) состоит из единственной компоненты u1(y)u_1(y)u1\\u200b(y), равной yϕ\\\\frac{y}{\\\\phi}ϕy\\u200b. По доказанной в предыдущем разделе лемме имеем тогда, что математическое ожидание μ\\\\muμ такой случайной величины равно\\nμ=ϕEu1(y)=ϕ∂∂θ(a(θ)ϕ)=a′(θ)\\\\mu = \\\\phi\\\\mathbb{E}u_1(y) = \\\\phi\\\\frac{\\\\partial}{\\\\partial\\\\theta}\\\\left(\\\\frac{a(\\\\theta)}{\\\\phi}\\\\right) = a'(\\\\theta)\\nμ=ϕEu1\\u200b(y)=ϕ∂θ∂\\u200b(ϕa(θ)\\u200b)=a′(θ)До сих пор мы рассуждали о распределении p(y)p(y)p(y) без xxx в условии. Что будет, если его добавить? Параметр ϕ\\\\phiϕ мы договорились сохранять постоянным, тогда от xxx должен зависеть единственный оставшийся параметр θ\\\\thetaθ.\\nСамый естественный в нашей ситуации вариант — это положить θ=⟨x,w⟩\\\\theta = \\\\langle x, w\\\\rangleθ=⟨x,w⟩. В GLM мы вводили функцию ggg, для которой g(E(y∣x))=⟨x,w⟩g(\\\\mathbb E(y \\\\vert x)) = \\\\langle x, w\\\\rangleg(E(y∣x))=⟨x,w⟩, то есть E(y∣x)=g−1(⟨x,w⟩)\\\\mathbb{E}(y\\\\vert x) = g^{-1}(\\\\langle x, w\\\\rangle)E(y∣x)=g−1(⟨x,w⟩). Но ведь матожидание yyy равно a′(θ)a'(\\\\theta)a′(θ), то есть a′(⟨x,w⟩)a'(\\\\langle x, w\\\\rangle)a′(⟨x,w⟩). Это позволяет нам однозначно определить функцию связи g=(a′)−1g = (a')^{-1}g=(a′)−1. Такая функция связи называется канонической функцией связи (canonical link function).\\nПримеры\\nПоговорим немного о том, как на практике подбирать ϕ,a,b\\\\phi, a, bϕ,a,b, чтобы по классу распределений y∣xy \\\\vert xy∣x определить каноническую функцию связи. Чтобы разобраться, рассмотрим несколько примеров.\\nПример 1. Пусть мы решили применить к данным линейную регрессию. Тогда\\np(y∣x,w,σ2)=12πσexp\\u2061(−(y−⟨x,w⟩)22σ2)p(y \\\\vert x, w, \\\\sigma^2) = \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}\\\\exp\\\\left(-\\\\frac{(y-\\\\langle x, w\\\\rangle)^2}{2\\\\sigma^2}\\\\right)\\np(y∣x,w,σ2)=2π\\u200bσ1\\u200bexp(−2σ2(y−⟨x,w⟩)2\\u200b)Обозначим для краткости μ=⟨x,w⟩\\\\mu = \\\\langle x, w\\\\rangleμ=⟨x,w⟩ и будем рассматривать p(y∣μ,σ2)p(y\\\\vert \\\\mu, \\\\sigma^2)p(y∣μ,σ2).\\nМы уже знаем, что семейство нормальных распределений относится к экспоненциальному классу, но давайте выразим эту плотность в описанном выше более частном виде:\\np(y∣μ,σ2)=exp\\u2061(−(y−μ)22σ2−log\\u2061(2πσ2))p(y \\\\vert \\\\mu, \\\\sigma^2) = \\\\exp\\\\left(-\\\\frac{(y-\\\\mu)^2 }{2\\\\sigma^2}- \\\\log(2\\\\pi\\\\sigma^2)\\\\right)\\np(y∣μ,σ2)=exp(−2σ2(y−μ)2\\u200b−log(2πσ2))В формуле экспоненциального семейства распределений единственная часть, не зависящая от θ\\\\thetaθ, — это функция bbb. Поскольку μ=a′(θ)\\\\mu=a'(\\\\theta)μ=a′(θ), функция bbb также не должна зависеть от μ\\\\muμ. Так что внутри экспоненты выделим в качестве функции bbb всё, что не зависит от μ\\\\muμ:\\np(y∣μ,σ2)=exp\\u2061(yμ⏞=yθ−μ2/2⏞=a(θ)σ2⏟=ϕ−(yi22σ2+log\\u2061(2πσ2))⏟=b(y,ϕ))p(y \\\\vert \\\\mu, \\\\sigma^2) = \\\\exp\\\\left(\\\\frac{\\\\overbrace{y\\\\mu}^{=y\\\\theta} - \\\\overbrace{\\\\mu^2/2}^{=a(\\\\theta)}}{\\\\underbrace{\\\\sigma^2}_{=\\\\phi}}\\\\underbrace{ - \\\\left(\\\\frac{y_i^2}{2\\\\sigma^2} + \\\\log(2\\\\pi\\\\sigma^2)\\\\right)}_{=b(y,\\\\phi)}\\\\right)\\np(y∣μ,σ2)=exp\\u200b=ϕσ2\\u200b\\u200byμ\\u200b=yθ\\u200b−μ2/2\\u200b=a(θ)\\u200b\\u200b=b(y,ϕ)−(2σ2yi2\\u200b\\u200b+log(2πσ2))\\u200b\\u200b\\u200bЭта формула уже похожа на формулу экспоненциального семейства распределений и видно, что ϕ=σ2\\\\phi=\\\\sigma^2ϕ=σ2, θ=g(μ)=μ\\\\theta=g(\\\\mu)=\\\\muθ=g(μ)=μ (коэффициент при yyy), a(θ)=μ2/2=θ2/2a(\\\\theta) = \\\\mu^2/2 = \\\\theta^2/2a(θ)=μ2/2=θ2/2, b(y,ϕ)=−y22σ2−log\\u2061(2πσ2)b(y, \\\\phi) = -\\\\frac{y^2}{2\\\\sigma^2} - \\\\log(2\\\\pi\\\\sigma^2)b(y,ϕ)=−2σ2y2\\u200b−log(2πσ2).\\nКаноническая функция связи является обратной к a′(θ)=θa'(\\\\theta) = \\\\thetaa′(θ)=θ, то есть ⟨x,w⟩=g(μ)=μ\\\\langle x, w\\\\rangle = g(\\\\mu) = \\\\mu⟨x,w⟩=g(μ)=μ, как мы и привыкли.\\nПример 2. Проделаем то же самое, но теперь для распределения Бернулли.\\nЕсли вам интересно — попробуйте сперва сами, прежде чем читать решение.Пусть μ=E(y∣x)\\\\mu = \\\\mathbb{E}(y\\\\vert x)μ=E(y∣x). Тогда функция вероятности имеет вид\\np(y∣μ)=μy(1−μ)1−y=exp\\u2061(ylog\\u2061(μ1−μ)+log\\u2061(1−μ))=p(y \\\\vert \\\\mu) = \\\\mu^y(1-\\\\mu)^{1-y} = \\\\exp\\\\left(y\\\\log\\\\left(\\\\frac{\\\\mu}{1-\\\\mu}\\\\right) + \\\\log\\\\left(1-\\\\mu\\\\right)\\\\right) =\\np(y∣μ)=μy(1−μ)1−y=exp(ylog(1−μμ\\u200b)+log(1−μ))==exp\\u2061(ylog\\u2061(μ1−μ)−[−log\\u2061(1−μ)]1+0)=\\\\exp\\\\left(\\\\frac{y\\\\log\\\\left(\\\\frac{\\\\mu}{1-\\\\mu}\\\\right) - \\\\left[-\\\\log\\\\left(1-\\\\mu\\\\right)\\\\right]}{1} + 0\\\\right)\\n=exp\\u200b1ylog(1−μμ\\u200b)−[−log(1−μ)]\\u200b+0\\u200bТо есть мы можем положить ϕ=1\\\\phi=1ϕ=1, θ=log\\u2061μ1−μ\\\\theta = \\\\log\\\\frac{\\\\mu}{1-\\\\mu}θ=log1−μμ\\u200b, a(θ)=−log\\u2061(1−μ)=log\\u2061(1+eθ)a(\\\\theta) = -\\\\log\\\\left(1-\\\\mu\\\\right) = \\\\log\\\\left(1 + e^{\\\\theta}\\\\right)a(θ)=−log(1−μ)=log(1+eθ), b=0b=0b=0.\\nНайдём каноническую функцию связи. Имеем a′(θ)=eθ1+eθa'(\\\\theta) = \\\\frac{e^{\\\\theta}}{1 + e^{\\\\theta}}a′(θ)=1+eθeθ\\u200b. Следовательно, g(t)=log\\u2061(t1−t)g(t) = \\\\log\\\\left(\\\\frac{t}{1-t}\\\\right)g(t)=log(1−tt\\u200b). Как и ожидалось, это функция logit=σ−1\\\\text{logit}=\\\\sigma^{-1}logit=σ−1, используемая в логистической регрессии. Соответственно, ⟨x,w⟩=σ−1(E(y∣x))\\\\langle x, w\\\\rangle = \\\\sigma^{-1}(\\\\mathbb{E}(y\\\\vert x))⟨x,w⟩=σ−1(E(y∣x)).\\nПример 3. Хорошо, про линейную и логистическую регрессию мы и так знали. Давайте попробуем решить с помощью GLM новую задачу.\\nПусть мы хотим по каким-то признакам XXX предсказать количество «лайков», которое пользователи поставят посту в социальной сети за первые 10 минут после публикации. Конечно, можно использовать для этого линейную регрессию. Однако предположение линейной регрессии, что Y∣X∼NY \\\\vert X\\\\sim\\\\mathcal NY∣X∼N, в данном случае странное по нескольким причинам.\\nВо-первых, количество лайков заведомо не может быть отрицательным, а нормальное распределение всегда будет допускать ненулевую вероятность отрицательного значения.\\nВо-вторых, количество лайков — всегда целое число.\\nВ-третьих, у распределения количества лайков, скорее всего, положительный коэффициент асимметрии (skewness). То есть, если модель предсказывает, что под постом будет 100 лайков, мы скорее можем ожидать, что под ним окажется 200 лайков, чем 0. Нормальное распределение симметрично и не может описать такие данные.\\nС другой стороны, если мы предположим, что в первые 10 минут после публикации есть какая-то постоянная частота (своя для каждого поста, зависящая от xxx), с которой пользователи ставят лайк, мы получим, что количество лайков имеет распределение Пуассона. Распределение Пуассона не имеет описанных выше проблем:\\n\\nНо какая будет каноническая функция связи, если мы считаем, что Y∣X∼PoissonY \\\\vert X\\\\sim\\\\text{Poisson}Y∣X∼Poisson? Аналогично первому и второму примерам:\\np(y∣μ)=e−μμyy!=exp\\u2061(ylog\\u2061μ−μ−log\\u2061y!)p(y \\\\vert \\\\mu) = \\\\frac{e^{-\\\\mu}\\\\mu^y}{y!} = \\\\exp\\\\left(y\\\\log\\\\mu - \\\\mu - \\\\log y!\\\\right)\\np(y∣μ)=y!e−μμy\\u200b=exp(ylogμ−μ−logy!)Откуда ϕ=1\\\\phi=1ϕ=1, θ=g(μ)=log\\u2061μ\\\\theta = g(\\\\mu) = \\\\log\\\\muθ=g(μ)=logμ, a(θ)=μ=exp\\u2061(θ)a(\\\\theta) = \\\\mu = \\\\exp(\\\\theta)a(θ)=μ=exp(θ), b(y,ϕ)=−log\\u2061y!b(y, \\\\phi) = -\\\\log y!b(y,ϕ)=−logy!\\nЗначит, эта модель (она называется пуассоновская регрессия), будет предсказывать с помощью формулы E(y∣x)=g−1(⟨x,w⟩)=exp\\u2061(⟨x,w⟩)\\\\mathbb E(y \\\\vert x) = g^{-1}(\\\\langle x, w\\\\rangle)= \\\\exp(\\\\langle x, w\\\\rangle)E(y∣x)=g−1(⟨x,w⟩)=exp(⟨x,w⟩).\\nВопрос на подумать. В каких ситуациях была бы полезной функция связи complementary log-log link (cloglog)\\ng(x)=log\\u2061(−log(1−x))?g(x) = \\\\log(-log(1 - x))?\\ng(x)=log(−log(1−x))?Ответ (не открывайте сразу; сначала подумайте сами!)Давайте разбираться. Предсказание с такой функцией связи будет иметь вид\\nE(y∣x)=g−1(⟨x,w⟩)=1−exp\\u2061(−exp\\u2061(⟨x,w⟩))\\\\mathbb{E}(y\\\\vert x) = g^{-1}(\\\\langle x, w\\\\rangle) = 1 - \\\\exp(-\\\\exp(\\\\langle x, w\\\\rangle))\\nE(y∣x)=g−1(⟨x,w⟩)=1−exp(−exp(⟨x,w⟩))Функция g−1g^{-1}g−1 принимает значения на отрезке от 000 до 111, так что речь идёт о предсказании вероятностей в задаче бинарной классификации. Сравним g−1g^{-1}g−1 с привычной нам сигмоидой:\\n\\nГрафик g−1g^{-1}g−1 не симметричен.  Можно сказать, что модель, основанная на функции связи cloglog, похожа на логистическую регрессию в области точек, уверенно отнесённых к нулевому классу, но с уменьшением этой уверенности они приписываются близкие к единице вероятности всё «охотнее».\\nЭта функция связи может оказаться полезной, если объекты положительного класса редки (обнаружение редких событий).\\nЧтобы добавить в заметки выделенный текст, нажмите Command + EДобавить в заметкиПараграф прочитанПройдите квиз по параграфуЧтобы закрепить пройденный материалНачатьСообщить об ошибкеПредыдущий параграф4.2. Экспоненциальный класс распределений и принцип максимальной энтропииСамые главные семейства распределений в\\xa0жизни любого data scientist’аСледующий параграф4.4. Как оценивать вероятностиКак правильно оценить вероятности классов в\\xa0задаче классификацииЯндекс УчебникЯндекс ЛицейЯндекс ПрактикумШкола анализа данныхПрограммы в университетахИсследованияХендбукиБаза знанийЖурналСобытияО насОбратная связьПользовательское соглашениеСайт образовательной организацииСведения об образовательной организацииРассылкаБотОбразовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\\xa0основании Лицензии №\\u2009Л035-01298-77/00185314 от 24 марта 2015 года.\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\")]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = parse_html_with_langchain(\"data\")\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
